in the probabilistic lr model, probabilities are assigned to tree 696 precision recall f-score time (min) best-first classifier-based (this paper) 88.1 87.8 87.9 17 deterministic (maxent) (this paper) 85.4 84.8 85.1 < 1 charniak & johnson (2005) 91.3 90.6 91.0 unk bod (2003) 90.8 90.7 90.7 145* charniak (2000) 89.5 89.6 89.5 23 collins (1999) 88.3 88.1 88.2 39 ratnaparkhi (1997) 87.5 86.3 86.9 unk tsuruoka & tsujii (2005): deterministic 86.5 81.2 83.8 < 1* tsuruoka & tsujii (2005): search 86.8 85.0 85.9 2* sagae & lavie (2005) 86.0 86.1 86.0 11* table 1: summary of results on labeled precision and recall of constituents, and time required to parse the test set. 
some of the more popular and more accurate of these approaches to data-driven parsing (charniak, 2000; collins, 1997; klein and manning, 2002) have been based on generative models that are closely related to probabilistic context-free grammars. 
we used the collins parser (1997) to generate the constituency parse and a dependency converter (hwa and lopez, 2004) to obtain the dependency parse of english sentences. 
this is well illustrated by the collins parser (collins, 1997; collins, 1999), scrutinized by bikel (2004), where several transformations are applied in order to improve the analysis of noun phrases, coordination and punctuation. 
training on about 40,000 sentences (collins, 1997) achieves a crossing brackets rate of 1.07, a better value than our 1.63 value for regular parsing or the 1.13 value assuming perfect segmentation/tagging, but even for similar text types, comparisons across languages are of course problematic. 
previous work the generative models described in collins (1996) and the earlier version of these models described in collins (1997) conditioned on punctuation as surface features of the string, treating it quite differently from lexical items. 
in particular, the model in collins (1997) failed to generate punctuation, a deficiency of the model. 
of particular relevance is other work on parsing the penn wsj treebank (jelinek et al 1994; magerman 1995; eisner 1996a, 1996b; collins 1996; charniak 1997; goodman 1997; ratnaparkhi 1997; chelba and jelinek 1998; roark 2001). 
the models were originally introduced in collins (1997); the current article 1 gives considerably more detail about the models and discusses them in greater depth. 
in the models described in collins (1997), there was a third question concerning punctuation: (3) does the string contain 0, 1, 2 or more than 2 commas? 
(note that conditioning on the rule's parent is needed to disallow the structure [np [np pp] pp]; see johnson [1997] for further discussion.) 
eisner (1996), charniak (1997), collins (1997), and many subsequent researchers1 annotated every node with lexical features passed up from its head-child in order to more precisely reflect the node's inside contents. 
317 citation observed data hidden data collins (1997) treebank tree with head child annotated on each nonterminal no hidden data. 
collins (1997)'s parser and its reimplementation and extension by bikel (2002) have by now been applied to a variety of languages: english (collins, 1999), czech (collins et al, 1999), german (dubey and keller, 2003), spanish (cowan and collins, 2005), french (arun and keller, 2005), chinese (bikel, 2002) and, according to dan bikel's web page, arabic. 
evaluation 8.1 effects of unpublished details in this section we present the results of effectively doing a â€œclean-roomâ€ implementation of collinsâ€™ parsing model, that is, using only information available in (collins 1997, 1999), as shown in table 4. the clean-room model has a 10.6% increase in f-measure error compared to collinsâ€™ parser and an 11.0% increase in f-measure error compared to our engine in its complete emulation of collinsâ€™ model 2. this is comparable to the increase in 32 although we have implemented a version of this type of pruning that limits the number of items that can be collected in any one cell, that is, the maximum number of items that cover a particular span. 
these confidence values can be derived in a number of sensible ways; the technique used by collins was adapted from that used in bikel et al (1997), which makes use of a quantity called the diversity of the history context (witten and bell 1991), which is equal to the number of unique futures observed in training for that history context. 
another consequence of not generating posthead conjunctions and punctuation as first-class words is that they 19 in fact, if punctuation occurs before the head, it is not generated at allâ€”a deficiency in the parsing model that appears to be a holdover from the deficient punctuation handling in the model of collins (1997). 
503 bikel intricacies of collinsâ€™ parsing model table 4 overall parsing results using only details found in collins (1997, 1999). 
michael collins (1996, 1997, 1999) parsing models have been quite influential in the field of natural language processing. 
this was done for supervised parsing in different ways by collins (1997), klein and manning (2003), and mcdonald et al (2005), all of whom considered intervening material or coarse distance classes when predicting children in a tree. 
as in most other statistical parsing systems we therefore use the pruning technique described in goodman (1997) and collins (1999: 263-264) which assigns a score to each item in the chart equal to the product of the inside probability of the item and its prior probability. 
the base line for our base line parse accuracy, we used the now standard division of the wsj (see collins 1997, 1999; charniak 1997, 2000; ratnaparkhi 1999) with sections 2 through 21 for training (approx. 
(collins 1997, 1999; charniak 2000), and the current paper has shown the importance of including two and more nonhead words. 
head-lexicalized stochastic grammars have recently become increasingly popular (see collins 1997, 1999; charniak 1997, 2000). 
context-free rules charniak (1996) collins (1996), eisner (1996) context-free rules, headwords charniak (1997) context-free rules, headwords, grandparent nodes collins (2000) context-free rules, headwords, grandparent nodes/rules, bigrams, two-level rules, two-level bigrams, nonheadwords bod (1992) all fragments within parse trees scope of statistical dependencies model figure 4. schematic overview of the increase of statistical dependencies by stochastic parsers thus there seems to be a convergence towards a maximalist model which "takes all fragments [...] and lets the statistics decide" (bod 1998: 5). 
a major difference between our approach and most other models tested on the wsj is that the dop model uses frontier lexicalization while most other models use constituent lexicalization (in that they associate each constituent non terminal with its lexical head -see collins 1996, 1999; charniak 1997; eisner 1997). 
collins 1996; eisner 1996), later models showed the importance of including context from higher nodes in the tree (charniak 1997; johnson 1998). 
many stochastic parsing models use linguistic intuitions to find this minimal set, for example by restricting the statistical dependencies to the locality of headwords of constituents (collins 1997, 1999; eisner 1997), leaving it as an open question whether there exist important statistical dependencies that go beyond linguistically motivated dependencies. 
table 1 shows the lp and lr scores obtained with our base line subtree set, and compares these scores with those of previous stochastic parsers tested on the wsj (respectively charniak 1997, collins 1999, ratnaparkhi 1999, and charniak 2000). 
we should note, however, that most other stochastic parsers do include counts of single nonheadwords: they appear in the backed-off statistics of these parsers (see collins 1997, 1999; charniak 1997; goodman 1998). 
in parsing, the most relevant previous work is due to collins (1997), who considered three binary features of the intervening material: did it contain (a) any word tokens at all, (b) any verbs, (c) any commas or colons? 
as discussed in footnote 3, collins (1997) and mcdonald et al (2005) considered the pos tags intervening between a head and child. 
statistical disambiguation such as (collins and brooks, 1995) for pp-attachment or (collins, 1997; charniak, 2000) for generative parsing greatly improve disambiguation, but as they model by imitation instead of by understanding, complete soundness has to remain elusive. 
nasrÂ©lia, univ-avignon, fr) :~cogentex, inc. (owenocogentex.com) 1 introduction dependency grammar has a long tradition in syntactic theory, dating back to at least tesni~re's work from the thirties3 recently, it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words (see, e.g., (collins, 1997)), which is what dependency grammars model explicitly do, but context-free phrasestructure grammars do not. 
most recently, mcdonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as charniak (2000)) and very impressive speed (it is about ten times faster than collins (1997) and four times faster than charniak (2000)). 
(collins, 1997) only the words that occur more than d times in training data. 
parsers that attempt to disambiguate the input completely â€” full parsing â€” typically first employ some kind of dynamic programming algorithm to derive a packed parse forest and then applies a probabilistic top-down model in order to select the most probable analysis (collins, 1997; charniak, 2000). 
here, we extract part-of-speech tags from the collins parserâ€™s output (collins, 1997) for section 23 instead of reinventing a tagger. 
previous parsing models (e.g., collins, 1997; charniak, 2000) maximize the joint probability p(s, t) of a sentence s and its parse tree t. we maximize the conditional probability p(t | s). 
there has been a great deal of progress in statistical parsing in the past decade (collins, 1996; collins, 1997; chaniak, 2000). 
this permits us to make exact comparisons with the parser of yamada and matsumoto (2003), but also the parsers of collins (1997) and charniak (2000), which are evaluated on the same data set in yamada and matsumoto (2003). 
moreover, the deterministic dependency parser of yamada and matsumoto (2003), when trained on the penn treebank, gives a dependency accuracy that is almost as good as that of collins (1997) and charniak (2000). 
table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (model 2 with label set b) in comparison with collins (1997) (model 3), charniak (2000), and yamada and matsumoto (2003). 
on the other hand, the best available parsers trained on the penn treebank, those of collins (1997) and charniak (2000), use statistical models for disambiguation that make crucial use of dependency relations. 
many probabilistic evaluation models have been published inspired by one or more of these feature types [black, 1992] [briscoe, 1993] [charniak, 1997] [collins, 1996] [collins, 1997] [magerman, 1995] [eisner, 1996], but discrepancies between training sets, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively. 
in the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [black, 1992] [briscoe, 1993] [brown, 1991] [charniak, 1997] [collins, 1996] [collins, 1997] [magerman, 1991] [magerman, 1992] [magerman, 1995] [eisner, 1996]. 
mainstream approaches in statistical parsing are based on nondeterministic parsing techniques, usually employing some kind of dynamic programming, in combination with generative probabilistic models that provide an n-best ranking of the set of candidate analyses derived by the parser (collins, 1997; collins, 1999; charniak, 2000). 
deterministic parsing algorithms for building dependency graphs (kudo and matsumoto, 2002; yamada and matsumoto, 2003; nivre, 2003) 2. history-based models for predicting the next parser action (black et al, 1992; magerman, 1995; ratnaparkhi, 1997; collins, 1999) 3. discriminative learning to map histories to parser actions (kudo and matsumoto, 2002; yamada and matsumoto, 2003; nivre et al, 2004) in this section we will define dependency graphs, describe the parsing algorithm used in the experiments and finally explain the extraction of features for the history-based models. 
thus, model c uses a reduced version of equation (12) and an extended version of 2other models, as (collins and brooks, 1995; merlo et al, 1998) for pp-attachment resolution, or (collins, 1997; samuelsson, 2000) for probabilistic parsing, are somewhat related, but their supervised nature makes any direct comparison impossible. 
collins et al (1999) applied the parser of collins (1997) developed for english, to czech, and found thatthe performance was substantially lower when compared to the results for english. 
