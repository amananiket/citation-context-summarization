Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 41?48,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Evaluating the Accuracy of an Unlexicalized
Statistical Parser on the PARC DepBank
Ted Briscoe
Computer Laboratory
University of Cambridge
John Carroll
School of Informatics
University of Sussex
Abstract
We evaluate the accuracy of an unlexi-
calized statistical parser, trained on 4K
treebanked sentences from balanced data
and tested on the PARC DepBank. We
demonstrate that a parser which is compet-
itive in accuracy (without sacrificing pro-
cessing speed) can be quickly tuned with-
out reliance on large in-domain manually-
constructed treebanks. This makes it more
practical to use statistical parsers in ap-
plications that need access to aspects of
predicate-argument structure. The com-
parison of systems using DepBank is not
straightforward, so we extend and validate
DepBank and highlight a number of repre-
sentation and scoring issues for relational
evaluation schemes.
1 Introduction
Considerable progress has been made in accu-
rate statistical parsing of realistic texts, yield-
ing rooted, hierarchical and/or relational repre-
sentations of full sentences. However, much
of this progress has been made with systems
based on large lexicalized probabilistic context-
free like (PCFG-like) models trained on the Wall
Street Journal (WSJ) subset of the Penn Tree-
Bank (PTB). Evaluation of these systems has been
mostly in terms of the PARSEVAL scheme using
tree similarity measures of (labelled) precision and
recall and crossing bracket rate applied to section
23 of the WSJ PTB. (See e.g. Collins (1999) for
detailed exposition of one such very fruitful line
of research.)
We evaluate the comparative accuracy of an un-
lexicalized statistical parser trained on a smaller
treebank and tested on a subset of section 23 of
the WSJ using a relational evaluation scheme. We
demonstrate that a parser which is competitive
in accuracy (without sacrificing processing speed)
can be quickly developed without reliance on large
in-domain manually-constructed treebanks. This
makes it more practical to use statistical parsers in
diverse applications needing access to aspects of
predicate-argument structure.
We define a lexicalized statistical parser as one
which utilizes probabilistic parameters concerning
lexical subcategorization and/or bilexical relations
over tree configurations. Current lexicalized sta-
tistical parsers developed, trained and tested on
PTB achieve a labelled F1-score ? the harmonic
mean of labelled precision and recall ? of around
90%. Klein and Manning (2003) argue that such
results represent about 4% absolute improvement
over a carefully constructed unlexicalized PCFG-
like model trained and tested in the same man-
ner.1 Gildea (2001) shows that WSJ-derived bilex-
ical parameters in Collins? (1999) Model 1 parser
contribute less than 1% to parse selection accu-
racy when test data is in the same domain, and
yield no improvement for test data selected from
the Brown Corpus. Bikel (2004) shows that, in
Collins? (1999)Model 2, bilexical parameters con-
tribute less than 0.5% to accuracy on in-domain
data while lexical subcategorization-like parame-
ters contribute just over 1%.
Several alternative relational evaluation
schemes have been developed (e.g. Carroll et al,
1998; Lin, 1998). However, until recently, no
WSJ data has been carefully annotated to support
relational evaluation. King et al (2003) describe
the PARC 700 Dependency Bank (hereinafter
DepBank), which consists of 700 WSJ sentences
randomly drawn from section 23. These sentences
have been annotated with syntactic features and
with bilexical head-dependent relations derived
from the F-structure representation of Lexical
Functional Grammar (LFG). DepBank facilitates
1Klein and Manning retained some functional tag infor-
mation from PTB, so it could be argued that their model re-
mains ?mildly? lexicalized since functional tags encode some
subcategorization information.
41
comparison of PCFG-like statistical parsers
developed from the PTB with other parsers whose
output is not designed to yield PTB-style trees,
using an evaluation which is closer to the protypi-
cal parsing task of recovering predicate-argument
structure.
Kaplan et al (2004) compare the accuracy and
speed of the PARC XLE Parser to Collins? Model
3 parser. They develop transformation rules for
both, designed to map native output to a subset of
the features and relations in DepBank. They com-
pare performance of a grammatically cut-down
and complete version of the XLE parser to the
publically available version of Collins? parser.
One fifth of DepBank is held out to optimize the
speed and accuracy of the three systems. They
conclude from the results of these experiments that
the cut-down XLE parser is two-thirds the speed
of Collins? Model 3 but 12% more accurate, while
the complete XLE system is 20% more accurate
but five times slower. F1-score percentages range
from the mid- to high-70s, suggesting that the re-
lational evaluation is harder than PARSEVAL.
Both Collins? Model 3 and the XLE Parser use
lexicalized models for parse selection trained on
the rest of the WSJ PTB. Therefore, although Ka-
plan et al demonstrate an improvement in accu-
racy at some cost to speed, there remain questions
concerning viability for applications, at some re-
move from the financial news domain, for which
substantial treebanks are not available. The parser
we deploy, like the XLE one, is based on a
manually-defined feature-based unification gram-
mar. However, the approach is somewhat differ-
ent, making maximal use of more generic struc-
tural rather than lexical information, both within
the grammar and the probabilistic parse selection
model. Here we compare the accuracy of our
parser with Kaplan et al?s results, by repeating
their experiment with our parser. This compari-
son is not straightforward, given both the system-
specific nature of some of the annotation in Dep-
Bank and the scoring reported. We, therefore, ex-
tend DepBank with a set of grammatical relations
derived from our own system output and highlight
how issues of representation and scoring can affect
results and their interpretation.
In ?2, we describe our development method-
ology and the resulting system in greater detail.
?3 describes the extended Depbank that we have
developed and motivates our additions. ?2.4 dis-
cusses how we trained and tuned our current sys-
tem and describes our limited use of information
derived from WSJ text. ?4 details the various ex-
periments undertaken with the extended DepBank
and gives detailed results. ?5 discusses these re-
sults and proposes further lines of research.
2 Unlexicalized Statistical Parsing
2.1 System Architecture
Both the XLE system and Collins? Model 3 pre-
process textual input before parsing. Similarly,
our baseline system consists of a pipeline of mod-
ules. First, text is tokenized using a deterministic
finite-state transducer. Second, tokens are part-of-
speech and punctuation (PoS) tagged using a 1st-
order Hidden Markov Model (HMM) utilizing a
lexicon of just over 50K words and an unknown
word handling module. Third, deterministic mor-
phological analysis is performed on each token-
tag pair with a finite-state transducer. Fourth, the
lattice of lemma-affix-tags is parsed using a gram-
mar over such tags. Finally, the n-best parses are
computed from the parse forest using a probabilis-
tic parse selection model conditioned on the struc-
tural parse context. The output of the parser can be
displayed as syntactic trees, and/or factored into a
sequence of bilexical grammatical relations (GRs)
between lexical heads and their dependents.
The full system can be extended in a variety of
ways ? for example, by pruning PoS tags but al-
lowing multiple tag possibilities per word as in-
put to the parser, by incorporating lexical subcate-
gorization into parse selection, by computing GR
weights based on the proportion and probability
of the n-best analyses yielding them, and so forth
? broadly trading accuracy and greater domain-
dependence against speed and reduced sensitivity
to domain-specific lexical behaviour (Briscoe and
Carroll, 2002; Carroll and Briscoe, 2002; Watson
et al, 2005; Watson, 2006). However, in this pa-
per we focus exclusively on the baseline unlexical-
ized system.
2.2 Grammar Development
The grammar is expressed in a feature-based, uni-
fication formalism. There are currently 676 phrase
structure rule schemata, 15 feature propagation
rules, 30 default feature value rules, 22 category
expansion rules and 41 feature types which to-
gether define 1124 compiled phrase structure rules
in which categories are represented as sets of fea-
42
tures, that is, attribute-value pairs, possibly with
variable values, possibly bound between mother
and one or more daughter categories. 142 of the
phrase structure schemata are manually identified
as peripheral rather than core rules of English
grammar. Categories are matched using fixed-
arity term unification at parse time.
The lexical categories of the grammar consist
of feature-based descriptions of the 149 PoS tags
and 13 punctuation tags (a subset of the CLAWS
tagset, see e.g. Sampson, 1995) which constitute
the preterminals of the grammar. The number
of distinct lexical categories associated with each
preterminal varies from 1 for some function words
through to around 35 as, for instance, tags for main
verbs are associated with a VSUBCAT attribute tak-
ing 33 possible values. The grammar is designed
to enumerate possible valencies for predicates by
including separate rules for each pattern of pos-
sible complementation in English. The distinc-
tion between arguments and adjuncts is expressed
by adjunction of adjuncts to maximal projections
(XP ? XP Adjunct) as opposed to government of
arguments (i.e. arguments are sisters within X1
projections; X1 ? X0 Arg1. . . ArgN).
Each phrase structure schema is associated with
one or more GR specifications which can be con-
ditioned on feature values instantiated at parse
time and which yield a rule-to-rule mapping from
local trees to GRs. The set of GRs associated with
a given derivation define a connected, directed
graph with individual nodes representing lemma-
affix-tags and arcs representing named grammati-
cal relations. The encoding of this mapping within
the grammar is similar to that of F-structure map-
ping in LFG. However, the connected graph is not
constructed and completeness and coherence con-
straints are not used to filter the phrase structure
derivation space.
The grammar finds at least one parse rooted in
the start category for 85% of the Susanne treebank,
a 140K word balanced subset of the Brown Cor-
pus, which we have used for development (Samp-
son, 1995). Much of the remaining data consists
of phrasal fragments marked as independent text
sentences, for example in dialogue. Grammati-
cal coverage includes the majority of construction
types of English, however the handling of some
unbounded dependency constructions, particularly
comparatives and equatives, is limited because of
the lack of fine-grained subcategorization infor-
mation in the PoS tags and by the need to balance
depth of analysis against the size of the deriva-
tion space. On the Susanne corpus, the geometric
mean of the number of analyses for a sentence of
length n is 1.31n. The microaveraged F1-score for
GR extraction on held-out data from Susanne is
76.5% (see section 4.2 for details of the evaluation
scheme).
The system has been used to analyse about 150
million words of English text drawn primarily
from the PTB, TREC, BNC, and Reuters RCV1
datasets in connection with a variety of projects.
The grammar and PoS tagger lexicon have been
incrementally improved by manually examining
cases of parse failure on these datasets. How-
ever, the effort invested amounts to a few days?
effort for each new dataset as opposed to the main
grammar development effort, centred on Susanne,
which has extended over some years and now
amounts to about 2 years? effort (see Briscoe, 2006
for further details).
2.3 Parser
To build the parsing module, the unification gram-
mar is automatically converted into an atomic-
categoried context free ?backbone?, and a non-
deterministic LALR(1) table is constructed from
this, which is used to drive the parser. The residue
of features not incorporated into the backbone
are unified on each rule application (reduce ac-
tion). In practice, the parser takes average time
roughly quadratic in the length of the input to cre-
ate a packed parse forest represented as a graph-
structured stack. The statistical disambiguation
phase is trained on Susanne treebank bracketings,
producing a probabilistic generalized LALR(1)
parser (e.g. Inui et al, 1997) which associates
probabilities with alternative actions in the LR ta-
ble.
The parser is passed as input the sequence of
most probable lemma-affix-tags found by the tag-
ger. During parsing, probabilities are assigned
to subanalyses based on the the LR table actions
that derived them. The n-best (i.e. most proba-
ble) parses are extracted by a dynamic program-
ming procedure over subanalyses (represented by
nodes in the parse forest). The search is effi-
cient since probabilities are associated with single
nodes in the parse forest and no weight function
over ancestor or sibling nodes is needed. Proba-
bilities capture structural context, since nodes in
43
the parse forest partially encode a configuration of
the graph-structured stack and lookahead symbol,
so that, unlike a standard PCFG, the model dis-
criminates between derivations which only differ
in the order of application of the same rules and
also conditions rule application on the PoS tag of
the lookahead token.
When there is no parse rooted in the start cat-
egory, the parser returns a connected sequence
of partial parses which covers the input based
on subanalysis probability and a preference for
longer and non-lexical subanalysis combinations
(e.g. Kiefer et al, 1999). In these cases, the GR
graph will not be fully connected.
2.4 Tuning and Training Method
The HMM tagger has been trained on 3M words
of balanced text drawn from the LOB, BNC and
Susanne corpora, which are available with hand-
corrected CLAWS tags. The parser has been
trained from 1.9K trees for sentences from Su-
sanne that were interactively parsed to manually
obtain the correct derivation, and also from 2.1K
further sentences with unlabelled bracketings de-
rived from the Susanne treebank. These brack-
etings guide the parser to one or possibly sev-
eral closely-matching derivations and these are
used to derive probabilities for the LR table us-
ing (weighted) Laplace estimation. Actions in the
table involving rules marked as peripheral are as-
signed a uniform low prior probability to ensure
that derivations involving such rules are consis-
tently lower ranked than those involving only core
rules.
To improve performance onWSJ text, we exam-
ined some parse failures from sections other than
section 23 to identify patterns of consistent fail-
ure. We then manually modified and extended the
grammar with a further 6 rules, mostly to handle
cases of indirect and direct quotation that are very
common in this dataset. This involved 3 days?
work. Once completed, the parser was retrained
on the original data. A subsequent limited inspec-
tion of top-ranked parses led us to disable 6 ex-
isting rules which applied too freely to the WSJ
text; these were designed to analyse auxiliary el-
lipsis which appears to be rare in this genre. We
also catalogued incorrect PoS tags fromWSJ parse
failures and manually modified the tagger lexicon
where appropriate. These modifications mostly
consisted of adjusting lexical probabilities of ex-
tant entries with highly-skewed distributions. We
also added some tags to extant entries for infre-
quent words. These modifications took a further
day. The tag transition probabilities were not rees-
timated. Thus, we have made no use of the PTB
itself and only limited use of WSJ text.
This method of grammar and lexicon devel-
opment incrementally improves the overall per-
formance of the system averaged across all the
datasets that it has been applied to. It is very
likely that retraining the PoS tagger on the WSJ
and retraining the parser using PTB would yield
a system which would perform more effectively
on DepBank. However, one of our goals is to
demonstrate that an unlexicalized parser trained
on a modest amount of annotated text from other
sources, coupled to a tagger also trained on
generic, balanced data, can perform competitively
with systems which have been (almost) entirely
developed and trained using PTB, whether or not
these systems deploy hand-crafted grammars or
ones derived automatically from treebanks.
3 Extending and Validating DepBank
DepBank was constructed by parsing the selected
section 23 WSJ sentences with the XLE system
and outputting syntactic features and bilexical re-
lations from the F-structure found by the parser.
These features and relations were subsequently
checked, corrected and extended interactively with
the aid of software tools (King et al, 2003).
The choice of relations and features is based
quite closely on LFG and, in fact, overlaps sub-
stantially with the GR output of our parser. Fig-
ure 1 illustrates some DepBank annotations used
in the experiment reported by Kaplan et al and
our hand-corrected GR output for the example
Ten of the nation?s governors meanwhile called
on the justices to reject efforts to limit abortions.
We have kept the GR representation simpler and
more readable by suppressing lemmatization, to-
ken numbering and PoS tags, but have left the
DepBank annotations unmodified.
The example illustrates some differences be-
tween the schemes. For instance, the subj and
ncsubj relations overlap as both annotations con-
tain such a relation between call(ed) and Ten), but
the GR annotation also includes this relation be-
tween limit and effort(s) and reject and justice(s),
while DepBank links these two verbs to a variable
pro. This reflects a difference of philosophy about
44
DepBank: obl(call?0, on?2)
stmt_type(call?0, declarative)
subj(call?0, ten?1)
tense(call?0, past)
number_type(ten?1, cardinal)
obl(ten?1, governor?35)
obj(on?2, justice?30)
obj(limit?7, abortion?15)
subj(limit?7, pro?21)
obj(reject?8, effort?10)
subj(reject?8, pro?27)
adegree(meanwhile?9, positive)
num(effort?10, pl)
xcomp(effort?10, limit?7)
GR: (ncsubj called Ten _)
(ncsubj reject justices _)
(ncsubj limit efforts _)
(iobj called on)
(xcomp to called reject)
(dobj reject efforts)
(xmod to efforts limit)
(dobj limit abortions)
(dobj on justices)
(det justices the)
(ta bal governors meanwhile)
(ncmod poss governors nation)
(iobj Ten of)
(dobj of governors)
(det nation the)
Figure 1: DepBank and GR annotations.
resolution of such ?understood? relations in differ-
ent constructions. Viewed as output appropriate to
specific applications, either approach is justifiable.
However, for evaluation, these DepBank relations
add little or no information not already specified
by the xcomp relations in which these verbs also
appear as dependents. On the other hand, Dep-
Bank includes an adjunct relation between mean-
while and call(ed), while the GR annotation treats
meanwhile as a text adjunct (ta) of governors, de-
limited by balanced commas, following Nunberg?s
(1990) text grammar but conveying less informa-
tion here.
There are also issues of incompatible tokeniza-
tion and lemmatization between the systems and
of differing syntactic annotation of similar infor-
mation, which lead to problems mapping between
our GR output and the current DepBank. Finally,
differences in the linguistic intuitions of the an-
notators and errors of commission or omission
on both sides can only be uncovered by manual
comparison of output (e.g. xmod vs. xcomp for
limit efforts above). Thus we reannotated the Dep-
Bank sentences with GRs using our current sys-
tem, and then corrected and extended this anno-
tation utilizing a software tool to highlight dif-
ferences between the extant annotations and our
own.2 This exercise, though time-consuming, un-
covered problems in both annotations, and yields
a doubly-annotated and potentially more valuable
resource in which annotation disagreements over
complex attachment decisions, for instance, can be
inspected.
The GR scheme includes one feature in Dep-
Bank (passive), several splits of relations in Dep-
Bank, such as adjunct, adds some of DepBank?s
featural information, such as subord form, as a
subtype slot of a relation (ccomp), merges Dep-
Bank?s oblique with iobj, and so forth. But it
does not explicitly include all the features of Dep-
Bank or even of the reduced set of semantically-
relevant features used in the experiments and eval-
uation reported in Kaplan et al. Most of these
features can be computed from the full GR repre-
sentation of bilexical relations between numbered
lemma-affix-tags output by the parser. For in-
stance, num features, such as the plurality of jus-
tices in the example, can be computed from the
full det GR (det justice+s NN2:4 the AT:3)
based on the CLAWS tag (NN2 indicating ?plu-
ral?) selected for output. The few features that can-
not be computed from GRs and CLAWS tags di-
rectly, such as stmt type, could be computed from
the derivation tree.
4 Experiments
4.1 Experimental Design
We selected the same 560 sentences as test data as
Kaplan et al, and all modifications that we made
to our system (see ?2.4) were made on the basis
of (very limited) information from other sections
of WSJ text.3 We have made no use of the further
140 held out sentences in DepBank. The results
we report below are derived by choosing the most
probable tag for each word returned by the PoS
tagger and by choosing the unweighted GR set re-
turned for the most probable parse with no lexical
information guiding parse ranking.
4.2 Results
Our parser produced rooted sentential analyses for
84% of the test items; actual coverage is higher
2The new version of DepBank along with evaluation
software is included in the current RASP distribution:
www.informatics.susx.ac.uk/research/nlp/rasp
3The PARC group kindly supplied us with the experimen-
tal data files they used to facilitate accurate reproduction of
this experiment.
45
Relation Precision Recall F1 P R F1 Relation
mod 75.4 71.2 73.3
ncmod 72.9 67.9 70.3
xmod 47.7 45.5 46.6
cmod 51.4 31.6 39.1
pmod 30.8 33.3 32.0
det 88.7 91.1 89.9
arg mod 71.9 67.9 69.9
arg 76.0 73.4 74.6
subj 80.1 66.6 72.7 73 73 73
ncsubj 80.5 66.8 73.0
xsubj 50.0 28.6 36.4
csubj 20.0 50.0 28.6
subj or dobj 82.1 74.9 78.4
comp 74.5 76.4 75.5
obj 78.4 77.9 78.1
dobj 83.4 81.4 82.4 75 75 75 obj
obj2 24.2 38.1 29.6 42 36 39 obj-theta
iobj 68.2 68.1 68.2 64 83 72 obl
clausal 63.5 71.6 67.3
xcomp 75.0 76.4 75.7 74 73 74
ccomp 51.2 65.6 57.5 78 64 70 comp
pcomp 69.6 66.7 68.1
aux 92.8 90.5 91.6
conj 71.7 71.0 71.4 68 62 65
ta 39.1 48.2 43.2
passive 93.6 70.6 80.5 80 83 82
adegree 89.2 72.4 79.9 81 72 76
coord form 92.3 85.7 88.9 92 93 93
num 92.2 89.8 91.0 86 87 86
number type 86.3 92.7 89.4 96 95 96
precoord form 100.0 16.7 28.6 100 50 67
pron form 92.1 91.9 92.0 88 89 89
prt form 71.1 58.7 64.3 72 65 68
subord form 60.7 48.1 53.6
macroaverage 69.0 63.4 66.1
microaverage 81.5 78.1 79.7 80 79 79
Table 1: Accuracy of our parser, and where
roughly comparable, the XLE as reported by King
et al
than this since some of the test sentences are el-
liptical or fragmentary, but in many cases are rec-
ognized as single complete constituents. Kaplan
et al report that the complete XLE system finds
rooted analyses for 79% of section 23 of the WSJ
but do not report coverage just for the test sen-
tences. The XLE parser uses several performance
optimizations which mean that processing of sub-
analyses in longer sentences can be curtailed or
preempted, so that it is not clear what proportion
of the remaining data is outside grammatical cov-
erage.
Table 1 shows accuracy results for each indi-
vidual relation and feature, starting with the GR
bilexical relations in the extended DepBank and
followed by most DepBank features reported by
Kaplan et al, and finally overall macro- and mi-
croaverages. The macroaverage is calculated by
taking the average of each measure for each indi-
vidual relation and feature; the microaverage mea-
sures are calculated from the counts for all rela-
tions and features.4 Indentation of GRs shows
degree of specificity of the relation. Thus, mod
scores are microaveraged over the counts for the
five fully specified modifier relations listed imme-
diately after it in Table 1. This allows comparison
of overall accuracy on modifiers with, for instance
overall accuracy on arguments. Figures in italics
to the right are discussed in the next section.
Kaplan et al?s microaveraged scores for
Collins? Model 3 and the cut-down and complete
versions of the XLE parser are given in Table 2,
along with the microaveraged scores for our parser
from Table 1. Our system?s accuracy results (eval-
uated on the reannotated DepBank) are better than
those for Collins and the cut-down XLE, and very
similar overall to the complete XLE (evaluated
on DepBank). Speed of processing is also very
competitive.5 These results demonstrate that a
statistical parser with roughly state-of-the-art ac-
curacy can be constructed without the need for
large in-domain treebanks. However, the perfor-
mance of the system, as measured by microrav-
eraged F1-score on GR extraction alone, has de-
clined by 2.7% over the held-out Susanne data,
so even the unlexicalized parser is by no means
domain-independent.
4.3 Evaluation Issues
The DepBank num feature on nouns is evalu-
ated by Kaplan et al on the grounds that it is
semantically-relevant for applications. There are
over 5K num features in DepBank so the overall
microaveraged scores for a system will be signifi-
cantly affected by accuracy on num. We expected
our system, which incorporates a tagger with good
empirical (97.1%) accuracy on the test data, to re-
cover this feature with 95% accuracy or better, as
it will correlate with tags NNx1 and NNx2 (where
?x? represents zero or more capitals in the CLAWS
4We did not compute the remaining DepBank features
stmt type, tense, prog or perf as these rely on information
that can only be extracted from the derivation tree rather than
the GR set.
5Processing time for our system was 61 seconds on one
2.2GHz Opteron CPU (comprising tokenization, tagging,
morphology, and parsing, including module startup over-
heads). Allowing for slightly different CPUs, this is 2.5?10
times faster than the Collins and XLE parsers, as reported by
Kaplan et al
46
System Eval corpus Precision Recall F1
Collins DepBank 78.3 71.2 74.6
Cut-down XLE DepBank 79.1 76.2 77.6
Complete XLE DepBank 79.4 79.8 79.6
Our system DepBank/GR 81.5 78.1 79.7
Table 2: Microaveraged overall scores from Kaplan et al and for our system.
tagset). However, DepBank treats the majority
of prenominal modifiers as adjectives rather than
nouns and, therefore, associates them with an ade-
gree rather than a num feature. The PoS tag se-
lected depends primarily on the relative lexical
probabilities of each tag for a given lexical item
recorded in the tagger lexicon. But, regardless
of this lexical decision, the correct GR is recov-
ered, and neither adegree(positive) or num(sg)
add anything semantically-relevant when the lex-
ical item is a nominal premodifier. A strategy
which only provided a num feature for nominal
heads would be both more semantically-relevant
and would also yield higher precision (95.2%).
However, recall (48.4%) then suffers against Dep-
Bank as noun premodifiers have a num feature.
Therefore, in the results presented in Table 1 we
have not counted cases where either DepBank or
our system assign a premodifier adegree(positive)
or num(sg).
There are similar issues with other DepBank
features and relations. For instance, the form of
a subordinator with clausal complements is anno-
tated as a relation between verb and subordina-
tor, while there is a separate comp relation be-
tween verb and complement head. The GR rep-
resentation adds the subordinator as a subtype of
ccomp recording essentially identical information
in a single relation. So evaluation scores based on
aggregated counts of correct decisions will be dou-
bled for a system which structures this informa-
tion as in DepBank. However, reproducing the ex-
act DepBank subord form relation from the GR
ccomp one is non-trivial because DepBank treats
modal auxiliaries as syntactic heads while the GR-
scheme treats the main verb as head in all ccomp
relations. We have not attempted to compensate
for any further such discrepancies other than the
one discussed in the previous paragraph. However,
we do believe that they collectively damage scores
for our system.
As King et al note, it is difficult to identify
such informational redundancies to avoid double-
counting and to eradicate all system specific bi-
ases. However, reporting precision, recall and F1-
scores for each relation and feature separately and
microaveraging these scores on the basis of a hi-
erarchy, as in our GR scheme, ameliorates many
of these problems and gives a better indication
of the strengths and weaknesses of a particular
parser, which may also be useful in a decision
about its usefulness for a specific application. Un-
fortunately, Kaplan et al do not report their re-
sults broken down by relation or feature so it is
not possible, for example, on the basis of the ar-
guments made above, to choose to compare the
performance of our system on ccomp to theirs for
comp, ignoring subord form. King et al do re-
port individual results for selected features and re-
lations from an evaluation of the complete XLE
parser on all 700 DepBank sentences with an al-
most identical overall microaveraged F1 score of
79.5%, suggesting that these results provide a rea-
sonably accurate idea of the XLE parser?s relative
performance on different features and relations.
Where we believe that the information captured
by a DepBank feature or relation is roughly com-
parable to that expressed by a GR in our extended
DepBank, we have included King et al?s scores
in the rightmost column in Table 1 for compari-
son purposes. Even if these features and relations
were drawn from the same experiment, however,
they would still not be exactly comparable. For in-
stance, as discussed in ?3 nearly half (just over 1K)
the DepBank subj relations include pro as one el-
ement, mostly double counting a corresponding
xcomp relation. On the other hand, our ta rela-
tion syntactically underspecifies many DepBank
adjunct relations. Nevertheless, it is possible to
see, for instance, that while both parsers perform
badly on second objects ours is worse, presumably
because of lack of lexical subcategorization infor-
mation.
47
5 Conclusions
We have demonstrated that an unlexicalized parser
with minimal manual modification for WSJ text ?
but no tuning of performance to optimize on this
dataset alne, and no use of PTB ? can achieve
accuracy competitive with parsers employing lex-
icalized statistical models trained on PTB.
We speculate that we achieve these results be-
cause our system is engineered to make minimal
use of lexical information both in the grammar and
in parse ranking, because the grammar has been
developed to constrain ambiguity despite this lack
of lexical information, and because we can com-
pute the full packed parse forest for all the test sen-
tences efficiently (without sacrificing speed of pro-
cessing with respect to other statistical parsers).
These advantages appear to effectively offset the
disadvantage of relying on a coarser, purely struc-
tural model for probabilistic parse selection. In fu-
ture work, we hope to improve the accuracy of the
system by adding lexical information to the statis-
tical parse selection component without exploiting
in-domain treebanks.
Clearly, more work is needed to enable more
accurate, informative, objective and wider com-
parison of extant parsers. More recent PTB-based
parsers show small improvements over Collins?
Model 3 using PARSEVAL, while Clark and Cur-
ran (2004) and Miyao and Tsujii (2005) report
84% and 86.7% F1-scores respectively for their
own relational evaluations on section 23 of WSJ.
However, it is impossible to meaningfully com-
pare these results to those reported here. The rean-
notated DepBank potentially supports evaluations
which score according to the degree of agreement
between this and the original annotation and/or de-
velopment of future consensual versions through
collaborative reannotation by the research com-
munity. We have also highlighted difficulties for
relational evaluation schemes and argued that pre-
senting individual scores for (classes of) relations
and features is both more informative and facili-
tates system comparisons.
6 References
Bikel, D.. 2004. Intricacies of Collins? parsing model, Com-
putational Linguistics, 30(4):479?512.
Briscoe, E.J.. 2006. An introduction to tag sequence gram-
mars and the RASP system parser, University of Cam-
bridge, Computer Laboratory Technical Report 662.
Briscoe, E.J. and J. Carroll. 2002. Robust accurate statistical
annotation of general text. In Proceedings of the 3rd Int.
Conf. on Language Resources and Evaluation (LREC),
Las Palmas, Gran Canaria. 1499?1504.
Carroll, J. and E.J. Briscoe. 2002. High precision extraction
of grammatical relations. In Proceedings of the 19th Int.
Conf. on Computational Linguistics (COLING), Taipei,
Taiwan. 134?140.
Carroll, J., E. Briscoe and A. Sanfilippo. 1998. Parser evalu-
ation: a survey and a new proposal. In Proceedings of the
1st International Conference on Language Resources and
Evaluation, Granada, Spain. 447?454.
Clark, S. and J. Curran. 2004. The importance of supertag-
ging for wide-coverage CCG parsing. In Proceedings of
the 20th International Conference on Computational Lin-
guistics (COLING-04), Geneva, Switzerland. 282?288.
Collins, M.. 1999. Head-driven Statistical Models for Nat-
ural Language Parsing. PhD Dissertation, Computer and
Information Science, University of Pennsylvania.
Gildea, D.. 2001. Corpus variation and parser performance.
In Proceedings of the Empirical Methods in Natural Lan-
guage Processing (EMNLP?01), Pittsburgh, PA.
Inui, K., V. Sornlertlamvanich, H. Tanaka and T. Tokunaga.
1997. A new formalization of probabilistic GLR parsing.
In Proceedings of the 5th International Workshop on Pars-
ing Technologies (IWPT?97), Boston, MA. 123?134.
Kaplan, R., S. Riezler, T. H. King, J. Maxwell III, A. Vasser-
man and R. Crouch. 2004. Speed and accuracy in shal-
low and deep stochastic parsing. In Proceedings of the
HLT Conference and the 4th Annual Meeting of the North
American Chapter of the ACL (HLT-NAACL?04), Boston,
MA.
Kiefer, B., H-U. Krieger, J. Carroll and R. Malouf. 1999.
A bag of useful techniques for efficient and robust pars-
ing. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics, University of
Maryland. 473?480.
King, T. H., R. Crouch, S. Riezler, M. Dalrymple and R. Ka-
plan. 2003. The PARC700 Dependency Bank. In Pro-
ceedings of the 4th International Workshop on Linguisti-
cally Interpreted Corpora (LINC-03), Budapest, Hungary.
Klein, D. and C. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, Sapporo,
Japan. 423?430.
Lin, D.. 1998. Dependency-based evaluation of MINIPAR.
In Proceedings of the Workshop at LREC?98 on The Eval-
uation of Parsing Systems, Granada, Spain.
Manning, C. and H. Schu?tze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press, Cambridge,
MA.
Miyao, Y. and J. Tsujii. 2005. Probabilistic disambiguation
models for wide-coverage HPSG parsing. In Proceedings
of the 43rd Annual Meeting of the Association for Compu-
tational Linguistics, Ann Arbor, MI. 83?90.
Nunberg, G.. 1990. The Linguistics of Punctuation. CSLI
Lecture Notes 18, Stanford, CA.
Sampson, G.. 1995. English for the Computer. Oxford Uni-
versity Press, Oxford, UK.
Watson, R.. 2006. Part-of-speech tagging models for parsing.
In Proceedings of the 9th Conference of Computational
Linguistics in the UK (CLUK?06), Open University, Mil-
ton Keynes.
Watson, R., J. Carroll and E.J. Briscoe. 2005. Efficient ex-
traction of grammatical relations. In Proceedings of the
9th Int. Workshop on Parsing Technologies (IWPT?05),
Vancouver, Ca..
48
