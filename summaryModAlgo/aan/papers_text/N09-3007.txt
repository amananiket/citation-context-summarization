Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 37?42,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Building a Semantic Lexicon of English Nouns via Bootstrapping
Ting Qian1, Benjamin Van Durme2 and Lenhart Schubert2
1Department of Brain and Cognitive Sciences
2Department of Computer Science
University of Rochester
Rochester, NY 14627 USA
ting.qian@rochester.edu, {vandurme, schubert}@cs.rochester.edu
Abstract
We describe the use of a weakly supervised
bootstrapping algorithm in discovering con-
trasting semantic categories from a source lex-
icon with little training data. Our method pri-
marily exploits the patterns in sentential con-
texts where different categories of words may
appear. Experimental results are presented
showing that such automatically categorized
terms tend to agree with human judgements.
1 Introduction
There are important semantic distinctions between
different types of English nouns. For example, some
nouns typically refer to a concrete physical object,
such as book, tree, etc. Others are used to represent
the process or the result of an event (e.g. birth, cele-
bration). Such information is useful in disambiguat-
ing syntactically similar phrases and sentences, so as
to provide more accurate semantic interpretations.
For instance, A MAN WITH HOBBIES and A MAN
WITH APPLES share the same structure, but convey
very different aspects about the man being referred
to (i.e. activities vs possessions).
Compiling such a lexicon by hand, e.g., WordNet
(Fellbaum, 1998), requires tremendous time and ex-
pertise. In addition, when new words appear, these
will have to be analyzed and added manually. Fur-
thermore, a single, global lexicon may contain er-
roneous categorizations when used within a specific
domain/genre; we would like a ?flexible? lexicon,
adaptable to a given corpus. Also, in adapting se-
mantic classifications of words to a particular genre
or domain, we would like to be able to exploit con-
tinuing improvements in methods of extracting se-
mantic occurrence patterns from text.
We present our initial efforts in discovering se-
mantic classes incrementally under a weakly super-
vised bootstrapping process. The approach is able
to selectively learn from its own discoveries, thereby
minimizing the effort needed to provide seed exam-
ples as well as maintaining a reasonable accuracy
rate. In what follows, we first focus on its appli-
cation to an event-noun classification task, and then
use a physical-object vs non-physical-object experi-
ment as a showcase for the algorithm?s generality.
2 Bootstrapping Algorithm
The bootstrapping algorithm discovers words with
semantic properties similar to a small set of labelled
seed examples. These examples can be manually se-
lected from an existing lexicon. By simply changing
the semantic property of the seed set, this algorithm
can be applied to the task of discovering a variety of
semantic classes.
Features Classification is performed using a
perceptron-based model (Rosenblatt, 1958) that ex-
amines features of each word. We use two kinds
of features in our model: morphological (affix and
word length), and contextual. Suffixes, such as -ion,
often reveal the semantic type that a noun belongs
to (e.g., destruction, explosion). Other suffixes like
-er typically suggest non-event nouns (e.g. waiter,
hanger). The set of affixes can be modified to re-
flect meaningful distinctions in the task at hand. Re-
garding word length, longer words tend to have more
37
syllables, and thus are more likely to contain affixes.
For example, if a word ends with -ment, its num-
ber of letters must be ? 5. We defined a partition
of words based on word length: shortest (fewer than
5 letters), short (5-7), medium (8-12), long (13-19),
and longest (> 19).
Besides morphological features, we also make use
of verbalized propositions resulting from the experi-
ments of Van Durme et al (2008) as contextual fea-
tures. These outputs are in the form of world knowl-
edge ?factoids? abstracted from texts, based on log-
ical forms from parsed sentences, produced by the
KNEXT system (see Schubert (2002) for details).
The followings are some sample factoids about the
word destruction, extracted from the British Na-
tional Corpus.
? A PERSON-OR-ORGANIZATION MAY UNDERGO A DE-
STRUCTION
? INDIVIDUAL -S MAY HAVE A DESTRUCTION
? PROPERTY MAY UNDERGO A DESTRUCTION
We take each verbalization (with the target word
removed) as a contextual feature, such as PROPERTY
MAY UNDERGO A . Words from the same seman-
tic category (e.g., event nouns) should have seman-
tic and syntactic similarities on the sentential level.
Thus their contextual features, which reflect the use
of words both semantically and syntactically, should
be similar. For instance, PROPERTY MAY UNDERGO
A PROTECTION is another verbalization produced
by KNEXT, suggesting the word protection may be-
long to the same category as destruction.
A few rough-and-ready heuristics are already em-
ployed by KNEXT to do the same task as we wish
to automate here. A built-in classifier judges nomi-
nals to be event or non-event ones based on analysis
of endings, plus a list of event nouns whose endings
are unrevealing, and a list of non-event nouns whose
endings tend to suggest they are event nouns. As a
result, the factoids used as contextual features in our
work already reflect the built-in classifier?s attempt
to distinguish event nouns from the rest. Thus, the
use of these contextual features may bias the algo-
rithm to perform seemingly well on event-noun clas-
sification. However, we will show that our algorithm
works for classification of other semantic categories,
for which KNEXT does not yet have discriminative
procedures.
Iterative Training We use a bootstrapping pro-
cedure to iteratively train a perceptron-based lin-
ear classifier. A perceptron algorithm determines
whether the active features of a test case are similar
to those learned from given categories of examples.
In an iterative training process, the classifier first
learns from a small seed set, which contains exam-
ples of all categories (in binary classification, both
positive and negative examples) manually selected
to reflect human knowledge of semantic categories.
The classifier then discovers new instances (and cor-
responding features) of each category. Based on
activation values, these newly discovered instances
are selectively admitted into the original training set,
which increases the size of training examples for the
next iteration.
The iterative training algorithm described above
is adopted from Klementiev and Roth (2006). The
advantage of bootstrapping is the ability to auto-
matically learn from new discoveries, which saves
both time and effort required to manually examine
a source lexicon. However, if implemented exactly
as described above, this process has two apparent
disadvantages: New examples may be wrongly clas-
sified by the model; and it is difficult to evaluate the
discriminative models produced in successive itera-
tions, as there are no standard data against which to
judge them (the new examples are by definition pre-
viously unexamined). We propose two measures to
alleviate these problems. First, we admit into the
training set only those instances whose activation
values are higher than the mean activation of their
corresponding categories in an iteration. This sets
a variable threshold that is correlated with the per-
formance of the model at each iteration. Second, we
evaluate iterative results post hoc, using a Bootstrap-
ping Score. This measures the efficacy of bootstrap-
ping (i.e. the ratio of correct newly discovered in-
stances to training examples) and precision (i.e. the
proportion of correct discoveries among all those re-
turned by the algorithm). We compute this score to
decide which iteration has yielded the optimal dis-
criminative model.
3 Building an Event-noun Lexicon
We applied the bootstrapping algorithm to the task
of discovering event nouns from a source lexicon.
38
Event nouns are words that typically describe the
occurrence, the process, or the result of an event.
We first explore the effectiveness of this algorithm,
and then describe a method of extracting the optimal
model. Top-ranked features in the optimal model are
used to find subcategories of event nouns.
Experimental Setup The WordNet noun-list is
chosen as the source lexicon (Fellbaum, 1998),
which consists of 21,512 nouns. The purpose of
this task is to explore the separability of event nouns
from this collection.
typical suffixes: appeasement, arrival, renewal,
construction, robbery, departure, happening
irregular cases: birth, collapse, crash, death, de-
cline, demise, loss, murder
Table 1: Examples of event-nouns in initial training set.
We manually selected 15 event nouns and 215
non-event nouns for the seed set. Event-noun exam-
ples are representative of subcategories within the
semantic class, as well as their commonly seen mor-
phological structures (Table 1). Non-event examples
are primarily exceptions to morphological regulari-
ties (to prevent the algorithm from overly relying on
affix features), such as, anything, ambition, diago-
nal. The subset of all contextual and morphological
features represented by both event and non-event ex-
amples are used to bootstrap the training process.
Event Noun Discovery Reducing the number of
working features is often an effective strategy in
training a perceptron. We experimented with two
cut-off thresholds for features: in Trial 1, features
must appear at least 10 times (55,354 remaining);
in Trial 2, features must appear at least 15 times
(35,457 remaining).
We set the training process to run for 20 iterations
in both trials. Classification results of each iteration
were collected. We expect the algorithm to discover
few event nouns during early iterations. But with
new instances found in each subsequent iteration,
it ought to utilize newly seen features and discover
more. Figure 1 confirms our intuition.
The number of classified event-noun instances in-
creased sharply at the 15th iteration in Trial 1 and the
11th iteration in Trial 2, which may suggest overfit-
ting of the training examples used in those iterations.
If so, this should also correlate with an increase of
error rate in the classification results (error rate de-
fined as the percentage of non-event nouns identi-
fied as event nouns in all discovered event nouns).
We manually marked all misclassified event noun in-
stances for the first 10 iterations in both trials. The
error rate in Trial 2 is expected to significantly in-
crease at the 10th iteration, while Trial 1 should ex-
hibit little increase in error rate within this interval.
This expectation is confirmed in Figure 2.
Extracting the Optimal Model We further pur-
sued the task of finding the iteration that has yielded
the best model. Optimality is judged from two as-
pects: 1) the number of correctly identified event
nouns should be significantly larger than the size of
seed examples; and 2) the accuracy of classification
results should be relatively high so that it takes lit-
tle effort to clean up the result. Once the optimal
model is determined, we analyze its most heavily
weighted features and try to derive finer categories
from them. Furthermore, the optimal model could
be used to discover new instances from other source
lexicons in the future.
We define a measure called the Bootstrapping
Score (BS), serving a similar purpose as an F-score.
BS is computed as in Formula (1).
BS = 2 ?BR ? PrecisionBR + Precision . (1)
Here the Bootstrapping Rate (BR) is computed as:
BR = |NEW ||NEW |+ |SEED| , (2)
where |NEW | is the number of correctly identi-
fied new instances (seed examples excluded), and
|SEED| is the size of seed examples. The rate
of bootstrapping reveals how large the effect of the
bootstrapping process is. Note that BR is different
from the classic measure recall, for which the total
number of relevent documents (i.e. true event nouns
in English) must be known a priori ? again, this
knowledge is what we are discovering. The score
is a post hoc solution; both BR and precision are
computed for analysis after the algorithm has fin-
ished. Combining Formulas (1) and (2), a higher
Bootstrapping Score means better model quality.
Bootstrapping scores of models in the first ten it-
erations are plotted in Figure 3. Model quality in
39
5 10 15 20
02
000
4000
6000
8000
10000
Iteration
Numb
er of E
vent N
ouns D
iscove
red
Trial 1Trial 2
Figure 1: Classification rate
2 4 6 8 100
.05
0.10
0.15
0.20
0.25
0.30
Iteration
Error r
ate
Trial 1Trial 2
Figure 2: Error rate
2 4 6 8 100
.82
0.84
0.86
0.88
0.90
0.92
0.94
Iteration
Boots
trappin
g Scor
e
Trial 1Trial 2
Figure 3: Bootstrapping score
1 . . . 6 . . . 10
incorrect 5 . . . 32 . . . 176
correct 79 . . . 236 . . . 497
error rate 5.9% . . . 11.9% . . . 26.2%
score 87.0% . . . 90.8% . . . 83.8%
Table 2: From iterations 1 to 10, comparison between
instance counts, error rates, and bootstrapping scores as
the measure of model quality.
Trial 2 is better than in Trial 1 on average. In ad-
dition, within Trial 2, Iteration 6 yielded the best
discriminative model with a bootstrapping score of
90.8%. Compared to instance counts and error rate
measures as shown in Table 2, this bootstrapping
score provides a balanced measure of model qual-
ity. The model at the 6th iteration (hereafter, Model
6) can be considered the optimal model generated
during the bootstrapping training process.
Top-ranked Features in the Optimal Model In
order to understand why Model 6 is optimal, we
extracted its top 15 features that activate the event-
noun target in Model 6, as listed in Table 3. Inter-
estingly, top-ranked features are all contextual ones.
In fact, in later models where the ranks of mor-
phological features are boosted, the algorithm per-
formed worse as a result of relying too much on
those context-insensitive features.
Collectively, top-ranked features define the con-
textual patterns of event nouns. We are interested
in finding semantic subcategories within the set of
event nouns (497 nouns, Trial 2) by exploiting these
features individually. For instance, some events typ-
ically happen to people only (e.g. birth, betrayal),
while others usually happen to inanimate objects
(e.g. destruction, removal). Human actions can also
be distinguished by the number of participants, such
as group activities (e.g. election) or individual ac-
tivities (e.g. death). It is thus worth distinguishing
nouns that describe different sorts of events.
Manual Classification We extracted the top 100
contextual features from Model 6 and grouped
them into feature classes. A feature class con-
sists of contextual features sharing similar mean-
ings. For instance, A COUNTRY MAY UNDERGO
and A STATE MAY UNDERGO both belong to the
class social activity. For each feature class, we enu-
merate all words that correspond to its feature in-
stances. Examples are shown in Table 4.
Not all events can be unambiguously classified
into one of the subcategories. However, this is also
not necessary because these categories overlap with
one another. For example, death describes an event
that tends to occur both individually and briefly. In
addition to the six categories listed here, new cate-
gories can be added by creating more feature classes.
Automatic Clustering Representing each noun as
a frequency vector over the top 100 most discrim-
inating contextual features, we employed k-means
clustering and compared the results to our manually
crafted subcategories.
Through trial-and-error, we set k to 12, with the
smallest resulting cluster containing 2 nouns (inter-
pretation and perception), while the biggest result-
ing cluster contained 320 event nouns (that seemed
to share no apparent semantic properties). Other
clusters varied from 5 to 50 words in size, with ex-
amples shown in Table 5.
The advantage of automatic clustering is that the
results may reflect an English speaker?s impression
of word similarity gained through language use. Un-
40
a person-or-organization may undergo a a state may undergo a a can be attempted
a country may undergo a a child may have a a can be for a country
a company may undergo a a project may undergo a authority may undergo a
an explanation can be for a an empire may undergo a a war may undergo a
days may have a a can be abrupt a can be rapid
Table 3: Top 15 features that promote activation of the event-noun target, ranked from most weighted to least.
human events: adoption, arrival, birth, betrayal,
death, development, disappearance, emancipation,
funeral . . .
events of inanimate objects: collapse, construc-
tion, definition, destruction, identification, incep-
tion, movement, recreation, removal . . .
individual activities: birth, death, execution, fu-
neral, promotion . . .
social activities: abolition, evolution, federation,
fragmentation, invasion . . .
lasting events: campaign, development, growth,
trial . . .
brief events: awakening, collapse, death, mention,
onset, resignation, thunderstorm . . .
Table 4: Six subcategories of event nouns.
fortunately, the discovered clusters do not typically
come with the same obvious semantic properties as
were defined in manual classification. In the exam-
ple given above, neither of Cluster 1 and Cluster 3
seems to have a centralized semantic theme. But
Cluster 2 seems to be mostly about human activities.
Comparison with WordNet To compare our re-
sults with WordNet resources, we enumerated all
children of the gloss ?something that happens at a
given place and time?, giving 7655 terms (phrases
excluded). This gave a broader range of event nouns,
such as proper nouns and procedures (e.g. 9/11, CT,
MRI), onomatopoeias (e.g. mew, moo), and words
whose event reading is only secondary (e.g. pic-
ture, politics, teamwork). These types of words tend
to have very different contextual features from what
our algorithm had discovered.
While our method may be limited by the choice of
seed examples, we were able to discover event nouns
not classified under this set by WordNet, suggest-
ing that the discovery mechanism itself is a robust
one. Among them were low-frequency nouns (e.g.
crescendo, demise, names of processes (e.g. absorp-
Cluster 1 (17): cancellation, cessation, closure,
crackle, crash, demise, disappearance, dismissal, dis-
solution, division, introduction, onset, passing, resig-
nation, reversal, termination, transformation
Cluster 2 (32): alienation, backing, betrayal, contem-
plation, election, execution, funeral, hallucination,
imitation, juxtaposition, killing, mention, moulding,
perfection, prosecution, recognition, refusal, removal,
resurrection, semblance, inspection, occupation, pro-
motion, trial . . .
Cluster 3 (7): development, achievement, arrival,
birth, death, loss, survival
Table 5: Examples resulting from automatic clustering.
tion, evolution), and particular cases like thunder-
storm.
4 Extension to Other Semantic Categories
To verify that our bootstrapping algorithm was not
simply relying on KNEXT?s own event classifica-
tion heuristics, we set the algorithm to learn the
distinction between physical and non-physical ob-
jects/entities.
(Non-)Physical-object Nouns 15 physical-object/
entity nouns (e.g. knife, ring, anthropologist) and
34 non-physical ones (e.g. happiness, knowledge)
were given to the model as the initial training set.
At the 9th iteration, the number of discovered physi-
cal objects (which form the minority group between
the two) approaches 2,200 and levels off. We ran-
domly sampled five 20-word groups (a subset of
these words are listed in Table 6) from this entire
set of discovered physical objects, and computed an
average error rate of 4%. Prominent features of the
model at the 9th iteration are shown in Table 7.
5 Related Work
The method of using distributional patterns in a
large text corpus to find semantically related En-
41
heifer, sheriff, collector, hippie, accountant, cape, scab,
pebble, box, dick, calculator, sago, brow, ship, ?john,
superstar, border, rabbit, poker, garter, grinder, million-
aire, ash, herdsman, ?cwm, pug, bra, fulmar, *cam-
paign, stallion, deserter, boot, tear, elbow, cavalry,
novel, cardigan, nutcase, ?bulge, businessman, cop, fig,
musician, spire, butcher, dog, elk, . . .
Table 6: Physical-object nouns randomly sampled from
results; words with an asterisk are misclassified, ones
with a question mark are doubtful.
a male-individual can be a a can be small
a person can be a a can be large
a can be young a can be german
-S*morphological feature a can be british
a can be old a can be good
Table 7: Top-10 features that promote activation of the
physical-object target in the model.
glish nouns first appeared in Hindle (1990). Roark
and Charniak (1998) constructed a semantic lexicon
using co-occurrence statistics of nouns within noun
phrases. More recently, Liakata and Pulman (2008)
induced a hierarchy over nominals using as features
knowledge fragments similar to the sort given by
KNEXT. Our work might be viewed as aiming for
the same goal (a lexico-semantic based partition-
ing over nominals, tied to corpus-based knowledge),
but allowing for an a priori bias regarding preferred
structure.
The idea of bootstrapping lexical semantic prop-
erties goes back at least to Hearst (1998), where the
idea is suggested of using seed examples of a rela-
tion to discover lexico-syntactic extraction patterns
and then using these to discover further examples
of the desired relation. The Basilisk system devel-
oped by Thelen and Riloff (2002) almost paralleled
our effort. However, negative features ? features
that would prevent a word from being classified into
a semantic category ? were not considered in their
model. In addition, in scoring candidate words, their
algorithm only looked at the average relevance of
syntactic patterns. Our perceptron-based algorithm
examines the combinatorial effect of those patterns,
which has yielded results suggesting improved ac-
curacy and bootstrapping efficacy.
Similar to our experiments here using k-means,
Lin and Pantel (2001) gave a clustering algorithm
for iteratively building semantic classes, using as
features argument positions within fragments from
a syntactic dependency parser.
6 Conclusion
We have presented a bootstrapping approach for cre-
ating semantically tagged lexicons. The method can
effectively classify nouns with contrasting semantic
properties, even when the initial training set is a very
small. Further classification is possible with both
manual and automatic methods by utilizing individ-
ual contextual features in the optimal model.
Acknowledgments
This work was supported by NSF grants IIS-
0328849 and IIS-0535105.
References
BNC Consortium. 2001. The British National Corpus,
version 2 (BNC World). Distributed by Oxford Uni-
versity Computing Services.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Marti A. Hearst. 1998. Automated discovery of Word-
Net relations. In (Fellbaum, 1998), pages 131?153.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In ACL.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In ACL.
Maria Liakata and Stephen Pulman. 2008. Auto-
matic Fine-Grained Semantic Classification for Do-
main Adaption. In Proceedings of Semantics in Text
Processing (STEP).
Dekang Lin and Patrick Pantel. 2001. Induction of se-
mantic classes from natural language text. In KDD.
Brian Roark and Eugene Charniak. 1998. Noun-phrase
co-occurrence statistics for semi-automatic semantic
lexicon construction. In ACL, pages 1110?1116.
Frank Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65(6):386?408.
Lenhart K. Schubert. 2002. Can we derive general world
knowledge from text? In HLT.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In EMNLP.
Benjamin Van Durme, Ting Qian, and Lenhart Schubert.
2008. Class-driven Attribute Extraction. In COLING.
42
