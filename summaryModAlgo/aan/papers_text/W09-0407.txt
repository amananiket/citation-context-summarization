Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 51?55,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The RWTH System Combination System for WMT 2009
Gregor Leusch, Evgeny Matusov, and Hermann Ney
RWTH Aachen University
Aachen, Germany
Abstract
RWTH participated in the System Combi-
nation task of the Fourth Workshop on Sta-
tistical Machine Translation (WMT 2009).
Hypotheses from 9 German?English MT
systems were combined into a consen-
sus translation. This consensus transla-
tion scored 2.1% better in BLEU and 2.3%
better in TER (abs.) than the best sin-
gle system. In addition, cross-lingual
output from 10 French, German, and
Spanish?English systems was combined
into a consensus translation, which gave
an improvement of 2.0% in BLEU/3.5% in
TER (abs.) over the best single system.
1 Introduction
The RWTH approach to MT system combination
is a refined version of the ROVER approach in
ASR (Fiscus, 1997), with additional steps to cope
with reordering between different hypotheses, and
to use true casing information from the input hy-
potheses. The basic concept of the approach has
been described by Matusov et al (2006). Several
improvements have been added later (Matusov et
al., 2008). This approach includes an enhanced
alignment and reordering framework. In con-
trast to existing approaches (Jayaraman and Lavie,
2005; Rosti et al, 2007), the context of the whole
corpus rather than a single sentence is considered
in this iterative, unsupervised procedure, yielding
a more reliable alignment. Majority voting on the
generated lattice is performed using the prior prob-
abilities for each system as well as other statistical
models such as a special n-gram language model.
2 System Combination Algorithm
In this section we present the details of our system
combination method. Figure 1 gives an overview
of the system combination architecture described
in this section. After preprocessing the MT hy-
potheses, pairwise alignments between the hy-
potheses are calculated. The hypotheses are then
reordered to match the word order of a selected
primary hypothesis. From this, we create a confu-
sion network (CN), which we then rescore using
Figure 1: The system combination architecture.
system prior weights and a language model (LM).
The single best path in this CN then constitutes the
consensus translation.
2.1 Word Alignment
The proposed alignment approach is a statistical
one. It takes advantage of multiple translations for
a whole corpus to compute a consensus translation
for each sentence in this corpus. It also takes ad-
vantage of the fact that the sentences to be aligned
are in the same language.
For each source sentence F in the test corpus,
we select one of its translations En, n=1, . . . ,M,
as the primary hypothesis. Then we align the sec-
ondary hypotheses Em(m = 1, . . . ,M ;n 6= m)
with En to match the word order in En. Since it is
not clear which hypothesis should be primary, i. e.
has the ?best? word order, we let every hypothesis
play the role of the primary translation, and align
all pairs of hypotheses (En, Em); n 6= m.
The word alignment is trained in analogy to
the alignment training procedure in statistical MT.
The difference is that the two sentences that have
to be aligned are in the same language. We use the
IBM Model 1 (Brown et al, 1993) and the Hid-
den Markov Model (HMM, (Vogel et al, 1996))
to estimate the alignment model.
The alignment training corpus is created from a
test corpus1 of effectively M ? (M ? 1) ? N sen-
tences translated by the involved MT engines. The
single-word based lexicon probabilities p(e|e?) are
initialized from normalized lexicon counts col-
lected over the sentence pairs (Em, En) on this
corpus. Since all of the hypotheses are in the same
language, we count co-occurring identical words,
i. e. whether em,j is the same word as en,i for some
i and j. In addition, we add a fraction of a count
for words with identical prefixes.
1A test corpus can be used directly because the align-
ment training is unsupervised and only automatically pro-
duced translations are considered.
51
The model parameters are trained iteratively us-
ing the GIZA++ toolkit (Och and Ney, 2003). The
training is performed in the directions Em ? En
and En ? Em. After each iteration, the updated
lexicon tables from the two directions are interpo-
lated. The final alignments are determined using
a cost matrix C for each sentence pair (Em, En).
Elements of this matrix are the local costs C(j, i)
of aligning a word em,j from Em to a word en,i
from En. Following Matusov et al (2004), we
compute these local costs by interpolating the
negated logarithms of the state occupation proba-
bilities from the ?source-to-target? and ?target-to-
source? training of the HMM model. Two differ-
ent alignments are computed using the cost matrix
C: the alignment a? used for reordering each sec-
ondary translation Em, and the alignment a? used
to build the confusion network.
In addition to the GIZA++ alignments, we have
also conducted preliminary experiments follow-
ing He et al (2008) to exploit character-based
similarity, as well as estimating p(e|e?) :=?
f p(e|f)p(f |e
?) directly from a bilingual lexi-
con. But we were not able to find improvements
over the GIZA++ alignments so far.
2.2 Word Reordering and Confusion
Network Generation
After reordering each secondary hypothesis Em
and the rows of the corresponding alignment cost
matrix according to a?, we determine M?1 mono-
tone one-to-one alignments between En as the pri-
mary translation and Em,m = 1, . . . ,M ;m 6= n.
We then construct the confusion network. In case
of many-to-one connections in a? of words in Em
to a single word from En, we only keep the con-
nection with the lowest alignment costs.
The use of the one-to-one alignment a? implies
that some words in the secondary translation will
not have a correspondence in the primary transla-
tion and vice versa. We consider these words to
have a null alignment with the empty word ?. In
the corresponding confusion network, the empty
word will be transformed to an ?-arc.
M ? 1 monotone one-to-one alignments can
then be transformed into a confusion network. We
follow the approach of Bangalore et al (2001)
with some extensions. Multiple insertions with re-
gard to the primary hypothesis are sub-aligned to
each other, as described by Matusov et al (2008).
Figure 2 gives an example for the alignment.
2.3 Voting in the confusion network
Instead of choosing a fixed sentence to define the
word order for the consensus translation, we gen-
erate confusion networks for all hypotheses as pri-
mary, and unite them into a single lattice. In our
experience, this approach is advantageous in terms
of translation quality, e.g. by 0.7% in BLEU com-
pared to a minimum Bayes risk primary (Rosti et
al., 2007). Weighted majority voting on a single
confusion network is straightforward and analo-
gous to ROVER (Fiscus, 1997). We sum up the
probabilities of the arcs which are labeled with the
same word and have the same start state and the
same end state. To exploit the true casing abilities
of the input MT systems, we sum up the scores of
arcs bearing the same word but in different cases.
Here, we leave the decision about upper or lower
case to the language model.
2.4 Language Models
The lattice representing a union of several confu-
sion networks can then be directly rescored with
an n-gram language model (LM). A transforma-
tion of the lattice is required, since LM history has
to be memorized.
We train a trigram LM on the outputs of the sys-
tems involved in system combination. For LM
training, we took the system hypotheses for the
same test corpus for which the consensus trans-
lations are to be produced. Using this ?adapted?
LM for lattice rescoring thus gives bonus to n-
grams from the original system hypotheses, in
most cases from the original phrases. Presum-
ably, many of these phrases have a correct word or-
der, since they are extracted from the training data.
Previous experimental results show that using this
LM in rescoring together with a word penalty (to
counteract any bias towards short sentences) no-
tably improves translation quality. This even re-
sults in better translations than using a ?classical?
LM trained on a monolingual training corpus. We
attribute this to the fact that most of the systems
we combine are phrase-based systems, which al-
ready include such general LMs. Since we are us-
ing a true-cased LM trained on the hypotheses, we
can exploit true casing information from the in-
put systems by using this LM to disambiguate be-
tween the separate arcs generated for the variants
(see Section 2.3).
After LM rescoring, we add the probabilities of
identical partial paths to improve the estimation
of the score for the best hypothesis. This is done
through determinization of the lattice.
2.5 Extracting Consensus Translations
To generate our consensus translation, we extract
the single-best path within the rescored confusion
network. With our approach, we could also extract
N -best hypotheses. In a subsequent step, these N -
best lists could be rescored with additional statis-
tical models (Matusov et al, 2008). But as we did
not have the resources in the WMT 2009 evalua-
tion, this step was dropped for our submission.
3 Tuning system weights
System weights, LM factor, and word penalty
need to be tuned to produce good consensus trans-
lations. We optimize these parameters using the
52
0.25 would your like coffee or tea
system 0.35 have you tea or Coffee
hypotheses 0.10 would like your coffee or
0.30 I have some coffee tea would you like
alignment have|would you|your $|like Coffee|coffee or|or tea|tea
and would|would your|your like|like coffee|coffee or|or $|tea
reordering I|$ would|would you|your like|like have|$ some|$ coffee|coffee $|or tea|tea
$ would your like $ $ coffee or tea
confusion $ have you $ $ $ Coffee or tea
network $ would your like $ $ coffee or $
I would you like have some coffee $ tea
$ would you $ $ $ coffee or tea
voting 0.7 0.65 0.65 0.35 0.7 0.7 0.5 0.7 0.9
(normalized) I have your like have some Coffee $ $
0.3 0.35 0.35 0.65 0.3 0.3 0.5 0.3 0.1
consensus translation would you like coffee or tea
Figure 2: Example of creating a confusion network from monotone one-to-one word alignments (denoted
with symbol |). The words of the primary hypothesis are printed in bold. The symbol $ denotes a null
alignment or an ?-arc in the corresponding part of the confusion network.
Table 1: Systems combined for the WMT 2009
task. Systems written in oblique were also used in
the Cross Lingual task (rbmt3 for FR?EN).
DE?EN google, liu, rbmt3, rwth, stutt-
gart, systran, uedin, uka, umd
ES?EN google, nict, rbmt4, rwth,
talp-upc, uedin
FR?EN dcu, google, jhu, limsi, lium-
systran, rbmt4, rwth, uedin, uka
publicly available CONDOR optimization toolkit
(Berghen and Bersini, 2005). For the WMT
2009 Workshop, we selected a linear combina-
tion of BLEU (Papineni et al, 2002) and TER
(Snover et al, 2006) as optimization criterion,
?? := argmax? {(2 ? BLEU)? TER}, based on
previous experience (Mauser et al, 2008). We
used the whole dev set as a tuning set. For more
stable results, we used the case-insensitive variants
for both measures, despite the explicit use of case
information in our approach.
4 Experimental results
Due to the large number of submissions (71 in
total for the language pairs DE?EN, ES?EN,
FR?EN), we had to select a reasonable number
of systems to be able to tune the parameters in
a reliable way. Based on previous experience,
we manually selected the systems with the best
BLEU/TER score, and tried different variations of
this selection, e.g. by removing systems which
had low weights after optimization, or by adding
promising systems, like rule based systems.
Table 1 lists the systems which made it into
our final submission. In our experience, if a large
number of systems is available, using n-best trans-
lations does not give better results than using sin-
gle best translations, but raises optimization time
significantly. Consequently, we only used single
best translations from all systems.
The results also confirm another observation:
Even though rule-based systems by itself may
have significantly lower automatic evaluation
scores (e.g. by 2% or more in BLEU on DE?EN),
they are often very important in system combina-
tion, and can improve the consensus translation
e.g. by 0.5% in BLEU.
Having submitted our translations to the WMT
workshop, we calculated scores on the WMT 2009
test set, to verify the results on the tuning data.
Both the results on the tuning set and on the test
set can be found in the following tables.
4.1 The Google Problem
One particular thing we noticed is that in the lan-
guage pairs of FR?EN and ES?EN, the trans-
lations from one provided single system (Google)
were much better in terms of BLEU and TER than
those of all other systems ? in the former case
by more than 4% in BLEU. In our experience,
our system combination approach requires at least
three ?comparably good? systems to be able to
achieve significant improvements. This was con-
firmed in the WMT 2009 task as well: Neither in
FR?EN nor in ES?EN we were able to achieve
an improvement over the Google system. For this
reason, we did not submit consensus translations
for these two language pairs. On the other hand,
we would have achieved significant improvements
over all (remaining) systems leaving out Google.
4.2 German?English (DE?EN)
Table 2 lists the scores on the tuning and test set
for the DE?EN task. We can see that the best
systems are rather close to each other in terms
of BLEU. Also, the rule-based translation system
(RBMT), here SYSTRAN, scores rather well. As
a consequence, we find a large improvement using
system combination: 2.9%/2.7% abs. on the tun-
ing set, and still 2.1%/2.3% on test, which means
that system combination generalizes well here.
4.3 Spanish?English (ES?EN),
French?English (FR?EN)
In Table 3, we see that on the ES?EN and
FR?EN tasks, a single system ? Google ? scores
significantly better on the TUNE set than any other
53
Table 2: German?English task: case-insensitive
scores. Best single system was Google, second
best UKA, best RBMT Systran. SC stands for sys-
tem combination output.
TUNE TEST
German?English BLEU TER BLEU TER
Best single 23.2 59.5 21.3 61.3
Second best single 23.0 58.8 21.0 61.7
Best RBMT 21.3 61.3 18.9 63.7
SC (9 systems) 26.1 56.8 23.4 59.0
w/o RBMT 24.5 57.3 22.5 59.2
w/o Google 24.9 57.4 23.0 59.1
Table 3: Spanish?English and French?English
task: scores on the tuning set after system combi-
nation weight tuning (case-insensitive). Best sin-
gle system was Google, second best was Uedin
(Spanish) and UKA (French). No results on TEST
were generated.
ES?EN FR?EN
Spanish?English BLEU TER BLEU TER
Best single 29.5 53.6 32.2 50.1
Second best single 26.9 56.1 28.0 54.6
SC (6/9 systems) 28.7 53.6 30.7 52.5
w/o Google 27.5 55.6 30.0 52.8
system, namely by 2.6%/4.2% resp. in BLEU. As
a result, a combination of these systems scores
better than any other system, even when leaving
out the Google system. But it gives worse scores
than the single best system. This is explainable,
because system combination is trying to find a
consensus translation. For example, in one case,
the majority of the systems leave the French term
?wagon-lit? untranslated; spurious translations in-
clude ?baggage car?, ?sleeping car?, and ?alive?.
As a result, the consensus translation also contains
?wagon-lit?, not the correct translation ?sleeper?
which only the Google system provides. Even tun-
ing all other system weights to zero would not re-
sult in pure Google translations, as these weights
neither affect the LM nor the selection of the pri-
mary hypothesis in our approach.
4.4 Cross-Lingual?English (XX?EN)
Finally, we have conducted experiments on cross-
lingual system combination, namely combining
the output from DE?EN, ES?EN, and FR?EN
systems to a single English consensus transla-
tion. Some interesting results can be found in
Table 4. We see that this consensus translation
scores 2.0%/3.5% better than the best single sys-
tem, and 4.4%/5.6% better than the second best
single system. While this is only 0.8%/2.5% bet-
ter than the combination of only the three Google
systems, the combination of the non-Google sys-
Table 4: Cross-lingual task: combination
of German?English, Spanish?English, and
French?English. Case-insensitive scores. Best
single system was Google for all language pairs.
Cross-lingual TUNE TEST
? English BLEU TER BLEU TER
Best single German 23.2 59.5 21.3 61.3
Best single Spanish 29.5 53.6 28.7 53.8
Best single French 32.2 50.1 31.1 51.7
SC (10 systems) 35.5 46.4 33.1 48.2
w/o RBMT 35.1 46.5 32.7 48.3
w/o Google 32.3 48.8 29.9 50.5
3 Google systems 34.2 48.0 32.3 49.2
w/o German 34.0 49.3 31.5 50.9
w/o Spanish 33.4 49.8 31.0 51.9
w/o French 30.5 51.4 28.6 52.3
tems leads to translations that could compete with
the FR?EN Google system. Again, we see that
RBMT systems lead to a small improvement of
0.4% in BLEU, although their scores are signif-
icantly worse than those of the competing SMT
systems.
Regarding languages, we see that despite the
large differences in the quality of the systems (10
points between DE?EN and FR?EN), all lan-
guages seem to provide significant information to
the consensus translation: While FR?EN cer-
tainly has the largest influence (?4.5% in BLEU
when left out), even DE?EN ?contributes? 1.6
BLEU points to the final submission.
5 Conclusions
We have shown that our system combination sys-
tem can lead to significant improvements over
single best MT output where a significant num-
ber of comparably good translations is available
on a single language pair. For cross-lingual sys-
tem combination, we observe even larger improve-
ments, even if the quality in terms of BLEU or
TER between the systems of different language
pairs varies significantly. While the input of high-
quality SMT systems has the largest weight for the
consensus translation quality, we find that RBMT
systems can give important additional information
leading to better translations.
Acknowledgments
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. This work was
partly supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract
No. HR0011-06-C-0023.
54
References
S. Bangalore, G. Bordel, and G. Riccardi. 2001.
Computing consensus translation from multiple ma-
chine translation systems. In IEEE Automatic
Speech Recognition and Understanding Workshop,
Madonna di Campiglio, Italy, December.
F. V. Berghen and H. Bersini. 2005. CONDOR,
a new parallel, constrained extension of Powell?s
UOBYQA algorithm: Experimental results and
comparison with the DFO algorithm. Journal of
Computational and Applied Mathematics, 181:157?
175.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
J. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recognizer output voting er-
ror reduction (ROVER). In IEEE Workshop on Au-
tomatic Speech Recognition and Understanding.
X. He, M. Yang, J. Gao, P. Nguyen, and R. Moore.
2008. Indirect-HMM-based hypothesis alignment
for combining outputs from machine translation sys-
tems. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 98?107, Honolulu, Hawaii, October.
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching.
In Proc. of the 10th Annual Conf. of the European
Association for Machine Translation (EAMT), pages
143?152, Budapest, Hungary, May.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
word alignments for statistical machine translation.
In COLING ?04: The 20th Int. Conf. on Computa-
tional Linguistics, pages 219?225, Geneva, Switzer-
land, August.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40, Trento, Italy, April.
E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y. S. Lee,
J. B. Marino, M. Paulik, S. Roukos, H. Schwenk,
and H. Ney. 2008. System combination for machine
translation of spoken and written language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237, September.
A. Mauser, S. Hasan, and H. Ney. 2008. Automatic
evaluation measures for statistical machine transla-
tion system optimization. In International Confer-
ence on Language Resources and Evaluation, Mar-
rakech, Morocco, May.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51, March.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318, Philadelphia, PA, July.
A. V. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 312?319, Prague, Czech Re-
public, June.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Error
Rate with Targeted Human Annotation. In Proc. of
the 7th Conf. of the Association for Machine Trans-
lation in the Americas (AMTA), pages 223?231,
Boston, MA, August.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational
Linguistics, pages 836?841, Copenhagen, Denmark,
August.
55
