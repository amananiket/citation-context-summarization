JEP-TALN-RECITAL 2012, Atelier DEFT 2012: D?fi Fouille de Textes, pages 1?13,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
Indexation libre et contr?l?e d?articles scientifiques
Pr?sentation et r?sultats du d?fi fouille de textes DEFT2012
Patrick Paroubek1 Pierre Zweigenbaum1 Dominic Forest2 Cyril Grouin1
(1) LIMSI?CNRS, Rue John von Neumann, 91403 Orsay, France
(2) EBSI, Universit? de Montr?al, C.P. 6128, succursale Centre-ville, Montr?al H3C 3J7, Canada
{pap,pz,grouin}@limsi.fr, dominic.forest@umontreal.ca
R?SUM?
Dans cet article, nous pr?sentons la campagne 2012 du d?fi fouille de texte (DEFT). Cette ?dition
traite de l?indexation automatique par des mots-cl?s d?articles scientifiques au travers de deux
pistes. La premi?re fournit aux participants la terminologie des mots-cl?s employ?s dans les
documents ? indexer tandis que la seconde ne fournit pas cette terminologie, rendant la t?che
plus complexe. Le corpus se compose d?articles scientifiques parus dans des revues de sciences
humaines, index?s par leurs auteurs. Cette indexation sert de r?f?rence pour l??valuation. Les
r?sultats ont ?t? ?valu?s en termes de micro-mesures sur les rappel, pr?cision et F-mesure calcul?s
apr?s lemmatisation de chaque mot-cl?. Dans la piste fournissant la terminologie des mots-cl?s
employ?s, la F-mesure moyenne est de 0,3575, la m?diane de 0,3321 et l??cart-type de 0,2985 ;
sur la seconde piste, en l?absence de terminologie, la F-mesure moyenne est de 0,2055, la m?diane
de 0,1901 et l??cart-type de 0,1516.
ABSTRACT
Controlled and free indexing of scientific papers
Presentation and results of the DEFT2012 text-mining challenge
In this paper, we present the 2012 edition of the DEFT text-mining challenge. This edition
addresses the automatic, keyword-based indexing of scientific papers through two tracks. The
first gives to the participants the terminology of keywords used to index the documents, while the
second does not provide this terminology. The corpus is composed of scientific papers published
in humanities journals, indexed by their authors. This indexing is used as a reference for the
evaluation. The results have been evaluated in terms of micro-measures on the recall, precision
and F-measure computed after keyword lemmatization. In the track giving the terminology of
used keywords, the mean F-measure is 0.3575, the median is 0.3321 and the standard deviation
is 0.2985 ; in the second track, the mean F-measure is 0.2055, the median is 0.1901 and the
standard deviation is 0.1516.
MOTS-CL?S : Campagne d??valuation, fouille de textes, indexation libre, indexation contr?l?e,
mots-cl?s, th?saurus.
KEYWORDS: Evaluation campaign, Text-Mining, Free Indexing, Controlled Indexing, Keywords,
Thesaurus.
1
1 Introduction
La r?daction d?un article scientifique s?accompagne g?n?ralement de m?ta-donn?es que l?auteur
de l?article doit tr?s souvent renseigner : titre, auteurs, affiliation des auteurs, et g?n?ralement
un r?sum? pr?sentant bri?vement le contenu de l?article et un ensemble de mots-cl?s d?crivant
les th?mes de l?article. Ces mots-cl?s visent ? aider la recherche des articles dans les bases de
donn?es bibliographiques.
La campagne DEFT 2012 s?int?resse ? la d?termination des mots-cl?s appropri?s pour un article.
Cela demande d?une part de d?terminer les th?mes principaux de l?article et d?autre part de
choisir des termes pour les nommer.
Certaines disciplines ont constitu? un th?saurus qui prescrit les termes ? employer pour cela. C?est
le cas par exemple des sciences de la vie avec le th?saurus MeSH (Medical Subject Headings)1
avec ses 26 142 descripteurs (version 2011), ou encore de l?informatique avec la classification
hi?rarchique de l?ACM2 et ses 368 classes. Des th?saurus ? vocation plus large ont aussi ?t? cr??s,
telle que la classification de la Biblioth?que du Congr?s des ?tats-Unis3. Ces r?f?rentiels visent ?
contr?ler l?indexation des documents et ? aider ainsi leur recherche.
? l?inverse, dans certaines revues ou conf?rences, aucun r?f?rentiel n?est impos? pour le choix des
mots-cl?s indexant un article. Dans cette indexation libre, g?n?ralement r?alis?e par les auteurs
eux-m?mes, le choix des mots-cl?s devient plus subjectif, chacun ayant une vision diff?rente des
termes ? utiliser pour caract?riser l?article.
C?est donc dans le cadre de cette probl?matique d?indexation libre ou contr?l?e des articles
scientifiques que nous avons inscrit cette nouvelle ?dition du d?fi fouille de texte (DEFT).
1.1 ?tat de l?art
Les travaux dans le domaine de l?indexation des documents, qu?elle soit automatique (Salton
et al, 1975) ou non (Lancaster, 2003), ne sont pas r?cents. Parmi les m?thodes g?n?ralement
appliqu?es pour la cr?ation de classes de termes, celles-ci comprennent traditionnellement deux
?tapes : l?identification de termes dans un premier temps, puis la s?lection des meilleurs candidats.
L??tude des cooccurrences de termes (avec pr?-traitements tels que ?tiquetage des parties du
discours et lemmatisation) et l?utilisation de connaissances du domaine permet d?obtenir des
r?sultats exploitables (Toussaint et al, 1998). Ces techniques reprennent celles en vigueur
en recherche d?information. Appliqu?es aux syntagmes nominaux, elles fournissent une base
qui ne peut cependant suffire pour l?indexation (Sidhom, 2002). Des exp?riences d?indexation
contr?l?e automatique (au moyen de l?algorithme Okapi) et manuelle sur un corpus en fran?ais
ont d?montr? l?int?r?t de combiner ces deux approches pour am?liorer les r?sultats (Savoy,
2005). Des approches plus r?centes en mati?re d?indexation automatiques prennent en compte la
coccurrence des termes associ?es ? la structure des documents (Pompidor et al, 2008). D?autres
m?thodes ont aussi ?t? exploit?es pour assister l?indexion automatique des documents, parmi
lesquelles on retrouve la s?mantique latente (Deerwester et al, 1990).
1MeSH (National Library of Medicine) : http://www.nlm.nih.gov/mesh/MBrowser.html.2Association for Computing Machinery, Computing Classification System : http://dl.acm.org/ccs.cfm?part=
author&coll=portal&dl=GUIDE.3Library of Congress Classification : http://www.loc.gov/catdir/cpso/lcc.html.
2
1.2 D?roulement
Un appel ? participation a ?t? lanc? le 5 f?vrier 2012 sur les principales listes de diffusion dans les
domaines des sciences de l?information (ASIS-L), de la fouille de textes (TextAnalytics, KDnuggets),
des humanit?s num?riques (DH, Humanist), du Traitement Automatique des Langues et de la
linguistique de corpus (Corpora, LN, etc.). Dix-huit ?quipes se sont inscrites, pour certaines alors
m?me que la phase de test avait d?j? commenc?, tandis que dix ?quipes ont poursuivi leurs efforts
jusqu?? la p?riode de tests. Ces ?quipes sont les suivantes, des inscriptions les plus anciennes (6
f?vrier) aux plus r?centes (11 avril) :
? FBK, Fondazione Bruno Kessler, Trento, Italie : Sara Tonelli, Elena Cabrio, Emanuele Pianta.
? LIM&BIO, Laboratoire d?Informatique M?dicale & bioinformatique, Universit? Paris 13 Nord,
Bobigny (93) : Thierry Hamon.
? URPAH, Unit? de Recherche en Programmation Algorithmique et Heuristique, Facult? des Sciences
de Tunis, Tunisie : Amine Amri, Mbarek Maroua, Chedi Bechikh, Chiraz Latiri, Hatem Haddad.
? GREYC, Groupe de Recherche en Informatique, Image, Automatique et Instrumentalisation de
Caen, Universit? de Caen Basse-Normandie, Caen (14) : Ga?lle Doualan, Mathieu Boucher,
Romain Brixtel, Ga?l Lejeune et Ga?l Dias.
? IRISA, Institut de Recherche en Informatique et Syst?mes Al?atoires, Universit? Rennes 1, Rennes
(35) : Vincent Claveau et Christian Raymond.
? LINA, Laboratoire d?Informatique de Nantes Atlantique, Universit? de Nantes/?cole des Mines
de Nantes, Nantes (44) : Florian Boudin, Amir Hazem, Nicolas Hernandez et Prajol Shrestha.
? LIMSI, Laboratoire d?Informatique pour la M?canique et les Sciences de l?Ing?nieur, Orsay (91) :
Alexander Pak.
? LUTIN, Laboratoire des Usages en Technologies d?Information Num?rique, Universit? Paris
8/UPMC/UTC/Universcience, Paris (75) : Adil El Ghali, Daniel Hromada et Kaoutar El Ghali.
? LORIA, Laboratoire Lorrain de Recherche en Informatique et ses Applications, Nancy (54) : Alain
Lelu et Martine Cadot.
? PRISM, laboratoire Parall?lisme, R?seaux, Syst?mes et Mod?lisation, Universit? Versailles?Saint-
Quentin-en-Yvelines (78) et LaISC Laboratoire d?Informatique et des Syst?mes Complexes, EPHE,
Paris (75) : Murat Ahat, Coralie Petermann, Yann Vigile Hoareau, Soufian Ben Amor et Marc
Bui.
Les corpus d?entra?nement ont ?t? diffus?s aux participants inscrits ayant retourn?s l?accord de
restriction d?usage des corpus sign?s ? partir du 6 f?vrier 2012. Chaque ?quipe a choisi une
fen?tre de trois jours durant la semaine du 9 au 15 avril 2012 pour appliquer ses m?thodes sur le
corpus de test. Les r?sultats ont ?t? communiqu?s aux participants le 17 avril. La version finale
des articles pr?sentant les m?thodes utilis?es ?tait attendue pour le 1er mai, pour un atelier de
cl?ture le 8 juin 2012 pendant la conf?rence jointe JEP/TALN ? Grenoble.
Pour la premi?re fois dans l?histoire de DEFT, nous avons voulu mettre en place une interface
de soumission des fichiers de r?sultats qui permettent de lancer une ?valuation. Cette interface,
d?riv?e d?une version utilis?e dans un projet d?annotation de corpus, a n?cessit? de nombreuses
adaptations et n?a pu ?tre utilis?e par les participants que trop tardivement (? partir du 6 avril, soit
une semaine avant le d?marrage de la phase de test) avec une fonction d??valuation r?ellement
op?rationnelle qu?en fin de p?riode de test. En cons?quence, les participants au d?fi n?ont pas pu
acc?der ? l?outil d??valuation de leurs r?sultats pendant la p?riode d?entra?nement, ce qui, nous
en convenons, ne facilite pas le d?veloppement de m?thodes ni l?appr?ciation des ?volutions
offertes par les tentatives de modifications de ces m?thodes durant cette p?riode.
3
2 Pr?sentation
Dans la continuit? de l??dition 2011 du d?fi (voir DEFT2011), nous proposons de travailler de
nouveau sur un corpus d?articles scientifiques parus dans le domaine des Sciences Humaines
et Sociales. Alors que l??dition 2011 visait l?appariement de r?sum? avec l?article scientifique
correspondant, nous proposons cette ann?e d?identifier les mots-cl?s, tels qu?ils ont ?t? choisis
par les auteurs, pour indexer ces m?mes types d?articles. Les m?thodes qui seront utilis?es pour
identifier les mots-cl?s devraient permettre de mettre en ?vidence les ?l?ments saillants qui
permettent d?indexer le contenu d?un article au moyen de mots-cl?s.
2.1 Pistes
Deux pistes sont propos?es autour de l?identification de mots-cl?s (chaque piste dispose de ses
propres corpus d?apprentissage et de test) :
? la premi?re piste renvoie ? l?indexation contr?l?e des articles scientifiques et fournit la termi-
nologie des mots-cl?s utilis?s dans le corpus de cette piste (avec cependant une terminologie
distincte pour chaque sous-corpus : une premi?re pour l?apprentissage, une seconde pour le
test), cette terminologie constituant une aide ? la d?couverte des mots-cl?s ;
? la seconde piste renvoie ? une indexation libre et ne fournit donc pas cette terminologie de
r?f?rence ; les participants doivent identifier par eux-m?mes, dans le contenu du r?sum? et du
corps de l?article, quels sont les mots-cl?s qui ont pu ?tre choisis par l?auteur de l?article.
Sur chacune des deux pistes, le nombre de mots-cl?s indexant chaque document dans la r?f?rence
est renseign?, tant dans le corpus d?apprentissage que dans le corpus de test. Les participants
peuvent ainsi fournir exactement le nombre de mots-cl?s attendus.
Le travail d?indexation, qu?il s?effectue dans un cadre contr?l? ou non, reste complexe (Moens,
2000). Dans le cadre d?une indexation contr?l?e, le choix de mots-cl?s parmi ceux propos?s dans
une terminologie reste difficile, l?indexeur, qu?il soit humain ou automatique, doit choisir parmi
les termes propos?s et uniquement parmi ceux-ci, les meilleurs candidats. Le travail consiste donc
? identifier, parmi les termes propos?s, quels sont ceux qui se rapprochent le plus de ceux que
l?on aurait naturellement eu tendance ? choisir. Dans le cadre d?une indexation libre, la premi?re
difficult? consiste ? d?terminer quels sont les meilleurs candidats ? l?indexation, g?n?ralement
en usant de m?thodes statistiques ?ventuellement compl?t?es par d?autres approches. Dans le
cadre de ce d?fi, l??valuation des termes qui auront ?t? automatiquement choisis constitue une
deuxi?me difficult? puisque la r?f?rence est constitu?e des mots-cl?s choisis par les auteurs des
articles, ce choix ?tant purement subjectif mais consid?r? comme le meilleur pour cette campagne
d??valuation. Les r?sultats des participants sont donc ?valu?s en comparaison d?une r?f?rence qui
reste hautement perfectible.
Les participants peuvent participer, ? leur convenance, aux pistes qu?ils souhaitent (seulement
l?une ou les deux). Chaque participant est autoris? ? soumettre jusqu?? trois fichiers de r?sultats
par piste (soit un maximum de six ex?cutions pour une ?quipe participant aux deux t?ches),
permettant de tester officiellement trois syst?mes ou trois configurations diff?rentes d?un m?me
syst?me.
Les participants peuvent utiliser n?importe quelle ressource externe sauf celles provenant du site
Erudit.org d?o? proviennent les corpus.
4
2.2 Corpus
Le corpus se compose d?articles scientifiques provenant du portail Erudit.org parus entre 2003 et
2008 dans quatre revues de Sciences Humaines et Sociales : Anthropologie et Soci?t?, M?ta, Revue
des Sciences de l??ducation et Traduction, terminologie, r?daction. Ces revues ont ?t? s?lectionn?es
car une majorit? d?articles qui y ont paru sont accompagn?s de mots-cl?s, choisis par les auteurs,
indexant le contenu des articles. Ces mots-cl?s constituent la r?f?rence de cette ?dition, utilis?e
par les participants lors de la phase d?apprentissage et par les organisateurs pour ?valuer les
r?sultats lors de la phase de tests.
Du corpus initial de quatre revues, nous avons donc extrait 468 articles index?s par des mots-cl?s.
Ces articles ont ?t? r?partis ?quitablement entre corpus des deux pistes, soit 234 articles par
piste. Pour chaque piste, nous avons ensuite op?r? une r?partition entre corpus d?apprentissage
et corpus de test selon le ratio 60/40% habituel, en nous assurant que ce ratio s?applique sur
chaque revue (soit 60% des articles de chaque revue dans l?apprentissage et les 40% restants de
chaque revue dans le test). Nous donnons ci-apr?s (Figure 1) un exemple de document tel qu?il
apparait dans le corpus d?apprentissage.
<doc id=" 0360 ">
<motsc les>
<nombre>5</nombre>
<mots>dimension ; concept ; c a r a c t ? r e ; s p a t i o l o g i e ; o rgan i s a t i on des connaissances</mots>
</motsc les>
<a r t i c l e>
<resume>
<p>? p a r t i r de l ? ana lyse de p l u s i e u r s termes se rappor tant au domaine de l a s p a t i o l o g i e , dansdes langues a u s s i d i f f ? r e n t e s que l ? a n g l a i s e t l e f r a n ? a i s d ? une par t e t l ? arabe d ? autrepart , nous nous proposons de d?montrer l ? importance de l a not ion de p l u r i d i m e n s i o n n a l i t ?du concept d a n s l ? o rgan i sa t i on des connaissances e t l a c l a s s i f i c a t i o n des o b j e t s du monde .Ce f a i s a n t , nous about i rons a u s s i ? l a conc lus ion que l a s t r u c t u r a t i o n d ? un domaine des p ? c i a l i t ? , l ? ? l abo ra t i on de son arborescence e t su r tou t l a formulat ion d ? une d ? f i n i t i o nd?pendent pr inc ipa lement des c a r a c t ? r e s p r i s en compte dans l ? appr?hension des concepts ,donc n?cessa i rement de l a ? d imens ion ? du concept .</p>
</resume>
<corps>
<p>Nous savons que l e concept e s t l ? un i t? de base de toute analyse terminolog ique . Que c e l l e?c is o i t synchronique ou diachronique , por tant sur l e terme ou sur l a d ? f i n i t i o n , i l f au ttou jour s r even i r au concept , ? sa d e s c r i p t i o n au se in du syst?me de concepts qu ? i lc o n s t i t u e avec l e s au t re s concepts appartenant au m?me domaine.</p>
<p>Le concept e s t une ? u n i t ? de connaissance cr??e par une combinaison unique de c a r a c t ? r e s ?( ISO 1087?1 2000 : 2) . Cet te d ? f i n i t i o n que donne l a norme ISO 1087?1 2000 du concept metsu r tou t l ? accent sur l a d?composit ion du concept en ca rac t ? re s , une d?composit ion quipermet une mei l l eure compr?hension du concept e t donc une mei l l eure o rgan i sa t i on dusyst?me de concepts auquel i l appa r t i en t .</p>. . .
</ corps>
</ a r t i c l e>
</doc>
FIG. 1 ? Extrait du corpus d?apprentissage avec m?ta-donn?es associ?es
Chaque document int?gre les ?l?ments suivants :
? Des m?ta-donn?es : la liste des mots-cl?s indexant le contenu de l?article (chaque mot cl?
est s?par? du suivant par un point-virgule, information uniquement fournie dans les corpus
d?apprentissage), mots-cl?s qu?il faudra identifier pour la phase de test (ligne 4) et le nombre de
mots-cl?s indexant le contenu de l?article (information fournie dans les corpus d?apprentissage
5
et de test, ligne 3) ;
? L?article scientifique : le r?sum? de l?article (ligne 8) et le corps de l?article au complet (? partir
de la ligne 11).
2.3 Terminologie
Sur la premi?re piste, la terminologie des mots-cl?s employ?s dans le corpus est fournie (Figure 2).
La terminologie du corpus d?apprentissage a ?t? constitu?e en relevant tous les mots-cl?s des
documents de ce corpus, class?s par ordre alphab?tique. La m?me proc?dure a ?t? suivie pour
constuire la terminologie du corpus de test. Puisque les mots-cl?s ont ?t? choisis par les auteurs
eux-m?mes, on constate que les mots-cl?s sont de diff?rents types : des mots simples (ethnologie),
des mots compos?s (Am?rique latine), des expressions complexes (Am?rindien du Nord-Est) et des
combinaisons d?informations pr?sentes dans l?article rassembl?es sous un m?me ? mot-cl? ? (1982,
droit constitutionnel canadien). Si la question de la difficult? de rattacher chaque mot-cl? de cette
terminologie aux documents du corpus se pose pour la premi?re piste, les exemples pr?sent?s
ici t?moignent ?galement de la difficult? ? venir pour identifier les mots-cl?s sur la seconde
piste, en l?absence de toute terminologie, compte-tenu de la grande variabilit? des modalit?s de
constitution des mots-cl?s.
1867 , C o n s t i t u t i o n Act1982 , d r o i t c o n s t i t u t i o n n e l canadienAb?l?sAf r iqueAf r ique de l ? Es tAgrawalA lg? r i ensAmazonieAmbedkarAm?rindien du Nord?EstAm?rique l a t i n eAncien R?gimeAubr?e. . .e t h n i c i t ?ethno?f i c t i o nethnographieethnographie m u l t i s i t e se t h n o l i n g u i s t i q u ee thno log ieexogamie. . .
FIG. 2 ? Extrait de la terminologie du corpus d?apprentissage
6
3 ?valuation
Les mesures qui ont ?t? retenues pour l??valuation 2012 sont les mesures de pr?cision, rappel, et
F-mesure (Manning et Sch?tze, 1999), calcul?es avec une micro-moyenne (Nakache et M?tais,
2005). Ce sont ces mesures qui ont ?t? utilis?es pour la piste 5 de la campagne SemEval-2010 :
Automatic Keyphrase Extraction from Scientific Articles (Kim et al, 2010).
Notons D l?ensemble des idenfiants de documents, K l?ensemble de tous les mots-cl?s utilis?s
par le syst?me, W l?ensemble des mots-cl?s utilis?s dans la base documentaire, les donn?es
hypoth?se H (formule 1), c?est-?-dire l?ensemble des paires associant un identifiant de document
? un mot cl? fourni par le syst?me participant et R les donn?es r?f?rence (formule 2), c?est-?-
dire l?ensemble des paires associant un identifiant de document ? un mot cl? issu de la base
documentaire. Naturellement, pour un m?me identifiant de document, il peut exister plusieurs
paires, aussi bien dans H que dans R, mais nous n?aurons pas de paire doublon au sein de l?un de
ces ensembles, car les mots-cl?s seront alors diff?rents. En effet, il n?y a aucun int?r?t ? annoter
un document plusieurs fois avec le m?me mot-cl?
H = (d, Lem(Norm(w))d ? D, w ?W, ((d, w1) ? H)? ((d, w2) ? H) ? w1 6= w2 (1)
R = (a, Lem(Norm(k))a ? D, k ? K , ((a, k1) ? R)? ((a, k2) ? R) ? k1 6= k2 (2)
Norm() est une fonction de normalisation de la typographie des mots-cl? (normalisation de la
casse) et Lem() est une fonction de lemmatisation des mots-cl?.
L?ensemble des mots-cl? correctement associ?s ? un document par le syst?me correspond au taux
de vrais positifs (TP, formule 3), l?ensemble des mots-cl? incorrectement associ?s ? un document
par le syst?me correspond au taux de faux positifs (FP, formule 4) et l?ensemble des mots-cl? non
trouv?s par le syst?me correspond au taux de faux n?gatifs (FN, formule 5).
TP = H ? R (3) FP = H(H ? R) (4) FN =
R
(H ? R) (5)
La pr?cision, le rappel et la F-mesure calcul?s en micro-moyenne correspondent aux formules 6 :
Pr?cision = |H ? R||H| Rappel =
|H ? R|
|R| F-mesure =
(2? p? r)
(p + r) (6)
Notons que nous utilisons l??galit? stricte sur les mots-cl?s sans avoir recourt ? une distance
s?mantique qui permettrait par exemple, de s?apercevoir que recherche d?information est plus
proche de fouille de donn?es que d?algorithmique afin de ne pas biaiser l??valuation par rapport
? une ontologie particuli?re. Nous avons ?galement d?cid? de ne pas prendre en compte les
recouvrements partiels de termes comme ayant une certaine validit? pour ?viter de recompenser
un syst?me qui retournerait fouilles arch?logiques alors que la bonne r?ponse est fouille de donn?es.
Bien entendu, ce choix a pour r?sultat que la fourniture de l?hyponyme d?un terme au lieu du
7
terme sera consid?r?e comme tout aussi fausse que la fourniture de n?importe quel autre terme.
La production de mesures de performance compl?mentaires peut ?tre envisag?e ? titre indicatif.
Pour les r?sultats officiels de la campagne, seule la performance en F-mesure en micro-moyenne
sera prise en compte.
4 Tests humains
Nous avons effectu? des tests humains sur les deux pistes aupr?s des ?tudiants du parcours
? Ing?nierie Multilingue ? du M2 Professionnel de l?INaLCO (formation sciences du langage avec
une dominante traitement automatique des langues, ?tudiants d?origine ?trang?re avec pour
certains une ma?trise moyenne de la langue fran?aise). Pour chaque piste, un sous-corpus compos?
de quatre fichiers chacun a ?t? produit (un fichier issu de chacune des quatre revues utilis?es
dans le corpus global). Nous remercions chacun des ?tudiants pour le travail accompli.
4.1 Premi?re piste, avec terminologie
Sur la premi?re piste, puisque la terminologie des mots-cl?s employ?s dans les quatre articles
composant le sous-corpus est disponible, une simple projection des mots-cl?s sur ce corpus au
moyen d?une commande informatique4 permet d?identifier dans quel fichier appara?t 14 des mots-
cl?s de la terminologie. Sur ces 14 mots-cl?s, un seul est attribu? ? deux fichiers ; l?attribution de
ce mot-cl? au fichier qui compte le plus d?occurrences de ce terme permet une indexation correcte.
Pour les 4 mots-cl?s restants qui n?ont pu faire l?objet d?une projection (g?n?ralement des mots-cl?s
compos?s : traduction fran?aise et alemande, ?ducation multiculturelle, ?ducation intellectuelle),
une recherche d?un des termes composant le mot-cl? permet d?identifier correctement l?article
auquel il doit ?tre associ?. Cette technique, sur un sous-corpus limit?, permet d?identifier 100%
des indexations (F-mesure de 1,000).
4.2 Seconde piste, sans terminologie
Sur la seconde piste, aucune terminologie des mots-cl?s n?ayant ?t? fournie, la t?che a ?t? jug?e
plus complexe par les ?tudiants comme en t?moignent les r?sultats obtenus (voir Tableau 1,
F-mesure moyenne de 0,216 et m?diane de 0,208). Afin de dresser grossi?rement le contenu
AM BM IP LM LT NS SK
Pr?cision 0.250 0.200 0.167 0.118 0.292 0.292 0.208
Rappel 0.250 0.208 0.167 0.083 0.292 0.292 0.208
F-mesure 0.250 0.204 0.167 0.098 0.292 0.292 0.208
TAB. 1 ? ?valuation des tests humains sur la seconde piste
de chaque article, un script qui extrait les tokens et les trigrammes de tokens utilis?s dans le
4
grep -of termino_appr.txt piste1/testSans/* | sort | uniq
8
document class?s par fr?quence d?utilisation d?croissante a ?t? mis ? contribution. ? charge pour
les ?tudiants de s?inspirer de ces listes et de les confronter au contenu r?el de l?article pour cr?er
des mots-cl?s potentiels.
En conclusion, la seconde piste (sans terminologie) a ?t? jug?e difficile. Les mots-cl?s employ?s
ne se retrouvent pas forc?ment ? l?identique (traduction fran?aise et alemande) mais peuvent
correspondre ? une concat?nation de plusieurs expressions (traduction allemande et traduction
fran?aise). Il apparait par ailleurs que les mots-cl?s employ?s peuvent ne pas apparaitre dans le
texte mais r?sulter d?une inf?rence (Colombie Britannique alors que le texte ne mentionne pas
le nom de cette province mais celui d?une ville de cette province). Enfin, la redondance d?une
th?matique d?un m?me champ s?mantique exprim?e au moyen de deux mots-cl?s (interpr?te
et interpr?tation) a ?t? jug?e complexe parce que contre-intuitif (un annotateur humain ayant
tendance ? choisir soit l?un, soit l?autre).
5 M?thodes des participants
La plupart des participants a consid?r? la premi?re piste (avec terminologie) comme une t?che
de recherche d?information dans laquelle les mots-cl?s constituent la requ?te ? traiter.
Pour la seconde piste (absence de terminologie), les participants ont utilis?s des outils d?extraction
de mots-cl?s apr?s avoir supprim? les mots non significatifs puis des m?thodes de r?ordonnan-
cement des mots-cl?s candidats. Concernant le niveau de granularit? sur lequel travailler, une
?quipe (n? 04) a tent? le niveau caract?re et le niveau mot (Doualan et al, 2012) tandis qu?une
autre ?quipe (n? 03) a fait le pari de travailler uniquement ? l??chelle du syntagme nominal,
consid?rant qu?un terme complexe est moins ambig? qu?un terme simple isol? (Amri et al, 2012).
Plusieurs outils d?extraction de termes ont ainsi ?t? mobilis?s : l?outil KX accorde ainsi un poids
aux termes extraits selon des annotations linguistiques et des relev?s statistiques (Tonelli et al,
2012) (n? 01), l?outil TermoStat qui repose sur des m?thodes symboliques puis effectue un
tri statistique (Claveau et Raymond, 2012) (n? 05), l?algorithme KEA (Keyphrase Extraction
Algorithm) utilis? par l??quipe 06 (Boudin et al, 2012). Une ?quipe (n? 02) a utilis? des outils
de constitution de terminologies structur?es pour reconna?tre les termes (librairie TermTagger
en Perl) extraire les termes (outil YaTeA) (Hamon, 2012). Les participants ont g?n?ralement
utilis? des m?thodes de pond?ration des mots-cl?s extraits reposant principalement sur le tf*idf,
parfois en compl?tant avec la position du mot dans le document (Boudin et al, 2012; Claveau
et Raymond, 2012; Doualan et al, 2012; Hamon, 2012; Tonelli et al, 2012), la fr?quence dans
l?article, dans le r?sum?, la longueur de la cha?ne, la pr?sence du terme dans l?introduction et la
conclusion (Doualan et al, 2012). Certaines ?quipes ont ?galement travaill? sur la reconnaissance
des variantes morpho-syntaxiques des termes candidats (Hamon, 2012; Claveau et Raymond,
2012) en utilisant notamment l?outil Fastr. Mais l?approche qui a permis d?obtenir les meilleurs
r?sultats (El Ghali et al, 2012) repose sur une combinaison de plusieurs modules linguistiques
d?ordre morphologique, s?mantique et pragmatique.
En ce qui concerne le choix des meilleurs candidats, le cosinus a g?n?ralement ?t? employ? (Ahat
et al, 2012; Hamon, 2012), parfois en combinaison avec d?autres techniques telles que les
graphes par l??quipe 18 (Ahat et al, 2012) ou les r?seaux bay?siens (El Ghali et al, 2012).
D?autres techniques fond?es sur l?apprentissage ont ?galement ?t? mobilis?es.
9
6 R?sultats des participants
? l?image des tests humains, les participants ont obtenu de meilleurs r?sultats sur la premi?re
piste (o? la terminologie des mots-cl?s employ?s ?tait fournie) que sur la seconde (absence de
terminologie). Nous renseignons dans le tableau 2 des r?sultats obtenus par les participants
pour chacun des fichiers soumis dans chacune des deux pistes. Nous int?grons ?galement une
?valuation dite ? hors comp?tition ? pour les fichiers re?us apr?s la fin de la p?riode de test ; ces
r?sultats ne sont pris en compte, ni dans le classement final, ni dans les statistiques globales
(moyenne, m?diane, ?cart-type).
?quipe Run T?CHE 1 T?CHE 2Pr?cision Rappel F-mesure Pr?cision Rappel F-mesure
01 ? FBK
1 0,2682 0,2682 0,2682 0,1880 0,1880 0,1880
2 0,2737 0,2737 0,2737 0,1452 0,1446 0,1449
3 0,1978 0,1974 0,1976 0,1901 0,1901 0,1901
02 ? LIM&BIO
1 0,3985 0,3985 0,3985 0,1798 0,1798 0,1798
2 0,3333 0,3333 0,3333 0,1612 0,1612 0,1612
3 0,2253 0,2253 0,2253 0,1921 0,1921 0,1921
03 ? URPAH 1 0,0857 0,0857 0,0857 0,0785 0,0785 0,07852 ? ? ? 0,0785 0,0785 0,0785
04 ? GREYC
1 0,0507 0,1769 0,0788 0,0469 0,1777 0,0742
2 0,1082 0,1322 0,1190 0,1108 0,1488 0,1270
3 0,4144 0,4730 0,4417 ? ? ?
05 ? IRISA
1 0,8017 0,7002 0,7475 0,2087 0,2087 0,2087
2 0,7114 0,7114 0,7114 0,1704 0,1694 0,1699
3 0,6760 0,6760 0,6760 ? ? ?
06 ? LINA
1 0,3812 0,4004 0,3906 0,1788 0,2128 0,1943
2 0,3759 0,3948 0,3851 0,1949 0,2355 0,2133
3 0,3343 0,4097 0,3682 0,1643 0,1880 0,1753
13 ? LIMSI 1 0,1378 0,1378 0,1378 0,1632 0,1632 0,1632
16 ? LUTIN
1 0,4618 0,4618 0,4618 0,2438 0,2438 0,2438
2 0,9480 0,9497 0,9488 0,3471 0,3471 0,3471
3 0,7486 0,7486 0,7486 0,5880 0,5868 0,5874
17 ? LORIA
1 0,0522 0,2737 0,0877 0,0446 0,2562 0,0759
2 0,0745 0,1955 0,1079 0,0603 0,1736 0,0895
3 0,0401 0,3147 0,0711 0,0350 0,3017 0,0627
18 ? PRISM 1 0,0428 0,0428 0,0428 ? ? ?2 0,0242 0,0242 0,0242 ? ? ?
?valuations hors comp?tition
HC 03 ? URPAH 1 0,1695 0,1695 0,1695 0,1203 0,1198 0,1201
HC 15 ? NOOPSIS 1 0,4587 0,2067 0,2850 0,0969 0,0909 0,0938
TAB. 2 ? R?sultats des participants pour chaque soumission sur les deux pistes
10
La correspondance entre num?ro d??quipe et article pr?sentant les m?thodes s??tablit comme suit :
01 ? FBK (Tonelli et al, 2012), 02 ? LIM&Bio (Hamon, 2012), 03 ? URPAH (Amri et al, 2012),
04 ? GREYC (Doualan et al, 2012), 05 ? IRISA (Claveau et Raymond, 2012), 06 ? LINA (Boudin
et al, 2012), 16 ? LUTIN (El Ghali et al, 2012), et 18 ? PRISM (Ahat et al, 2012).
Sur la premi?re piste, nous constatons des ?carts extr?mement importants entre participants,
avec des F-mesures qui varient de 0,0242 ? 0,9488 ! On observe ?galement des ?carts ?lev?s
entre les diff?rentes soumissions d?un m?me participant variant du simple au quadruple. Sur
cette piste, si l?on se fonde sur les meilleures soumissions de chaque ?quipe, la F-mesure moyenne
est de 0,3575, la m?diane de 0,3321 et l??cart-type de 0,2985.
Sur la seconde piste, les ?carts entre participants sont moindres, les F-mesures variant de 0,0627
? 0,5874. On observe ?galement qu?un grand nombre de participants obtient, sur la meilleure
soumission de son syst?me, une F-mesure qui varie autour de 0,2. En se focalisant sur la meilleure
soumission de chaque participant, la F-mesure moyenne est de 0,2055, la m?diane de 0,1901 et
l??cart-type de 0,1516.
Nous renseignons dans le tableau 3 du nombre de mots-cl?s int?gr?s dans chaque fichier de
soumission. Sur la premi?re piste, 443 mots-cl?s ?taient attendus tandis que la seconde en
attendait 391. Nombreux sont les participants qui ont fournis autant de mots-cl?s que le nombre
attendu (ce nombre ?tant renseign? dans les m?ta-donn?es de chaque document ? traiter). Deux
?quipes ont fait le choix de retourner davantage de mots-cl?s que le nombre attendu.
?quipe 01 ? FBK 02 ? LIM&BIO 03 ? URPAH 04 ? GREYC 05 ? IRISA
Run 1 2 3 1 2 3 1 2 1 2 3 1 2 3
T?che 1 443 443 442 443 443 443 443 ? 1786 657 519 375 443 443
T?che 2 391 390 391 391 391 391 391 391 1748 650 ? 391 388 ?
?quipe 06 ? LINA 13 ? LIMSI 16 ? LUTIN 17 ? LORIA 18 ? PRISM
Run 1 2 3 1 1 2 3 1 2 3 1 2
T?che 1 470 470 564 443 443 444 443 2725 1315 4134 443 443
T?che 2 483 492 461 391 391 391 391 2697 1302 4092 ? ?
TAB. 3 ? Nombre de mots-cl?s renseign?s par fichier et par ex?cution sur chaque piste
Le GREYC (?quipe 04) d?abord, avec environ quatre fois plus de mots-cl?s sur la premi?re
ex?cution, environ une fois et demie de plus sur la seconde soumission et ? peine 1,17 fois de
plus sur la troisi?me. Rapport? aux r?sultats obtenus, la troisi?me soumission ? parce qu?elle
correspond globalement au nombre attendu de mots-cl?s ? obtient les meilleurs r?sultats. Le
LORIA (?quipe 17) enfin, avec environ six fois plus de mots-cl?s sur la premi?re ex?cution,
environ 3 fois plus sur la seconde et 9,33 fois plus sur la troisi?me soumission. ? l?image du
GREYC, la soumission dont le nombre de mots-cl?s se rapproche de celui attendu obtient les
meilleurs r?sultats. Pour ces deux ?quipes, ces strat?gies permettent d?obtenir un rappel meilleur
que la pr?cision mais les valeurs calcul?es restent faibles.
11
7 Conclusion
Les t?ches d?indexation, bien que r?alis?es depuis de nombreuses ann?es, ne constituent plus
des pistes exploratoires. ? ce titre, les r?sultats obtenus par les participants sur cette cam-
pagne t?moignent des ?carts importants entre ?quipes, selon que l??quipe dispose d?un syst?me
d?indexation ou bien part uniquement d?un syst?me de base.
Les participants ont mieux r?ussi la premi?re piste que la seconde, parce qu?elle fournissait
la terminologie des mots-cl?s employ?s dans les documents du corpus ? traiter. La F-mesure
moyenne passe de 0,3575 sur la premi?re piste ? 0,2045 sur la seconde avec des ?cart-types
variant de 0,2985 ? 0,1522 de l?une ? l?autre. On constate ?galement des ?carts ?lev?s (jusqu??
0,5388 d??cart de F-mesure), pour une m?me ?quipe, entre la meilleure soumission sur chaque
piste.
Compte-tenu des modalit?s d??valuation, les strat?gies visant ? fournir davantage de mots-cl?s
que le nombre attendu (cette information ayant ?t? fournie dans les corpus d?apprentissage et de
test) ont permis d?accro?tre le rappel au d?triment d?une pr?cision tr?s faible.
Remerciements
L?interface de soumission et d??valuation des r?sultats a ?t? d?velopp?e par Pierre Albert dans
le cadre du projet DoXa (financement CapDigital, convention DGE no 08 2 93 0888). Nous
remercions les organisateurs des conf?rences JEP/TALN pour l?organisation logistique de l?atelier
et l?ATALA pour la mise ? disposition d?une salle.
Nous remercions les ?tudiants du M2 Professionnel ? Ing?nierie Multilingue ? 2011/2012 de
l?INaLCO pour les tests humains qu?ils ont effectu?s, leur permettant ainsi de d?couvrir l?une
des ?tapes essentielles lors de l?organisation d?une campagne d??valuation : Alexandra Moraru,
Benjamin Marie, Irina Poltavchenko, Leidiana Martins, L?vana Thammavongsa, Nazim Saadi,
Sofiane Kerroua.
R?f?rences
AHAT, M., PETERMANN, C., HOAREAU, Y. V., BEN AMOR, S. et BUI, M. (2012). Algorithme automa-
tique non supervis? pour le deft 2012. In Actes de l?atelier de cl?ture du huiti?me d?fi fouille de
texte (DEFT), pages 73?79.
AMRI, A., MBAREK, M., BECHIKH, C., LATIRI, C. et HADDAD, H. (2012). Indexation ? base des
syntagmes nominaux. In Actes de l?atelier de cl?ture du huiti?me d?fi fouille de texte (DEFT), pages
37?43.
BOUDIN, F., HAZEM, A., HERNANDEZ, N. et SHRESTHA, P. (2012). Participation du lina ? deft 2012.
In Actes de l?atelier de cl?ture du huiti?me d?fi fouille de texte (DEFT), pages 65?72.
CLAVEAU, V. et RAYMOND, C. (2012). Participation de l?irisa ? deft2012 : recherche d?information
et apprentissage pour la g?n?ration de mots-cl?s. In Actes de l?atelier de cl?ture du huiti?me d?fi
fouille de texte (DEFT), pages 53?64.
12
DEERWESTER, S., DUMAIS, S. T., FURNAS, G. W., LANDAUER, T. K. et HARSHMAN, R. (1990). Indexing
by latent semantic analysis. In Journal of the American Society for Information Science, volume 41,
pages 391?407.
DOUALAN, G., BOUCHER, M., BRIXTEL, R., LEJEUNE, G. et DIAS, G. (2012). D?tection de mots-cl?s
par approches au grain caract?re et au grain mot. In Actes de l?atelier de cl?ture du huiti?me d?fi
fouille de texte (DEFT), pages 45?52.
EL GHALI, A., HROMADA, D. et EL GHALI, K. (2012). Enrichir et raisonner sur des espaces
s?mantiques pour l?attribution de mots-cl?s. In Actes de l?atelier de cl?ture du huiti?me d?fi fouille
de texte (DEFT), pages 81?94.
HAMON, T. (2012). Acquisition terminologique pour identifier les mots-cl?s d?articles scientifiques.
In Actes de l?atelier de cl?ture du huiti?me d?fi fouille de texte (DEFT), pages 29?35.
KIM, S. N., MEDELYAN, O., KAN, M.-Y. et BALDWIN, T. (2010). Semeval-2010 task 5 : Automatic
keyphrase extraction from scientific articles. In Proc. of SemEval, pages 21?26, Stroudsburg, PA.
Association for Computational Linguistics.
LANCASTER, F. W. (2003). Indexing and abstracting in theory and practice. Facet, London.
MANNING, C. D. et SCH?TZE, H. (1999). Foundations of Statistical Natural Language Processing.
MIT Press, Cambridge, Massashusetts.
MOENS, M. F. (2000). Indexing and abstracting of Document Texts. Kluwer Academic Publishers.
NAKACHE, D. et M?TAIS, E. (2005). Evaluation : nouvelle approche avec juges. In INFORSID,
pages 555?570, Grenoble.
POMPIDOR, P., CARBONNEILL, B. et SALA, M. (2008). Indexation de co-occurrences guid?e par
la structure des documents et contr?l?e par une ontologie et l?exploitation du corpus. In
INFORSID?08, Fontainebleau, France. Lavoisier-Herm?s.
SALTON, G., WONG, A. et YANG, C. S. (1975). A vector space model for automatic indexing. In
Communications of the ACM, volume 18, pages 613?620.
SAVOY, J. (2005). Indexation manuelle et automatique : une ?valuation comparative bas?e sur
un corpus en langue fran?aise. In Actes de Coria, pages 9?23, Grenoble.
SIDHOM, S. (2002). Plate-forme d?analyse morpho-syntaxique pour l?indexation automatique et la
recherche d?information : de l??crit vers la gestion des connaissances. Th?se de doctorat, Universit?
Claude Bernard ? Lyon I.
TONELLI, S., CABRIO, E. et PIANTA, E. (2012). Key-concept extraction from french articles with kx.
In Actes de l?atelier de cl?ture du huiti?me d?fi fouille de texte (DEFT), pages 19?28.
TOUSSAINT, Y., NAMER, F., DAILLE, B., JACQUEMIN, C., ROYAUT?, J. et HATHOUT, N. (1998). Une
approche linguistique et statistique pour l?analyse de l?information en corpus. In ZWEIGENBAUM,
P., ?diteur : Actes de TALN 1998 (Traitement automatique des langues naturelles), pages 1?10,
Paris. ATALA.
13

