Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 244?253,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Incorporating Temporal and Semantic Information with Eye Gaze for
Automatic Word Acquisition in Multimodal Conversational Systems
Shaolin Qu Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
{qushaoli,jchai}@cse.msu.edu
Abstract
One major bottleneck in conversational sys-
tems is their incapability in interpreting un-
expected user language inputs such as out-of-
vocabulary words. To overcome this problem,
conversational systems must be able to learn
new words automatically during human ma-
chine conversation. Motivated by psycholin-
guistic findings on eye gaze and human lan-
guage processing, we are developing tech-
niques to incorporate human eye gaze for au-
tomatic word acquisition in multimodal con-
versational systems. This paper investigates
the use of temporal alignment between speech
and eye gaze and the use of domain knowl-
edge in word acquisition. Our experiment re-
sults indicate that eye gaze provides a poten-
tial channel for automatically acquiring new
words. The use of extra temporal and domain
knowledge can significantly improve acquisi-
tion performance.
1 Introduction
Interpreting human language is a challenging prob-
lem in human machine conversational systems due
to the flexibility of human language behavior. When
the encountered vocabulary is outside of the sys-
tem?s knowledge, conversational systems tend to
fail. It is desirable that conversational systems can
learn new words automatically during human ma-
chine conversation. While automatic word acquisi-
tion in general is quite challenging, multimodal con-
versational systems offer an unique opportunity to
explore word acquisition. In a multimodal conversa-
tional system where users can talk and interact with
a graphical display, users? eye gaze, which occurs
naturally with speech production, provides a poten-
tial channel for the system to learn new words auto-
matically during human machine conversation.
Psycholinguistic studies have shown that eye gaze
is tightly linked to human language processing. Eye
gaze is one of the reliable indicators of what a per-
son is ?thinking about? (Henderson and Ferreira,
2004). The direction of eye gaze carries informa-
tion about the focus of the user?s attention (Just and
Carpenter, 1976). The perceived visual context in-
fluences spoken word recognition and mediates syn-
tactic processing of spoken sentences (Tanenhaus et
al., 1995). In addition, directly before speaking a
word, the eyes move to the mentioned object (Grif-
fin and Bock, 2000).
Motivated by these psycholinguistic findings, we
are investigating the use of eye gaze for automatic
word acquisition in multimodal conversation. Par-
ticulary, this paper investigates the use of tempo-
ral information about speech and eye gaze and do-
main semantic relatedness for automatic word ac-
quisition. The domain semantic and temporal in-
formation are incorporated in statistical translation
models for word acquisition. Our experiments show
that the use of domain semantic and temporal infor-
mation significantly improves word acquisition per-
formance.
In the following sections, we first describe the ba-
sic translation models for word acquisition. Then,
we describe the enhanced models that incorporate
temporal and semantic information about speech
and eye gaze for word acquisition. Finally, we
present the results of empirical evaluation.
244
(a) Raw gaze points (b) Processed gaze fixations
Figure 1: Domain scene with a user?s gaze fixations
2 Related Work
Word acquisition by grounding words to visual en-
tities has been studied in many language ground-
ing systems. For example, given speech paired with
video images of single objects, mutual information
between audio and visual signals was used to acquire
words by associating acoustic phone sequences with
the visual prototypes (e.g., color, size, shape) of ob-
jects (Roy and Pentland, 2002). Generative mod-
els were used to acquire words by associating words
with image regions given parallel data of pictures
and description text (Barnard et al, 2003). Differ-
ent from these works, in our work, the visual atten-
tion foci accompanying speech are indicated by eye
gaze. Eye gaze is an implicit and subconscious in-
put, which brings additional challenges in word ac-
quisition.
Eye gaze has been explored for word acquisition
in previous work. In (Yu and Ballard, 2004), given
speech paired with eye gaze information and video
images, a translation model was used to acquire
words by associating acoustic phone sequences with
visual representations of objects and actions. A re-
cent investigation on word acquisition from tran-
scribed speech and eye gaze in human machine con-
versation was reported in (Liu et al, 2007). In this
work, a translation model was developed to asso-
ciate words with visual objects on a graphical dis-
play. Different from these previous works, here
we investigate the incorporation of extra knowledge,
specifically speech-gaze temporal information and
domain knowledge, with eye gaze to facilitate word
acquisition.
3 Data Collection
We recruited users to interact with a simplified mul-
timodal conversational system to collect speech and
eye gaze data.
3.1 Domain
We are working on a 3D room decoration domain.
Figure 1 shows the 3D room scene that was shown
to the user in the experiments. There are 28 3D
objects (bed, chairs, paintings, lamp, etc.) in the
room scene. During the human machine conversa-
tion, the system verbally asked the user a question
(e.g., ?what do you dislike about the arrangement
of the room??) or issued a request (e.g., ?describe
the left wall?) about the room. The user provided
responses by speaking to the system.
During the experiments, users? speech was
recorded through an open microphone and users?
eye gaze was captured by an Eye Link II eye tracker.
Eye gaze data consists of the screen coordinates of
each gaze point that was captured by the eye tracker
at a sampling rate of 250hz.
3.2 Data Preprocessing
As for speech data, we collected 357 spoken utter-
ances from 7 users? experiments. The vocabulary
size is 480, among which 227 words are nouns and
adjectives. We manually transcribed the collected
speech.
As for gaze data, the first step is to identify gaze
fixation from raw gaze points. As shown in Fig-
ure 1(a), the collected raw gaze points are very noisy.
They can not be used directly for identifying gaze
fixated entities in the scene. We processed the raw
245
gaze data to eliminate invalid and saccadic gaze
points. Invalid gaze points occur when users look
off the screen. Saccadic gaze points occur during
ballistic eye movements between gaze fixations. Vi-
sion studies have shown that no visual processing
occurs in the human mind during saccades (i.e., sac-
cadic suppression) (Matin, 1974). Since eyes do not
stay still but rather make small, frequent jerky move-
ments, we average nearby gaze points to better iden-
tify gaze fixations. The processed eye gaze fixations
are shown in Figure 1(b).
1668 2096 32522692
[19] [22] [ ] [10]
[11]
[10]
[11]
[10]
[11]
This room has a chandelier
2572 2872 3170 3528 3736
speech stream
gaze stream
(ms)
(ms)
[fixated entity ID]
ts te
f: gaze fixation
( [19] ? bed_frame; [22] ? door; [10] ? bedroom; [11] ? chandelier )
Figure 2: Parallel speech and gaze streams
Figure 2 shows an excerpt of the collected speech
and gaze fixation in one experiment. In the speech
stream, each word starts at a particular timestamp.
In the gaze stream, each gaze fixation has a start-
ing timestamp ts and an ending timestamp te. Each
gaze fixation also has a list of fixated entities (3D ob-
jects). An entity e on the graphical display is fixated
by gaze fixation f if the area of e contains fixation
point of f .
Given the collected speech and gaze fixations, we
build parallel speech-gaze data set as follows. For
each spoken utterance and its accompanying gaze
fixations, we construct a pair of word sequence and
entity sequence (w, e). The word sequence w con-
sists of only nouns and adjectives in the utterance.
Each gaze fixation results in a fixated entity in the
entity sequence e. When multiple entities are fix-
ated by one gaze fixation due to the overlapping of
the entities, the forefront one is chosen. Also, we
merge the neighboring gaze fixations that contain
the same fixated entities. For the parallel speech and
gaze streams shown in Figure 2, the resulting word
sequence is w = [room chandelier] and the entity
sequence is e = [bed frame door chandelier].
4 Translation Models for Automatic Word
Acquisition
Since we are working on conversational systems
where users interact with a visual scene, we consider
the task of word acquisition as associating words
with visual entities in the domain. Given the par-
allel speech and gaze fixated entities {(w, e)}, we
formulate word acquisition as a translation problem
and use translation models to estimate word-entity
association probabilities p(w|e). The words with the
highest association probabilities are chosen as ac-
quired words for entity e.
4.1 Base Model I
Using the translation model I (Brown et al, 1993),
where each word is equally likely to be aligned with
each entity, we have
p(w|e) =
1
(l + 1)m
m?
j=1
l?
i=0
p(wj |ei) (1)
where l and m are the lengths of entity and word
sequences respectively. This is the model used in
(Liu et al, 2007) and (Yu and Ballard, 2004). We
refer to this model as Model-1 throughout the rest
of this paper.
4.2 Base Model II
Using the translation model II (Brown et al, 1993),
where alignments are dependent on word/entity po-
sitions and word/entity sequence lengths, we have
p(w|e) =
m?
j=1
l?
i=0
p(aj = i|j,m, l)p(wj |ei) (2)
where aj = i means that wj is aligned with ei.
When aj = 0, wj is not aligned with any entity (e0
represents a null entity). We refer to this model as
Model-2.
Compared to Model-1, Model-2 considers the or-
dering of words and entities in word acquisition.
EM algorithms are used to estimate the probabilities
p(w|e) in the translation models.
5 Using Speech-Gaze Temporal
Information for Word Acquisition
In Model-2, word-entity alignments are estimated
from co-occurring word and entity sequences in an
246
unsupervised way. The estimated alignments are de-
pendent on where the words/entities appear in the
word/entity sequences, not on when those words and
gaze fixated entities actually occur. Motivated by the
finding that users move their eyes to the mentioned
object directly before speaking a word (Griffin and
Bock, 2000), we make the word-entity alignments
dependent on their temporal relation in a new model
(referred as Model-2t):
p(w|e) =
m?
j=1
l?
i=0
pt(aj = i|j, e,w)p(wj |ei) (3)
where pt(aj = i|j, e,w) is the temporal alignment
probability computed based on the temporal dis-
tance between entity ei and word wj .
We define the temporal distance between ei and
wj as
d(ei, wj) =
?
??
??
0 ts(ei) ? ts(wj) ? te(ei)
te(ei) ? ts(wj) ts(wj) > te(ei)
ts(ei) ? ts(wj) ts(wj) < ts(ei)
(4)
where ts(wj) is the starting timestamp (ms) of word
wj , ts(ei) and te(ei) are the starting and ending
timestamps (ms) of gaze fixation on entity ei.
The alignment of word wj and entity ei is de-
cided by their temporal distance d(ei, wj). Based
on the psycholinguistic finding that eye gaze hap-
pens before a spoken word, wj is not allowed to
be aligned with ei when wj happens earlier than ei
(i.e., d(ei, wj) > 0). When wj happens no earlier
than ei (i.e., d(ei, wj) ? 0), the closer they are, the
more likely they are aligned. Specifically, the tem-
poral alignment probability of wj and ei in each co-
occurring instance (w, e) is computed as
pt(aj = i|j, e,w) =
{
0 d(ei, wj) > 0
exp[??d(ei,wj)]
?i exp[??d(ei,wj)]
d(ei, wj) ? 0
(5)
where ? is a constant for scaling d(ei, wj). In our
experiments, ? is set to 0.005.
An EM algorithm is used to estimate probabilities
p(w|e) in Model-2t.
?5000 ?4000 ?3000 ?2000 ?1000 0 10000
20
40
60
80
100
120
140
temporal distance of aligned word and entity (ms)
alig
nm
ent
 cou
nt
Figure 3: Histogram of truly aligned word and entity
pairs over temporal distance (bin width = 200ms)
For the purpose of evaluation, we manually anno-
tated the truly aligned word and entity pairs. Fig-
ure 3 shows the histogram of those truly aligned
word and entity pairs over the temporal distance of
aligned word and entity. We can observe in the fig-
ure that 1) almost no eye gaze happens after a spo-
ken word, and 2) the number of word-entity pairs
with closer temporal distance is generally larger than
the number of those with farther temporal distance.
This is consistent with our modeling of the tempo-
ral alignment probability of word and entity (Equa-
tion (5)).
6 Using Domain Semantic Relatedness for
Word Acquisition
Speech-gaze temporal alignment and occurrence
statistics sometimes are not sufficient to associate
words to an entity correctly. For example, suppose
a user says ?there is a lamp on the dresser? while
looking at a lamp object on a table object. Due
to their co-occurring with the lamp object, words
dresser and lamp are both likely to be associated
with the lamp object in the translation models. As
a result, word dresser is likely to be incorrectly ac-
quired for the lamp object. For the same reason, the
word lamp could be acquired incorrectly for the ta-
ble object. To solve this type of association prob-
lem, the semantic knowledge about the domain and
words can be helpful. For example, the knowledge
that the word lamp is more semantically related to
the object lamp can help the system avoid associat-
247
ing the word dresser to the lamp object. Therefore,
we are interested in investigating the use of semantic
knowledge in word acquisition.
On one hand, each conversational system has a
domain model, which is the knowledge representa-
tion about its domain such as the types of objects
and their properties and relations. On the other hand,
there are available resources about domain indepen-
dent lexical knowledge (e.g., WordNet (Fellbaum,
1998)). The question is whether we can utilize the
domain model and external lexical knowledge re-
source to improve word acquisition. To address this
question, we link the domain concepts in the domain
model with WordNet concepts, and define semantic
relatedness of word and entity to help the system ac-
quire domain semantically compatible words.
In the following sections, we first describe our
domain modeling, then define the semantic related-
ness of word and entity based on domain modeling
and WordNet semantic lexicon, and finally describe
different ways of using the semantic relatedness of
word and entity to help word acquisition.
6.1 Domain Modeling
We model the 3D room decoration domain as shown
in Figure 4. The domain model contains all do-
main related semantic concepts. These concepts are
linked to the WordNet concepts (i.e., synsets in the
format of ?word#part-of-speech#sense-id?). Each of
the entities in the domain has one or more properties
(e.g., semantic type, color, size) that are denoted by
domain concepts. For example, the entity dresser 1
has domain concepts SEM DRESSER and COLOR.
These domain concepts are linked to ?dresser#n#4?
and ?color#n#1? in WordNet.
Note that in the domain model, the domain con-
cepts are not specific to a certain entity, they are gen-
eral concepts for a certain type of entity. Multiple
entities of the same type have the same properties
and share the same set of domain concepts.
6.2 Semantic Relatedness of Word and Entity
We compute the semantic relatedness of a word w
and an entity e based on the semantic similarity be-
tween w and the properties of e. Specifically, se-
mantic relatedness SR(e, w) is defined as
SR(e, w) = max
i,j
sim(s(cie), sj(w)) (6)
?bed#n#1?
?picture#n#2? ?size#n#1?
?color#n#1?
?dresser#n#4?
COLOR
bed_framedresser_1
SIZESEM_DRESSER SEM_BED COLOR
Entities:
Domain 
concepts:
WordNet 
concepts:
Dom ain Model
Figure 4: Domain model with domain concepts linked to
WordNet synsets
where cie is the i-th property of entity e, s(c
i
e) is the
synset of property cie as designed in domain model,
sj(w) is the j-th synset of word w as defined in
WordNet, and sim(?, ?) is the similarity score of two
synsets.
We computed the similarity score of two synsets
based on the path length between them. The similar-
ity score is inversely proportional to the number of
nodes along the shortest path between the synsets as
defined in WordNet. When the two synsets are the
same, they have the maximal similarity score of 1.
The WordNet-Similarity tool (Pedersen et al, 2004)
was used for the synset similarity computation.
6.3 Word Acquisition with Word-Entity
Semantic Relatedness
We can use the semantic relatedness of word and
entity to help the system acquire semantically com-
patible words for each entity, and therefore improve
word acquisition performance. The semantic relat-
edness can be applied for word acquisition in two
ways: post process learned word-entity association
probabilities by rescoring them with semantic relat-
edness, or directly affect the learning of word-entity
associations by constraining the alignment of word
and entity in the translation models.
6.3.1 Rescoring with semantic relatedness
In the acquired word list for an entity ei, each
word wj has an association probability p(wj |ei) that
is learned from a translation model. We use the
248
semantic relatedness SR(ei, wj) to redistribute the
probability mass for each wj . The new association
probability is given by:
p?(wj |ei) =
p(wj |ei)SR(ei, wj)
?
j p(wj |ei)SR(ei, wj)
(7)
6.3.2 Semantic alignment constraint in
translation model
When used to constrain the word-entity alignment
in the translation model, semantic relatedness can be
used alone or used together with speech-gaze tempo-
ral information to decide the alignment probability
of word and entity.
? Using only semantic relatedness to constrain
word-entity alignments in Model-2s, we have
p(w|e) =
m?
j=1
l?
i=0
ps(aj = i|j, e,w)p(wj |ei)
(8)
where ps(aj = i|j, e,w) is the alignment prob-
ability based on semantic relatedness,
ps(aj = i|j, e,w) =
SR(ei, wj)
?
i SR(ei, wj)
(9)
? Using semantic relatedness and temporal infor-
mation to constrain word-entity alignments in
Model-2ts, we have
p(w|e) =
m?
j=1
l?
i=0
pts(aj = i|j, e,w)p(wj |ei)
(10)
where pts(aj = i|j, e,w) is the alignment
probability that is decided by both temporal re-
lation and semantic relatedness of ei and wj ,
pts(aj = i|j, e,w) =
ps(aj = i|j, e,w)pt(aj = i|j, e,w)
?
i ps(aj = i|j, e,w)pt(aj = i|j, e,w)
(11)
where ps(aj = i|j, e,w) is the semantic align-
ment probability in Equation (9), and pt(aj =
i|j, e,w) is the temporal alignment probability
given in Equation (5).
EM algorithms are used to estimate p(w|e) in
Model-2s and Model-2ts.
7 Grounding Words to Domain Concepts
As discussed above, based on translation models, we
can incorporate temporal and domain semantic in-
formation to obtain p(w|e). This probability only
provides a means to ground words to entities. In
conversational systems, the ultimate goal of word
acquisition is to make the system understand the se-
mantic meaning of new words. Word acquisition by
grounding words to objects is not always sufficient
for identifying their semantic meanings. Suppose
the word green is grounded to a green chair object,
so is the word chair. Although the system is aware
that green is some word describing the green chair,
it does not know that word green refers to the chair?s
color while the word chair refers to the chair?s se-
mantic type. Thus, after learning the word-entity as-
sociations p(w|e) by the translation models, we need
to further ground words to domain concepts of entity
properties.
We further apply WordNet to ground words to do-
main concepts. For each entity e, based on asso-
ciation probabilities p(w|e), we can choose the n-
best words as acquired words for e. Those n-best
words have the n highest association probabilities.
For each word w acquired for e, the grounded con-
cept c?e forw is chosen as the one that has the highest
semantic relatedness with w:
c?e = argmax
i
[
max
j
sim(s(cie), sj(w))
]
(12)
where sim(s(cie), sj(w)) is the semantic similarity
score defined in Equation (6).
8 Evaluation
We evaluate word acquisition performance of differ-
ent models on the data collected from our user stud-
ies (see Section 3).
8.1 Evaluation Metrics
The following metrics are used to evaluate the words
acquired for domain concepts (i.e., entity properties)
{cie}.
? Precision
?
e
?
i # words correctly acquired for c
i
e?
e
?
i # words acquired for c
i
e
249
? Recall
?
e
?
i # words correctly acquired for c
i
e
?
e
?
i # ground-truth
1 words of cie
? F-measure
2 ? precision ? recall
precision + recall
The metrics of precision, recall, and F-measure
are based on the n-best words acquired for the entity
properties. Therefore, we have different precision,
recall, and F-measure when n changes.
The metrics of precision, recall, and F-measure
only provide evaluation on the top n candidate
words. To measure the acquisition performance on
the entire ranked list of candidate words, we define
a new metric as follows:
? Mean Reciprocal Rank Rate (MRRR)
MRRR =
?
e
?Nei=1
1
index(wie)
?Nei=1
1
i
#e
where Ne is the number of all ground-truth
words {wie} for entity e, index(w
i
e) is the in-
dex of word wie in the ranked list of candidate
words for entity e.
Entities may have a different number of ground-
truth words. For each entity e, we calculate a Recip-
rocal Rank Rate (RRR), which measures how close
the ranks of the ground-truth words in the candidate
word list is to the best scenario where the top Ne
words are the ground-truth words for e. RRR is in
the range of (0, 1]. The higher the RRR, the better
is the word acquisition performance. The average of
RRRs across all entities gives the Mean Reciprocal
Rank Rate (MRRR).
Note that MRRR is directly based on the learned
word-entity associations p(w|e), it is in fact a mea-
sure of grounding words to entities.
1The ground-truth words were compiled and agreed upon by
two human judges.
8.2 Evaluation Results
To compare the effects of different speech-gaze
alignments on word acquisition, we evaluate the fol-
lowing models:
? Model-1 ? base model I without word-entity
alignment (Equation (1)).
? Model-2 ? base model II with positional align-
ment (Equation (2)).
? Model-2t ? enhanced model with temporal
alignment (Equation (3)).
? Model-2s ? enhanced model with semantic
alignment (Equation (8)).
? Model-2ts ? enhanced model with both tempo-
ral and semantic alignment (Equation (10)).
To compare the different ways of incorporating
semantic relatedness in word acquisition as dis-
cussed in Section 6.3.1, we also evaluate the follow-
ing models:
? Model-1-r ?Model-1 with semantic relatedness
rescoring of word-entity association.
? Model-2t-r ? Model-2t with semantic related-
ness rescoring of word-entity association.
Figure 5 shows the results of models with differ-
ent speech-gaze alignments. Figure 6 shows the re-
sults of models with semantic relatedness rescoring.
In Figure 5 & 6, n-best means the top n word candi-
dates are chosen as acquired words for each entity.
The Mean Reciprocal Rank Rates of all models are
compared in Figure 7.
8.2.1 Results of using different speech-gaze
alignments
As shown in Figure 5, Model-2 does not show a
consistent improvement compared to Model-1 when
a different number of n-best words are chosen as ac-
quired words. This result shows that it is not very
helpful to consider the index-based positional align-
ment of word and entity for word acquisition.
Figure 5 also shows that models considering
temporal or/and semantic information (Model-2t,
Model-2s, Model-2ts) consistently perform better
than the models considering neither temporal nor
250
1 2 3 4 5 6 7 8 9 100.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
n?best
prec
ision
Model?1Model?2Model?2tModel?2sModel?2ts
(a) precision
1 2 3 4 5 6 7 8 9 100.1
0.2
0.3
0.4
0.5
0.6
n?best
rec
all
Model?1Model?2Model?2tModel?2sModel?2ts
(b) recall
1 2 3 4 5 6 7 8 9 100.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
n?best
F?m
easu
re
Model?1Model?2Model?2tModel?2sModel?2ts
(c) F-measure
Figure 5: Performance of word acquisition when different types of speech-gaze alignment are applied
1 2 3 4 5 6 7 8 9 100.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
n?best
prec
ision
Model?1Model?2tModel?1?rModel?2t?r
(a) precision
1 2 3 4 5 6 7 8 9 100.1
0.2
0.3
0.4
0.5
0.6
n?best
rec
all
Model?1Model?2tModel?1?rModel?2t?r
(b) recall
1 2 3 4 5 6 7 8 9 100.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
n?best
F?m
easu
re
Model?1Model?2tModel?1?rModel?2t?r
(c) F-measure
Figure 6: Performance of word acquisition when semantic relatedness rescoring of word-entity association is applied
M?1 M?2 M?2t M?2s M?2ts M?1?r M?2t?r0.5
0.55
0.6
0.65
0.7
0.75
0.8
Models
Mea
n R
ecip
roca
l Ra
nk R
ate
Figure 7: MRRRs achieved by different models
semantic information (Model-1, Model-2). Among
Model-2t, Model-2s, and Model-2ts, it is found that
they do not make consistent differences.
As shown in Figure 7, the MRRRs of different
models are consistent with their performances on F-
measure. A t-test has shown that the difference be-
tween the MRRRs of Model-1 and Model-2 is not
statistically significant. Compared to Model-1, t-
tests have confirmed that MRRR is significantly im-
proved by Model-2t (t = 2.27, p < 0.02), Model-2s
(t = 3.40, p < 0.01), and Model-2ts(t = 2.60, p <
0.01). T-tests have shown no significant differences
among Model-2t, Model-2s, and Model-2ts.
8.2.2 Results of applying semantic relatedness
rescoring
Figure 6 shows that semantic relatedness rescor-
ing improves word acquisition. After semantic re-
latedness rescoring of the word-entity associations
learned by Model-1, Model-1-r improves the F-
measure consistently when a different number of
n-best words are chosen as acquired words. Com-
pared to Model-2t, Model-2t-r also improves the F-
measure consistently.
Comparing the two ways of using semantic relat-
edness for word acquisition, it is found that rescor-
ing word-entity association with semantic related-
ness works better. When semantic relatedness is
used together with temporal information to constrain
word-entity alignments in Model-2ts, word acqui-
251
Model Rank 1 Rank 2 Rank 3 Rank 4 Rank 5
M-1 table(0.173) dresser(0.067) area(0.058) picture(0.053) dressing(0.041)
M-2t table(0.146) dresser(0.125) dressing(0.061) vanity(0.051) fact(0.050)
M-2t-r table(0.312) dresser(0.241) vanity(0.149) desk(0.047) area(0.026)
Table 1: N-best candidate words acquired for the entity dresser 1 by different models
sition performance is not improved compared to
Model-2t. However, using semantic relatedness to
rescore word-entity association learned by Model-
2t, Model-2t-r further improves word acquisition.
As shown in Figure 7, the MRRRs of Model-
1-r and Model-2t-r are consistent with their per-
formances on F-measure. Compared to Model-2t,
Model-2t-r improves MRRR. A t-test has confirmed
that this is a significant improvement (t = 1.97, p <
0.03). Compared to Model-1, Model-1-r signifi-
cantly improves MRRR (t = 2.33, p < 0.02). There
is no significant difference between Model-1-r and
Model-2t/Model-2s/Model-2ts.
In Figures 5&6, we also notice that the recall
of the acquired words is still comparably low even
when 10 best word candidates are chosen for each
entity. This is mainly due to the scarcity of those
words that are not acquired in the data. Many of
the words that are not acquired appear less than 3
times in the data, which makes them unlikely to be
associated with any entity by the translation models.
When more data is available, we expect to see higher
recall.
8.3 An Example
Table 1 shows the 5-best words acquired by different
models for the entity dresser 1 in the 3d room scene
(see Figure 1). In the table, each word is followed by
its word-entity association probability p(w|e). The
correctly acquired words are shown in bold font.
As shown in the example, the baseline Model-1
learned 2 correct words in the 5-best list. Consid-
ering speech-gaze temporal information, Model-2t
learned one more correct word vanity in the 5-best
list. With semantic relatedness rescoring, Model-
2t-r further acquired word desk in the 5-best list
because of the high semantic relatedness of word
desk and the type of entity dresser 1. Although nei-
ther Model-1 nor Model-2t successfully acquired the
word desk in the 5-best list, the rank (=7) of the word
desk in Model-2t?s n-best list is much higher than the
rank (=21) in Model-1?s n-best list.
9 Conclusion
Motivated by the psycholinguistic findings, we in-
vestigate the use of eye gaze for automatic word ac-
quisition in multimodal conversational systems. Par-
ticularly, we investigate the use of speech-gaze tem-
poral information and word-entity semantic related-
ness to facilitate word acquisition. Our experiments
show that word acquisition is significantly improved
when temporal information is considered, which is
consistent with the previous psycholinguistic find-
ings about speech and eye gaze. Moreover, using
temporal information together with semantic relat-
edness rescoring further improves word acquisition.
Eye tracking systems are no longer bulky sys-
tems that prevent natural human machine commu-
nication. Display mounted gaze tracking systems
(e.g., Tobii) are completely non-intrusive, can toler-
ate head motion, and provide high tracking quality.
Integrating eye tracking with conversational inter-
faces is no longer beyond reach. Recent works have
shown that eye gaze can facilitate spoken language
processing in conversational systems (Qu and Chai,
2007; Prasov and Chai, 2008). Incorporating eye
gaze with automatic word acquisition provides an-
other potential approach to improve the robustness
of human machine conversation.
Acknowledgments
This work was supported by IIS-0347548 and IIS-
0535112 from the National Science Foundation.
The authors would like to thank Zahar Prasov for his
contribution on data collection. The authors would
also like to thank anonymous reviewers for their
valuable comments and suggestions.
References
Kobus Barnard, Pinar Duygulu, Nando de Freitas, David
Forsyth, David Blei, and Michael I. Jordan. 2003.
252
Matching words and pictures. Journal of Machine
Learning Research, 3:1107?1135.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematic
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Zenzi M. Griffin and Kathryn Bock. 2000. What the eyes
say about speaking. Psychological Science, 11:274?
279.
JohnM. Henderson and Fernanda Ferreira, editors. 2004.
The interface of language, vision, and action: Eye
movements and the visual world. New York: Taylor
& Francis.
Marcel A. Just and Patricia A. Carpenter. 1976. Eye fix-
ations and cognitive processes. Cognitive Psychology,
8:441?480.
Yi Liu, Joyce Y. Chai, and Rong Jin. 2007. Au-
tomated vocabulary acquisition and interpretation in
multimodal conversational systems. In Proceedings of
the 45th Annual Meeting of the Association of Compu-
tational Linguistics (ACL).
E. Matin. 1974. Saccadic suppression: a review and an
analysis. Psychological Bulletin, 81:899?917.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the relat-
edness of concepts. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence (AAAI-
04).
Zahar Prasov and Joyce Y. Chai. 2008. What?s in a
gaze? the role of eye-gaze in reference resolution in
multimodal conversational interfaces. In Proceedings
of ACM 12th International Conference on Intelligent
User interfaces (IUI).
Shaolin Qu and Joyce Y. Chai. 2007. An exploration
of eye gaze in spoken language processing for multi-
modal conversational interfaces. In Proceedings of the
Conference of the North America Chapter of the Asso-
ciation of Computational Linguistics (NAACL).
Deb K. Roy and Alex P. Pentland. 2002. Learning words
from sights and sounds, a computational model. Cog-
nitive Science, 26(1):113?146.
Michael K. Tanenhaus, Michael J. Spivey-Knowiton,
Kathleen M. Eberhard, and Julie C. Sedivy. 1995. In-
tegration of visual and linguistic information in spoken
language comprehension. Science, 268:1632?1634.
Chen Yu and Dana H. Ballard. 2004. A multimodal
learning interface for grounding spoken language in
sensory perceptions. ACM Transactions on Applied
Perceptions, 1(1):57?80.
253
