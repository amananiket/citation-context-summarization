F IN ITE-STATE APPROXIMATION 
OF PHRASE STRUCTURE GRAMMARS 
Fernando C. N. Pereira 
AT&T Bell Laboratories 
600 Mountain Ave. 
Murray Hill, NJ 07974 
Rebecca N. Wright 
Dept. of Computer Science, Yale University 
PO Box 2158 Yale Station 
New Haven, CT 06520 
Abstract  
Phrase-structure grammars are an effective rep- 
resentation for important syntactic and semantic 
aspects of natural anguages, but are computa- 
tionally too demanding for use as language mod- 
els in real-time speech recognition. An algorithm 
is described that computes finite-state approxi- 
mations for context-free grammars and equivalent 
augmented phrase-structure grammar formalisms. 
The approximation is exact for certain context- 
free grammars generating regular languages, in- 
cluding all left-linear and right-linear context-free 
grammars. The algorithm has been used to con- 
struct finite-state language models for limited- 
domain speech recognition tasks. 
1 Mot ivat ion 
Grammars for spoken language systems are sub- 
ject to the conflicting requirements of language 
modeling for recognition and of language analysis 
for sentence interpretation. Current recognition 
algorithms can most directly use finite-state ac- 
ceptor (FSA) language models. However, these 
models are inadequate for language interpreta- 
tion, since they cannot express the relevant syntac- 
tic and semantic regularities. Augmented phrase 
structure grammar (APSG) formalisms, such as 
unification-based grammars (Shieber, 1985a), can 
express many of those regularities, but they are 
computationally ess suitable for language mod- 
eling, because of the inherent cost of computing 
state transitions in APSG parsers. 
The above problems might be circumvented by 
using separate grammars for language modeling 
and language interpretation. Ideally, the recog- 
nition grammar should not reject sentences ac- 
ceptable by the interpretation grammar and it 
should contain as much as reasonable of the con- 
straints built into the interpretation grammar. 
However, if the two grammars are built indepen- 
dently, those goals are difficult to maintain. For 
this reason, we have developed a method for con- 
structing automatically a finite-state approxima- 
tion for an APSG. Since the approximation serves 
as language model for a speech-recognition front- 
end to the real parser, we require it to be sound 
in the sense that the it accepts all strings in the 
language defined by the APSG. Without qualifica- 
tion, the term "approximation" will always mean 
here "sound approximation." 
If no further constraints were placed on the 
closeness of the approximation, the trivial al- 
gorithm that assigns to any APSG over alpha- 
bet E the regular language E* would do, but of 
course this language model is useless. One pos- 
sible criterion for "goodness" of approximation 
arises from the observation that many interest- 
ing phrase-structure grammars have substantial 
parts that accept regular languages. That does 
not mean that the grammar rules are in the stan- 
dard forms for defining regular languages (left- 
linear or right-linear), because syntactic and se- 
mantic onsiderations often require that strings in 
a regular set be assigned structural descriptions 
not definable by left- or right-linear rules. A use- 
ful criterion is thus that if a grammar generates 
a regular language, the approximation algorithm 
yields an acceptor for that regular language. In 
other words, one would like the algorithm to be ex- 
act for APSGs yielding regular languages. 1 While 
we have not proved that in general our method 
satisfies the above exactness criterion, we show in 
Section 3.2 that the method is exact for left-linear 
and right-linear grammars, two important classes 
of context-free grammars generating regular lan- 
guages. 
1 At first sight, this requirement may be seen as conflict- 
ing with the undecidability ofdetermining whether a CFG 
generates a regular language (Harrison, 1978). However, 
note that the algorithm just produces an approximation, 
but cannot say whether the approximation is exact. 
246 
2 The Algorithm 
Our approximation method applies to any 
context-free grammar (CFG), or any unification- 
based grammar (Shieber, 1985a) that can be fully 
expanded into a context-free grammar. 2 The re- 
sulting FSA accepts all the sentences accepted 
by the input grammar, and possibly some non- 
sentences as well. 
The current implementation accepts as input 
a form of unification grammar in which features 
can take only atomic values drawn from a speci- 
fied finite set. Such grammars can only generate 
context-free languages, since an equivalent CFG 
can be obtained by instantiating features in rules 
in all possible ways. 
The heart of our approximation method is an 
algorithm to convert he LR(0) characteristic ma- 
chine .Ad(G) (Aho and Ullman, 1977; Backhouse, 
1979) of a CFG G into an FSA for a superset of 
the language L(G) defined by G. The characteris- 
tic machine for a CFG G is an FSA for the viable 
prefixes of G, which are just the possible stacks 
built by the standard shift-reduce recognizer for 
G when recognizing strings in L(G).  
This is not the place to review the character- 
istic machine construction in detail. However, to 
explain the approximation algorithm we will need 
to recall the main aspects of the construction. The 
states of .~4(G) are sets of dotted rules A ---* a . \[3 
where A ---, a/~ is some rule of G. .A4(G) is the 
determinization by the standard subset construc- 
tion (Aho and Ullman, 1977) of the FSA defined 
as follows: 
? The initial state is the dotted rule f f  ---, -S 
where S is the start symbol of G and S' is a 
new auxiliary start symbol. 
? The final state is S' --~ S.. 
? The other states are all the possible dotted 
rules of G. 
? There is a transition labeled X, where X is a 
terminal or nonterminal symbol, from dotted 
rule A -+ a .  X~ to A --+ c~X.//. 
? There is an e-transition from A --~ a ? B/~ to 
B --~ "7, where B is a nonterminal symbol 
and B -+ 7 a rule in G. 
2Unification-based grammars not in this class would 
have to be weakened first, using techniques akin to those of 
Sato and Tamaki (1984), Shieber (1985b) and Haas (1989). 
I S' -> .  S S -> .  Ab A ->. A a A->. 
1 
Is'->s.\] 
'Aqk~ SA'>A'.ba Ja~\ [A .>Aa.  j 
Figure 1: Characteristic Machine for G1 
.A~(G) can be seen as the finite state control for 
a nondeterministic shift-reduce pushdown recog- 
nizer TO(G) for G. A state transition labeled by a 
terminal symbol z from state s to state s' licenses 
a shift move, pushing onto the stack of the recog- 
nizer the pair (s, z). Arrival at a state containing 
a completed dotted rule A --~ a. licenses a reduc- 
tion move. This pops from the stack as many pairs 
as the symbols in a, checking that the symbols in 
the pairs match the corresponding elements of a, 
and then takes the transition out of the last state 
popped s labeled by A, pushing (s, A) onto the 
stack. (Full definitions of those concepts are given 
in Section 3.) 
The basic ingredient of our approximation algo- 
rithm is the f lattening of a shift-reduce recognizer 
for a grammar G into an FSA by eliminating the 
stack and turning reduce moves into e-transitions. 
It will be seen below that flattening 7~(G) directly 
leads to poor approximations in many interesting 
cases. Instead, .bq(G) must first be unfolded into 
a larger machine whose states carry information 
about the possible stacks of g (G) .  The quality of 
the approximation is crucially influenced by how 
much stack information is encoded in the states of 
the unfolded machine: too little leads to coarse ap- 
proximations, while too much leads to redundant 
automata needing very expensive optimization. 
The algorithm is best understood with a simple 
example. Consider the left-linear grammar G1 
S---. Ab 
A---* Aa Je 
AJ(G1) is shown on Figure 1. Unfolding is not re- 
quired for this simple example, so the approximat- 
ing FSA is obtained from .Ad(G1) by the flatten- 
ing method outlined above. The reducing states in 
AJ(G1), those containing completed otted rules, 
are states 0, 3 and 4. For instance, the reduction 
at state 4 would lead to a transition on nonter- 
247 
Figure 2: Flattened FSA 
0 
a 
Figure 3: Minimal Acceptor 
minal A, to state 2, from the state that activated 
the rule being reduced. Thus the corresponding 
e-transition goes from state 4 to state 2. Adding 
all the transitions that arise in this way we ob- 
tain the FSA in Figure 2. From this point on, the 
arcs labeled with nonterminals can be deleted, and 
after simplification we obtain the deterministic fi-
nite automaton (DFA) in Figure 3, which is the 
minimal DFA for L(G1). 
If flattening were always applied to the LR(0) 
characteristic machine as in the example above, 
even simple grammars defining regular languages 
might be inexactly approximated by the algo- 
rithm. The reason for this is that in general the 
reduction at a given reducing state in the char- 
acteristic machine transfers to different states de- 
pending on context. In other words, the reducing 
state might be reached by different routes which 
use the result of the reduction in different ways. 
Consider for example the grammar G2 
S ~ aXa \] bXb 
X -'* c 
which accepts just the two strings aca and bcb. 
Flattening J~4(G2) will produce an FSA that will 
also accept acb and bca, an undesirable outcome. 
The reason for this is that the e-transitions leav- 
ing the reducing state containing X ~ c. do not 
distinguish between the different ways of reach- 
ing that state, which are encoded in the stack of 
One way of solving the above problem is to un- 
fold each state of the characteristic machine into 
a set of states corresponding to different stacks at 
that state, and flattening the corresponding recog- 
nizer rather than the original one. However, the 
set of possible stacks at a state is in general infi- 
nite. Therefore, it is necessary to do the unfolding 
not with respect o stacks, but with respect o a 
finite partition of the set of stacks possible at the 
state, induced by an appropriate equivalence r la- 
tion. The relation we use currently makes two 
stacks equivalent if they can be made identical 
by collapsing loops, that is, removing portions of 
stack pushed between two arrivals at the same 
state in the finite-state control of the shift-reduce 
recognizer. The purpose of collapsing loops is to 
~forget" stack segments that may be arbitrarily 
repeated, s Each equivalence class is uniquely de- 
fined by the shortest stack in the class, and the 
classes can be constructed without having to con- 
sider all the (infinitely) many possible stacks. 
3 Formal Properties 
In this section, we will show here that the approx- 
imation method described informally in the pre- 
vious section is sound for arbitrary CFGs and is 
exact for left-linear and right-linear CFGs. 
In what follows, G is a fixed CFG with termi- 
nal vocabulary ~, nonterminal vocabulary N, and 
start symbol S; V = ~ U N. 
3.1 Soundness  
Let J~4 be the characteristic machine for G, with 
state set Q, start state so, set of final states F, 
and transition function ~ : S x V --* S. As usual, 
transition functions uch as 6 are extended from 
input symbols to input strings by defining 6(s, e) -- 
s and 6is , a/~) = 5(6(s, a),/~). The shift-reduce 
recognizer 7~ associated to A4 has the same states, 
start state and final states. Its configurations are 
triples Is, a, w) of a state, a stack and an input 
string. The stack is a sequence of pairs / s, X) of a 
state and a symbol. The transitions of the shift- 
reduce recognizer are given as follows: 
Shift: is, a, zw) t- (s', a/s, z), w) if 6(s, z) = s' 
Reduce:  is, err, w) ~- /5( s ' ,  A), cr/s', A/, w) if ei- 
ther (1) A --~ ? is a completed otted rule 
3Since possible stacks can be shown to form a regular 
language, loop collapsing has a direct connection to the 
pumping lemma for regular languages. 
248 
in s, s"  = s and r is empty, or (2) A 
X1. . .Xn .  is a completed dotted rule in s, 
T = is1,  X l )  . .  . ( sn ,Xn)  and s" = 81. 
The initial configurations of ~ are (so, e, w} for 
some input string w, and the final configurations 
are ( s, (so, S), e) for some state s E F.  A deriva- 
tion of a string w is a sequence of configura- 
tions c0 , . . . , cm such that  c0 = (s0,e,w), c,~ = 
( s, (so, S), e) for some final state s, and ei-1 l- ci 
for l< i<n.  
Let s be a state. We define the set Stacks(s) to 
contain every sequence (s0 ,X0) . . .  (sk,Xk) such 
that si = 6(s i - l ,X i -1 ) , l  < i < k and s = 
6(st,  Xk). In addition, Stacks(s0) contains the 
empty  sequence . By construction, it is clear that 
if ( s, a, w) is reachable from an initial configura- 
tion in ~,  then o- E Stacks(s). 
A stack congruence on 7? is a family of equiv- 
alence relations _=o on Stacks(s) for each state 
s E 8 such that  if o- = ,  a '  and/f(s ,  X)  = d then 
o-(s,X} =,,  , r (s ,X) .  A stack congruence ---- par- 
titions each set Stacks(s) into equivalence classes 
\[<r\]? of the stacks in Stacks(s) equivalent o o- un- 
der --_,. 
Each stack congruence - on ~ induces a cor- 
responding unfolded recognizer 7~-. The states of 
the unfolded recognizer axe pairs i s, M , ) ,  notated 
more concisely as \[~\]?, of a state and stack equiv- 
alence class at that state. The initial state is \[e\],o, 
and the final states are all \[o-\]? with s E F and 
o- E Stacks(s). The transit ion function 6- of the 
unfolded recognizer is defined by 
t-(\[o-\]', x )  = \[o-is, x) \ ]  '( ' 'x) 
That  this is well-defined follows immediately from 
the definition of stack congruence. 
The  definitions of dotted rules in states, config- 
urations, shift and reduce transitions given above 
carry over immediately to unfolded recognizers. 
Also, the characteristic recognizer can also be seen 
as an unfolded recognizer for the trivial coarsest 
congruence. 
Unfolding a characteristic recognizer does not 
change the language accepted: 
P ropos i t ion  1 Let G be a CFG, 7~ its charac- 
teristic recognizer with transition function ~, and 
= a stack congruence on T?. Then the unfolded 
recognizer ~=_ and 7~ are equivalent recognizers. 
Proof :  We show first that any string w accepted 
by T?--- is accepted by 7~. Let do, . . . ,dm be a 
derivation of w in ~=.  Each di has the form 
di = ( \ [P / \ ] " ,  o'i ,  ul), and can be mapped to an T? 
configuration di = (sl, 8i, ul), where ? = E and 
((s, C), X) = 8i  s, X) .  It  is straightforward to ver- 
ify that  do , . . . ,  d,, is a derivation of w in ~.  
Conversely, let w E L(G),  and c0 , . . . ,em be 
a derivation of w in 7~, with ci = isl,o-i, ui). 
We define el = (\[~ri\] s~, hi, ui), where ~ = e and 
o-is, x )  = aito-\]', x ) .  
If ci-1 P ci is a shift move, then ui-1 = zui  and 
6(s i - l ,  z)  = si. Therefore, 
6-@,_ , \ ] " - ' ,~)  = \[o-~-,(s~-,,~)\]~("- '") 
= \[o-,\]', 
Furthermore, 
~ = o-~- l (S , -  1, ~) = ~, -1  (\[o-,- 1 \ ] " - ' ,  ~) 
Thus we have 
~',-x = ( \ [o - l -d" - ' ,a i -x , *u , )  
~, = @d",e~- l (P~-d"- ' , * ) ,~'~)  
with 6_=(\[o-i-1\]"-', z) = \[o-i\]". Thus, by definition 
of shift move, 6i-1 I- 6i in 7?_--. 
Assume now that  ei-1 I- ci is a reduce move in 
~.  Then ui = ui-1 and we have a state s in 7~, 
a symbol  A E N,  a stack o- and a sequence r of 
state-symbol pairs such that  
si = 6(s,A) 
o-i-1 = o"1" 
o-, = o-(s,a) 
and either 
(a) A --* ? is in s i - t ,  s = si-1 and r = e, or 
(b) A ---, X I . . .Xn .  is in si-1 , r = 
(ql, Xd . . .  (q., X . )  and s = ql- 
Let ~ = \[o-\]*. Then 
6=(~,A) = \[o-(s,A)p0,A) 
= \[o-d" 
We now define a pair sequence ~ to play the  
same role in 7~- as r does in ~.  In case (a) 
above, ~ = e. Otherwise, let rl = e and ri = 
r i - l (q i - l ,X i -1 )  for 2 < i ( n, and define ~ by 
= (\[d q', x l ) . . .  @h i  q', xi)  ? ? ? ( \ [~ .p - ,  x . )  
Then 
O'i-- 1 --~- 0"7" 
= o- (q1 ,X1) . . . (q . -x ,x . -x )  
249 
Thus 
x.)  
-- ? r (q~,X ,} . . . (q i -hX i - l )  
xd-- .  x.)  
= 
= a(\[d',A) 
= a(#,A) 
~i = (~f=(&A),a(~,A),ui) 
which by construction of e immediately entails 
that ~_ 1 ~- Ci is a reduce move in ~=.  fl 
For any unfolded state p, let Pop(p) be the set 
of states reachable from p by a reduce transition. 
More precisely, Pop(p) contains any state pl such 
that there is a completed otted rule A --* (~. in 
p and a state pll such that 6-(p I~, ~) - p and 
6 - ( f * ,A )  -- f .  Then the flattening ~r= of~-  is 
a nondeterministic FSA with the same state set, 
start state and final states as ~-  and nondeter- 
ministic transition function @= defined as follows: 
? I f  6=(p,z) - pt for some z E E, then f E 
? I f  p~ E Pop(p) then f E ~b=(p, ~). 
Let co , . . . ,  cm be a derivation of string w in ~,  
and put ei -- (q~,~q, wl), and p~ = \[~\]~'. By 
construction, if ci_~ F ci is a shift move on z 
(wi-x -- zw~), then 6=(pi - l ,Z)  = Pi, and thus 
p~ ~ ~-(p~_~, z). Alternatively, assume the transi- 
tion is a reduce move associated to the completed 
dotted rule A --* a..  We consider first the case 
a ~ ~. Put a -- X1 . . .  X~. By definition of reduce 
move, there is a sequence of states r l , . . . ,  r~ and 
a stack # such that o'i-x = ?(r~, X1)... (rn, Xn), 
qi -- #(r~,A), 5(r~,A) = qi, and 5(rj,X1) - ri+~ 
for 1 ~ j < n. By  definition of stack congruence, 
we will then have 
= 
where rx = ? and rj = ( r~,X , ) . . . ( r~-x ,X~- , )  for 
j > 1. Furthermore, again by definition of stack 
congruence we have 6=(\[cr\] r*,A) = Pi. Therefore, 
Pi 6 Pop(pi_l) and thus pi e ~_--(pi-x,?). A sim- 
ilar but simpler argument allows us to reach the 
same conclusion for the case a = e. Finally, the 
definition of final state for g= and ~r__ makes Pm 
a final state. Therefore the sequence P0, . - . ,Pm 
is an accepting path for w in ~r_. We have thus 
proved 
Propos i t ion  2 For any CFG G and stack con- 
gruence =_ on the canonical LR(0) shift-reduce rec- 
ognizer 7~(G) of G, L(G) C_ L(~r-(G)), where 
~r-(G) is the flattening of ofT~(G)--. 
Finally, we should show that the stack collaps- 
ing equivalence described informally earlier is in- 
deed a stack congruence. A stack r is a loop if 
' / "  - "  (81, X1)... (sk, Xk) and 6(sk, Xt )  = sz. A 
stack ~ collapses to a stack ~' if cr = pry, cr ~ = pv 
and r is a loop. Two stacks are equivalent if they 
can be collapsed to the same stack. This equiv- 
alence relation is closed under suffixing, therefore 
it is a stack congruence. 
3 .2  Exactness  
While it is difficult to decide what should be meant 
by a "good" approximation, we observed earlier 
that a desirable feature of an approximation algo- 
r ithm would be that it be exact for a wide class of 
CFGs generating regular languages. We show in 
this section that our algorithm is exact both for 
left-linear and for right-linear context-free gram- 
mars, which as is well-known generate regular lan- 
guages. 
The proofs that follow rely on the following ba- 
sic definitions and facts about the LR(0) construc- 
tion. Each LR(0) state s is the closure of a set of 
a certain set of dotted rules, its core. The closure 
\[R\] of a set R of dotted rules is the smallest set 
of dotted rules containing R that contains B --~ "7 
whenever it contains A --~ a ? Bfl and B ---* 7 is 
in G. The core of the initial state so contains just 
the dotted rule f f  ~ .S. For any other state s, 
there is a state 8 ~ and a symbol X such that 8 is 
the closure of the set core consisting of all dotted 
rules A ~ aX. /~ where A --* a .  X/~ belongs to s'. 
3 .3  Le f t -L inear  Grammars  
In this section, we assume that the CFG G is left- 
linear, that is, each rule in G is of the form A 
B/~ or A --+/~, where A, B E N and/3 E ~*. 
P ropos i t ion  3 Let G be a left-linear CFG, and 
let gz be the FSA produced by the approximation 
algorithm from G. Then L(G) = L(3r). 
Proof :  By Proposition 2, L(G) C. L(.~'). Thus we 
need only show L(~)  C_ L(G). 
The proof hinges on the observation that each 
state s of At(G) can be identified with a string 
E V* such that every dotted rule in s is of the 
fo rmA ~ ~.a  for some A E N and c~ E V*. 
250 
Clearly, this is true for so = \[S' --* .S\], with ~0 = e. 
The core k of any other state s will by construction 
contain only dotted rules of the form A ~ a .  
with a ~ e. Since G is left linear, /3 must be 
a terminal string, ensuring that s = \[h\]. There- 
fore, every dotted rule A --* a .  f in s must result 
from dotted rule A ~ .aft in so by the sequence 
of transitions determined by a (since ?tq(G) is de- 
terministic). This means that if A ~ a .  f and 
A' --* a ' .  fl' are in s, it must be the case that 
a - a ~. In the remainder of this proof, let ~ = s 
whenever a = ~. 
To go from the characteristic machine .M(G) to 
the FSA ~', the algorithm first unfolds Ad(G) us- 
ing the stack congruence relation, and then flat- 
tens the unfolded machine by replacing reduce 
moves with e-transitions. However, the above ar- 
gument shows that the only stack possible at a 
state s is the one corresponding to the transitions 
given by $, and thus there is a single stack con- 
gruence state at each state. Therefore, .A4(G) 
will only be flattened, not unfolded. Hence the 
transition function ? for the resulting flattened 
automaton ~" is defined as follows, where a E 
N~* U \]~*,a E ~, and A E N: 
(a) ?(~,a)  = {~} 
(b) ?(5, e) = {.4 I A --, a e G} 
The start state of ~" is ~. The only final state is S. 
We will establish the connection between Y~ 
derivations and G derivations. We claim that if 
there is a path from ~ to S labeled by w then ei- 
ther there is a rule A --* a such that w = xy and 
S :~ Ay  =~ azy ,  or a = S and w = e. The claim 
is proved by induction on Iw\[. 
For the base case, suppose. \[w I = 0 and there is a 
path from & to .~ labeled by w. Then w = e, and 
either a - S, or there is a path of e-transitions 
from ~ to S. In the latter case, S =~ A =~ e for 
some A E N and rule A --~ e, and thus the claim 
holds. 
Now, assume that the claim is true for all Iwl < 
k, and suppose there is a path from & to ,~ labeled 
w I, for some \[wl\[ = k. Then w I - aw for some ter- 
minal a and Iw\[ < k, and there is a path from ~-~ 
to S labeled by w. By the induction hypothesis, 
S =~. Ay  =~ aaz 'y ,  where A --.* aaz  ~ is a rule and 
z ly  - w (since aa y? S).  Letting z -- ax I, we have 
the desired result. 
If w E L(~),  then there is a path from ~ to 
labeled by w. Thus, by claim just proved, S =~ 
Ay ::~ :cy, where A ~ ? is a rule and w = ~y 
(since e # S). Therefore, S =~ w, so w ~ L(G),  as 
desired. 
3 .4  R ight -L inear  Grammars  
A CFG G is right linear if each rule in G is of the 
form A --~ fB  or A --* /3, where A, B E N and 
Propos i t ion  4 Let G be a right-linear CFG and 
9 e be the unfolded, f lattened automaton produced 
by the approximation algorithm on input G. Then 
L(G)  = L(Yz). 
Proof :  As before, we need only show L(~') C 
L(G).  
Let ~ be the shift-reduce recognizer for G. The 
key fact to notice is that, because G is right-linear, 
no shift transition may follow a reduce transition. 
Therefore, no terminal transition in 3 c may follow 
an e-transition, and after any e-transition, there 
is a sequence of G-transitions leading to the final 
state \[$' --* S.\]. Hence ~" has the following kinds of 
states: the start state, the final state, states with 
terminal transitions entering or leaving them (we 
call these reading states), states with e-transitions 
entering and leaving them (prefinal states), and 
states with terminal transitions entering them and 
e-transitions leaving them (cr0ssover states). Any 
accepting path through ~" will consist of a se- 
quence of a start state, reading states, a crossover 
state, prefinal states, and a final state. The excep- 
tion to this is a path accepting the empty string, 
which has a start state, possibly some prefinal 
states, and a final state. 
The above argument also shows that unfolding 
does not change the set of strings accepted by ~,  
because any reduction in 7~= (or e-transition in 
jc), is guaranteed to be part of a path of reductions 
(e-transitions) leading to a final state of 7~_- (~). 
Suppose now that w = w: . . .  wn is accepted by 
~'. Then there is a path from the start state So 
through reading states s l , . . . ,  s,,-1, to crossover 
state sn, followed by e-transitions to the final 
state. We claim that if there there is a path from 
sl to sn labeled wi+l . . .wn ,  then there is a dot- 
ted rule A ---* x ? yB  in si such B :~ z and yz = 
w~+1.. .wn,  where A E N ,B  E NU~*,y ,z  ~ ~*, 
and one of the following holds: 
(a) z is a nonempty suffix of wt . . .  wi, 
(b) z = e, A" =~ A, A'  --* z ' .  A"  is a dotted rule 
in sl, and z t is a nonempty suffix ofT1 . . .wi,  
or  
(c) z=e,  s i=s0 ,  andS=~A.  
We prove the claim by induction on n - i. For 
the base case, suppose there is an empty path from 
251 
Sn to s , .  Because sn  is the crossover state, there 
must be some dotted rule A ~ x. in sn .  Letting 
y = z = B = e, we get that A ---* z .  yB  is a dotted 
rule of s ,  and B = z. The dotted rule A --', z .  yB  
must have either been added to 8n by closure or 
by shifts. I f  it arose from a shift, z must be a 
nonempty suffix of wl . . .wn.  If  the dotted rule 
arose by closure, z = e, and there is some dotted 
rule A ~ --~ z t ? A" such that A" =~ A and ~l is a 
nonempty suffix of Wl . . .  wn. 
Now suppose that the claim holds for paths from 
si to sn, and look at a path labeled w i . . .wn  
from si-1 to sn. By the induction hypothesis, 
A ~ z ? yB  is a dotted rule of st, where B =~ z, 
uz  = w i+ l . . .wn ,  and (since st ~ s0), either z is a 
nonempty suffix of wl . . .  wi or z = e, A ~ - .  z ~. A" 
is a dotted rule of si, A" :~ A, and z ~ is a 
nonempty suffix of wl  . . .  w l .  
In the former case, when z is a nonempty suffix 
of wl  . . .  w l ,  then z = w j  . . .  w i  for some 1 < j < 
i. Then A ---, w j  . . .w l  ? yB  is a dotted rule of 
sl, and thus A ---* w j  . . .w i -1  ? w iyB  is a dotted 
rule o fs i _ l .  I f j  < i -  1, then wj . . .w i _ l  is a 
nonempty suffix of w l . . .w i -1 ,  and we are done. 
Otherwise, wj  . . .w i -1  = e, and so  A --* .w iyB  is a 
dotted rule ofs i -1 .  Let y~ = w iy .  Then A ~ .yJB 
is a dotted rule of si-1, which must have been 
added by closure. Hence there are nonterminals 
A I and A" such that A" :~ A and A I ~ z I ? A"  
is a dotted rule of s t - l ,  where z ~ is a nonempty 
sUtTLX of Wl .. ? wi -  1. 
In the latter case, there must be a dotted rule 
A ~ ~ w j  . . .w i -1  ? w iA"  in si-1. The rest of the 
conditions are exactly as in the previous case. 
Thus, if w - w l . . .wn  is accepted by ~c, then 
there is a path from so to sn labeled by wl . . .  w,.  
Hence, by the claim just proved, A ~ z .  yB  is 
a dotted rule of sn, and B :~ z, where yz  -" 
wl . . .wa  -- w. Because the st in the claim is 
so, and all the dotted rules of si can have nothing 
before the dot, and z must be the empty string. 
Therefore, the only possible case is case 3. Thus, 
S :~ A ---, yz  = w, and hence w E L (G) .  The 
proof that the empty string is accepted by ~" only 
if it is in L(G)  is similar to the proof of the claim. 
D 
4 A Complete Example 
The appendix shows an APSG for a small frag- 
ment of English, written in the notation accepted 
by the current version of our grammar compiler. 
The categories and features used in the grammar 
are described in Tables 1 and 2 (categories without 
features are omitted). Features enforce person- 
number agreement, personal pronoun case, and a 
limited verb subcategorization scheme. 
Grammar compilation has three phrases: (i) 
construction of an equivalent CFG, (ii) approxi- 
mation, and (iii) determinization and minimiza- 
tion of the resulting FSA. The equivalent CFG is 
derived by finding all full instantiations of the ini- 
tial APSG rules that are actually reachable in a 
derivation from the grammar's  start symbol. In 
the current implementation, the construction of 
the equivalent CFG is is done by a Prolog pro- 
gram, while the approximator, determinizer and 
minimizer are written in C. 
For the example grammar, the equivalent CFG 
has 78 nonterminals and 157 rules, the unfolded 
and flattened FSA 2615 states and 4096 transi- 
tions, and the determinized and minimized final 
DFA 16 states and 97 transitions. The runtime 
for the whole process is 4.91 seconds on a Sun 
SparcStation 1. 
Substantially larger grammars, with thousands 
of instantiated rules, have been developed for a 
speech-to-speech translation project. Compilation 
times vary widely, but very long compilations ap- 
pear to be caused by a combinatorial explosion in 
the unfolding of right recursions that will be dis- 
cussed further in the next section. 
5 Informal Analysis 
In addition to the cases of left-linear and right- 
linear grammars discussed in Section 3, our algo- 
r ithm is exact in a variety of interesting cases, in- 
cluding the examples of Church and Patil (1982), 
which illustrate how typical attachment ambigu- 
ities arise as structural ambiguities on regular 
string sets. 
The algorithm is also exact for some self- 
embedding rammars 4 of regular languages, such 
as 
S --+ aS  l Sb  l c 
defining the regular language a*eb*.  
A more interesting example is the following sim- 
plified grammar for the structure of English noun 
4 A grammar isself-embedding if and only if licenses the 
derivation X ~ c~X~ for nonempty c~ and/3. A language 
is regular if and only if it can be described by some non- 
self-embedding grammar. 
252 
Figure 4: Acceptor for Noun Phrases 
phrases: 
NP -+ Det Nom \[ PN 
Det -+ Art \] NP's 
Nom -+ N I Nom PP J Adj Nom 
PP --* P NP 
The symbols Art, N, PN and P correspond to the 
parts of speech article, noun, proper noun and 
preposition. From this grammar, the algorithm 
derives the DFA in Figure 4. 
As an example of inexact approximation, con- 
sider the the self-embedding CFG 
S -+ aSb I ~ 
for the nonregular language a'~b'~,n > O. This 
grammar is mapped by the algorithm into an FSA 
accepting ~ I a+b+. The effect of the algorithm is 
thus to "forget" the pairing between a's and b's 
mediated by the stack of the grammar's charac- 
teristic recognizer. 
Our algorithm has very poor worst-case perfor- 
mance. First, the expansion of an APSG into a 
CFG, not described here, can lead to an exponen- 
tial blow-up in the number of nonterminals and 
rules. Second, the subset calculation implicit in 
the LR(0) construction can make the number of 
states in the characteristic machine xponential 
on the number of CF rules. Finally, unfolding can 
yield another exponential b ow-up in the number 
of states. 
However, in the practical examples we have con- 
sidered, the first and the last problems appear to 
be the most serious. 
The rule instantiation problem may be allevi- 
ated by avoiding full instantiation of unification 
grammar rules with respect o "don't care" fea- 
tures, that is, features that are not constrained by 
the rule. 
The unfolding problem is particularly serious in 
grammars with subgrammars of the form 
S -+ X IS  I ""  J X, ,S J Y (I) 
It is easy to see that the number of unfolded states 
in the subgrammar is exponential in n. This kind 
of situation often arises indirectly in the expan- 
sion of an APSG when some features in the right- 
hand side of a rule are unconstrained and thus 
lead to many different instantiated rules. In fact, 
from the proof of Proposition 4 it follows immedi- 
ately that unfolding is unnecessary for right-linear 
grammars. Ultimately, by dividing the gram- 
mar into non-mutually recursive (strongly con- 
nected) components and only unfolding center- 
embedded components, this particular problem 
could he avoided, s In the meanwhile, the prob- 
lem can be circumvented by left factoring (1) as 
follows: 
S -+ ZS\ [Y  
z -+x ,  I...IX. 
6 Related Work and Conclu- 
sions 
Our work can be seen as an algorithmic realization 
of suggestions ofChurch and Patil (1980; 1982) on 
algebraic simplifications of CFGs of regular lan- 
guages. Other work on finite state approximations 
of phrase structure grammars has typically re- 
lied on arbitrary depth cutoffs in rule application. 
While this is reasonable for psycholinguistic mod- 
eling of performance r strictions on center embed- 
ding (Pulman, 1986), it does not seem appropriate 
for speech recognition where the approximating 
FSA is intended to work as a filter and not re- 
ject inputs acceptable by the given grammar. For 
instance, depth cutoffs in the method escribed by 
Black (1989) lead to approximating FSAs whose 
language is neither a subset nor a superset of the 
language of the given phrase-structure grammar. 
In contrast, our method will produce an exact FSA 
for many interesting grammars generating regular 
languages, uch as those arising from systematic 
attachment ambiguities (Church and Patil, 1982). 
It important to note, however, that even when the 
result FSA accepts the same language, the origi- 
nal grammar is still necessary because interpreta- 
SWe have already implemented a version of the algo- 
rithm that splits the grammar into strongly connected com- 
ponents, approximates and minimizes separately each com- 
ponent and combines the results, but the main purpose of 
this version is to reduce approximation a d determinization 
costs for some grmmmars. 
253 
tion algorithms are generally expressed in terms of 
phrase structures described by that grammar,  not 
in terms of the states of the FSA. 
Although the algorithm described here has 
mostly been adequate for its intended applica- 
tion - -  grammars ufficiently complex not to be 
approximated within reasonable time and space 
bounds usually yield automata  that are far too 
big for our current real-time speech recognition 
hardware - -  it would be eventually of interest to 
handle right-recursion i  a less profligate way. In a 
more theoretical vein, it would also be interesting 
to characterize more tightly the class of exactly 
approximable grammars.  Finally, and most spec- 
ulatively, one would like to develop useful notions 
of degree of approximation of  a language by a reg- 
ular language. Formal-language-theoretic notions 
such as the rational index (Boason et al, 1981) 
or probabilistic ones (Soule, 1974) might be prof- 
itably investigated for this purpose. 
Acknowledgments 
We thank Mark Liberman for suggesting that we 
look into finite-state approximations and Pedro 
Moreno, David Roe, and Richard Sproat for try- 
ing out several prototypes of the implementation 
and supplying test grammars.  
References 
Alfred V. Aho and Jeffrey D. Ullman. 1977. Princi. 
pies of Compiler Design. Addison-Wesley, Reading, 
Massachusetts. 
Roland C. Backhouse. 1979. Syntaz o\] Programming 
Languages--Theorll and Practice. Series in Com- 
puter Science. Prentice-Hall, Englewood Cliffs, New 
Jersey. 
Alan W. Black. 1989. Finite state machines from fea- 
ture grammars. In Masaru Tomita, editor, Inter. 
national Workshop on Parsing Technologies, pages 
277-285, Pittsburgh, Pennsylvania. Carnegie Mel- 
lon University. 
Luc Boason, Bruno Courcelle, and Maurice Nivat. 
1981. The rational index: a complexity measure for 
languages. SIAM Journal o\] Computing, 10(2):284- 
296. 
Kenneth W. Church and Ramesh Patil. 1982. Coping 
with syntactic ambiguity or how to put the block 
in the box on the table. Computational Linguistics, 
8(3--4):139-149. 
Kenneth W. Church. 1980. On memory \]imitations in
? natural anguage processing. Master's thesis, M.I.T. 
Published as Report MIT/LCS/TR-245. 
Andrew Haas. 1989. A parsing algorithm for 
unification grammar. Computational Linguistics, 
15(4):219-232. 
Michael A. Harrison. 1978. Introduction to Formal 
Language Theor~l. Addison-Wesley, Reading, Mas- 
sachussets. 
Steven G. Pulman. 1986. Grammars, parsers, and 
memory limitations. Language and Cognitive Pro- 
cesses, 1(3):197-225. 
Taisuke Sato and Hisao Tamaki. 1984. Enumeration 
of success patterns in logic programs. Theoretical 
Computer Science, 34:227-240. 
Stuart M. Shieber. 1985a. An Introduction to 
Unification-Based Approaches to Grammar. Num- 
ber 4 in CSLI Lecture Notes. Center for the Study 
of Language and Information, Stanford, California. 
Distributed by Chicago University Press. 
Stuart M. Shieber. 1985b. Using restriction to ex- 
tend parsing algorithms for complex-feature-based 
formalisms. In ~3rd Annual Meeting of the Asso- 
ciation \]or Computational Linguistics, pages 145- 
152, Chicago, Illinois. Association for Computa- 
tionai Linguistics, Morristown, New Jersey. 
Stephen Soule. 1974. Entropies of probabilistic gram- 
mars. In\]ormation and Control, 25:57-74. 
Appendix APSG Formalism 
and Example 
Nonterminal symbols (syntactic ategories) may have 
features that specify variants of the category (eg. sin- 
gular or plural noun phrases, intransitive or transitive 
verbs). A category cat with feature constraints i writ- 
ten 
cat# \[ca, ? ? ?, em3. 
Feature constraints for feature f have one of the 
forms 
.f  = ,, (2)  
\] = c (3) 
.f = (c~ . . . . .  c . )  (4)  
where v is a variable name (which must be capitalized) 
and c, c l , . . . ,  c,  are feature values. 
All occurrences of a variable v in a rule stand for 
the same unspecified value. A constraint with form (2) 
specifies a feature as having that value. A constraint 
of form (3) specifies an actual value for a feature, and 
a constraint of form (4) specifies that a feature may 
have any value from the specified set of values. The 
symbol "!" appearing as the value of a feature in the 
right-hand side of a rule indicates that that feature 
must have the same value as the feature of the same 
name of the category in the left-hand side of the rule. 
This notation, as well as variables, can be used to en- 
force feature agreement between categories in a rule, 
? 
254 
Symbol Category Features 
s sentence 
np 
vp 
args 
det 
n 
pron 
V 
noun phrase 
verb phrase 
verb arguments 
determiner 
noun 
pronoun 
verb 
n (number), p 
(person) 
n, p,  c (case) 
n, p, t (verb type) 
t 
n 
n 
n, p, C 
n, p, t 
Table 1: Categories of Example Grammar  
Feature 
n' (number) 
p (person) 
c (case) 
t (verb type) 
Values 
s (singular), p (plural) 
! (first), 2 (second), 3 (third) 
s (subject), o (nonsubject) 
i (intransitive), t (transitive), d 
(ditransitive) 
Table 2: Features of  Example Grammar  
for instance, number agreement between Subject and 
verb. 
It is convenient to declare the features and possible 
values of categories with category declarations appear- 
ing before the grammar ules. Category declarations 
have the form 
cat CatS\[  /1 = (V l l  . . . .  ,V2kl), 
. . o ,  
fm = (vml . . . .  ,Vmk,) \ ] .  
giving all the possible values of all the features for the 
category. 
The declaration 
s tar t  cat. 
declares cat as the start symbol of the grammar. 
In  the grammar ules, the symbol " ' "  prefixes ter- 
minal symbols, commas are used for sequencing and 
\[" for alternation. 
s ta r t  s. 
cat sg\[n=Cs,p),p=(1,2,3)\]. 
cat npg\[n=(s,p) ,p=(1,2,3) ,c=(s,o)\].  
cat vpg\[n=(s,p) , l>=(1,2,3),type=(i,t ,d)\].  
cat argsg\[type=(i . t ,d) \ ] .  
cat detg\[n=(s,p)\].  
cat ng\[n=(s,p)\]. 
cat prong\[n=(s,p),p=(1,2,3),c=(s,o)\]. 
cat vg\[n-(s ,p) ,p=(1,2,3) , type=(i , t ,d) \ ] .  
s => npg\[n=! ,pffi! ,c=s\],  vpg\[n=! ,p=!\].  
npg\[p=3\] => detg\ [n=!\ ] ,  adjs ,  ng\[n=!\] .  
n l~\ [n=s ,p -3 \ ]  -> pn. 
np => prong In= !, p= !, c= ! \ ] .  
prong \[n=s,p-1, c=s\]  => ' i .  
prong \[p=2\] => ' you. 
prong\[n=s,p=3,c=s\]  => 'he I 'she.  
prong\[n-s ,p-3\]  => ' i t .  
prong\[nffip,l~l,c-s\] => 'vs .  
prong\[n=p,p=3,c=s\] > ' they.  
prong\[n=s,p- l ,c=o\]  => 'me. 
prong\[n=s,p=3,c=o\]  => 'him \[ 
prong\[n=p,p=1,c=o\] > 'us .  
prong\[n=p,p-3,c=o\] => 'them. 
'her. 
vp => vg\[n=! ,p=! , type=: \ ] ,  argsg\ [ type=!\ ] .  
adjs -> ~.  
adjs => adj,  adjs .  
args#\[type=i\]  => \ [ \ ] .  
args#\[type=t\]  => npg\[c=o\].  
argsg\[type-d\]  =>npg\[c=o\],  ' to ,  npg\[cfo\ ] .  
pn => ' ton  I 'd ick  \[ 'har ry .  
det => 'soaeJ ' the .  
det#\[n=s\] =>'every  \[ 'a ,  
det#\[n-p\]  => 'a l l  \[ 'most. 
n#\[n=s\] => ' ch i ld  \[ 'cake.  
n#\[n~p\] => ' ch i ld ren  I 'cakes.  
adj . -> 'n ice  J ' sgeet .  
v#\[n=s, l~3,type=i\]  > ' s leeps .  
v#\[nffip,type=i\] > ' s leep .  
v#\[n=s, l~,(1,2),type=/\]  => 's leep.  
v#\[n-s ,p-3 , type=t\ ]  -> 'eats .  
v#\[n~p,type-t \ ]  => 'eat .  
v#\[n=s,p- (1 ,2) , type=t\ ]  ffi> 'eat .  
v#\[n=s,pffi3,type=d\] > 'g ives .  
v#\[nffip,type-d\] =>'g ive.  
v#\[n=s,p=(1,2),type=d\] => 'g ive .  
255 
