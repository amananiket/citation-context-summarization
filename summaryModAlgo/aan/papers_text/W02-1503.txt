The Parallel Grammar Project
Miriam Butt
Cent. for Computational Linguistics
UMIST
Manchester M60 1QD GB
mutt@csli.stanford.edu
Helge Dyvik
Dept. of Linguistics
University of Bergen
N5007 Bergen NORWAY
helge.dyvik@lili.uib.no
Tracy Holloway King
Palo Alto Research Center
Palo Alto, CA 94304 USA
thking@parc.com
Hiroshi Masuichi
Corporate Research Center
Fuji Xerox Co., Ltd.
Kanagawa 259-0157, JAPAN
hiroshi.masuichi@fujixerox.co.jp
Christian Rohrer
IMS Universita?t Stuttgart
D-70174 Stuttgart GERMANY
rohrer@ims.uni-stuttgart.de
Abstract
We report on the Parallel Grammar (ParGram)
project which uses the XLE parser and grammar
development platform for six languages: English,
French, German, Japanese, Norwegian, and Urdu.1
1 Introduction
Large-scale grammar development platforms are ex-
pensive and time consuming to produce. As such, a
desideratum for the platforms is a broad utilization
scope. A grammar development platform should be
able to be used to write grammars for a wide variety
of languages and a broad range of purposes. In this
paper, we report on the Parallel Grammar (ParGram)
project (Butt et al, 1999) which uses the XLE parser
and grammar development platform (Maxwell and
Kaplan, 1993) for six languages: English, French,
German, Japanese, Norwegian, and Urdu. All of
the grammars use the Lexical-Functional Gram-
mar (LFG) formalism which produces c(onstituent)-
structures (trees) and f(unctional)-structures (AVMs)
as the syntactic analysis.
LFG assumes a version of Chomsky?s Universal
Grammar hypothesis, namely that all languages are
structured by similar underlying principles. Within
LFG, f-structures are meant to encode a language
universal level of analysis, allowing for cross-
linguistic parallelism at this level of abstraction. Al-
though the construction of c-structures is governed
1We would like to thank Emily Bender, Mary Dalrymple,
and Ron Kaplan for help with this paper. In addition, we would
like to acknowledge the other grammar writers in the Par-
Gram project, both current: Stefanie Dipper, Jean-Philippe Mar-
cotte, Tomoko Ohkuma, and Victoria Rose?n; and past: Caroline
Brun, Christian Fortmann, Anette Frank, Jonas Kuhn, Veronica
Lux, Yukiko Morimoto, Mar??a-Eugenia Nin?o, and Fre?de?rique
Segond.
by general wellformedness principles, this level of
analysis encodes language particular differences in
linear word order, surface morphological vs. syntac-
tic structures, and constituency.
The ParGram project aims to test the LFG formal-
ism for its universality and coverage limitations and
to see how far parallelism can be maintained across
languages. Where possible, the analyses produced
by the grammars for similar constructions in each
language are parallel. This has the computational
advantage that the grammars can be used in simi-
lar applications and that machine translation (Frank,
1999) can be simplified.
The results of the project to date are encouraging.
Despite differences between the languages involved
and the aims and backgrounds of the project groups,
the ParGram grammars achieve a high level of paral-
lelism. This parallelism applies to the syntactic anal-
yses produced, as well as to grammar development
itself: the sharing of templates and feature decla-
rations, the utilization of common techniques, and
the transfer of knowledge and technology from one
grammar to another. The ability to bundle grammar
writing techniques, such as templates, into transfer-
able technology means that new grammars can be
bootstrapped in a relatively short amount of time.
There are a number of other large-scale gram-
mar projects in existence which we mention briefly
here. The LS-GRAM project (Schmidt et al, 1996),
funded by the EU-Commission under LRE (Lin-
guistic Research and Engineering), was concerned
with the development of grammatical resources for
nine European languages: Danish, Dutch, English,
French, German, Greek, Italian, Portuguese, and
Spanish. The project started in January 1994 and
ended in July 1996. Development of grammatical
resources was carried out in the framework of the
Advanced Language Engineering Platform (ALEP).
The coverage of the grammars implemented in LS-
GRAM was, however, much smaller than the cov-
erage of the English (Riezler et al, 2002) or Ger-
man grammar in ParGram. An effort which is closer
in spirit to ParGram is the implemention of gram-
mar development platforms for HPSG. In the Verb-
mobil project (Wahlster, 2000), HPSG grammars for
English, German, and Japanese were developed on
two platforms: LKB (Copestake, 2002) and PAGE.
The PAGE system, developed and maintained in the
Language Technology Lab of the German National
Research Center on Artificial Intelligence DFKI
GmbH, is an advanced NLP core engine that facili-
tates the development of grammatical and lexical re-
sources, building on typed feature logics. To evalu-
ate the HPSG platforms and to compare their mer-
its with those of XLE and the ParGram projects, one
would have to organize a special workshop, partic-
ularly as the HPSG grammars in Verbmobil were
written for spoken language, characterized by short
utterances, whereas the LFG grammars were devel-
oped for parsing technical manuals and/or newspa-
per texts. There are some indications that the Ger-
man and English grammars in ParGram exceed the
HPSG grammars in coverage (see (Crysmann et al,
2002) on the German HPSG grammar).
This paper is organized as follows. We first pro-
vide a history of the project. Then, we discuss how
parallelism is maintained in the project. Finally, we
provide a summary and discussion.
2 Project History
The ParGram project began in 1994 with three lan-
guages: English, French, and German. The gram-
mar writers worked closely together to solidify the
grammatical analyses and conventions. In addition,
as XLE was still in development, its abilities grew
as the size of the grammars and their needs grew.
After the initial stage of the project, more lan-
guages were added. Because Japanese is typolog-
ically very different from the initial three Euro-
pean languages of the project, it represented a chal-
lenging case. Despite this typological challenge, the
Japanese grammar has achieved broad coverage and
high performance within a year and a half. The
South Asian language Urdu also provides a widely
spoken, typologically distinct language. Although it
is of Indo-European origin, it shares many character-
istics with Japanese such as verb-finality, relatively
free word order, complex predicates, and the abil-
ity to drop any argument (rampant pro-drop). Nor-
wegian assumes a typological middle position be-
tween German and English, sharing different prop-
erties with each of them. Both the Urdu and the Nor-
wegian grammars are still relatively small.
Each grammar project has different goals, and
each site employs grammar writers with different
backgrounds and skills. The English, German, and
Japanese projects have pursued the goal of hav-
ing broad coverage, industrial grammars. The Nor-
wegian and Urdu grammars are smaller scale but
are experimenting with incorporating different kinds
of information into the grammar. The Norwegian
grammar includes a semantic projection; their anal-
yses produce not only c- and f-structures, but also
semantic structures. The Urdu grammar has imple-
mented a level of argument structure and is test-
ing various theoretical linguistic ideas. However,
even when the grammars are used for different pur-
poses and have different additional features, they
have maintained their basic parallelism in analysis
and have profited from the shared grammar writing
techniques and technology.
Table (1) shows the size of the grammars. The first
figure is the number of left-hand side categories in
phrase-structure rules which compile into a collec-
tion of finite-state machines with the listed number
of states and arcs.
(1)
Language Rules States Arcs
German 444 4883 15870
English 310 4935 13268
French 132 1116 2674
Japanese 50 333 1193
Norwegian 46 255 798
Urdu 25 106 169
3 Parallelism
Maintaining parallelism in grammars being devel-
oped at different sites on typologically distinct lan-
guages by grammar writers from different linguis-
tic traditions has proven successful. At project meet-
ings held twice a year, analyses of sample sentences
are compared and any differences are discussed; the
goal is to determine whether the differences are jus-
tified or whether the analyses should be changed
to maintain parallelism. In addition, all of the f-
structure features and their values are compared; this
not only ensures that trivial differences in naming
conventions do not arise, but also gives an overview
of the constructions each language covers and how
they are analyzed. All changes are implemented be-
fore the next project meeting. Each meeting also in-
volves discussion of constructions whose analysis
has not yet been settled on, e.g., the analysis of parti-
tives or proper names. If an analysis is agreed upon,
all the grammars implement it; if only a tentative
analysis is found, one grammar implements it and
reports on its success. For extremely complicated or
fundamental issues, e.g., how to represent predicate
alternations, subcommittees examine the issue and
report on it at the next meeting. The discussion of
such issues may be reopened at successive meetings
until a concensus is reached.
Even within a given linguistic formalism, LFG for
ParGram, there is usually more than one way to an-
alyze a construction. Moreover, the same theoreti-
cal analysis may have different possible implemen-
tations in XLE. These solutions often differ in effi-
ciency or conceptual simplicity and one of the tasks
within the ParGram project is to make design deci-
sions which favor one theoretical analysis and con-
comitant implementation over another.
3.1 Parallel Analyses
Whenever possible, the ParGram grammars choose
the same analysis and the same technical solution
for equivalent constructions. This was done, for
example, with imperatives. Imperatives are always
assigned a null pronominal subject within the f-
structure and a feature indicating that they are im-
peratives, as in (2).
(2) a. Jump! Saute! (French)
Spring! (German) Tobe! (Japanese)
Hopp! (Norwegian) kuudoo! (Urdu)
b. PRED jump SUBJ
SUBJ PRED pro
STMT-TYPE imp
Another example of this type comes from the
analysis of specifiers. Specifiers include many dif-
ferent types of information and hence can be ana-
lyzed in a number of ways. In the ParGram analysis,
the c-structure analysis is left relatively free accord-
ing to language particular needs and slightly vary-
ing theoretical assumptions. For instance, the Nor-
wegian grammar, unlike the other grammars, im-
plements the principles in (Bresnan, 2001) concern-
ing the relationship between an X -based c-structure
and the f-structure. This allows Norwegian speci-
fiers to be analyzed as functional heads of DPs etc.,
whereas they are constituents of NPs in the other
grammars. However, at the level of f-structure, this
information is part of a complex SPEC feature in
all the grammars. Thus parallelism is maintained
at the level of f-structure even across different the-
oretical preferences. An example is shown in (3)
for Norwegian and English in which the SPEC con-
sists of a QUANT(ifier) and a POSS(essive) (SPEC
can also contain information about DETerminers and
DEMONstratives).
(3) a. alle mine hester (Norwegian)
all my horses
?all my horses?
b. PRED horse
SPEC
QUANT PRED all
POSS
PRED pro
PERS 1
NUM sg
Interrogatives provide an interesting example be-
cause they differ significantly in the c-structures of
the languages, but have the same basic f-structure.
This contrast can be seen between the German ex-
ample in (4) and the Urdu one in (5). In German,
the interrogative word is in first position with the
finite verb second; English and Norwegian pattern
like German. In Urdu the verb is usually in final po-
sition, but the interrogative can appear in a number
of positions, including following the verb (5c).
(4) Was hat John Maria gegeben? (German)
what has John Maria give.PerfP
?What did John give to Mary??
(5) a. jon=nee marii=koo kyaa diiyaa? (Urdu)
John=Erg Mary=Dat what gave
?What did John give to Mary?
b. jon=nee kyaa marii=koo diiyaa?
c. jon=nee marii=ko diiyaa kyaa?
Despite these differences in word order and hence in
c-structure, the f-structures are parallel, with the in-
terrogative being in a FOCUS-INT and the sentence
having an interrogative STMT-TYPE, as in (6).
(6) PRED give SUBJ,OBJ,OBL
FOCUS-INT
PRED pro
PRON-TYPE int
SUBJ PRED John
OBJ [ ]
OBL PRED Mary
STMT-TYPE int
In the project grammars, many basic construc-
tions are of this type. However, as we will see in
the next section, there are times when parallelism is
not possible and not desirable. Even in these cases,
though, the grammars which can be parallel are;
so, three of the languages might have one analysis,
while three have another.
3.2 Justified Differences
Parallelism is not maintained at the cost of misrepre-
senting the language. This is reflected by the fact that
the c-structures are not parallel because word order
varies widely from language to language, although
there are naming conventions for the nodes. Instead,
the bulk of the parallelism is in the f-structure. How-
ever, even in the f-structure, situations arise in which
what seems to be the same construction in different
languages do not have the same analysis. An exam-
ple of this is predicate adjectives, as in (7).
(7) a. It is red.
b. Sore wa akai. (Japanese)
it TOP red
?It is red.?
In English, the copular verb is considered the syn-
tactic head of the clause, with the pronoun being the
subject and the predicate adjective being an XCOMP.
However, in Japanese, the adjective is the main pred-
icate, with the pronoun being the subject. As such,
these receive the non-parallel analyses seen in (8a)
for Japanese and (8b) for English.
(8) a. PRED red SUBJ
SUBJ PRED pro
b. PRED be XCOMP SUBJ
SUBJ PRED pro
XCOMP
PRED red SUBJ
SUBJ [ ]
Another situation that arises is when a feature
or construction is syntactically encoded in one lan-
guage, but not another. In such cases, the informa-
tion is only encoded in the languages that need it.
The equivalence captured by parallel analyses is not,
for example, translational equivalence. Rather, par-
allelism involves equivalence with respect to gram-
matical properties, e.g. construction types. One con-
sequence of this is that a typologically consistent
use of grammatical terms, embodied in the feature
names, is enforced. For example, even though there
is a tradition for referring to the distinction between
the pronouns he and she as a gender distinction in
English, this is a different distinction from the one
called gender in languages like German, French,
Urdu, and Norwegian, where gender refers to nom-
inal agreement classes. Parallelism leads to the sit-
uation where the feature GEND occurs in German,
French, Urdu, and Norwegian, but not in English
and Japanese. That is, parallelism does not mean
finding the same features in all languages, but rather
using the same features in the same way in all lan-
guages, to the extent that they are justified there. A
French example of grammatical gender is shown in
(9); note that determiner, adjective, and participle
agreement is dependent on the gender of the noun.
The f-structure for the nouns crayon and plume are
as in (10) with an overt GEND feature.
(9) a. Le petit crayon est casse?. (French)
the-M little-M pencil-M is broken-M.
?The little pencil is broken.?
b. La petite plume est casse?e. (French)
the-F little-F pen-F is broken-F.
?The little pen is broken.?
(10)
PRED crayon
GEND masc
PERS 3
PRED plume
GEND fem
PERS 3
F-structures for the equivalent words in English and
Japanese will not have a GEND feature.
A similar example comes from Japanese dis-
course particles. It is well-known that Japanese has
syntactic encodings for information such as honori-
fication. The verb in the Japanese sentence (11a)
encodes information that the subject is respected,
while the verb in (11b) shows politeness from the
writer (speaker) to the reader (hearer) of the sen-
tence. The f-structures for the verbs in (11) are as in
(12) with RESPECT and POLITE features within the
ADDRESS feature.
(11) a. sensei ga hon wo oyomininaru.
teacher Nom book Acc read-Respect
?The teacher read the book.? (Japanese)
b. seito ga hon wo yomimasu.
student Nom book Acc read-Polite
?The student reads the book.? (Japanese)
(12) a. PRED yomu SUBJ,OBJ
ADDRESS RESPECT +
b. PRED yomu SUBJ,OBJ
ADDRESS POLITE +
A final example comes from English progres-
sives, as in (13). In order to distinguish these two
forms, the English grammar uses a PROG feature
within the tense/aspect system. (13b) shows the f-
structure for (13a.ii).
(13) a. John hit Bill. i. He cried.
ii. He was crying.
b. PRED cry SUBJ
SUBJ PRED pro
TNS-ASP
TENSE past
PROG +
However, this distinction is not found in the other
languages. For example, (14a) is used to express
both (13a.i) and (13a.ii) in German.
(14) a. Er weinte. (German)
he cried
?He cried.?
b. PRED weinen SUBJ
SUBJ PRED pro
TNS-ASP TENSE past
As seen in (14b), the German f-structure is left un-
derspecified for PROG because there is no syntactic
reflex of it. If such a feature were posited, rampant
ambiguity would be introduced for all past tense
forms in German. Instead, the semantics will deter-
mine whether such forms are progressive.
Thus, there are a number of situations where hav-
ing parallel analyses would result in an incorrect
analysis for one of the languages.
3.3 One Language Shows the Way
Another type of situation arises when one language
provides evidence for a certain feature space or type
of analysis that is neither explicitly mirrored nor
explicitly contradicted by another language. In the-
oretical linguistics, it is commonly acknowledged
that what one language codes overtly may be harder
to detect for another language. This situation has
arisen in the ParGram project. Case features fall un-
der this topic. German, Japanese, and Urdu mark
NPs with overt case morphology. In comparison,
English, French, and Norwegian make relatively lit-
tle use of case except as part of the pronominal sys-
tem. Nevertheless, the f-structure analyses for all the
languages contain a case feature in the specification
of noun phrases.
This ?overspecification? of information expresses
deeper linguistic generalizations and keeps the f-
structural analyses as parallel as possible. In addi-
tion, the features can be put to use for the isolated
phenomena in which they do play a role. For exam-
ple, English does not mark animacy grammatically
in most situations. However, providing a ANIM +
feature to known animates, such as people?s names
and pronouns, allows the grammar to encode infor-
mation that is relevant for interpretation. Consider
the relative pronoun who in (15).
(15) a. the girl[ANIM +] who[ANIM +] left
b. the box[ANIM +] who[ANIM +] left
The relative pronoun has a ANIM + feature that is as-
signed to the noun it modifies by the relative clause
rules. As such, a noun modified by a relative clause
headed by who is interpreted as animate. In the case
of canonical inanimates, as in (15b), this will result
in a pragmatically odd interpretation, which is en-
coded in the f-structure.
Teasing apart these different phenomena crosslin-
guistically poses a challenge that the ParGram mem-
bers are continually engaged in. As such, we have
developed several methods to help maintain paral-
lelism.
3.4 Mechanics of Maintaining Parallelism
The parallelism among the grammars is maintained
in a number of ways. Most of the work is done dur-
ing two week-long project meetings held each year.
Three main activities occur during these meetings:
comparison of sample f-structures, comparison of
features and their values, and discussions of new or
problematic constructions.
A month before each meeting, the host site
chooses around fifteen sentences whose analysis is
to be compared at the meeting. These can be a ran-
dom selection or be thematic, e.g., all dealing with
predicatives or with interrogatives. The sentences
are then parsed by each grammar and the output is
compared. For the more recent grammars, this may
mean adding the relevant rules to the grammars, re-
sulting in growth of the grammar; for the older gram-
mars, this may mean updating a construction that has
not been examined in many years. Another approach
that was taken at the beginning of the project was to
have a common corpus of about 1,000 sentences that
all of the grammars were to parse. For the English,
French, and German grammars, this was an aligned
tractor manual. The corpus sentences were used for
the initial f-structure comparisons. Having a com-
mon corpus ensured that the grammars would have
roughly the same coverage. For example, they all
parsed declarative and imperative sentences. How-
ever, the nature of the corpus can leave major gaps
in coverage; in this case, the manual contained no in-
terrogatives.
The XLE platform requires that a grammar de-
clare all the features it uses and their possible val-
ues. Part of the Urdu feature table is shown in (16)
(the notation has been simplified for expository pur-
poses). As seen in (16) for QUANT, attributes which
take other attributes as their values must also be de-
clared. An example of such a feature was seen in
(3b) for SPEC which takes QUANT and POSS fea-
tures, among others, as its values.
(16) PRON-TYPE: pers poss null .
PROPER: date location name title .
PSEM: locational directional .
PTYPE: sem nosem .
QUANT: PRED QUANT-TYPE
QUANT-FORM .
The feature declarations of all of the languages are
compared feature by feature to ensure parallelism.
The most obvious use of this is to ensure that the
grammars encode the same features in the same way.
For example, at a basic level, one feature declaration
might have specified GEN for gender while the oth-
ers had chosen the name GEND; this divergence in
naming is regularized. More interesting cases arise
when one language uses a feature and another does
not for analyzing the same phenomena. When this is
noticed via the feature-table comparison, it is deter-
mined why one grammar needs the feature and the
other does not, and thus it may be possible to elim-
inate the feature in one grammar or to add it to an-
other.
On a deeper level, the feature comparison is use-
ful for conducting a survey of what constructions
each grammar has and how they are implemented.
For example, if a language does not have an ADE-
GREE (adjective degree) feature, the question will
arise as to whether the grammar analyzes compar-
ative and superlative adjectives. If they do not, then
they should be added and should use the ADEGREE
feature; if they do, then the question arises as to why
they do not have this feature as part of their analysis.
Finally, there is the discussion of problematic
constructions. These may be constructions that al-
ready have analyses which had been agreed upon in
the past but which are not working properly now that
more data has been considered. More frequently,
they are new constructions that one of the grammars
is considering adding. Possible analyses for the con-
struction are discussed and then one of the gram-
mars will incorporate the analysis to see whether it
works. If the analysis works, then the other gram-
mars will incorporate the analysis. Constructions
that have been discussed in past ParGram meet-
ings include predicative adjectives, quantifiers, par-
titives, and clefts. Even if not all of the languages
have the construction in question, as was the case
with clefts, the grammar writers for that language
may have interesting ideas on how to analyze it.
These group discussions have proven particularly
useful in extending grammar coverage in a parallel
fashion.
Once a consensus is reached, it is the responsi-
bility of each grammar to make sure that its anal-
yses match the new standard. As such, after each
meeting, the grammar writers will rename features,
change analyses, and implement new constructions
into their grammars. Most of the basic work has now
been accomplished. However, as the grammars ex-
pand coverage, more constructions need to be inte-
grated into the grammars, and these constructions
tend to be ones for which there is no standard analy-
sis in the linguistic literature; so, differences can eas-
ily arise in these areas.
4 Conclusion
The experiences of the ParGram grammar writers
has shown that the parallelism of analysis and imple-
mentation in the ParGram project aids further gram-
mar development efforts. Many of the basic deci-
sions about analyses and formalism have already
been made in the project. Thus, the grammar writer
for a new language can use existing technology to
bootstrap a grammar for the new language and can
parse equivalent constructions in the existing lan-
guages to see how to analyze a construction. This
allows the grammar writer to focus on more diffi-
cult constructions not yet encountered in the existing
grammars.
Consider first the Japanese grammar which was
started in the beginning of 2001. At the initial stage,
the work of grammar development involved imple-
menting the basic constructions already analyzed in
the other grammars. It was found that the grammar
writing techniques and guidelines to maintain par-
allelism shared in the ParGram project could be ef-
ficiently applied to the Japanese grammar. During
the next stage, LFG rules needed for grammatical is-
sues specific to Japanese have been gradually incor-
porated, and at the same time, the biannual ParGram
meetings have helped significantly to keep the gram-
mars parallel. Given this system, in a year and a half,
using two grammar writers, the Japanese grammar
has attained coverage of 99% for 500 sentences of a
copier manual and 95% for 10,000 sentences of an
eCRM (Voice-of-Customer) corpus.
Next consider the Norwegian grammar which
joined the ParGram group in 1999 and also empha-
sized slightly different goals from the other groups.
Rather than prioritizing large textual coverage from
the outset, the Norwegian group gave priority to the
development of a core grammar covering all major
construction types in a principled way based on the
proposals in (Bresnan, 2001) and the inclusion of a
semantic projection in addition to the f-structure. In
addition, time was spent on improving existing lexi-
cal resources ( 80,000 lemmas) and adapting them
to the XLE format. Roughly two man-years has been
spent on the grammar itself. The ParGram cooper-
ation on parallelism has ensured that the derived f-
structures are interesting in a multilingual context,
and the grammar will now serve as a basis for gram-
mar development in other closely related Scandina-
vian languages.
Thus, the ParGram project has shown that it is
possible to use a single grammar development plat-
form and a unified methodology of grammar writing
to develop large-scale grammars for typologically
different languages. The grammars? analyses show a
large degree of parallelism, despite being developed
at different sites. This is achieved by intensive meet-
ings twice a year. The parallelism can be exploited in
applications using the grammars: the fewer the dif-
ferences, the simpler a multilingual application can
be (see (Frank, 1999) on a machine-translation pro-
totype using ParGram).
References
Joan Bresnan. 2001. Lexical-Functional Syntax.
Blackwell.
Miriam Butt, Tracy Holloway King, Mar??a-Eugenia
Nin?o, and Fre?de?rique Segond. 1999. A Grammar
Writer?s Cookbook. CSLI Publications.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI Publications.
Berthold Crysmann, Anette Frank, Bernd Keifer, St.
Mu?ller, Gu?nter Neumann, Jakub Piskorski, Ulrich
Scha?fer, Melanie Siegel, Hans Uszkoreit, Feiyu
Xu, Markus Becker, and Hans-Ulrich Krieger.
2002. An integrated architecture for shallow and
deep parsing. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguis-
tics, University of Pennsylvania.
Anette Frank. 1999. From parallel grammar devel-
opment towards machine translation. In Proceed-
ings of MT Summit VII, pages 134?142.
John T. Maxwell, III and Ron Kaplan. 1993. The
interface between phrasal and functional con-
straints. Computational Lingusitics, 19:571?589.
Stefan Riezler, Tracy Holloway King, Ronald Ka-
plan, Dick Crouch, John T. Maxwell, III, and
Mark Johnson. 2002. Parsing the wall street jour-
nal using a lexical-functional grammar and dis-
criminative estimation techniques. In Proceed-
ings of the Annual Meeting of the Association for
Computational Linguistics, University of Penn-
sylvania.
Paul Schmidt, Sibylle Rieder, Axel Theofilidis, and
Thierry Declerck. 1996. Lean formalisms, lin-
guistic theory, and applications: Grammar devel-
opment in alep. In Proceedings of COLING.
Wolfgang Wahlster, editor. 2000. Verbmobil:
Foundations of Speech-to-Speech Translation.
Springer.
