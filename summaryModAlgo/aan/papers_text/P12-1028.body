Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 263?272,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Verb Classification using Distributional Similarity
in Syntactic and Semantic Structures
Danilo Croce
University of Tor Vergata
00133 Roma, Italy
croce@info.uniroma2.it
Alessandro Moschitti
University of Trento
38123 Povo (TN), Italy
moschitti@disi.unitn.it
Roberto Basili
University of Tor Vergata
00133 Roma, Italy
basili@info.uniroma2.it
Martha Palmer
University of Colorado at Boulder
Boulder, CO 80302, USA
mpalmer@colorado.edu
Abstract
In this paper, we propose innovative repre-
sentations for automatic classification of verbs
according to mainstream linguistic theories,
namely VerbNet and FrameNet. First, syntac-
tic and semantic structures capturing essential
lexical and syntactic properties of verbs are
defined. Then, we design advanced similarity
functions between such structures, i.e., seman-
tic tree kernel functions, for exploiting distri-
butional and grammatical information in Sup-
port Vector Machines. The extensive empir-
ical analysis on VerbNet class and frame de-
tection shows that our models capture mean-
ingful syntactic/semantic structures, which al-
lows for improving the state-of-the-art.
1 Introduction
Verb classification is a fundamental topic of com-
putational linguistics research given its importance
for understanding the role of verbs in conveying se-
mantics of natural language (NL). Additionally, gen-
eralization based on verb classification is central to
many NL applications, ranging from shallow seman-
tic parsing to semantic search or information extrac-
tion. Currently, a lot of interest has been paid to
two verb categorization schemes: VerbNet (Schuler,
2005) and FrameNet (Baker et al., 1998), which
has also fostered production of many automatic ap-
proaches to predicate argument extraction.
Such work has shown that syntax is necessary
for helping to predict the roles of verb arguments
and consequently their verb sense (Gildea and Juras-
fky, 2002; Pradhan et al., 2005; Gildea and Palmer,
2002). However, the definition of models for opti-
mally combining lexical and syntactic constraints is
still far for being accomplished. In particular, the ex-
haustive design and experimentation of lexical and
syntactic features for learning verb classification ap-
pears to be computationally problematic. For exam-
ple, the verb order can belongs to the two VerbNet
classes:
? The class 60.1, i.e., order someone to do some-
thing as shown in: The Illinois Supreme Court or-
dered the commission to audit Commonwealth Edi-
son ?s construction expenses and refund any unrea-
sonable expenses .
? The class 13.5.1: order or request something like
in: ... Michelle blabs about it to a sandwich man
while ordering lunch over the phone .
Clearly, the syntactic realization can be used to dis-
cern the cases above but it would not be enough to
correctly classify the following verb occurrence: ..
ordered the lunch to be delivered .. in Verb class
13.5.1. For such a case, selectional restrictions are
needed. These have also been shown to be use-
ful for semantic role classification (Zapirain et al.,
2010). Note that their coding in learning algorithms
is rather complex: we need to take into account syn-
tactic structures, which may require an exponential
number of syntactic features (i.e., all their possible
substructures). Moreover, these have to be enriched
with lexical information to trig lexical preference.
In this paper, we tackle the problem above
by studying innovative representations for auto-
matic verb classification according to VerbNet and
FrameNet. We define syntactic and semantic struc-
tures capturing essential lexical and syntactic prop-
erties of verbs. Then, we apply similarity between
263
such structures, i.e., kernel functions, which can also
exploit distributional lexical semantics, to train au-
tomatic classifiers. The basic idea of such functions
is to compute the similarity between two verbs in
terms of all the possible substructures of their syn-
tactic frames. We define and automatically extract
a lexicalized approximation of the latter. Then, we
apply kernel functions that jointly model structural
and lexical similarity so that syntactic properties are
combined with generalized lexemes. The nice prop-
erty of kernel functions is that they can be used
in place of the scalar product of feature vectors to
train algorithms such as Support Vector Machines
(SVMs). This way SVMs can learn the association
between syntactic (sub-) structures whose lexical ar-
guments are generalized and target verb classes, i.e.,
they can also learn selectional restrictions.
We carried out extensive experiments on verb
class and frame detection which showed that our
models greatly improve on the state-of-the-art (up
to about 13% of relative error reduction). Such re-
sults are nicely assessed by manually inspecting the
most important substructures used by the classifiers
as they largely correlate with syntactic frames de-
fined in VerbNet.
In the rest of the paper, Sec. 2 reports on related
work, Sec. 3 and Sec. 4 describe previous and our
models for syntactic and semantic similarity, respec-
tively, Sec. 5 illustrates our experiments, Sec. 6 dis-
cusses the output of the models in terms of error
analysis and important structures and finally Sec. 7
derives the conclusions.
2 Related work
Our target task is verb classification but at the same
time our models exploit distributional models as
well as structural kernels. The next three subsec-
tions report related work in such areas.
Verb Classification. The introductory verb classi-
fication example has intuitively shown the complex-
ity of defining a comprehensive feature representa-
tion. Hereafter, we report on analysis carried out in
previous work.
It has been often observed that verb senses tend
to show different selectional constraints in a specific
argument position and the above verb order is a clear
example. In the direct object position of the example
sentence for the first sense 60.1 of order, we found
commission in the role PATIENT of the predicate. It
clearly satisfies the +ANIMATE/+ORGANIZATION
restriction on the PATIENT role. This is not true
for the direct object dependency of the alternative
sense 13.5.1, which usually expresses the THEME
role, with unrestricted type selection. When prop-
erly generalized, the direct object information has
thus been shown highly predictive about verb sense
distinctions.
In (Brown et al., 2011), the so called dynamic
dependency neighborhoods (DDN), i.e., the set of
verbs that are typically collocated with a direct ob-
ject, are shown to be more helpful than lexical in-
formation (e.g., WordNet). The set of typical verbs
taking a noun n as a direct object is in fact a strong
characterization for semantic similarity, as all the
nounsm similar to n tend to collocate with the same
verbs. This is true also for other syntactic depen-
dencies, among which the direct object dependency
is possibly the strongest cue (as shown for example
in (Dligach and Palmer, 2008)).
In order to generalize the above DDN feature, dis-
tributional models are ideal, as they are designed
to model all the collocations of a given noun, ac-
cording to large scale corpus analysis. Their abil-
ity to capture lexical similarity is well established in
WSD tasks (e.g. (Schutze, 1998)), thesauri harvest-
ing (Lin, 1998), semantic role labeling (Croce et al.,
2010)) as well as information retrieval (e.g. (Furnas
et al., 1988)).
Distributional Models (DMs). These models fol-
low the distributional hypothesis (Firth, 1957) and
characterize lexical meanings in terms of context of
use, (Wittgenstein, 1953). By inducing geometrical
notions of vectors and norms through corpus analy-
sis, they provide a topological definition of seman-
tic similarity, i.e., distance in a space. DMs can
capture the similarity between words such as dele-
gation, deputation or company and commission. In
case of sense 60.1 of the verb order, DMs can be
used to suggest that the role PATIENT can be inher-
ited by all these words, as suitable Organisations.
In supervised language learning, when few exam-
ples are available, DMs support cost-effective lexi-
cal generalizations, often outperforming knowledge
based resources (such as WordNet, as in (Pantel et
al., 2007)). Obviously, the choice of the context
264
type determines the type of targeted semantic prop-
erties. Wider contexts (e.g., entire documents) are
shown to suggest topical relations. Smaller con-
texts tend to capture more specific semantic as-
pects, e.g. the syntactic behavior, and better capture
paradigmatic relations, such as synonymy. In partic-
ular, word space models, as described in (Sahlgren,
2006), define contexts as the words appearing in a
n-sized window, centered around a target word. Co-
occurrence counts are thus collected in a words-by-
words matrix, where each element records the num-
ber of times two words co-occur within a single win-
dow of word tokens. Moreover, robust weighting
schemas are used to smooth counts against too fre-
quent co-occurrence pairs: Pointwise Mutual Infor-
mation (PMI) scores (Turney and Pantel, 2010) are
commonly adopted.
Structural Kernels. Tree and sequence kernels
have been successfully used in many NLP applica-
tions, e.g., parse reranking and adaptation, (Collins
and Duffy, 2002; Shen et al., 2003; Toutanova et
al., 2004; Kudo et al., 2005; Titov and Hender-
son, 2006), chunking and dependency parsing, e.g.,
(Kudo and Matsumoto, 2003; Daume? III and Marcu,
2004), named entity recognition, (Cumby and Roth,
2003), text categorization, e.g., (Cancedda et al.,
2003; Gliozzo et al., 2005), and relation extraction,
e.g., (Zelenko et al., 2002; Bunescu and Mooney,
2005; Zhang et al., 2006).
Recently, DMs have been also proposed in in-
tegrated syntactic-semantic structures that feed ad-
vanced learning functions, such as the semantic
tree kernels discussed in (Bloehdorn and Moschitti,
2007a; Bloehdorn and Moschitti, 2007b; Mehdad et
al., 2010; Croce et al., 2011).
3 Structural Similarity Functions
In this paper we model verb classifiers by exploiting
previous technology for kernel methods. In particu-
lar, we design new models for verb classification by
adopting algorithms for structural similarity, known
as Smoothed Partial Tree Kernels (SPTKs) (Croce et
al., 2011). We define new innovative structures and
similarity functions based on LSA.
The main idea of SPTK is rather simple: (i) mea-
suring the similarity between two trees in terms of
the number of shared subtrees; and (ii) such number
also includes similar fragments whose lexical nodes
are just related (so they can be different). The con-
tribution of (ii) is proportional to the lexical similar-
ity of the tree lexical nodes, where the latter can be
evaluated according to distributional models or also
lexical resources, e.g., WordNet.
In the following, we define our models based on
previous work on LSA and SPTKs.
3.1 LSA as lexical similarity model
Robust representations can be obtained through
intelligent dimensionality reduction methods. In
LSA the original word-by-context matrix M is de-
composed through Singular Value Decomposition
(SVD) (Landauer and Dumais, 1997; Golub and Ka-
han, 1965) into the product of three new matrices:
U , S, and V so that S is diagonal and M = USV T .
M is then approximated by Mk = UkSkV Tk , where
only the first k columns of U and V are used,
corresponding to the first k greatest singular val-
ues. This approximation supplies a way to project
a generic term wi into the k-dimensional space us-
ing W = UkS
1/2
k , where each row corresponds to
the representation vectors ~wi. The original statisti-
cal information about M is captured by the new k-
dimensional space, which preserves the global struc-
ture while removing low-variant dimensions, i.e.,
distribution noise. Given two words w1 and w2,
the term similarity function ? is estimated as the
cosine similarity between the corresponding projec-
tions ~w1, ~w2 in the LSA space, i.e ?(w1, w2) =
~w1? ~w2
? ~w1?? ~w2?
. This is known as Latent Semantic Ker-
nel (LSK), proposed in (Cristianini et al., 2001),
as it defines a positive semi-definite Gram matrix
G = ?(w1, w2) ?w1, w2 (Shawe-Taylor and Cris-
tianini, 2004). ? is thus a valid kernel and can be
combined with other kernels, as discussed in the
next session.
3.2 Tree Kernels driven by Semantic Similarity
To our knowledge, two main types of tree kernels
exploit lexical similarity: the syntactic semantic tree
kernel defined in (Bloehdorn and Moschitti, 2007a)
applied to constituency trees and the smoothed
partial tree kernels (SPTKs) defined in (Croce et
al., 2011), which generalizes the former. We report
the definition of the latter as we modified it for our
purposes. SPTK computes the number of common
substructures between two trees T1 and T2 without
explicitly considering the whole fragment space. Its
265
SVP
S
-
NP-1
NN
commission::n
DT
the::d
VBD
TARGET-order::v
NP-SBJ
NNP
court::n
NNP
supreme::n
NNP
illinois::n
DT
the::d
Figure 1: Constituency Tree (CT) representation of verbs.
ROOT
OPRD
IM
VB
audit::v
TO
to::t
OBJ
NN
commission::n
NMOD
DT
the::d
VBD
TARGET-order::v
SBJ
NNP
court::n
NMOD
NNP
supreme::n
NMOD
NNP
illinois::n
NMOD
DT
the::d
Figure 2: Representation of verbs according to the Grammatical Relation Centered Tree (GRCT)
general equations are reported hereafter:
TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2), (1)
where NT1 and NT2 are the sets of the T1?s and T2?s
nodes, respectively and ?(n1, n2) is equal to the
number of common fragments rooted in the n1 and
n2 nodes1. The ? function determines the richness
of the kernel space and thus induces different tree
kernels, for example, the syntactic tree kernel (STK)
(Collins and Duffy, 2002) or the partial tree kernel
(PTK) (Moschitti, 2006).
The algorithm for SPTK?s ? is the follow-
ing: if n1 and n2 are leaves then ??(n1, n2) =
???(n1, n2); else
??(n1, n2) = ??(n1, n2)?
(
?2 +
?
~I1,~I2,l(~I1)=l(~I2)
?d(
~I1)+d(~I2)
l(~I1)?
j=1
??(cn1(~I1j), cn2(~I2j))
)
, (2)
where (1) ? is any similarity between nodes, e.g., be-
tween their lexical labels; (2) ?, ? ? [0, 1] are decay
factors; (3) cn1(h) is the h
th child of the node n1;
(4) ~I1 and ~I2 are two sequences of indexes, i.e., ~I =
(i1, i2, .., l(I)), with 1 ? i1 < i2 < .. < il(I); and (5)
d(~I1) = ~I1l(~I1)?
~I11+1 and d(~I2) = ~I2l(~I2)?
~I21+1.
Note that, as shown in (Croce et al., 2011), the av-
erage running time of SPTK is sub-quadratic in the
number of the tree nodes. In the next section we
show how we exploit the class of SPTKs, for verb
classification.
1To have a similarity score between 0 and 1, a normalization
in the kernel space, i.e. TK(T1,T2)?
TK(T1,T1)?TK(T2,T2)
is applied.
4 Verb Classification Models
The design of SPTK-based algorithms for our verb
classification requires the modeling of two differ-
ent aspects: (i) a tree representation for the verbs;
and (ii) the lexical similarity suitable for the task.
We also modified SPTK to apply different similarity
functions to different nodes to introduce flexibility.
4.1 Verb Structural Representation
The implicit feature space generated by structural
kernels and the corresponding notion of similarity
between verbs obviously depends on the input struc-
tures. In the cases of STK, PTK and SPTK different
tree representations lead to engineering more or less
expressive linguistic feature spaces.
With the aim of capturing syntactic features, we
started from two different parsing paradigms: phrase
and dependency structures. For example, for repre-
senting the first example of the introduction, we can
use the constituency tree (CT) in Figure 1, where the
target verb node is enriched with the TARGET label.
Here, we apply tree pruning to reduce the computa-
tional complexity of tree kernels as it is proportional
to the number of nodes in the input trees. Accord-
ingly, we only keep the subtree dominated by the
target VP by pruning from it all the S-nodes along
with their subtrees (i.e, all nested sentences are re-
moved). To further improve generalization, we lem-
matize lexical nodes and add generalized POS-Tags,
i.e., noun (n::), verb (v::), adjective (::a), determiner
(::d) and so on, to them. This is useful for constrain-
ing similarity to be only contributed by lexical pairs
of the same grammatical category.
266
TARGET-order::v
VBDROOTto::t
TOOPRDaudit::v
VBIM
commission::n
NNOBJthe::d
DTNMOD
court::n
NNPSBJsupreme::n
NNPNMOD
illinois::n
NNPNMOD
the::d
DTNMOD
Figure 3: Representation of verbs according to the Lexical Centered Tree (LCT)
To encode dependency structure information in a
tree (so that we can use it in tree kernels), we use
(i) lexemes as nodes of our tree, (ii) their dependen-
cies as edges between the nodes and (iii) the depen-
dency labels, e.g., grammatical functions (GR), and
POS-Tags, again as tree nodes. We designed two
different tree types: (i) in the first type, GR are cen-
tral nodes from which dependencies are drawn and
all the other features of the central node, i.e., lexi-
cal surface form and its POS-Tag, are added as ad-
ditional children. An example of the GR Centered
Tree (GRCT) is shown in Figure 2, where the POS-
Tags and lexemes are children of GR nodes. (ii) The
second type of tree uses lexicals as central nodes on
which both GR and POS-Tag are added as the right-
most children. For example, Figure 3 shows an ex-
ample of a Lexical Centered Tree (LCT). For both
trees, the pruning strategy only preserves the verb
node, its direct ancestors (father and siblings) and
its descendants up to two levels (i.e., direct children
and grandchildren of the verb node). Note that, our
dependency tree can capture the semantic head of
the verbal argument along with the main syntactic
construct, e.g., to audit.
4.2 Generalized node similarity for SPTK
We have defined the new similarity ?? to be used in
Eq. 2, which makes SPTK more effective as shown
by Alg. 1. ?? takes two nodes n1 and n2 and applies
a different similarity for each node type. The latter is
derived by ? and can be: GR (i.e., SYNT), POS-Tag
(i.e., POS) or a lexical (i.e., LEX) type. In our exper-
iment, we assign 0/1 similarity for SYNT and POS
nodes according to string matching. For LEX type,
we apply a lexical similarity learned with LSA to
only pairs of lexicals associated with the same POS-
Tag. It should be noted that the type-based similarity
allows for potentially applying a different similarity
for each node. Indeed, we also tested an amplifica-
tion factor, namely, leaf weight (lw), which ampli-
fies the matching values of the leaf nodes.
Algorithm 1 ?? (n1, n2, lw)
?? ? 0,
if ?(n1) = ?(n2) = SYNT ? label(n1) = label(n2) then
?? ? 1
end if
if ?(n1) = ?(n2) = POS ? label(n1) = label(n2) then
?? ? 1
end if
if ?(n1) = ?(n2) = LEX ? pos(n1) = pos(n2) then
?? ? ?LEX(n1, n2)
end if
if leaf(n1) ? leaf(n2) then
?? ? ?? ? lw
end if
return ??
5 Experiments
In these experiments, we tested the impact of our dif-
ferent verb representations using different kernels,
similarities and parameters. We also compared with
simple bag-of-words (BOW) models and the state-
of-the-art.
5.1 General experimental setup
We consider two different corpora: one for VerbNet
and the other for FrameNet. For the former, we used
the same verb classification setting of (Brown et al.,
2011). Sentences are drawn from the Semlink cor-
pus (Loper et al., 2007), which consists of the Prop-
Banked Penn Treebank portions of the Wall Street
Journal. It contains 113K verb instances, 97K of
which are verbs represented in at least one VerbNet
class. Semlink includes 495 verbs, whose instances
are labeled with more than one class (including one
single VerbNet class or none). We used all instances
of the corpus for a total of 45,584 instances for 180
verb classes. When instances labeled with the none
class are not included, the number of examples be-
comes 23,719.
The second corpus refers to FrameNet frame clas-
sification. The training and test data are drawn from
the FrameNet 1.5 corpus2, which consists of 135K
sentences annotated according the frame semantics
2http://framenet.icsi.berkeley.edu
267
(Baker et al., 1998). We selected the subset of
frames containing more than 100 sentences anno-
tated with a verbal predicate for a total of 62,813
sentences in 187 frames (i.e., very close to the Verb-
Net datasets). For both the datasets, we used 70% of
instances for training and 30% for testing.
Our verb (multi) classifier is designed with
the one-vs-all (Rifkin and Klautau, 2004) multi-
classification schema. This uses a set of binary
SVM classifiers, one for each verb class (frame) i.
The sentences whose verb is labeled with the class
i are positive examples for the classifier i. The sen-
tences whose verbs are compatible with the class i
but evoking a different class or labeled with none
(no current verb class applies) are added as negative
examples. In the classification phase the binary clas-
sifiers are applied by (i) only considering classes that
are compatible with the target verbs; and (ii) select-
ing the class associated with the maximum positive
SVM margin. If all classifiers provide a negative
score the example is labeled with none.
To learn the binary classifiers of the schema
above, we coded our modified SPTK in SVM-Light-
TK3 (Moschitti, 2006). The parameterization of
each classifier is carried on a held-out set (30% of
the training) and is concerned with the setting of the
trade-off parameter (option -c) and the leaf weight
(lw) (see Alg. 1), which is used to linearly scale
the contribution of the leaf nodes. In contrast, the
cost-factor parameter of SVM-Light-TK is set as the
ratio between the number of negative and positive
examples for attempting to have a balanced Preci-
sion/Recall.
Regarding SPTK setting, we used the lexical simi-
larity ? defined in Sec. 3.1. In more detail, LSA was
applied to ukWak (Baroni et al., 2009), which is a
large scale document collection made up of 2 billion
tokens. M is constructed by applying POS tagging to
build rows with pairs ?lemma, ::POS? (lemma::POS
in brief). The contexts of such items are the columns
of M and are short windows of size [?3,+3], cen-
tered on the items. This allows for better captur-
ing syntactic properties of words. The most frequent
20,000 items are selected along with their 20k con-
texts. The entries of M are the point-wise mutual
3(Structural kernels in SVMLight (Joachims, 2000)) avail-
able at http://disi.unitn.it/moschitti/Tree-Kernel.htm
STK PTK SPTK
lw Acc. lw Acc. lw Acc.
CT - 83.83% 8 84.57% 8 84.46%
GRCT - 84.83% 8 85.15% 8 85.28%
LCT - 77.73% 0.1 86.03% 0.2 86.72%
Br. et Al. 84.64%
BOW 79.08%
SK 82.08%
Table 1: VerbNet accuracy with the none class
STK PTK SPTK
lw Acc. lw Acc. lw Acc.
GRCT - 92.67% 6 92.97% 0.4 93.54%
LCT - 90.28% 6 92.99% 0.3 93.78%
BOW 91.13%
SK 91.84%
Table 2: FrameNet accuracy without the none class
information between them. SVD reduction is then
applied to M, with a dimensionality cut of l = 250.
For generating the CT, GRCT and LCT struc-
tures, we used the constituency trees generated by
the Charniak parser (Charniak, 2000) and the de-
pendency structures generated by the LTH syntactic
parser (described in (Johansson and Nugues, 2008)).
The classification performance is measured with
accuracy (i.e., the percentage of correct classifica-
tion). We also derive statistical significance of the
results by using the model described in (Yeh, 2000)
and implemented in (Pado?, 2006).
5.2 VerbNet and FrameNet Classification
Results
To assess the performance of our settings, we also
derive a simple baseline based on the bag-of-words
(BOW) model. For it, we represent an instance of
a verb in a sentence using all words of the sentence
(by creating a special feature for the predicate word).
We also used sequence kernels (SK), i.e., PTK ap-
plied to a tree composed of a fake root and only one
level of sentence words. For efficiency reasons4, we
only consider the 10 words before and after the pred-
icate with subsequence features of length up to 5.
Table 1 reports the accuracy of different mod-
els for VerbNet classification. It should be noted
that: first, SK produces a much higher accuracy than
BOW, i.e., 82.08 vs. 79.08. On one hand, this is
4The average running time of the SK is much higher than the
one of PTK. When a tree is composed by only one level PTK
collapses to SK.
268
STK PTK SPTK
lw Acc. lw Acc. lw Acc.
CT - 91.14% 8 91.66% 6 91.66%
GRCT - 91.71% 8 92.38% 4 92.33%
LCT - 89.20% 0.2 92.54% 0.1 92.55%
BOW 88.16%
SK 89.86%
Table 3: VerbNet accuracy without the none class
generally in contrast with standard text categoriza-
tion tasks, for which n-gram models show accuracy
comparable to the simpler BOW. On the other hand,
it simply confirms that verb classification requires
the dependency information between words (i.e., at
least the sequential structure information provided
by SK).
Second, SK is 2.56 percent points below the state-
of-the-art achieved in (Brown et al., 2011) (BR), i.e,
82.08 vs. 84.64. In contrast, STK applied to our rep-
resentation (CT, GRCT and LCT) produces compa-
rable accuracy, e.g., 84.83, confirming that syntactic
representation is needed to reach the state-of-the-art.
Third, PTK, which produces more general struc-
tures, improves over BR by almost 1.5 (statistically
significant result) when using our dependency struc-
tures GRCT and LCT. CT does not produce the same
improvement since it does not allow PTK to directly
compare the lexical structure (lexemes are all leaf
nodes in CT and to connect some pairs of them very
large trees are needed).
Finally, the best model of SPTK (i.e, using LCT)
improves over the best PTK (i.e., using LCT) by al-
most 1 point (statistically significant result): this dif-
ference is only given by lexical similarity. SPTK im-
proves on the state-of-the-art by about 2.08 absolute
percent points, which, given the high accuracy of the
baseline, corresponds to 13.5% of relative error re-
duction.
We carried out similar experiments for frame clas-
sification. One interesting difference is that SK im-
proves BOW by only 0.70, i.e., 4 times less than in
the VerbNet setting. This suggests that word order
around the predicate is more important for deriving
the VerbNet class than the FrameNet frame. Ad-
ditionally, LCT or GRCT seems to be invariant for
both PTK and SPTK whereas the lexical similarity
still produces a relevant improvement on PTK, i.e.,
13% of relative error reduction, for an absolute accu-
racy of 93.78%. The latter improves over the state-
50% 
60% 
70% 
80% 
90% 
0% 20% 40% 60% 80% 100% 
Accu
racy 
Percentage of train examples 
SPTK 
BOW 
Brown et al. 
Figure 4: Learning curves: VerbNet accuracy with the
none Class
of-the-art, i.e., 92.63% derived in (Giuglea and Mos-
chitti, 2006), by using STK on CT on 133 frames.
We also carried out experiments to understand
the role of the none class. Table 3 reports on the
VerbNet classification without its instances. This is
of course an unrealistic setting as it would assume
that the current VerbNet release already includes all
senses for English verbs. In the table, we note that
the overall accuracy highly increases and the differ-
ence between models reduces. The similarities play
no role anymore. This may suggest that SPTK can
help in complex settings, where verb class character-
ization is more difficult. Another important role of
SPTK models is their ability to generalize. To test
this aspect, Figure 4 illustrates the learning curves
of SPTK with respect to BOW and the accuracy
achieved by BR (with a constant line). It is impres-
sive to note that with only 40% of the data SPTK can
reach the state-of-the-art.
6 Model Analysis and Discussion
We carried out analysis of system errors and its in-
duced features. These can be examined by apply-
ing the reverse engineering tool5 proposed in (Pighin
and Moschitti, 2010; Pighin and Moschitti, 2009a;
Pighin and Moschitti, 2009b), which extracts the
most important features for the classification model.
Many mistakes are related to false positives and neg-
atives of the none class (about 72% of the errors).
This class also causes data imbalance. Most errors
are also due to lack of lexical information available
to the SPTK kernel: (i) in 30% of the errors, the
argument heads were proper nouns for which the
lexical generalization provided by the DMs was not
5http://danielepighin.net/cms/software/flink
269
VerbNet class 13.5.1
(IM(VB(target))(OBJ))
(VC(VB(target))(OBJ))
(VC(VBG(target))(OBJ))
(OPRD(TO)(IM(VB(target))(OBJ)))
(PMOD(VBG(target))(OBJ))
(VB(target))
(VC(VBN(target)))
(PRP(TO)(IM(VB(target))(OBJ)))
(IM(VB(target))(OBJ)(ADV(IN)(PMOD)))
(OPRD(TO)(IM(VB(target))(OBJ)(ADV(IN)(PMOD))))
VerbNet class 60
(VC(VB(target))(OBJ))
(NMOD(VBG(target))(OPRD))
(VC(VBN(target))(OPRD))
(NMOD(VBN(target))(OPRD))
(PMOD(VBG(target))(OBJ))
(ROOT(SBJ)(VBD(target))(OBJ)(P(,)))
(VC(VB(target))(OPRD))
(ROOT(SBJ)(VBZ(target))(OBJ)(P(,)))
(NMOD(SBJ(WDT))(VBZ(target))(OPRD))
(NMOD(SBJ)(VBZ(target))(OPRD(SBJ)(TO)(IM)))
Table 4: GRCT fragments
available; and (ii) in 76% of the errors only 2 or less
argument heads are included in the extracted tree,
therefore tree kernels cannot exploit enough lexical
information to disambiguate verb senses. Addition-
ally, ambiguity characterizes errors where the sys-
tem is linguistically consistent but the learned selec-
tional preferences are not sufficient to separate verb
senses. These errors are mainly due to the lack of
contextual information. While error analysis sug-
gests that further improvement is possible (e.g. by
exploiting proper nouns), the type of generalizations
currently achieved by SPTK are rather effective. Ta-
ble 4 and 5 report the tree structures characterizing
the most informative training examples of the two
senses of the verb order, i.e. the VerbNet classes
13.5.1 (make a request for something) and 60 (give
instructions to or direct somebody to do something
with authority).
In line with the method discussed in (Pighin and
Moschitti, 2009b), these fragments are extracted as
they appear in most of the support vectors selected
during SVM training. As easily seen, the two classes
are captured by rather different patterns. The typ-
ical accusative form with an explicit direct object
emerges as characterizing the sense 13.5.1, denot-
ing the THEME role. All fragments of the sense 60
emphasize instead the sentential complement of the
verb that in fact expresses the standard PROPOSI-
TION role in VerbNet. Notice that tree fragments
correspond to syntactic patterns. The a posteriori
VerbNet class 13.5.1
(VP(VB(target))(NP))
(VP(VBG(target))(NP))
(VP(VBD(target))(NP))
(VP(TO)(VP(VB(target))(NP)))
(S(NP-SBJ)(VP(VBP(target))(NP)))
VerbNet class 60
(VBN(target))
(VP(VBD(target))(S))
(VP(VBZ(target))(S))
(VBP(target))
(VP(VBD(target))(NP-1)(S(NP-SBJ)(VP)))
Table 5: CT fragments
analysis of the learned models (i.e. the underlying
support vectors) confirm very interesting grammati-
cal generalizations, i.e. the capability of tree kernels
to implicitly trigger useful linguistic inductions for
complex semantic tasks. When SPTK are adopted,
verb arguments can be lexically generalized into
word classes, i.e., clusters of argument heads (e.g.
commission vs. delegation, or gift vs. present). Au-
tomatic generation of such classes is an interesting
direction for future research.
7 Conclusion
We have proposed new approaches to characterize
verb classes in learning algorithms. The key idea is
the use of structural representation of verbs based on
syntactic dependencies and the use of structural ker-
nels to measure similarity between such representa-
tions. The advantage of kernel methods is that they
can be directly used in some learning algorithms,
e.g., SVMs, to train verb classifiers. Very interest-
ingly, we can encode distributional lexical similar-
ity in the similarity function acting over syntactic
structures and this allows for generalizing selection
restrictions through a sort of (supervised) syntactic
and semantic co-clustering.
The verb classification results show a large im-
provement over the state-of-the-art for both Verb-
Net and FrameNet, with a relative error reduction
of about 13.5% and 16.0%, respectively. In the fu-
ture, we plan to exploit the models learned from
FrameNet and VerbNet to carry out automatic map-
ping of verbs from one theory to the other.
Acknowledgements This research is partially sup-
ported by the European Community?s Seventh Frame-
work Programme (FP7/2007-2013) under grant numbers
247758 (ETERNALS), 288024 (LIMOSINE) and 231126
(LIVINGKNOWLEDGE). Many thanks to the reviewers
for their valuable suggestions.
270
References
