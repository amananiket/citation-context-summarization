Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 276?285,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Wikipedia as Frame Information Repository
Sara Tonelli
FBK-irst
I-38100, Trento, Italy
satonelli@fbk.eu
Claudio Giuliano
FBK-irst
I-38100, Trento, Italy
giuliano@fbk.eu
Abstract
In this paper, we address the issue of au-
tomatic extending lexical resources by ex-
ploiting existing knowledge repositories.
In particular, we deal with the new task
of linking FrameNet and Wikipedia us-
ing a word sense disambiguation system
that, for a given pair frame ? lexical unit
(F, l), finds the Wikipage that best ex-
presses the the meaning of l. The mapping
can be exploited to straightforwardly ac-
quire new example sentences and new lex-
ical units, both for English and for all lan-
guages available in Wikipedia. In this way,
it is possible to easily acquire good-quality
data as a starting point for the creation of
FrameNet in new languages. The evalua-
tion reported both for the monolingual and
the multilingual expansion of FrameNet
shows that the approach is promising.
1 Introduction
Many applications in the context of natural lan-
guage processing or information retrieval have
proved to convey significant improvement by ex-
ploiting lexical databases with high-quality anno-
tation such as FrameNet (Fillmore et al, 2003)
and WordNet (Fellbaum, 1998). Nevertheless, the
practical use of similar resources is often biased
by their limited coverage because manual anno-
tation is time-consuming and requires a relevant
financial effort. For this reason, some research ac-
tivities have focused on the automatic enrichment
of such resources with annotated information in
(near) manual quality. The main strategy proposed
was the mapping between resources in order to
reciprocally enrich different lexical databases by
linking their information layers. This has proved
to be useful in several tasks, from verb classifica-
tion (Chow and Webster, 2007) to semantic role
labeling (Giuglea and Moschitti, 2006), open text
semantic parsing (Shi and Mihalcea, 2004) and
textual entailment (Burchardt and Frank, 2006).
In this work, we focus on the automatic enrich-
ment of the FrameNet database for English and we
propose a new framework to extend this procedure
to new languages. While similar works in the past
have mainly proposed to automatically extend the
FrameNet database by mapping frames and Word-
Net synsets (Shi and Mihalcea (2005), Johans-
son and Nugues (2007), and Tonelli and Pighin
(2009)), we present an explorative approach that
for the first time exploits Wikipedia to this pur-
pose. In particular, given a lexical unit l belong-
ing to a frame F , we devise a strategy to link
l to the Wikipedia article that best captures the
sense of l in F . This is basically a word disam-
biguation (WSD) problem (Erk, 2004) and to this
purpose we employ a state-of-the-art WSD sys-
tem (Gliozzo et al, 2005). The mapping between
(F, l) pairs and Wikipedia pages could then be ex-
ploited for three further subtasks: (a) automati-
cally extract from Wikipedia all sentences point-
ing to the Wikipage mapped with (F, l) and assign
them to F ; (b) automatically expand the lexical
units sets in the English FrameNet by exploiting
the redirecting and linking strategy of Wikipedia;
and (c) since Wikipedia is available in 260 lan-
guages, use the English Wikipedia article linked to
(F, l) as a bridge to carry out sentence and lexical
unit retrieval in other languages. The set of auto-
matically collected data would represent the start-
ing point for the creation of FrameNet in new lan-
guages. In fact, having a repository of sentences
extracted from Wikipedia which have already been
divided by sense would significantly speed up the
annotation process. In this way, the annotators
would not need to extract all sentences in a cor-
pus containing l and classify them by sense. In-
stead, they should simply validate the given sen-
tences and assign the correct frame elements.
276
In the following, we start by providing a brief
overview of FrameNet and Wikipedia and we
present their structure and organization. Next, we
describe the algorithm for mapping lexical units
and Wikipages and the word sense disambigua-
tion algorithm employed by the system. In Sec-
tion 5 we describe the dataset used in the first ex-
periment and report evaluation results of the map-
ping between (F, l) pairs and Wikipedia senses. In
Section 6 we describe an application of the map-
ping, i.e. the automatic enrichment of English
FrameNet. We describe the data extraction pro-
cess and evaluate the quality of the data. In Section
7 we describe and evaluate another application of
the mapping, i.e. the acquisition of data for the
automatic creation of Italian FrameNet using the
Italian Wikipedia. Finally, we draw conclusions
and present future research directions.
2 FrameNet and Wikipedia
FrameNet (Fillmore et al, 2003) is a lexical re-
source for English based on corpus evidence,
whose conceptual model comprises a set of proto-
typical situations called frames, the frame-evoking
words or expressions called lexical units (LUs)
and the roles or participants involved in these situ-
ations, called frame elements. All lexical units be-
longing to the same frame have similar semantics
but, differently from WordNet synsets, they can
belong to different categories and present differ-
ent parts of speech. For example, the KILLING
frame is described in the FrameNet database
1
as ?A Killer or Cause causes the death of the
Victim?. The elements in capitals are the se-
mantic roles (frame elements) typically involved
in the KILLING situation. The frame definition
comes also with the list of frame-evoking lexical
units, namely annihilate.v, annihilation.n, butch-
ery.n, carnage.n, crucify.v, deadly.a, etc. Since
FrameNet is a corpus-based resource, every lexi-
cal unit should be instantiated by a set of exam-
ple sentences, where the frame elements are anno-
tated as well. Instead, FrameNet is still an ongoing
project and in the latest release (v. 1.3) there are
about 3,380 lexical units out of 10,195 that come
with no example sentences. In this work we focus
on these lexical units and propose how to automat-
ically collect the missing sentences. Anyhow, the
algorithm we propose is suitable also for expand-
ing sentence sets already present in FrameNet.
1
http://framenet.icsi.berkeley.edu
Wikipedia
2
is one of the largest online reposito-
ries of encyclopedic knowledge, with millions of
articles available for a large number of languages
(>2,800,000 for English). The article (or page)
is the basic entry in Wikipedia. Every article has
an unique reference, i.e., one or more words that
identify the page and are present in its URL. For
example, Ball (dance) identifies the page that de-
scribes several types of ball intended as formal
dance, while Dance (musical form) describes the
dance as musical genre. Every Wikipedia article
is linked to others, and in the body of every page
there are plenty of links to connect the most rel-
evant terms to other pages. Another important
attribute is the presence of about 3,000,000 redi-
rection pages, that given an identifier that is not
present in Wikipedia, automatically display the
page with the most semantically similar identi-
fier (for example Killing is redirected to the Mur-
der page). Wikipedia contains also more than
100,000 disambiguation pages listing all senses
(pages) for an ambiguous entity. For example,
Book has 9 senses, which correspond to 9 dif-
ferent articles. Wikipedia structure and quality
make this resource particularly suitable for infor-
mation extraction and word sense disambiguation
tasks (Csomai and Mihalcea (2008) and Milne and
Witten (2008)). In fact, page references can be
seen as senses and Wikipedia as a large sense in-
ventory. From this point of view, also linking
a lexical unit to the correct Wikipedia page is a
word sense disambiguation issue because it im-
plies recognizing what meaning the lexical unit
has in the given frame. For example, dance.n
in the SOCIAL EVENT frame should be linked to
Ball (dance) and not to Dance (musical form).
3 The Mapping Algorithm
In this section, we describe how to map a frame
? lexical unit pair (F, l) into the Wikipedia arti-
cle that best captures the sense of l as defined in
F . The mapping problem is casted as a supervised
WSD problem, in which l must be disambiguated
using F to provide the context and Wikipedia to
provide the sense inventory and the training data.
Even if the idea of using Wikipedia links for dis-
ambiguation is not novel (Cucerzan, 2007), it is
applied for the first time to FrameNet lexical units,
considering a frame as a sense definition. The pro-
posed algorithm is summarized as follows:
2
http://en.wikipedia.org
277
Step 1 For each lexical unit l, we collect from
the English Wikipedia dump
3
all contexts
4
where l
is the anchor of an internal link (wiki link). The set
of targets represents the senses of l in Wikipedia
and the contexts are used as labelled training ex-
amples. For example, the lexical unit building.n in
the frame Buildings is an anchor in 708 different
contexts that point to 42 different Wikipedia pages
(senses).
Step 2 The set of contexts with their correspond-
ing senses is then used to train the WSD system
described in Section 4. For example, the context
?The building, which date from the mid-to-late
19th century, were built in a variety of High Victo-
rian architectural styles.? is a training example for
the sense defined by the Wikipedia page Building.
Step 3 Finally, the disambiguation model
learned in the previous step is used to map a pair
(F, l) to a Wikipedia article. (F, l) is represented
as a fictitious-context derived by aggregating the
frame definition and all lexical units associated to
F . We used the term ?fictitious-context? to re-
mark the slight difference in structure compared
with the training contexts (i.e., the Wikipedia
paragraphs). For example, ?. . . structures form-
ing an enclosure and providing protection from
the elements . . . acropolis arena auditorium bar
building . . . ? is the fictitious-context built for
the pair (Buildings, building.n). The sense, i.e.,
the Wikipedia article, assigned to the fictitious-
context by the disambiguation algorithm uniquely
defines the mapping. The previous example is as-
signed to the Wikipedia page Building.
4 The WSD Algorithm
Gliozzo et al (2005) proposed an elegant approach
to WSD based on kernel methods. The algorithm
proved effective at Senseval-3 (Mihalcea and Ed-
monds, 2004) and, nowadays, it still represents
the state-of-the-art in WSD (Pradhan et al, 2007).
Specifically, they addressed these issues: (i) inde-
pendently modeling domain and syntagmatic as-
pects of sense distinction to improve feature rep-
resentativeness; and (ii) exploiting external knowl-
edge acquired from unlabeled data, with the pur-
pose of drastically reducing the amount of labeled
3
http://download.wikimedia.org/enwiki/
20090306
4
A context corresponds to a line of text in the Wikipedia
dump and it is represented as a paragraph in a Wikipedia ar-
ticle.
training data. The first direction is based on the
linguistic assumption that syntagmatic and domain
(associative) relations are crucial for representing
sense distictions, but they are originated by differ-
ent phenomena. Regarding the second direction, it
is possible to obtain a more accurate prediction by
taking into account unlabeled data relevant for the
learning problem (Chapelle et al, 2006).
On the other hand, kernel methods are theoret-
ically well founded in statistical learning theory
and shown good empirical results in many appli-
cations (Shawe-Taylor and Cristianini, 2004). The
strategy adopted by kernel methods consists of
splitting the learning problem into two parts. They
first embed the input data in a suitable feature
space, and then use a linear algorithm (e.g., sup-
port vector machines) to discover nonlinear pat-
terns in the input space. The kernel function is
the only task-specific component of the learning
algorithm. Thus, to develop a WSD system, one
only needs to define appropriate kernel functions
to represent the domain and syntagmatic aspects
of sense distinction and to exploit the properties
of kernel functions in order to define a composite
kernel that combines and extends individual ker-
nels.
The WSD system described in the following
consists of a composite kernel (Section 4.3) that
combines the domain and syntagmatic kernels.
The former (Section 4.1) models the domain as-
pects of sense distinction, the latter (Section 4.2)
represents the syntagmatic aspects of sense dis-
tinction.
4.1 Domain Kernel
It is been shown that domain information is fun-
damental for WSD (Magnini et al, 2002). For in-
stance, the (domain) polysemy between the com-
puter science and the medicine senses of the word
?virus? can be solved by considering the domain
of the context in which it appears.
In the context of kernel methods, domain infor-
mation can be exploited by defining a kernel func-
tion that estimates the domain similarity between
the contexts of the word to be disambiguated. The
simplest method to estimate the domain similarity
between two texts is to compute the cosine simi-
larity of their vector representations in the vector
space model (VSM). The VSM is a k-dimensional
space R
k
, in which the text t
j
is represented by
a vector
~
t
j
, where the i
th
component is the term
278
frequency of the term w
i
in t
j
. However, such an
approach does not deal well with lexical variabil-
ity and ambiguity. For instance, despite the fact
that the sentences ?he is affected by AIDS? and
?HIV is a virus? express concepts closely related,
their similarity is zero in the VSM because they
have no words in common (they are represented
by orthogonal vectors). On the other hand, due
to the ambiguity of the word ?virus? , the simi-
larity between the sentences ?the laptop has been
infected by a virus? and ?HIV is a virus? is greater
than zero, even though they convey very different
messages.
To overcome this problem, Gliozzo et al (2005)
introduced the domain model (DM) and show how
to define a domain VSM in which texts and terms
are represented in a uniform way. A DM is com-
posed of soft clusters of terms. Each cluster rep-
resents a semantic domain, that is, a set of terms
that often co-occur in texts having similar topics.
A DM is represented by a k ? k
?
rectangular ma-
trix D, containing the degree of association among
terms and domains.
The matrix D is used to define a function D :
R
k
? R
k
?
, that maps the vector
~
t
j
represented in
the standard VSM, into the vector
~
t
?
j
in the domain
VSM. D is defined by
D(
~
t
j
) =
~
t
j
(I
IDF
D) =
~
t
?
j
, (1)
where
~
t
j
is represented as a row vector, I
IDF
is a
k?k diagonal matrix such that i
IDF
i,i
= IDF (w
i
),
and IDF (w
i
) is the inverse document frequency
of w
i
.
In the domain space, the similarity is esti-
mated by taking into account second order rela-
tions among terms. For example, the similarity of
the two sentences ?He is affected by AIDS? and
?HIV is a virus? is very high, because the terms
AIDS, HIV and virus are strongly associated with
the domain medicine.
Singular valued decomposition (SVD) is used to
acquire in a unsupervised way the DM from a cor-
pus represented by its term-by-document matrix
T. SVD decomposes the term-by-document ma-
trix T into three matrixes T ' V?
k
?
U
T
, where
V and U are orthogonal matrices (i.e., V
T
V = I
and U
T
U = I) whose columns are the eigenvec-
tors of TT
T
and T
T
T respectively, and ?
k
?
is
the diagonal k ? k matrix containing the highest
k
?
 k eigenvalues of T, and all the remaining
elements set to 0. The parameter k
?
is the dimen-
sionality of the domain VSM and can be fixed in
advance. Under this setting, the domain matrix D
is defined by
D = I
N
V
p
?
k
?
(2)
where I
N
is a diagonal matrix such that i
N
i,i
=
1
q
?
~
w
?
i
,
~
w
?
i
?
,
~
w
?
i
is the i
th
row of the matrix V
?
?
k
?
.
The domain kernel is explicitly defined by
K
D
(t
i
, t
j
) = ?D(t
i
),D(t
j
)?, (3)
where D is the domain mapping defined in Equa-
tion 1. Finally, the domain kernel is further ex-
tended to include the standard bag-of-word kernel.
4.2 Syntagmatic Kernel
Kernel functions are not restricted to operate on
vectorial objects ~x ? R
k
. In principle, kernels
can be defined for any kind of object representa-
tion, such as strings and trees. As syntagmatic re-
lations hold among words collocated in a partic-
ular temporal order, they can be modeled by ana-
lyzing sequences of words. Therefore, the string
kernel (Shawe-Taylor and Cristianini, 2004) is a
valid tool to represent such relations. It counts
how many times a (non-contiguous) subsequence
of symbols u of length n occurs in the input string
s, and penalizes non-contiguous occurrences ac-
cording to the number of gaps they contain. For-
mally, let V be the vocabulary, the feature space
associated with the string kernel of length n is in-
dexed by a set I of subsequences over V of length
n. The (explicit) mapping function is defined by
?
n
u
(s) =
X
i:u=s(i)
?
l(i)
, u ? V
n
, (4)
where u = s(i) is a subsequence of s in the posi-
tions given by the tuple i, l(i) is the length spanned
by u, and ? ?]0, 1] is the decay factor used to pe-
nalize non-contiguous subsequences.
The associated string kernel is defined by
K
n
(s
i
, s
j
) = ??
n
(s
i
), ?
n
(s
j
)? =
X
u?V
n
?
n
(s
i
)?
n
(s
j
)
(5)
Gliozzo et al (2005) modified the generic def-
inition of the string kernel in order to take into
account (sparse) collocations. Specifically, they
defined syntagmatic kernels as a combination of
string kernels applied to sequences of words in a
fixed-size window centered on the word to be dis-
ambiguated. This formulation allows estimating
the number of common (sparse) subsequences of
279
words (i.e., collocations) between two examples,
in order to capture syntagmatic similarity. The
syntagmatic kernel is defined by
K
S
(s
i
, s
j
) =
p
X
n=1
K
n
(s
i
, s
j
), (6)
where K
n
is the string kernel defined in Equation
5 and the parameter n represents the length of the
subsequences analyzed when estimating the sim-
ilarity between contexts. Notice that the syntag-
matic kernel is only effective for those fictitious
contexts in which the lexical units do occur in
meaningful sentences, however this is not guaran-
teed for the lexical units without examples.
4.3 Composite Kernel
Finally, to combine domain and syntagmatic infor-
mation, the composite kernel is defined by
K
WSD
(t
i
, t
j
) =
?
K
D
(t
i
, t
j
) +
?
K
S
(t
i
, t
j
), (7)
where
?
K
D
and
?
K
S
are normalized kernels defined
in Equation 3 and 6, respectively.
5
It follows di-
rectly from the explicit construction of the feature
space and from closure properties of kernels that
it is a valid kernel.
5 Mapping task
In this section we report the first experiment,
namely the mapping between (F, l) pairs and a
Wikipedia pages. We describe the experimental
setup and then present the corresponding evalua-
tion.
5.1 Experimental setup
We applied our algorithm to all lexical units that
do not have any example sentence in the FrameNet
database. In principle, the proposed approach can
be applied to every lexical unit, and we expect the
algorithm performance to improve if some exam-
ple sentences are already available because they
could be added to the fictitious-context used to
represent (F, l) in the system. Nevertheless, in this
explorative study we wanted to focus on the harder
cases, even if results are likely to be worse than on
the whole FrameNet database.
In FrameNet, 3,305 (F, l) pairs have no exam-
ple sentences (536 pairs with adjectival LU, 1313
verbal LU, 1456 nominal LU). Since Wikipedia is
basically a resource organized by concepts, which
5
?
K(x
i
, x
j
) =
K(x
i
,x
j
)
?
K(x
j
,x
j
)K(x
i
,x
i
)
are generally expressed by nouns, we decided to
restrict our experiment to nominal lexical units.
Besides, many verbal and adjectival concepts in
Wikipedia are redirected to nominal identifiers.
So, we randomly selected 900 pairs with nominal
lexical units. For the moment, we decided to dis-
card lexical units expressed by multiwords (about
150), which will be taken into account in a future
version of our system. The average ambiguity of
the 900 LUs considered is 1.24 in FrameNet. In-
stead, every LU corresponds to about 35 candidate
senses in Wikipedia.
In order to perform WSD, we built the domain
model from the 200,000 most visited Wikipedia
articles. After removing terms that occur less than
5 times, the resulting dictionaries contain about
300,000 terms. We used the SVDLIBC pack-
age
6
to compute the SVD, truncated to 100 di-
mensions. The experiments were performed using
the SVM package LIBSVM (Chang and Lin, 2001)
customized to embed the kernels described in Sec-
tion 4.
5.2 Evaluation
In this first evaluation step, we focus on the quality
of the mapping between (F, l) pairs and Wikipedia
articles. In order to evaluate the system output,
we created a gold standard where 250 (F, l) pairs
randomly extracted from the nominal subset de-
scribed above have been manually linked to the
Wikipedia page (if available) that best corresponds
to the meaning of l in F . The pairs have been cho-
sen in order to maximize the frame variability, i.e.
every pair corresponds to a different frame. Since
our gold standard contains 34% of all frames in
the FrameNet database, we believe that, despite its
limited size, it is well representative of FrameNet
characteristics. Evaluation was carried out com-
paring the system output against the gold stan-
dard. Results are reported in Table 1. The base-
line was computed considering the most frequent
sense of every lexical unit in Wikipedia. This ele-
ment is obtained by taking into account all occur-
rences in Wikipedia where the lexical unit LU we
consider is anchored to a given page. The most
frequent sense for LU is the page to which LU is
most frequently linked in Wikipedia. Since about
14% of the lexical units in the gold standard are
not present in Wikipedia, we also estimated an up-
per bound accuracy of 0.86. This confirms our in-
6
http://tedlab.mit.edu/
?
dr/svdlibc/
280
tuition that FrameNet and Wikipedia are linkable
resources to a large extent and that our task is well-
founded.
Accuracy
Baseline 0.66
System output 0.71
Upper bound 0.86
Table 1: Accuracy evaluation.
Wrong assignments include also problematic
cases that are not directly connected to proper sys-
tem errors. One of the most relevant issues is the
different granularity between FrameNet frames
and Wikipages. For example, the NETWORK
frame is defined as ?a set of entities of the same
or similar types (Nodes) are linked to each other
by Connections to form a Network allowing for
the flow of information, resources, etc.?. Even
if the listed lexical units (network.n and web.n)
and some examples refer to the informatics do-
main, the situation described in the FrameNet
database is more general. Wikipedia instead lists
several pages that may be seen as subdomains
of NETWORK such as Computer network, So-
cial network, Telecommunications network, etc.
In the future, it may be worth modifying the sys-
tem in order to allow multiple assignments of
Wikipages for every frame.
In other cases, frame definitions seem not to
be very consistent and it is very difficult to dis-
criminate between two frames even for a human
annotator. For example, ESTIMATED VALUE and
ESTIMATING include both estimation.n as lexical
unit, but since their frame definitions are almost
the same and the other lexical units in the same
frame are not discriminative, the system links both
(F, l) pairs to the same Wikipedia article.
6 English FrameNet expansion
In the following part of the experiment, we want
to investigate to what extent the FrameNet ?
Wikipedia mapping can be effectively applied to
automatically expand the FrameNet database with
new example sentences, and eventually to acquire
new lexical units. For every (F, l) pair, we con-
sider the linked Wikipedia sense s and extract all
sentences C
s
in Wikipedia with a reference to
s. In this way, we can assume that, if s was
linked to (F, l), C
s
can be included in the exam-
ple sentences of F . This repository of sentences
is already divided by sense and can significantly
speed-up manual annotation. On the other hand,
the extracted sentences could enrich the training
set of machine learning systems for frame annota-
tion to improve the frame identification step. In
fact, this task has raised growing interest in the
NLP community, with a devoted challenge at the
last SemEval campaign (Baker et al, 2007).
This retrieval process allows also to ex-
tract from C
s
all words W
s
that have an
embedded reference to s in the form <a
href=?/wiki/Wiki Sense?...>word</a>. In this
way, W
s
are automatically included in F as new
lexical units. In this phase, redirecting links are
very useful because they automatically connect a
word or expression to its nearest sense in case
there is no specific page for this word. The infor-
mation about redirecting allows also to account for
orthographic variations of the same lexical unit,
for example collectible is redirected to collectable.
We explain the data extraction process in
the light of an example from our dataset.
Our WSD system assigned to the (F, l) pair
(WORD RELATIONS ? homonym.n) the Wikipage
http://en.wikipedia.org/wiki/Homonym.
So, we extracted from the English Wikipedia
dump all sentences where the anchor <a
href=?/wiki/Homonym?... > appears and as-
sumed that the word or multiword expression that
is linked to the Homonym site may be a good can-
didate as lexical unit for the WORD RELATIONS
frame. In this case, the example sentences were
186. Apart from homonym, the candidate lexical
units are homograph, homophone, homophonous,
homonymic, heteronym, same. Among them, only
the latter is not appropriate, even if the sentence
where it occurs is semantically connected to the
WORD RELATIONS frame: ?In Hebrew the word
?thus? has the same triconsonantal root?. Instead,
homonymic and heteronym can be acquired as
new lexical units for WORD RELATIONS, and
homograph, for which no example sentences
are provided in FrameNet, can be automatically
instantiated by a set of examples.
6.1 Experimental setup
We considered 893 frame ? lexical unit pairs as-
signed to Wikipedia pages following the algorithm
described in Section 3. We discarded 7 pairs for
which the system reported an assignment failure,
i.e. the best sense delivered is the disambigua-
281
tion page. Then we extracted a set of sentences
for every (F, l) pair as described in the previous
paragraph. Statistics about the retrieved data is re-
ported in Table 2.
English Wikipedia
(F, l) pairs 893
N. of extracted sents 964,268
Avg. sents per (F, l) 1,080
Table 2: Extracted data from English Wikipedia
6.2 Evaluation
The dimension of the extracted corpus does not
allow to carry out a comprehensive evaluation.
For this reason, we manually evaluated 1,000 sen-
tences, i.e. we considered 20 (F, l) pairs, and for
each of them we evaluated 50 sentences extracted
from our large repository. Both (F, l) pairs and
the assigned sentences were randomly selected.
In particular, the 20 (F, l) pairs do not contain
only correctly assigned pairs, in fact three of them
are wrong. Anyhow, the 20 pairs seem to be
a representative subset of the 893 pairs consid-
ered in our experiment because they include both
monosemic lexical units (gynaecology.n in MED-
ICAL SPECIALTIES) and more ambiguous ones
(club.n in the WEAPON frame).
Our evaluation shows that 78% of the sentences
were correctly linked to (F, l) pairs. This value is
higher that the mapping accuracy between (F, l)
and Wikipages reported in Section 5.2. In fact,
we noticed that even if the Wikipage assigned to
(F, l) is not the article that best corresponds to the
meaning of l in F , some sentences pointing to it
may be appropriate to express l.
As we already mentioned in Section 5.2,
the different granularity of the information en-
coded by frames and Wikipages impacts on
the output quality. For example, conversion.n
in CAUSE CHANGE has a causative meaning,
while it implies a personal process in UN-
DERGO CHANGE. The mapping, instead, links
(CAUSE CHANGE ? conversion.n) to the Reli-
gious conversion page, and all the sentences col-
lected point to religious conversion, regardless of
their causative form or not. Another characteristic
of this approach is that we can acquire new lexi-
cal units regardless of their part-of-speech, even if
we start from nominal lexical units. This proves
that we do not need to apply the initial mapping to
verbal or adjectival LUs to obtain new data for all
parts of speech. For example, we linked (MEDI-
CAL SPECIALTIES ? gynaecology.n) to the Gynae-
cology Wikipage. Consequently, we could include
the adjective gynaecologic, pointing to the Gy-
naecology page, into the MEDICAL SPECIALTIES
frame for sentences like ?Fellowship training in a
gynaecologic subspeciality can range from one to
four years?. However, this advantage can also turn
into a weakness, because gynaecologist is also
redirected to the Gynaecology page, but it belongs
to MEDICAL PROFESSIONALS and should not be
included into MEDICAL SPECIALTIES.
For the 20 (F, l) pairs considered in the given
sentences, it was possible also to retrieve 8 lex-
ical units that are not present in FrameNet, for
example billy-club for the WEAPON frame. Ex-
ploiting redirections and anchoring strategies, our
induction method can account for orthographical
variations, for example it acquires both memorize
and memorise. On the other hand, also misspelled
words may be collected, for instance gynaecolo-
gial instead of gynaecological.
7 Multilingual FrameNet expansion
One of the great advantages of Wikipedia is its
availability in several languages. The English ver-
sion is by far the most extended, but a considerable
repository of pages is available also for other lan-
guages, esp. European ones. In general, articles on
the same object in different languages are edited
independently and do not have to be translations
of one another, but are linked to each other by their
authors. In this way, the multilingual versions of
Wikipedia can be easily exploited to build compa-
rable corpora, with connected Wikipages in differ-
ent languages dealing with the same contents.
In this research step, we focus on this aspect of
Wikipedia and propose a methodology that, using
the English Wikipages as a bridge, automatically
acquires new lexical units and example sentences
also for other languages. This would represent the
starting point towards the creation of FrameNet
for new languages. Indeed, FrameNet structure
comprises a language-independent level of infor-
mation, namely frame and frame element defini-
tions, and a language-dependent one, i.e. the lex-
ical units and the example sentences. This makes
the resource particularly suitable to corpus-based
(semi) automatic creation of FrameNet for new
languages, because the descriptive part can be pre-
282
served and the language-dependent layer can be
populated with new instances in other languages
(Crespo and Buitelaar, 2008).
We apply our extraction algorithm to the Italian
Wikipedia. Since several approaches have been
experimented to (semi) automatically build Italian
FrameNet using WordNet (De Cao et al (2008)
and Tonelli and Pighin (2009)), we believe that
our new proposal to exploit Wikipedia may be of
interest in the research community. Anyhow, the
approach can be exploited in principle for every
language available in Wikipedia.
7.1 Experimental setup
Similarly to the data extraction process described
in Section 6, we consider for every (F, l) pair in
English the linked Wikipedia sense s, in English
as well. Then, we retrieve the Italian Wikipedia
sense s
i
linked to s and extract all sentences C
i
in the Italian Wikipedia dump
7
with a reference to
s
i
. In this way, we can assume that C
i
are exam-
ple sentences of F and that the words or expres-
sions W
i
in C
i
containing an embedded reference
to s
i
are good candidate lexical units of F in the
Italian FrameNet. For example, if we link http:
//en.wikipedia.org/wiki/Court to the JUDI-
CIAL BODY frame, we first retrieve the Italian
version of the site http://it.wikipedia.org/
wiki/Tribunale. Then, with a top-down strat-
egy, we further extract all Italian sentences point-
ing to the Tribunale page and acquire as lexi-
cal units all words with an embedded reference to
this concept, for example tribunale and corte. In
this way, we can include the extracted lexical units
and the sentences where they occur in the JUDI-
CIAL BODY frame for Italian.
Given the 893 (F, l) pairs in English and the
linked Wikipedia senses described in 6.2, we first
extracted the Italian Wikipages that are linked to
the English ones. Then for every linked Wikipage
in Italian, we retrieved all sentences with a refer-
ence pointing to that page in the Italian Wikipedia
dump. Statistics about the extracted data are re-
ported in Table 3.
Since the Italian Wikipedia is about one fifth of
the English one, it was not possible to map ev-
ery English Wikipage with an Italian article. In
fact, only 371 senses out of 893 in English were
linked to an Italian page. Also the average num-
7
http://download.wikimedia.org/itwiki/
20090203
Italian Wikipedia
Linked Wikipages in Italian 371
N. of extracted sents 23,078
Avg. sents per Italian sense 62
Table 3: Extracted data from Italian Wikipedia
ber of sentences extracted for every sense is much
smaller (62 vs. 1,080). Anyhow, this does not rep-
resent a problem because in the English FrameNet,
the lexical units whose annotation is considered
to be complete are usually instantiated by set of
20 annotated sentences on average. So, according
to the FrameNet standard, 60 sentences are more
than enough to represent the meaning of a lexical
unit in a frame.
7.2 Evaluation
In this evaluation part, we took into account 1,000
sentences, in order to have a comparable dataset
w.r.t. the evaluation for English. However, the sets
of Italian sentences extracted for every (F, l), i.e.
for every Wikipedia article, were much smaller,
so we increased the number of randomly chosen
(F, l) pairs to 80. Our evaluation is focused on the
quality of the sentences and aims at assessing if the
given sentences are correctly assigned to the (F, l)
pairs. We report 69% accuracy, which is 9% lower
than for English. Apart from the same errors and
issues reported for English, a decrease in perfor-
mance can be explained by the fact that, since less
articles are present w.r.t. the English version, redi-
rections and internal links tend to be less precise
and fine-grained. For example, the word ?diritti?
in the sense of ?(human) rights? redirects to the ar-
ticle about Diritto, corresponding to Law as a sys-
tem of rules. On the contrary, Law and Rights have
two different pages in English. Besides, the differ-
ent quality of the two resources can also depend
on the smaller number of users that edit and check
the Italian articles. From the 1,000 sentences eval-
uated we extracted 145 new lexical units: since
Italian FrameNet does not exist yet, every lexical
unit in a sentence that is correct can be straightfor-
wardly included in the first version of the resource.
8 Conclusions and Future work
In this work, we have proposed to apply a
word sense disambiguation system to a new
task, namely the linking between FrameNet and
Wikipedia. Results are promising and show that
283
the task is adequately substantiated. The proposed
approach can help enriching FrameNet with new
example sentences and lexical units and provide a
starting point for the creation of FrameNet-like re-
sources in all Wikipedia languages. On the one
hand, the retrieved data could speed up human
annotation, requiring only a manual validation.
On the other hand, the extracted sentences could
provide enough training data to machine learning
systems for frame assignment, since insufficient
frame attestations in the FrameNet database are a
major problem for such systems.
In the next research step, we plan to carry out an
extended evaluation process in order to compute
inter-annotator agreement and eventually point out
validation problems. Then, we want to extend
the mapping and the data extraction process to all
(F, l) pairs in FrameNet (about 10,000). The re-
trieved sentences will be made available as train-
ing or annotation material. Besides, we want
to create an online resource where the links be-
tween (F, l) pairs and Wikipages are made explicit
and where users can browse the retrieved sen-
tences. The resource can be produced and made
available with a reduced effort for every language
in Wikipedia. Anyway, the English version has
proved to be more precise, while the resource for
new languages would require a more accurate re-
vision.
Acknowledgments
Claudio Giuliano is supported by the ITCH
project (http://itch.fbk.eu), sponsored
by the Italian Ministry of University and Re-
search and by the Autonomous Province of
Trento and the X-Media project (http://www.
x-media-project.org), sponsored by the
European Commission as part of the Information
Society Technologies (IST) programme under EC
grant number IST-FP6-026978.
References
Collin F. Baker, Michael Ellsworth, and Katrin Erk.
2007. SemEval-2007 Task 10: Frame Semantic
Structure Extraction. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 99?104, Prague, CZ, June.
Aljoscha Burchardt and Anette Frank. 2006. Approxi-
mating Textual Entailment with LFG and FrameNet
Frames. In Proceedings of the 2nd PASCAL RTE
Workshop, pages 92?97, Venice, Italy.
Diego De Cao, Danilo Croce, Marco Pennacchiotti,
and Roberto Basili. 2008. Combining Word Sense
and Usage for modeling Frame Semantics. In Pro-
ceedings of STEP 2008, Venice, Italy.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/
?
cjlin/libsvm.
Olivier Chapelle, Bernhard Sch?olkopf, and Alexander
Zien. 2006. Semi-Supervised Learning. MIT Press,
Cambridge, MA.
Ian Chow and Jonathan Webster. 2007. Integra-
tion of Linguistic Resources for Verb Classification:
FrameNet Frame, WordNet Verb and Suggested Up-
per Merged Ontology. Computational Linguistics
and Intelligent Text Processing, pages 1?11.
Mario Crespo and Paul Buitelaar. 2008. Domain-
specific English-to-Spanish Translation of
FrameNet. In Proc. of LREC 2008, Marrakech.
Andras Csomai and Rada Mihalcea. 2008. Linking
Documents to Encyclopedic Knowledge. IEEE In-
telligent Systems, special issue on ?Natural Lan-
guage Processing for the Web?.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 708?716, Prague, Czech Republic,
June. Association for Computational Linguistics.
Katrin Erk. 2004. Frame assignment as Word
Sense Disambiguation. In Proceedings of IWCS-6,
Tilburg, NL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
C.J. Fillmore, C.R. Johnson, and M. R. L. Petruck.
2003. Background to FrameNet. International
Journal of Lexicography, 16:235?250, September.
Ana-Maria Giuglea and Alessandro Moschitti. 2006.
Semantic role labeling via FrameNet, VerbNet and
PropBank. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual ACL meeting, pages 929?936, Morris-
town, US.
A. Gliozzo, C. Giuliano, and C. Strapparava. 2005.
Domain kernels for word sense disambiguation. In
Proceedings of the 43
rd
annual meeting of the As-
sociation for Computational Linguistics (ACL-05),
pages 403?410, Ann Arbor, Michigan, June.
R. Johansson and P. Nugues. 2007. Using Word-
Net to extend FrameNet coverage. In Proc. of the
Workshop on Building Frame-semantic Resources
for Scandinavian and Baltic Languages, at NODAL-
IDA, Tartu.
284
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The Role of Domain Information
in Word Sense Disambiguation. Natural Language
Engineering, 8(4):359?373.
R. Mihalcea and P. Edmonds, editors. 2004. Proceed-
ings of SENSEVAL-3, Barcelona, Spain, July.
David Milne and Ian H. Witten. 2008. Learning to
link with Wikipedia. In CIKM ?08: Proceedings of
the 17th ACM conference on Information and knowl-
edge management, pages 509?518, NY, USA. ACM.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 Task-17: En-
glish Lexical Sample, SRL and All Words. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 87?
92, Prague, Czech Republic, June. Association for
Computational Linguistics.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge Univer-
sity Press.
Lei Shi and Rada Mihalcea. 2004. Open Text Seman-
tic Parsing Using FrameNet and WordNet. In Pro-
ceedings of HLT-NAACL 2004.
Lei Shi and Rada Mihalcea. 2005. Putting Pieces To-
gether: Combining FrameNet, VerbNet and Word-
Net for Robust Semantic Parsing. In Proceedings of
CICLing 2005, pages 100?111. Springer.
Sara Tonelli and Daniele Pighin. 2009. New features
for FrameNet - WordNet Mapping. In Proceedings
of the Thirteenth Conference on Computational Nat-
ural Language Learning, Boulder, CO, USA.
285
