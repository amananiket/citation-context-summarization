Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 79?88,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Scaling Textual Inference to the Web
Stefan Schoenmackers, Oren Etzioni, and Daniel S. Weld
Turing Center
University of Washington
Computer Science and Engineering
Box 352350
Seattle, WA 98195, USA
stef,etzioni,weld@cs.washington.edu
Abstract
Most Web-based Q/A systems work by find-
ing pages that contain an explicit answer to
a question. These systems are helpless if the
answer has to be inferred from multiple sen-
tences, possibly on different pages. To solve
this problem, we introduce the HOLMES sys-
tem, which utilizes textual inference (TI) over
tuples extracted from text.
Whereas previous work on TI (e.g., the lit-
erature on textual entailment) has been ap-
plied to paragraph-sized texts, HOLMES uti-
lizes knowledge-based model construction to
scale TI to a corpus of 117 million Web pages.
Given only a few minutes, HOLMES doubles
recall for example queries in three disparate
domains (geography, business, and nutrition).
Importantly, HOLMES?s runtime is linear in
the size of its input corpus due to a surprising
property of many textual relations in the Web
corpus?they are ?approximately? functional
in a well-defined sense.
1 Introduction and Motivation
Numerous researchers have identified the Web as
a rich source of answers to factual questions, e.g.,
(Kwok et al, 2001; Brill et al, 2002), but often the
desired information is not stated explicitly even in a
textual corpus as massive as the Web. Consider the
question ?What vegetables help prevent osteoporo-
sis?? Since there is likely no sentence on the Web
directly stating ?Kale prevents osteoporosis?, a sys-
tem must infer that kale is an answer by combining
facts from multiple sentences, possibly from differ-
ent pages, which justify that conclusion: i.e., that
kale is a vegetable, kale contains calcium, and cal-
cium helps prevent osteoporosis.
Figure 1: The architecture of HOLMES.
Textual Inference (TI) methods have advanced in
recent years. For example, textual entailment tech-
niques aim to determine whether one textual frag-
ment (the hypothesis) follows from another (the text)
(Dagan et al, 2005). While most TI researchers have
focused on high-quality inferences from a small
source text, we seek to utilize sizable chunks of the
Web corpus as our source text. In order to do this,
we must confront two major challenges. The first is
uncertainty: TI is an imperfect process, particularly
when applied to the Web corpus, hence probabilistic
methods help to assess the confidence in inferences.
The second challenge is scalability: how does infer-
ence time scale given increasingly large corpora as
input?
1.1 HOLMES: A Scalable TI System
This paper describes HOLMES, an implemented sys-
tem, which addresses both challenges by carrying
out scalable, probabilistic inference over ground
assertions extracted from the Web. The input to
HOLMES is a conjunctive query, a set of inference
rules expressed as Horn clauses, and large sets of
ground assertions extracted from theWeb, WordNet,
and other knowledge bases. As shown in Figure 1,
HOLMES chains backward from the query, using the
inference rules to construct a forest of proof trees
from the ground assertions. This forest is converted
79
into a Markov network (a form of Knowledge-
Based Model Construction (KBMC) (Wellman et
al., 1992)) and evaluated using approximate prob-
abilistic inference. HOLMES operates in an anytime
fashion ? if desired it can keep iterating: search-
ing for more proofs, and elaborating the Markov net-
work.
HOLMES makes some important simplifying as-
sumptions. Specifically, we use simple ground
tuples to represent extracted assertions (e.g.,
contains(kale, calcium)). Syntactic prob-
lems (e.g., anaphora, relative clauses) and seman-
tic challenges (e.g., quantification, counterfactuals,
temporal qualification) are delegated to the extrac-
tion system or simply ignored. This paper focuses
on scalability for this subset of the TI task.
1.2 Summary of Experimental Results
We tested HOLMES on 183 million distinct ground
assertions extracted from the Web by the TEX-
TRUNNER system (Banko et al, 2007), coupled
with 159 thousand ground assertions from Word-
Net (Miller et al, 1990), and a compact set of hand-
coded inference rules. Given a total of 55 to 145
seconds, HOLMES was able to produce high-quality
inferences that doubled the number of answers to
example queries in three disparate domains: geog-
raphy, business, and nutrition.
We also evaluated how the speed of HOLMES
scaled with the size of its input corpus. In the
general case, logical inference over a Horn theory
(needed in order to produce the probabilistic net-
work) is polynomial in the number of ground asser-
tions, and hence in the size of the textual corpus.1
Unfortunately, this is prohibitive, since even low-
order polynomial growth is fatal on a 117 million-
page corpus, let alne the full Web.
1.3 Why HOLMES Scales Linearly
Fortunately, the Web?s long tail works in our favor.
The relations we extract from text are approximately
pseudo-functional (APF), as we formalize in Sec-
tion 3, and this property leads to runtime that scales
linearly with the corpus. To see the underlying in-
tuition, consider the APF relation denoted by the
phrase ?is married to;? most of the time it maps a
person?s name to a small number of spousal names
1In fact, it is P-complete ? as hard as any polynomial-time
problem.
so this relation is APF. Section 3 shows why this
APF property ensures linear scaling, and Section 4
demonstrates linear scaling in practice.
2 An Overview of HOLMES
HOLMES is a system designed to answer complex
queries over large, noisy knowledge bases. As a mo-
tivating example, we consider the question ?What
vegetables help prevent osteoporosis?? As of this
writing, Google has no pages explicitly stating ?kale
helps prevent osteoporosis?, making it challenging
to return ?kale? as an answer. However, there are
numerous web pages stating that ?kale is high in cal-
cium? and others declaring that ?calcium helps pre-
vent osteoporosis?. If we could combine those facts
we could easily infer that ?kale? is an answer to the
question ?What vegetables help prevent osteoporo-
sis?? HOLMES was designed to make such infer-
ences while accounting for uncertainty in the pro-
cess.
Given a query, expressed as a conjunctive
Datalog rule, HOLMES generates a probabilistic
model using knowledge-based model construction
(KBMC) (Wellman et al, 1992). Specifically,
HOLMES utilizes fast, logical inference to find the
subset of ground assertions and inference rules that
may influence the answers to the query ? enabling
the construction of a small and focused Markov net-
work. Since this graphical model is much smaller
than one incorporating all ground assertions, prob-
abilistic inference will be much faster than if naive
compilation were used.
Figure 1 summarizes the operation of HOLMES.
As with many theorem provers or KBMC systems,
HOLMES takes three inputs:
1. A set of knowledge bases ? databases of
ground relational assertions, each with an
estimate of its probability, which can be
generated by TextRunner (Banko et al,
2007) or Kylin (Wu and Weld, 2007). In
our example, we would extract the as-
sertions IsHighIn(kale, calcium) and
Prevents(calcium, osteoporosis) from
those sentences.
2. A domain theory ? A set of probabilis-
tic inference rules written as Markov logic
Horn clauses, which can be used to de-
rive new assertions. The weight associ-
ated with each clause specifies its reliability.
80
kaleis high incalcium(TextRunner : 0.39)kaleis high inmagnesium(TextRunner : 0.39) magnesiumhelps preventosteoporosis(TextRunner : 0.39) calciumhelps preventosteoporosis(TextRunner : 0.68) broccoliis high incalcium(TextRunner : 0.39)
kalehelps preventosteoporosis(Inferred : 0.88) broccolihelps preventosteoporosis(Inferred : 0.49)kaleIS-Avegetable(WordNet : 0.9) broccoliIS-Avegetable(WordNet : 0.9)Inf. Rule:Transitive-Throughhigh in Inf. Rule:Transitive-Throughhigh in Inf. Rule:Transitive-Throughhigh in
kale matches the query(Inferred : 0.91) broccoli matches the query(Inferred : 0.58)Query Result Query Result
Figure 2: Partial proof ?tree? (DAG) for the query ?What
vegetables help prevent osteoporosis?? Rectangles de-
pict ground assertions from a knowledge base, rounded
boxes are inferred assertions, and shaded squared repre-
sent the application of inference rules. HOLMES converts
this DAG into a Markov network in order to estimate the
probability of each node.
In Section 2.3 we identify several domain-
independent rules, but a user may (optionally)
specify additional, domain-specific rules if de-
sired. In our example, we assume we are given
the domain-specific rule: Prevents(X,Z) :-
IsHighIn(X,Y) ? Prevents(Y,Z)
3. A conjunctive query is specified as a Datalog
rule. For example, the question ?What vegeta-
bles help prevent osteoporosis?? could be writ-
ten as: query(X) :- IS-A(X,Vegetable)
? Prevents(X,osteoporosis)
and returns a set of answers to the query, each with
an associated probability.
2.1 Basic Operation
To find these answers and their associated proba-
bilities, HOLMES first finds all ground assertions in
the knowledge bases that are potentially relevant to
the query. This is efficiently done using the infer-
ence rules to chain backwards from the query. Note
that the generated candidate answers, themselves,
are less important than the associated proof trees.
Furthermore, since HOLMES uses these ?trees? (ac-
tually, DAGs) to generate a probabilistic graphical
model, HOLMES seeks to find as many proof trees
as possible for each query result ? each may influ-
ence the final belief in that result. Figure 2 shows a
partial proof tree for our example query.
To handle uncertainty, HOLMES now constructs a
ground Markov network from the proof trees and the
Markov-logic-encoded inference rules. Markov net-
works (Pearl, 1988) model the joint distribution of a
set of variables by creating an undirected graph with
one node for each random variable, and represent-
ing dependencies between variables with cliques in
the graph. Each clique has a corresponding poten-
tial function ?k, which returns a non-negative value
based on the state of variables in the clique. The
probability of a state, x, is given by
P (x) =
1
Z
?
?k(x{k})
where the partition functionZ is a normalizing term,
and x{k} denotes the state of all the variables in
clique k.
HOLMES converts the proof trees into a Markov
network in a manner pioneered by the Markov Logic
framework of Richardson and Domingos (2006). A
Boolean variable is created to represent the truth of
each assertion in the proof forest. Next, HOLMES
adds edges to the Markov network to create a clique
corresponding to each application of an inference
rule in the proof forest.
Following the Markov Logic framework, the po-
tential function of a clique has form ?(x) = ew if all
member nodes are true (w denotes the weight of the
inference rule), and ?(x) = 1 otherwise. The proba-
bilities of leaf nodes are derived from the underlying
knowledge base,2 and inferred nodes are biased with
an exponential prior.
Finally, HOLMES computes the approximate
probability of each answer by running a variant
of loopy belief propagation (Pearl, 1988) over the
Markov network. In our experience this method
performs well on networks derived from our Horn
clause proof forest, but one could use Monte Carlo
techniques or even exact methods if desired.
Note that this architecture allows HOLMES to
combine information from multiple web pages to in-
fer assertions not explicitly seen in the textual cor-
pus. Because this inference is done using a Markov
network, it correctly handles uncertain extractions
and probabilistic dependencies. By using KBMC to
create a custom, focused network for each query, the
2In our experiments, ground assertions from WordNet get
a uniformly high probability of correctness (0.9), but those ex-
tracted from the Web are assigned probabilities derived from
redundancy statistics, following the intuition that frequently ex-
tracted facts are more likely to be true (Etzioni et al, 2005).
81
amount of probabilistic inference is reduced to man-
ageable proportions.
2.2 Anytime, Incremental Expansion
Because exact probabilistic inference is #P-
complete, HOLMES uses approximate methods, but
even these techniques have problems if the Markov
network gets too large. As a result, HOLMES creates
the network incrementally. After the first proof trees
are generated, HOLMES creates the model and per-
forms approximate probabilistic inference. If more
time is available then HOLMES searches for addi-
tional proof trees and updates the network (Fig-
ure 1). This incremental process allows HOLMES
to return initial results (with preliminary probability
estimates) as soon as they are discovered.
For efficiency, HOLMES exploits standard Data-
log optimizations (e.g., it only expands proofs of re-
cently added nodes and it uses an approximation to
magic sets (Ullman, 1989), rather than simple back-
wards chaining). For tractability, we also allow the
user to limit the number of transitive inference steps
for any inference rule.
HOLMES also includes a few enhancements for
dealing with information extracted from natural lan-
guage. For example, HOLMES?s inference rules sup-
port substring/regex matching of ground assertions,
to accommodate simple variations in text. HOLMES
also can be restricted to only operate over proper
nouns, which is useful for queries involving named
entities.
2.3 Markov Logic Inference Rules
HOLMES is given the following set of six domain-
independent rules, which are similar to the up-
ward monotone rules introduced by (MacCartney
and Manning, 2007).
1. Observed relations are likely to be true:
R(X,Y) :- ObservedInCorpus(X, R, Y)
2. Synonym substitution preserves meaning:
RTR(X?,Y) :- RTR(X,Y) ? Synonym(X, X?)
3. RTR(X,Y?) :- RTR(X,Y) ? Synonym(Y, Y?)
4. Generalizations preserve meaning:
RTR(X?,Y) :- RTR(X,Y) ? IS-A(X, X?)
5. RTR(X,Y?) :- RTR(X,Y) ? IS-A(Y, Y?)
6. Transitivity of Part Meronyms:
RTR(X,Y?) :- RTR(X,Y) ? Part-Of(Y, Y?)
where RTR matches ?* in? (e.g., ?born in?).
For example, if Q(X):-BornIn(X,?France?),
and we know from WordNet that Paris is in
France, then by inference rule 6, we know that
BornIn(X,?Paris?) will yield valid results for
Q(X). Although all of these rules contain at most
two relations in the body, HOLMES allows an
arbitrary number of relations in the query and rule
bodies. However, we have found that even simple
rules can dramatically improve some queries.
We set the rule weights to capture the intuition
that deeper inferences decrease the likelihood (as
there are more chances to make mistakes), whereas
additional, independent proof trees increase the
likelihood (as there is more supporting evidence).
Specifically, in our experiments we set the prior on
inferred facts to -0.75, the weight on rule 1 to 1.5,
and the weights on all other rules to 0.6.
At present, we define these weights manually, but
we expect to learn the parameter values in the future.
3 Scaling Inference to the Web
If TI is applied to a corpus containing hundreds of
millions or even billions of pages, its run time has to
be at most linear in the size of the corpus. This sec-
tion shows that under some reasonable assumptions
inference does scale linearly.
We start our analysis with two simplifications.
First, we assume that the number of distinct, ground
assertions in the KBs, |A|, grows at most linearly
with the size of the textual corpus. This is cer-
tainly true for assertions extracted by TextRunner
and Kylin, and follows from our exclusion of texts
with complex quantified sentences. Our analysis
now proceeds to consider scaling with respect to |A|
for a fixed query and set of inference rules.
Our second assumption is that the size of every
proof tree is bounded by some constant, m. This
is a strong assumption and one that depends on the
precise set of inference rules and pattern of ground
assertions. However, it holds in our experience, and
if necessary could be enforced by terminating the
search for proof trees at a certain depth, e.g., log(m).
HOLMES?s knowledge-based model construction
has two parts: construction of the proof forest and
conversion of the forest into a Markov network.
Since the Markov network is essentially isomorphic
to the proof forest, the conversion will be O(|A|) if
the forest is linear in size, which is ensured if the
time to construct the proof trees isO(|A|). We show
82
this in the remainder of this section.
Recall that HOLMES requires inference rules to
be function-free Horn clauses. While this limits ex-
pressivity to some degree, it provides a huge speed
benefit ? logical inference over Horn clauses can
be done in polynomial time, whereas general propo-
sitional inference (i.e., from grounded first-order
rules) is NP-complete.
Alas, even low-order polynomial blowup is un-
acceptable when the textual corpus reaches Web
scale; we seek linear growth. Intuitively, there are
two places where polynomial expansion could cause
trouble. First, the number of different types of proofs
(i.e., first order proofs) could grow too quickly, and
secondly, a given type of proof tree might apply
to too many ground assertions (?tuples? in database
lingo). We treat these issues in turn.
Under our assumptions, each proof tree can be
represented as an expression in relational algebra
with at most m equijoins (Ullman, 1989),3 each
stemming from the application of an inference rule.
Since the number of rules is fixed, as is m, there are
a constant number of possible first-order proof trees.
The bigger concern is that any one of these first-
order trees might result in a polynomial number of
ground trees; if so, the size of the ground forest
(and corresponding Markov network) could grow
too quickly. In fact, polynomial growth is a common
phenomena in database query evaluation. Luckily,
most relations in the Web corpus behave more fa-
vorably. We introduce a property of relations that
ensures m-way joins, and therefore all proof trees
up to size m, can be computed in O(|A|) time.
The intuition is that most relations derived from
large corpora have a ?heavy-tailed? distribution,
wherein a few objects appear many times in a rela-
tion, but most appear only once or twice, thus joins
involving rare objects lead to a small number of re-
sults, and so the main limitation on scalability is
common objects. We now prove that if these com-
mon objects account for a small enough fraction of
the relation, then joins will still scale linearly. We
focus on binary relations, but these results can eas-
ily be extended to relations of larger arity.
3Note that an inference rule of the form H(X) :-
R1(X,Y),R2(Y,Z) is equivalent to the algebraic expression
piX(R1 ./ R2). First a join is performed between R1 and R2
testing for equality between values of Y ; then a projection elim-
inates all columns besides X .
Definition 1 A relation, R = {(xi, yi)} ? X ?
Y , is pseudo-functional (PF) in x with degree k, if
?x ? X : |{y|(x, y) ? R}| ? k. When the precise
variable and degree is irrelevant to discussion, we
simply say ?R is PF.?
An m-way equijoin over relations that are PF in
the join variables will have at most km ? |R| results.
Since km is constant for a given join and |R| scales
linearly in the size of the textual corpus, proof tree
construction over PF relations also scales linearly.
However, due to their heavy-tailed distributions,
most relations extracted from theWeb fit the pseudo-
functional definition in most, but not all values of
X . Fortunately, it turns out that in most cases these
?bad? values ofX are rare and hence don?t influence
the join size significantly. We formalize this intu-
ition by defining a class of approximately pseudo-
functional (APF) relations and proving that joining
two APF relations produces at most a linear number
of results.
Definition 2 A relation, R, is approximately
pseudo-functional (APF) in x with degree k, if X
can be partitioned into two sets XG and XB such
that for all x ? XG R is PF with degree k and?
x?XB
|{y|(x, y) ? R}| ? k ? log(|R|)
Theorem 1. If relation R1 is APF in y with de-
gree k1 and R2 is APF in y with degree k2 then
the relation Q = R1 ./ R2 has size at most
O(max(|R1|, |R2|)).
Proof. Since R1 and R2 are APF, we know that
Y can be partitioned into four groups: YBB =
YB1
?
YB2, YBG = YB1
?
YG2, YGB = YG1
?
YB2,
YGG = YG1
?
YG2.4 We can show that each group
leads to at most O(|A|) entries in Q. For y ? YBB
there are at most k1 ? k2 ? log(|R1|) ? log(|R2|) en-
tries in Q. The y ? YGB and y ? YBG lead to at
most k1 ? k2 ? log(|R2|) and k1 ? k2 ? log(|R1|)
entries, respectively. For y ? YGG there are at
most k1 ? k2 ? max(|R1|, |R2|). Summing the re-
sults from the four partitions, we see that |Q| is
O(max(|R1|, |R2|)), thus it is O(|A|).
This theorem and proof can easily be extended to
4YBB are the ?doubly bad? values of y that violate the PF
definition for both relations, YGG are the values that do not vio-
late the PF definition for either relation, and YBG and YGB are
the values that violate it in only R1 or R2, resp.
83
an m-way equijoin, as long as each relation is APF
in all arguments that are being joined.
Theorem 2. IfQ is the relation obtained by an equi-
join over m relationsR1..m, each having size at most
O(|A|), and if all R1..m are APF in all arguments
that they are joined in with degree at most kmax, and
if
?
1?i?m
log(|Ri|) ? |A|, then |Q| is O(|A|).
The inequality in Theorem 2 relates the sizes of
the relations (|R|), the join (m) and the number of
ground assertions (|A|). However, in many cases we
are interested in much smaller values of m than the
inequality enables. We can relax the APF definition
to allow a broader, but still scalable, class ofm-way-
APF relations.
Corollary 3. If Q is the relation obtained by an m-
way join, and if each participating relation is APF
in their joined variables with a bound of ki ? m
?
|Ri|
instead of ki ? log(|Ri|), then the join is O(|A|).
The final step in our scaling argument concerns
probabilistic inference, which is #P-Complete if per-
formed exactly. This is addressed in two ways. First,
HOLMES uses approximate methods, e.g., loopy be-
lief propagation, which avoids the cost of exact in-
ference ? at the cost of reduced precision. Sec-
ondly, at a practical level, HOLMES?s incremental
construction of the graphical model (Figure 1) al-
lows it to bound the size of the network by terminat-
ing the search for additional proofs.
4 Experimental Results
This section reports on measurements that confirm
that linear scaling with |A| occurs in practice, and
that HOLMES?s inference is not only scalable but
also improves precision/recall on sample queries in
a diverse set of domains. After describing the exper-
imental domains and queries, Section 4.2 reports on
the boost to the area under the precision/recall curve
for a set of example queries in three domains: ge-
ography, business, and nutrition. Section 4.3 then
shows that APF relations are very common in the
Web corpus, and finally Section 4.4 demonstrates
empirically that HOLMES?s inference time scales
linearly with the number of pages in the corpus.
4.1 Experimental Setup
HOLMES utilized two knowledge bases in these ex-
periments: TEXTRUNNER and WordNet. TEX-
TRUNNER contains approximately 183 million dis-
tinct ground assertions extracted from over 117 mil-
lion web pages, and WordNet contains 159 thousand
manually created IS-A, Part-Of, and Synonym asser-
tions.
In all queries, HOLMES utilizes the domain-
independent inference rules described in Sec-
tion 2.3. HOLMES additionally makes use of two
domain-specific inference rules in the Nutrition
domain, to demonstrate the benefits of including
domain-specific information. Estimating the preci-
sion and relative recall of HOLMES requires exten-
sive and careful manual tagging of HOLMES output.
To make this feasible, we restricted ourselves to a
set of twenty queries in three domains, but made the
domains diverse to illustrate the broad scope of the
system.
We now describe each domain briefly.
Geography: the query issued is: ?Who was born in
one of the following countries?? More formally,
Q(X) :- BornIn(X,{country}) where {country}
is bound to each of the following nine countries
in turn {France, Germany, China, Thailand, Kenya,
Morocco, Peru, Columbia, Guatemala}, yielding a
total of nine queries.
Because Web text often refers to a person?s
birth city rather than birth country, this query il-
lustrates how combining an ground assertion (e.g.,
BornIn(Alberto Fujimori, Lima)) with back-
ground knowledge (e.g., LocatedIn(Lima, Peru))
enables the system to draw new conclusions (e.g.,
BornIn(Alberto Fujimori, Peru)).
Business: we issued the following two queries.
1) Which companies are acquiring software com-
panies? Formally, Q(X) :- Acquired(X, Y)
? Develops(Y, ?software?) This query tests
HOLMES?s ability to scalably join a large number of
assertions from multiple pages.
2) Which companies are headquartered in the
USA? Q(X) :- HeadquarteredIn(X, ?USA?)
? IS-A(X, ?company?)
Answering this query comprehensively requires
HOLMES to combine a join (over the relations Head-
quarteredIn and IS-A) with transitive inference on
PartOf (e.g., Seattle is PartOf Washington which is
PartOf the USA) and on IS-A (e.g., Microsoft IS-A
software company which IS-A company). The IS-
A assertions came from both TEXTRUNNER (using
patterns from (Hearst, 1992)) and WordNet.
84
0
0.2
0.4
0.6
0.8
1
0 1000 2000 3000 4000 5000Estimated Recall
Precis
ion
BaselineHolmes Increase in AuC
Figure 3: PR Curve for BornIn(X, {country}). Inference
boosts the Area under the PR Curve (AuC) by 102 %.
Domain Increase Total Inference
in AuC Time
Geography +102% 55 s
Business +2,643% 145 s
Nutrition +5,595% 64 s
Table 1: Improvement in the AuC of HOLMES over the
BASELINE and total inference time taken by HOLMES.
Results are summed over all queries in the geography,
business, and nutrition domains. Inference time mea-
sured on unoptimized prototype.
Nutrition: the nine queries issued are instances
of ?What foods prevent disease?? Where a food is
a member of one of the classes: fruit, vegetable, or
grain, and a disease is one of: anemia, scurvy, or
osteoporosis. More formally, Q(X, {disease}) :-
Prevents(X, {disease}) ? IS-A(X, {food})
Our experiments in the nutrition domain utilized
two domain-specific inference rules in addition to
the ones presented in Section 2.3:
Prevents(X,Y):-HighIn(X,Z) ? Prevents(Z,Y)
Prevents(X,Y):-Contains(X,Z) ? Prevents(Z,Y)
4.2 Effect of Inference on Recall
To measure the cost and benefit of HOLMES?s in-
ference we need to define a baseline for compar-
ison. Answering the conjunctive queries in the
business and nutrition domains requires computing
joins, which TEXTRUNNER does not do. Thus, we
defined a baseline system, BASELINE, which has
access to the underlying Knowledge Bases (KBs)
(TEXTRUNNER and WordNet), and the ability to
compute joins using information explicitly stated in
either KB, but does not have the ability to infer new
assertions.
We compared HOLMES with BASELINE in all
three domains. Figure 3 depicts the combined pre-
cision/relative recall curves for the nine Geography
queries. HOLMES yields substantially higher re-
call (the shaded region) at modestly lower preci-
sion, doubling the area under the precision/recall
curve (AuC). The other precision/recall curves also
showed a slight drop in precision for substantial
gains in recall. Table 1 summarizes the results, along
with the total runtime needed for inference. Because
relations in the business domain are much larger
than in the other domains (i.e., 100x ground asser-
tions), inference is slower in this domain.
We note that inference is particularly helpful with
rarely mentioned instances. However, inference can
lead to errors when the proof tree contains joins on
generic terms (e.g., ?company?) or common extrac-
tion errors (e.g., ?LLC? as a company name). This
is a key area for future work.
4.3 Prevalence of APF Relations
To determine the prevalence of APF relations inWeb
text, we examined a sample of 500 binary relations
selected randomly from TEXTRUNNER?s ground as-
sertions. The surface forms of the relations and ar-
guments may misrepresent the true properties of the
underlying concepts, so to better estimate the true
properties we merged synonymous values as given
by Resolver (Yates and Etzioni, 2007) or the most
frequent sense of the word in WordNet. For exam-
ple, we would consider BornIn(baby, hospital)
and BornAt(infant, infirmary) to represent the
same concept, and so would merge them into one
instance of the ?Born In? relation. The largest two re-
lations had over 1.25 million unique instances each,
and 52% of the relations had more than 10,000 in-
stances.
For each relation R, we first found all instances
of R extracted by TEXTRUNNER and merged all
synonymous instances as described above. Then,
for each argument of R we computed the smallest
value, Kmin, such that R is APF with degree Kmin.
Since many interesting assertions can be inferred by
simply joining two relations, we also considered the
special case of 2-way joins using Corollary 3. We
computed the smallest value, K2./, such that the re-
lation is two-way-APF with degree K2./.
Figure 4 shows the fraction of relations with
Kmin andK2./ of at mostK as a function of varying
85
0%
20%
40%
60%
80%
100%
0 1000 2000 3000 4000 5000 6000Degree of Approximate Pseudo-Functionality
APFAPF for two-way join
Figure 4: Prevalence of APF relations in Web text. The
x-axis depicts the degree of pseudo-functionality, e.g.,
Kmin and K2./, (see definition 2); the y-axis lists the
percent of relations that are APF with that degree. Re-
sults are averaged over both arguments.
values of K. The results are averaged over both ar-
guments of each binary relation. For arbitrary joins
in this KB, 80% of the relations are APF with de-
gree less than 496; for 2-way joins (like the ones in
our inference rules and test queries), 80% of the rela-
tions are APF with degree less than 65. These results
indicate that the majority of relations TEXTRUNNER
extracted from text are APF, and so we can expect
HOLMES?s techniques will allow efficient inference
over most relations.
While Theorem 2 guarantees that joins over those
relations will beO(|R|), that notation hides a poten-
tially large constant factor of Kminm. Fortunately
the constant factor is significantly smaller in prac-
tice. To see why, we re-examine the proof: the large
factor comes from assuming that all of R?s first ar-
guments which meet the PF definition are associated
with exactly Kmin distinct second arguments. How-
ever, in our corpus 83% of first arguments are as-
sociated with only one second argument. Clearly,
our worst-case analysis substantially over-estimates
inference time for most queries. Moreover, in ad-
ditional experiments (omitted due to space limita-
tions), measured join sizes grew linearly in the size
of the corpus, but were on average two to three or-
ders of magnitude smaller than the bounds given in
the theory. This observation held across relations
with different sizes and values of Kmin.
While the results in Figure 4 may vary for other
sets of relations, we believe the general trends
hold. This is promising for Question Answering and
Textual Inference systems, since if true it implies
R2 = 0.9881
R2 = 0.9808
R2 = 0.9931
020
4060
80100
120140
160
0% 20% 40% 60% 80% 100%Fraction of Corpus
GeographyBusinessNutrition
Figure 5: The effects of corpus size on total inference
time. We see approximately linear growth in all domains,
and display the best fit lines and coefficient of determina-
tion (R2) of each.
that combining information frommultiple difference
source is feasible, and can allow such systems to in-
fer answers not explicitly seen in any source.
4.4 Scalability of Inference Speed
Since the previous subsection showed that most re-
lations are APF in their arguments, our theory pre-
dicts HOLMES?s inference will scale linearly. We
tested this hypothesis empirically by running infer-
ence over the test queries in our three domains, while
varying the number of pages in the textual corpus.
Figure 5 shows how the inference time HOLMES
used to answer all queries in each domain scales
with KB size. For these queries, and several oth-
ers we tested (not shown here), inference time grows
linearly with the size of the KB. Based on these re-
sults we believe that HOLMES can provide scalable
inference over a wide variety of domains.
5 Related Work
Textual Entailment systems are given two textual
fragments, text T and hypothesis H , and attempt to
decide if the meaning of H can be inferred from
the meaning of T (Dagan et al, 2005). While
many approaches have addressed this problem, our
work is most closely related to that of (Raina et al,
2005; MacCartney and Manning, 2007; Tatu and
Moldovan, 2006; Braz et al, 2005), which convert
the inputs into logical forms and then attempt to
?prove? H from T plus a set of axioms. For in-
stance, (Braz et al, 2005) represents T , H , and a
set of rewrite rules in a description logic framework,
and determines entailment by solving an integer lin-
86
ear program derived from that representation.
These approaches and related ones (e.g.,
(Van Durme and Schubert, 2008)) use highly
expressive representations, enabling them to ex-
press negation, temporal information, and more.
HOLMES?s representation is much simpler?
Markov Logic Horn Clauses for inference rules
coupled with a massive database of ground asser-
tions. However, this simplification allows HOLMES
to tackle a ?text? of enormously larger size: 117
million Web pages versus a single paragraph. A sec-
ond, if smaller, difference stems from the fact that
instead of determining whether a single hypothesis
sentence, H , follows from the text, HOLMES tries to
find all consequents that match a conjunctive query.
HOLMES is also related to open-domain question-
answering systems such as Mulder (Kwok et al,
2001), AskMSR (Brill et al, 2002), and others
(Harabagiu et al, 2000; Brill et al, 2001). How-
ever, these Q/A systems attempt to find individual
documents or sentences containing the answer. They
often perform deep analysis on promising texts, and
back off to shallower, less reliable methods if those
fail. In contrast, HOLMES utilizes TI and attempts
to combine information from multiple different sen-
tences in a scalable way.
While its ability to combine information from
multiple sources is promising, HOLMES has several
limitations these Q/A systems do not have. Since
HOLMES relies on an information extraction sys-
tem to convert sentences into ground predicates,
any limitations of the IE system will be propagated
to HOLMES. Additionally, the logical representa-
tion HOLMES uses limits the reasoning and types
of questions it can answer. HOLMES is geared to-
wards answering questions which are naturally ex-
pressed as properties and relations of entities, and is
not well suited to answering more abstract or open
ended questions. Although we have demonstrated
that HOLMES is scalable, further work is needed to
make it to run at interactive speeds.
Finally, research in statistical relational learning
such as MLNs (Richardson and Domingos, 2006),
RMNs (Taskar et al, 2002), and others (Getoor
and Taskar, 2007) have studied techniques for com-
bining logical and probabilistic inference. Our in-
ference rules are more restrictive than those al-
lowed in MLNs, but this trade-off allows us to ef-
ficiently scale inference to large, open domain cor-
pora. By constructing only cliques for satisfied in-
ference rules, HOLMES explicitly models the intu-
ition behind LazySAT inference (Singla and Domin-
gos, 2006) as used in MLNs. I.e., most Horn clause
inference rules will be trivially satisfied since their
antecedents will be false, so we only need to worry
about ones where the antecedent is true.
6 Conclusions
This paper makes three main contributions:
1. We introduce and evaluate the HOLMES sys-
tem, which leverages KBMC methods in order
to scale a class of TI methods to the Web.
2. We define the notion of Approximately Pseudo-
Functional (APF) relations and prove that, for
a APF relations, HOLMES?s inference time in-
creases linearly with the size of the input cor-
pus. We show empirically that APF relations
appear to be prevalent in our Web corpus (Fig-
ure 4), and that HOLMES?s runtime does scale
linearly with the size of its input (Figure 5), tak-
ing only a few CPU minutes when run over 183
million distinct ground assertions.
3. We present experiments demonstrating that, for
a set of queries in the domains of geography,
business, and nutrition, HOLMES substantially
improves the quality of answers (measured by
AuC) relative to a ?no inference? baseline.
In the future, we plan more extensive tests to char-
acterize when HOLMES?s inference is helpful. We
also hope to examine in what cases jointly perform-
ing extraction and inference (as opposed to perform-
ing them separately) is feasible at scale. Finally, we
plan to examine methods for HOLMES to learn both
rule weights and new inference rules.
Acknowledgements
We thank the following for helpful comments on
previous drafts: Fei Wu, Michele Banko, Mausam,
Doug Downey, and Alan Ritter. This research was
supported in part by NSF grants IIS-0535284, IIS-
0312988, and IIS-0307906, ONR grants N00014-
08-1-0431 and N00014-06-1-0147, CALO grant 03-
000225, the WRF / TJ Cable Professorship as well
as gifts from Google. The work was performed at
the University of Washington?s Turing Center.
87
References
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
R. Braz, R. Girju, V. Punyakanok, D. Roth, and M. Sam-
mons. 2005. An inference model for semantic en-
tailment in natural language. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 1678?1679.
E. Brill, J. Lin, M. Banko, S. T. Dumais, and A. Y. Ng.
2001. Data-intensive question answering. In Procs.
of Text REtrieval Conference (TREC-10), pages 393?
400.
Eric Brill, Susan Dumais, and Michele Banko. 2002. An
analysis of the AskMSR question-answering system.
In EMNLP ?02: Proceedings of the ACL-02 conference
on Empirical methods in natural language processing,
pages 257?264, Morristown, NJ, USA. Association for
Computational Linguistics.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL Recognising Textual Entailment Challenge.
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?8.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91?134.
L. Getoor and B. Taskar. 2007. Introduction to Statistical
Relational Learning. MIT Press.
S. Harabagiu, M. Pasca, and S. Maiorano. 2000. Exper-
iments with open-domain textual question answering.
In Procs. of the COLING-2000.
M. Hearst. 1992. Automatic Acquisition of Hyponyms
from Large Text Corpora. In Procs. of the 14th In-
ternational Conference on Computational Linguistics,
pages 539?545, Nantes, France.
C.C.T. Kwok, O. Etzioni, and D.S. Weld. 2001. Scal-
ing question answering to the Web. Proceedings of
the 10th international conference on World Wide Web,
pages 150?161.
B. MacCartney and C.D. Manning. 2007. Natural Logic
for Textual Inference. In Workshop on Textual Entail-
ment and Paraphrasing.
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1990. Introduction to wordnet: An on-line
lexical database. International Journal of Lexicogra-
phy, 3(4):235?312.
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks of plausible inference. Morgan
Kaufmann Publishers Inc. San Francisco, CA, USA.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI 2005.
AAAI Press.
M. Richardson and P. Domingos. 2006. Markov Logic
Networks. Machine Learning, 62(1-2):107?136.
Parag Singla and Pedro Domingos. 2006. Memory-
efficient inference in relational domains. In AAAI.
B. Taskar, P. Abbeel, and D. Koller. 2002. Discrimi-
native probabilistic models for relational data. Eigh-
teenth Conference on Uncertainty in Artificial Intelli-
gence (UAI02).
Marta Tatu and Dan Moldovan. 2006. A logic-based
semantic approach to recognizing textual entailment.
In Proceedings of the COLING/ACL on Main confer-
ence poster sessions, pages 819?826, Morristown, NJ,
USA. Association for Computational Linguistics.
J. Ullman. 1989. Database and knowledge-base systems.
Computer Science Press.
B. Van Durme and L.K. Schubert. 2008. Open knowl-
edge extraction through compositional language pro-
cessing. In Symposium on Semantics in Systems for
Text Processing.
M. Wellman, J. Breese, and R. Goldman. 1992. From
knowledge bases to decision models. The Knowledge
Engineering Review, 7(1):35?53.
F. Wu and D. Weld. 2007. Autonomously semantifying
Wikipedia. In Proceedings of the ACM Sixteenth Con-
ference on Information and Knowledge Management
(CIKM-07), Lisbon, Porgugal.
A. Yates and O. Etzioni. 2007. Unsupervised resolution
of objects and relations on the Web. In Procs. of HLT.
88
