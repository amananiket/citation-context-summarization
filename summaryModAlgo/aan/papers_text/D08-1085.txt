Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 812?819,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Attacking Decipherment Problems Optimally with Low-Order N-gram
Models
Sujith Ravi and Kevin Knight
University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
{sravi,knight}@isi.edu
Abstract
We introduce a method for solving substi-
tution ciphers using low-order letter n-gram
models. This method enforces global con-
straints using integer programming, and it
guarantees that no decipherment key is over-
looked. We carry out extensive empirical ex-
periments showing how decipherment accu-
racy varies as a function of cipher length and
n-gram order. We also make an empirical in-
vestigation of Shannon?s (1949) theory of un-
certainty in decipherment.
1 Introduction
A number of papers have explored algorithms
for automatically solving letter-substitution ciphers.
Some use heuristic methods to search for the best de-
terministic key (Peleg and Rosenfeld, 1979; Gane-
san and Sherman, 1993; Jakobsen, 1995; Olson,
2007), often using word dictionaries to guide that
search. Others use expectation-maximization (EM)
to search for the best probabilistic key using letter
n-gram models (Knight et al, 2006). In this paper,
we introduce an exact decipherment method based
on integer programming. We carry out extensive de-
cipherment experiments using letter n-gram models,
and we find that our accuracy rates far exceed those
of EM-based methods.
We also empirically explore the concepts in Shan-
non?s (1949) paper on information theory as applied
to cipher systems. We provide quantitative plots for
uncertainty in decipherment, including the famous
unicity distance, which estimates how long a cipher
must be to virtually eliminate such uncertainty.
We find the ideas in Shannon?s (1949) paper rel-
evant to problems of statistical machine translation
and transliteration. When first exposed to the idea
of statistical machine translation, many people natu-
rally ask: (1) how much data is needed to get a good
result, and (2) can translation systems be trained
without parallel data? These are tough questions by
any stretch, and it is remarkable that Shannon was
already in the 1940s tackling such questions in the
realm of code-breaking, creating analytic formulas
to estimate answers.
Our novel contributions are as follows:
? We outline an exact letter-substitution deci-
pherment method which:
- guarantees that no key is overlooked, and
- can be executed with standard integer pro-
gramming solvers
? We present empirical results for decipherment
which:
- plot search-error-free decipherment results at
various cipher lengths, and
- demonstrate accuracy rates superior to EM-
based methods
? We carry out empirical testing of Shannon?s
formulas for decipherment uncertainty
2 Language Models
We work on letter substitution ciphers with spaces.
We look for the key (among 26! possible ones)
that, when applied to the ciphertext, yields the most
English-like result. We take ?English-like? to mean
812
most probable according to some statistical lan-
guage model, whose job is to assign some proba-
bility to any sequence of letters. According to a 1-
gram model of English, the probability of a plaintext
p1...pn is given by:
P (p1...pn) = P (p1) ? P (p2) ? ... ? P (pn)
That is, we obtain the probability of a sequence
by multiplying together the probabilities of the in-
dividual letters that make it up. This model assigns
a probability to any letter sequence, and the proba-
bilities of all letter sequences sum to one. We col-
lect letter probabilities (including space) from 50
million words of text available from the Linguistic
Data Consortium (Graff and Finch, 1994). We also
estimate 2- and 3-gram models using the same re-
sources:
P (p1...pn) = P (p1 | START ) ? P (p2 | p1) ? P (p3 | p2) ?
... ? P (pn | pn?1) ? P (END | pn)
P (p1...pn) = P (p1 | START ) ? P (p2 | START p1) ?
P (p3 | p1 p2) ? ... ? P (pn | pn?2 pn?1) ?
P (END | pn?1 pn)
Unlike the 1-gram model, the 2-gram model will
assign a low probability to the sequence ?ae? be-
cause the probability P (e | a) is low. Of course, all
these models are fairly weak, as already known by
(Shannon, 1949). When we stochastically generate
text according to these models, we get, for example:
1-gram: ... thdo detusar ii c ibt deg irn toihytrsen ...
2-gram: ... itariaris s oriorcupunond rke uth ...
3-gram: ... ind thnowelf jusision thad inat of ...
4-gram: ... rece bence on but ther servier ...
5-gram: ... mrs earned age im on d the perious ...
6-gram: ... a party to possible upon rest of ...
7-gram: ... t our general through approve the ...
We can further estimate the probability of a whole
English sentence or phrase. For example, the prob-
abilities of two plaintext phrases ?het oxf? and
?the fox? (which have the same letter frequency
distribution) is shown below. The 1-gram model
which counts only the frequency of occurrence of
each letter in the phrase, estimates the same proba-
bility for both the phrases ?het oxf? and ?the fox?,
since the same letters occur in both phrases. On the
other hand, the 2-gram and 3-gram models, which
take context into account, are able to distinguish be-
tween the English and non-English phrases better,
and hence assign a higher probability to the English
phrase ?the fox?.
Model P(het oxf) P(the fox)
1-gram 1.83? 10?9 1.83? 10?9
2-gram 3.26? 10?11 1.18? 10?7
3-gram 1.89? 10?13 1.04? 10?6
Over a longer sequence X of length N , we can
also calculate ?log2(P (X))/N , which (per Shan-
non) gives the compression rate permitted by the
model, in bits per character. In our case, we get:1
1-gram: 4.19
2-gram: 3.51
3-gram: 2.93
3 Decipherment
Given a ciphertext c1...cn, we search for the key that
yields the most probable plaintext p1...pn. There are
26! possible keys, too many to enumerate. How-
ever, we can still find the best one in a guaranteed
fashion. We do this by taking our most-probable-
plaintext problem and casting it as an integer pro-
gramming problem.2
Here is a sample integer programming problem:
variables: x, y
minimize:
2x+ y
subject to:
x+ y < 6.9
y ? x < 2.5
y > 1.1
We require that x and y take on integer values. A
solution can be obtained by typing this integer pro-
gram into the publicly available lp solve program,
1Because spacing is fixed in our letter substitution ciphers,
we normalize P (X) by the sum of probabilities of all English
strings that match the spacing pattern of X .
2For an overview of integer and linear programming, see for
example (Schrijver, 1998).
813
1    2    3    4    5    6    7    8  ?
_    Q    W    B    S    Q    W    _  ?
a    a    a    a    a    a    a    a  ?
b    b    b    b    b    b    b    b  ?
c    c    c    c    c    c    c    c  ?
d    d    d    d    d    d    d    d  ?
e    e    e    e    e    e    e    e  ?
?    ?    ?    ?    ?    ?    ?    ?  ?
z    z    z    z    z    z    z    z  ?
_    _    _    _    _    _    _    _  ?
ciphertext
network of
possible
plaintexts
link-2de link-5ad link-7e_
Figure 1: A decipherment network. The beginning of the ciphertext is shown at the top of the figure (underscores
represent spaces). Any left-to-right path through the network constitutes a potential decipherment. The bold path
corresponds to the decipherment ?decade?. The dotted path corresponds to the decipherment ?ababab?. Given a
cipher length of n, the network has 27 ? 27 ? (n ? 1) links and 27n paths. Each link corresponds to a named variable
in our integer program. Three links are shown with their names in the figure.
or the commercially available CPLEX program,
which yields the result: x = 4, y = 2.
Suppose we want to decipher with a 2-gram lan-
guage model, i.e., we want to find the key that yields
the plaintext of highest 2-gram probability. Given
the ciphertext c1...cn, we create an integer program-
ming problem as follows. First, we set up a net-
work of possible decipherments (Figure 1). Each
of the 27 ? 27 ? (n ? 1) links in the network is a
binary variable in the integer program?it must be
assigned a value of either 0 or 1. We name these
variables linkXY Z , where X indicates the column
of the link?s source, and Y and Z represent the rows
of the link?s source and destination (e.g. variables
link1aa, link1ab, link5qu, ...).
Each distinct left-to-right path through the net-
work corresponds to a different decipherment. For
example, the bold path in Figure 1 corresponds to
the decipherment ?decade?. Decipherment amounts
to turning some links ?on? (assigning value 1 to the
link variable) and others ?off? (assigning value 0).
Not all assignments of 0?s and 1?s to link variables
result in a coherent left-to-right path, so we must
place some ?subject to? constraints in our integer
program.
We observe that a set of variables forms a path if,
for every node in columns 2 through n?1 of the net-
work, the following property holds: the sum of the
values of the link variables entering the node equals
the sum of the link variables leaving the node. For
nodes along a chosen decipherment path, this sum
will be 1, and for others, it will be 0.3 Therefore,
we create one ?subject to? constraint for each node
(? ? stands for space). For example, for the node in
column 2, row e we have:
subject to:
link1ae + link1be + link1ce + ...+ link1 e
= link2ea + link2eb + link2ec + ...+ link2e
Now we set up an expression for the ?minimize?
part of the integer program. Recall that we want
to select the plaintext p1...pn of highest probability.
For the 2-gram language model, the following are
equivalent:
(a) Maximize P (p1...pn)
(b) Maximize log2 P (p1...pn)
(c) Minimize ?log2 P (p1...pn)
(d) Minimize ?log2 [ P (p1 |START )
3Strictly speaking, this constraint over nodes still allows
multiple decipherment paths to be active, but we can rely on
the rest of our integer program to select only one.
814
?P (p2 | p1)
? ...
?P (pn | pn?1)
?P (END | pn) ]
(e) Minimize ?log2 P (p1 |START )
?log2 P (p2 | p1)
? ...
?log2 P (pn | pn?1)
?log2 P (END | pn)
We can guarantee this last outcome if we con-
struct our minimization function as a sum of 27 ?27 ?
(n? 1) terms, each of which is a linkXY Z variable
multiplied by ?log2P (Z|Y ):
Minimize link1aa ? ?log2 P (a | a)
+ link1ab ? ?log2 P (b | a)
+ link1ac ? ?log2 P (c | a)
+ ...
+ link5qu ? ?log2 P (u | q)
+ ...
When we assign value 1 to link variables along
some decipherment path, and 0 to all others, this
function computes the negative log probability of
that path.
We must still add a few more ?subject to? con-
straints. We need to ensure that the chosen path im-
itates the repetition pattern of the ciphertext. While
the bold path in Figure 1 represents the fine plain-
text choice ?decade?, the dotted path represents the
choice ?ababab?, which is not consistent with the
repetition pattern of the cipher ?QWBSQW?. To
make sure our substitutions obey a consistent key,
we set up 27 ? 27 = 729 new keyxy variables to
represent the choice of key. These new variables
are also binary, taking on values 0 or 1. If variable
keyaQ = 1, that means the key maps plaintext a to
ciphertext Q. Clearly, not all assignments to these
729 variables represent valid keys, so we augment
the ?subject to? part of our integer program by re-
quiring that for any letter x,
subject to:
keyxA + keyxB + ...+ keyxZ + keyx = 1
keyAx + keyBx + ...+ keyZx + key x = 1
That is, every plaintext letter must map to exactly
one ciphertext letter, and every ciphertext letter must
map to exactly one plaintext letter. We also add a
constraint to ensure that the ciphertext space charac-
ter maps to the plaintext space character:
subject to:
key = 1
Finally, we ensure that any chosen decipherment
path of linkXY Z variables is consistent with the
chosen key. We know that for every node A along
the decipherment path, exactly one active link has
A as its destination. For all other nodes, zero active
links lead in. Suppose node A represents the de-
cipherment of ciphertext letter ci as plaintext letter
pj?for all such nodes, we stipulate that the sum of
values for link(i?1)xpj (for all x) equals the value of
keypjci . In other words, whether a node lies along
the chosen decipherment path or not, the chosen key
must support that decision.
Figure 2 summarizes the integer program that we
construct from a given ciphertext c1...cn. The com-
puter code that transforms any given cipher into a
corresponding integer program runs to about one
page. Variations on the decipherment network yield
1-gram and 3-gram decipherment capabilities. Once
an integer program is generated by machine, we
ask the commercially-available CPLEX software
to solve it, and then we note which keyXY variables
are assigned value 1. Because CPLEX computes
the optimal key, the method is not fast?for ciphers
of length 32, the number of variables and constraints
encoded in the integer program (IP) along with aver-
age running times are shown below. It is possible to
obtain less-than-optimal keys faster by interrupting
the solver.
Model # of IP # of IP Average
variables constraints running time
1-gram 1, 755 1, 083 0.01 seconds
2-gram 27, 700 2, 054 50 seconds
3-gram 211, 600 27, 326 450 seconds
4 Decipherment Experiments
We create 50 ciphers each of lengths 2, 4, 8, ..., 256.
We solve these with 1-gram, 2-gram, and 3-gram
language models. We record the average percentage
of ciphertext tokens decoded incorrectly. 50% error
means half of the ciphertext tokens are deciphered
wrong, while 0% means perfect decipherment. Here
815
variables:
linkipr 1 if the ith cipher letter is deciphered as plaintext letter p AND the (i+1)th cipher letter is
deciphered as plaintext letter r
0 otherwise
keypq 1 if decipherment key maps plaintext letter p to ciphertext letter q
0 otherwise
minimize:?n?1
i=1
?
p,r
linkipr ? ?log P (r|p) (2-gram probability of chosen plaintext)
subject to:
for all p:
?
r
keypr = 1 (each plaintext letter maps to exactly one ciphertext letter)
for all p:
?
r
keyrp = 1 (each ciphertext letter maps to exactly one plaintext letter)
key = 1 (cipher space character maps to plain space character)
for (i=1...n-2), for all r: [
?
p
linkipr =
?
p
link(i+1)rp ] (chosen links form a left-to-right path)
for (i=1...n-1), for all p:
?
r
linkirp = keypci+1 (chosen links are consistent with chosen key)
Figure 2: Summary of how to build an integer program for any given ciphertext c1...cn. Solving the integer program
will yield the decipherment of highest probability.
we illustrate some automatic decipherments with er-
ror rates:
42% error: the avelage ongrichman hal cy wiof a
sevesonme qus antizexty that he buprk lathes we blung
than soment - fotes mmasthes
11% error: the average englishman has so week a
reference for antiality that he would rather be prong than
recent - deter quarteur
2% error: the average englishman has so keep a
reference for antiquity that he would rather be wrong than
recent - peter mcarthur
0% error: the average englishman has so deep a
reverence for antiquity that he would rather be wrong
than recent - peter mcarthur
Figure 3 shows our automatic decipherment re-
sults. We note that the solution method is exact, not
heuristic, so that decipherment error is not due to
search error. Our use of global key constraints also
leads to accuracy that is superior to (Knight et al,
2006). With a 2-gram model, their EM algorithm
gives 10% error for a 414-letter cipher, while our
method provides a solution with only 0.5% error.
At shorter cipher lengths, we observe much higher
improvements when using our method. For exam-
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
De
cip
he
rm
en
t E
rro
r (
%)
Cipher Length (letters)2 4 8 16 32 64 128 256 512 1024
1-gram
2-gram
3-gram
Figure 3: Average decipherment error using integer pro-
gramming vs. cipher length, for 1-gram, 2-gram and 3-
gram models of English. Error bars indicate 95% confi-
dence intervals.
ple, on a 52-letter textbook cipher, using a 2-gram
model, the solution from our method resulted in 21%
error as compared to 85% error given by the EM so-
lution.
We see that deciphering with 3-grams works well
on ciphers of length 64 or more. This confirms
816
that such ciphers can be attacked with very limited
knowledge of English (no words or grammar) and
little custom programming.
The 1-gram model works badly in this scenario,
which is consistent with Bauer?s (2006) observation
that for short texts, mechanical decryption on the ba-
sis of individual letter frequencies does not work. If
we had infinite amounts of ciphertext and plaintext
drawn from the same stochastic source, we would
expect the plain and cipher frequencies to eventually
line up, allowing us to read off a correct key from the
frequency tables. The upper curve in Figure 3 shows
that convergence to this end is slow.
5 Shannon Equivocation and Unicity
Distance
Very short ciphers are hard to solve accurately.
Shannon (1949) pinpointed an inherent difficulty
with short ciphers, one that is independent of the so-
lution method or language model used; the cipher
itself may not contain enough information for its
proper solution. For example, given a short cipher
like XY Y X , we can never be sure if the answer is
peep, noon, anna, etc. Shannon defined a mathemat-
ical measure of our decipherment uncertainty, which
he called equivocation (now called entropy).
Let C be a cipher, M be the plaintext message it
encodes, and K be the key by which the encoding
takes place. Before even seeing C, we can compute
our uncertainty about the key K by noting that there
are 26! equiprobable keys:4
H(K) = ?(26!) ? (1/26!) ? log2 (1/26!)
= 88.4 bits
That is, any secret key can be revealed in 89 bits.
When we actually receive a cipher C, our uncer-
tainty about the key and the plaintext message is re-
duced. Shannon described our uncertainty about the
plaintext message, letting m range over all decipher-
ments:
H(M |C) = equivocation of plaintext message
= ?
?
m
P (m|C) ? log2 P (m|C)
4(Shannon, 1948) The entropy associated with a set of pos-
sible events whose probabilities of occurrence are p1, p2, ..., pn
is given by H = ?
?n
i=1 pi ? log2(pi).
P (m|C) is probability of plaintext m (according
to the language model) divided by the sum of proba-
bilities of all plaintext messages that obey the repeti-
tion pattern of C. While integer programming gives
us a method to find the most probable decipherment
without enumerating all keys, we do not know of a
similar method to compute a full equivocation with-
out enumerating all keys. Therefore, we sample up
to 100,000 plaintext messages in the neighborhood
of the most probably decipherment5 and compute
H(M |C) over that subset.6
Shannon also described H(K|C), the equivoca-
tion of key. This uncertainty is typically larger than
H(M |C), because a given message M may be de-
rived from C via more than one key, in case C does
not contain all 26 letters of the alphabet.
We compute H(K|C) by letting r(C) be the
number of distinct letters in C, and letting q(C) be
(26 ? r(C))!. Letting i range over our sample of
plaintext messages, we get:
H(K|C) = equivocation of key
= ?
?
i
q(C) ? (P (i)/q(C)) ? log2 (P (i)/q(C))
= ?
?
i
P (i) ? log2 (P (i)/q(C))
= ?
?
i
P (i) ? (log2 P (i)? log2 q(C))
= ?
?
i
P (i) ? log2 P (i) +
?
i
P (i) ? log2 q(C)
= H(M |C) + log2 q(C)
Shannon (1949) used analytic means to roughly
sketch the curves for H(K|C) and H(M |C), which
we reproduce in Figure 4. Shannon?s curve is drawn
for a human-level language model, and the y-axis is
given in ?decimal digits? instead of bits.
5The sampling used to compute H(M |C) starts with the
optimal key and expands out a frontier, by swapping letters in
the key, and recursing to generate new keys (and corresponding
plaintext message decipherments). The plaintext messages are
remembered so that the frontier expands efficiently. The sam-
pling stops if 100,000 different messages are found.
6Interestingly, as we grow our sample out from the most
probable plaintext, we do not guarantee that any intermediate
result is a lower bound on the equivocation. An example is pro-
vided by the growing sample (0.12, 0.01, 0.01, 0.01, 0.01, 0.01,
0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01), whose entropy steadily
increases. However, if we add a 14th item whose P (m) is 0.12,
the entropy suddenly decreases from 2.79 to 2.78.
817
Unicity Distance
Key Equivocation
Message Equivocation
Figure 4: Equivocation for simple substitution on English (Shannon, 1949).
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 0  10  20  30  40  50  60  70  80  90  100
 110
 120
 130
 140
 150
 160
 170
 180
 190
 200
 210
 220
 230
 240
 250
 260
Eq
uiv
oc
ati
on
 of
 ke
y (
bit
s)
Cipher Length
1-gram
2-gram
3-gram
Figure 5: Average key equivocation observed (bits) vs.
cipher length (letters), for 1-gram, 2-gram and 3-gram
models of English.
For comparison, we plot in Figures 5 and 6 the av-
erage equivocations as we empirically observe them
using our 1-, 2-, and 3-gram language models.
The shape of the key equivocation curve follows
Shannon, except that it is curved from the start,
rather than straight.
The message equivocation curve follows Shan-
non?s prediction, rising then falling. Because very
short ciphers have relatively few solutions (for ex-
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 0  10  20  30  40  50  60  70  80  90  100
 110
 120
 130
 140
 150
 160
 170
 180
 190
 200
 210
 220
 230
 240
 250
 260
Eq
uiv
oc
ati
on
 of
 m
es
sa
ge
 (b
its
)
Cipher Length
1-gram
2-gram
3-gram
Figure 6: Average message equivocation observed (bits)
vs. cipher length (letters), for 1-gram, 2-gram and 3-gram
models of English.
ample, a one-letter cipher has only 26), the overall
uncertainty is not that great.7 As the cipher gets
longer, message equivocation rises. At some point,
it then decreases, as the cipher begins to reveal its
secret through patterns of repetition.
Shannon?s analytic model also predicts a sharp
decline of message equivocation towards zero. He
7Uncertainty is only loosely related to accuracy?even if we
are quite certain about a solution, it may still be wrong.
818
defines the unicity distance (U ) as the cipher length
at which we have virtually no more uncertainty
about the plaintext. Using analytic means (and vari-
ous approximations), he gives:
U = H(K)/(A?B)
where:
A = bits per character of a 0-gram model (4.7)
B = bits per character of the model used to decipher
For a human-level language model (B ? 1.2), he
concludes U ? 25, which is confirmed by practice.
For our language models, the formula gives:
U = 173 (1-gram)
U = 74 (2-gram)
U = 50 (3-gram)
These numbers are in the same ballpark as
Bauer (2006), who gives 167, 74, and 59. We note
that these predicted unicity distances are a bit too
rosy, according to our empirical message equivoca-
tion curves. Our experience confirms this as well, as
1-gram frequency counts over a 173-letter cipher are
generally insufficient to pin down a solution.
6 Conclusion
We provide a method for deciphering letter substi-
tution ciphers with low-order models of English.
This method, based on integer programming, re-
quires very little coding and can perform an opti-
mal search over the key space. We conclude by not-
ing that English language models currently used in
speech recognition (Chelba and Jelinek, 1999) and
automated language translation (Brants et al, 2007)
are much more powerful, employing, for example,
7-gram word models (not letter models) trained on
trillions of words. Obtaining optimal keys accord-
ing to such models will permit the automatic deci-
pherment of shorter ciphers, but this requires more
specialized search than what is provided by gen-
eral integer programming solvers. Methods such
as these should also be useful for natural language
decipherment problems such as character code con-
version, phonetic decipherment, and word substitu-
tion ciphers with applications in machine translation
(Knight et al, 2006).
7 Acknowledgements
The authors wish to gratefully acknowledge
Jonathan Graehl, for providing a proof to support
the argument that taking a larger number of samples
does not necessarily increase the equivocation. This
research was supported by the Defense Advanced
Research Projects Agency under SRI International?s
prime Contract Number NBCHD040058.
References
Friedrich L. Bauer. 2006. Decrypted Secrets: Methods
and Maxims of Cryptology. Springer-Verlag.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of EMNLP-
CoNLL.
Ciprian Chelba and Frederick Jelinek. 1999. Structured
language modeling for speech recognition. In Pro-
ceedings of NLDB: 4th International Conference on
Applications of Natural Language to Information Sys-
tems.
Ravi Ganesan and Alan T. Sherman. 1993. Statistical
techniques for language recognition: An introduction
and guide for cryptanalysts. Cryptologia, 17(4):321?
366.
David Graff and Rebecca Finch. 1994. Multilingual text
resources at the linguistic data consortium. In Pro-
ceedings of the HLT Workshop on Human Language
Technology.
Thomas Jakobsen. 1995. A fast method for cryptanalysis
of substitution ciphers. Cryptologia, 19(3):265?274.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipherment
problems. In Proceedings of the COLING/ACL.
Edwin Olson. 2007. Robust dictionary attack of short
simple substitution ciphers. Cryptologia, 31(4):332?
342.
Shmuel Peleg and Azriel Rosenfeld. 1979. Break-
ing substitution ciphers using a relaxation algorithm.
Comm. ACM, 22(11):598?605.
Alexander Schrijver. 1998. Theory of Linear and Integer
Programming. John Wiley & Sons.
Claude E. Shannon. 1948. A mathematical theory
of communication. Bell System Technical Journal,
27:379?423 and 623?656.
Claude E. Shannon. 1949. Communication theory
of secrecy systems. Bell System Technical Journal,
28:656?715.
819
