Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 324?332,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Topological Ordering of Function Words
in Hierarchical Phrase-based Translation
Hendra Setiawan
1
and Min-Yen Kan
2
and Haizhou Li
3
and Philip Resnik
1
1
University of Maryland Institute for Advanced Computer Studies
2
School of Computing, National University of Singapore
3
Human Language Technology, Institute for Infocomm Research, Singapore
{hendra,resnik}@umiacs.umd.edu,
kanmy@comp.nus.edu.sg, hli@i2r.a-star.edu.sg
Abstract
Hierarchical phrase-based models are at-
tractive because they provide a consis-
tent framework within which to character-
ize both local and long-distance reorder-
ings, but they also make it difcult to
distinguish many implausible reorderings
from those that are linguistically plausi-
ble. Rather than appealing to annotation-
driven syntactic modeling, we address this
problem by observing the inuential role
of function words in determining syntac-
tic structure, and introducing soft con-
straints on function word relationships as
part of a standard log-linear hierarchi-
cal phrase-based model. Experimentation
on Chinese-English and Arabic-English
translation demonstrates that the approach
yields signicant gains in performance.
1 Introduction
Hierarchical phrase-based models (Chiang, 2005;
Chiang, 2007) offer a number of attractive bene-
ts in statistical machine translation (SMT), while
maintaining the strengths of phrase-based systems
(Koehn et al, 2003). The most important of these
is the ability to model long-distance reordering ef-
ciently. To model such a reordering, a hierar-
chical phrase-based system demands no additional
parameters, since long and short distance reorder-
ings are modeled identically using synchronous
context free grammar (SCFG) rules. The same
rule, depending on its topological ordering ? i.e.
its position in the hierarchical structure ? can af-
fect both short and long spans of text. Interest-
ingly, hierarchical phrase-based models provide
this benet without making any linguistic commit-
ments beyond the structure of the model.
However, the system's lack of linguistic com-
mitment is also responsible for one of its great-
est drawbacks. In the absence of linguistic knowl-
edge, the system models linguistic structure using
an SCFG that contains only one type of nontermi-
nal symbol
1
. As a result, the system is susceptible
to the overgeneration problem: the grammar may
suggest more reordering choices than appropriate,
and many of those choices lead to ungrammatical
translations.
Chiang (2005) hypothesized that incorrect re-
ordering choices would often correspond to hier-
archical phrases that violate syntactic boundaries
in the source language, and he explored the use
of a ?constituent feature? intended to reward the
application of hierarchical phrases which respect
source language syntactic categories. Although
this did not yield signicant improvements, Mar-
ton and Resnik (2008) and Chiang et al (2008)
extended this approach by introducing soft syn-
tactic constraints similar to the constituent feature,
but more ne-grained and sensitive to distinctions
among syntactic categories; these led to substan-
tial improvements in performance. Zollman et al
(2006) took a complementary approach, constrain-
ing the application of hierarchical rules to respect
syntactic boundaries in the target language syn-
tax. Whether the focus is on constraints from the
source language or the target language, the main
ingredient in both previous approaches is the idea
of constraining the spans of hierarchical phrases to
respect syntactic boundaries.
In this paper, we pursue a different approach
to improving reordering choices in a hierarchical
phrase-based model. Instead of biasing the model
toward hierarchical phrases whose spans respect
syntactic boundaries, we focus on the topologi-
cal ordering of phrases in the hierarchical struc-
ture. We conjecture that since incorrect reorder-
ing choices correspond to incorrect topological or-
derings, boosting the probability of correct topo-
1
In practice, one additional nonterminal symbol is used in
?glue rules?. This is not relevant in the present discussion.
324
logical ordering choices should improve the sys-
tem. Although related to previous proposals (cor-
rect topological orderings lead to correct spans
and vice versa), our proposal incorporates broader
context and is structurally more aware, since we
look at the topological ordering of a phrase relative
to other phrases, rather than modeling additional
properties of a phrase in isolation. In addition, our
proposal requires no monolingual parsing or lin-
guistically informed syntactic modeling for either
the source or target language.
The key to our approach is the observation that
we can approximate the topological ordering of
hierarchical phrases via the topological ordering
of function words. We introduce a statistical re-
ordering model that we call the pairwise domi-
nance model, which characterizes reorderings of
phrases around a pair of function words. In mod-
eling function words, our model can be viewed as
a successor to the function words-centric reorder-
ing model (Setiawan et al, 2007), expanding on
the previous approach by modeling pairs of func-
tion words rather than individual function words
in isolation.
The rest of the paper is organized as follows. In
Section 2, we briey review hierarchical phrase-
based models. In Section 3, we rst describe the
overgeneration problem in more detail with a con-
crete example, and then motivate our idea of us-
ing the topological ordering of function words to
address the problem. In Section 4, we develop
our idea by introducing the pairwise dominance
model, expressing function word relationships in
terms of what we call the the dominance predi-
cate. In Section 5, we describe an algorithm to es-
timate the parameters of the dominance predicate
from parallel text. In Sections 6 and 7, we describe
our experiments, and in Section 8, we analyze the
output of our system and lay out a possible future
direction. Section 9 discusses the relation of our
approach to prior work and Section 10 wraps up
with our conclusions.
2 Hierarchical Phrase-based System
Formally, a hierarchical phrase-based SMT sys-
tem is based on a weighted synchronous context
free grammar (SCFG) with one type of nonter-
minal symbol. Synchronous rules in hierarchical
phrase-based models take the following form:
X ? ??, ?,?? (1)
where X is the nonterminal symbol and ? and ?
are strings that contain the combination of lexical
items and nonterminals in the source and target
languages, respectively. The ? symbol indicates
that nonterminals in ? and ? are synchronized
through co-indexation; i.e., nonterminals with the
same index are aligned. Nonterminal correspon-
dences are strictly one-to-one, and in practice the
number of nonterminals on the right hand side is
constrained to at most two, which must be sepa-
rated by lexical items.
Each rule is associated with a score that is com-
puted via the following log linear formula:
w(X ? ??, ?,??) =
?
i
f?ii (2)
where fi is a feature describing one particular as-
pect of the rule and ?i is the corresponding weight
of that feature. Given e? and f? as the source
and target phrases associated with the rule, typi-
cal features used are rule's translation probability
Ptrans(f? |e?) and its inverse Ptrans(e?|f?), the lexi-
cal probability Plex(f? |e?) and its inverse Plex(e?|f?).
Systems generally also employ a word penalty, a
phrase penalty, and target language model feature.
(See (Chiang, 2005) for more detailed discussion.)
Our pairwise dominance model will be expressed
as an additional rule-level feature in the model.
Translation of a source sentence e using hier-
archical phrase-based models is formulated as a
search for the most probable derivationD? whose
source side is equal to e:
D? = argmax P (D),where source(D)=e.
D = Xi, i ? 1...|D| is a set of rules following a
certain topological ordering, indicated here by the
use of the superscript.
3 Overgeneration and Topological
Ordering of Function Words
The use of only one type of nonterminal allows a
exible permutation of the topological ordering of
the same set of rules, resulting in a huge number of
possible derivations from a given source sentence.
In that respect, the overgeneration problem is not
new to SMT: Bracketing Transduction Grammar
(BTG) (Wu, 1997) uses a single type of nontermi-
nal and is subject to overgeneration problems, as
well.
2
2
Note, however, that overgeneration in BTG can be
viewed as a feature, not a bug, since the formalism was origi-
325
The problem may be less severe in hierarchi-
cal phrase-based MT than in BTG, since lexical
items on the rules' right hand sides often limit the
span of nonterminals. Nonetheless overgeneration
of reorderings is still problematic, as we illustrate
using the hypothetical Chinese-to-English exam-
ple in Fig. 1.
Suppose we want to translate the Chinese sen-
tence in Fig. 1 into English using the following set
of rules:
1. Xa ? ??Z X1, computers andX1?
2. Xb ? ?X14 X2, X1 are X2?
3. Xc ? ?C? , cell phones ?
4. Xd ? ?X1{? , inventions of X1?
5. Xe ? ???- , the last century ?
Co-indexation of nonterminals on the right hand
side is indicated by subscripts, and for our ex-
amples the label of the nonterminal on the left
hand side is used as the rule's unique identier.
To correctly translate the sentence, a hierarchical
phrase-based system needs to model the subject
noun phrase, object noun phrase and copula con-
structions; these are captured by rulesXa,Xd and
Xb respectively, so this set of rules represents a
hierarchical phrase-based system that can be used
to correctly translate the Chinese sentence. Note
that the Chinese word order is correctly preserved
in the subject (Xa) as well as copula constructions
(Xb), and correctly inverted in the object construc-
tion (Xd).
However, although it can generate the correct
translation in Fig. 2, the grammar has no mech-
anism to prevent the generation of an incorrect
translation like the one illustrated in Fig. 3. If
we contrast the topological ordering of the rules
in Fig. 2 and Fig. 3, we observe that the difference
is small but quite signicant. Using precede sym-
bol (?) to indicate the rst operand immediately
dominates the second operand in the hierarchical
structure, the topological orderings in Fig. 2 and
Fig. 3 are Xa ? Xb ? Xc ? Xd ? Xe and
Xd ? Xa ? Xb ? Xc ? Xe, respectively. The
only difference is the topological ordering of Xd:
in Fig. 2, it appears below most of the other hier-
archical phrases, while in Fig. 3, it appears above
all the other hierarchical phrases.
nally introduced for bilingual analysis rather than generation
of translations.
Modeling the topological ordering of hierarchi-
cal phrases is computationally prohibitive, since
there are literally millions of hierarchical rules in
the system's automatically-learned grammar and
millions of possible ways to order their applica-
tion. To avoid this computational problem and
still model the topological ordering, we propose
to use the topological ordering of function words
as a practical approximation. This is motivated by
the fact that function words tend to carry crucial
syntactic information in sentences, serving as the
?glue? for content-bearing phrases. Moreover, the
positional relationships between function words
and content phrases tends to be xed (e.g., in En-
glish, prepositions invariably precede their object
noun phrase), at least for the languages we have
worked with thus far.
In the Chinese sentence above, there are three
function words involved: the conjunctionZ (and),
the copula 4 (are), and the noun phrase marker
{ (of).3 Using the function words as approximate
representations of the rules in which they appear,
the topological ordering of hierarchical phrases in
Fig. 2 is Z(and) ? 4(are) ? {(of), while that
in Fig. 3 is {(of) ? Z(and) ? 4(are).4 We
can distinguish the correct and incorrect reorder-
ing choices by looking at this simple information.
In the correct reordering choice,{(of) appears at
the lower level of the hierarchy while in the incor-
rect one,{(of) appears at the highest level of the
hierarchy.
4 Pairwise Dominance Model
Our example suggests that we may be able to im-
prove the translation model's sensitivity to correct
versus incorrect reordering choices by modeling
the topological ordering of function words. We do
so by introducing a predicate capturing the domi-
nance relationship in a derivation between pairs of
neighboring function words.
5
Let us dene a predicate d(Y ?, Y ??) that takes
two function words as input and outputs one of
3
We use the term ?noun phrase marker? here in a general
sense, meaning that in this example it helps tell us that the
phrase is part of an NP, not as a technical linguistic term. It
serves in other grammatical roles, as well. Disambiguating
the syntactic roles of function words might be a particularly
useful thing to do in the model we are proposing; this is a
question for future research.
4
Note that for expository purposes, we designed our sim-
ple grammar to ensure that these function words appear in
separate rules.
5
Two function words are considered neighbors iff no other
function word appears between them in the source sentence.
326
? Z C? 4 ?{??-
?
XXXXXz
?????9?? ? ?
arecomputers and cell phones inventions of the last century
Figure 1: A running example of Chinese-to-English translation.
Xa???Z Xb, computers andXb?
???Z Xc4 Xd, computers andXc are Xd?
???ZC?4 Xd, computers and cell phones areXd?
???ZC?4 Xe{? , computers and cell phones are inventions ofXe?
???ZC?4??-{? , computers and cell phones are inventions of the last century?
Figure 2: The derivation that leads to the correct translation
Xd??Xa{? , inventions of Xa?
???Z Xb{? , inventions of computers andXb?
???Z Xc4 Xe{? , inventions of computers andXc are Xe?
???ZC?4 Xe{? , inventions of computers and cell phones areXe?
???ZC?4??-{? , inventions of computers and cell phones are the last century?
Figure 3: The derivation that leads to the incorrect translation
four values: {leftFirst, rightFirst, dontCare, nei-
ther}, where Y ? appears to the left of Y ?? in the
source sentence. The value leftFirst indicates that
in the derivation's topological ordering, Y ? pre-
cedes Y ?? (i.e. Y ? dominates Y ?? in the hierarchi-
cal structure), while rightFirst indicates that Y ??
dominates Y ?. In Fig. 2, d(Y ?, Y ??) = leftFirst
for Y ? = the copula 4 (are) and Y ?? = the noun
phrase marker{ (of).
The dontCare and neither values capture two
additional relationships: dontCare indicates that
the topological ordering of the function words is
exible, and neither indicates that the topologi-
cal ordering of the function words is disjoint. The
former is useful in cases where the hierarchical
phrases suggest the same kind of reordering, and
therefore restricting their topological ordering is
not necessary. This is illustrated in Fig. 2 by the
pairZ(and) and the copula 4(are), where putting
either one above the other does not change the -
nal word order. The latter is useful in cases where
the two function words do not share a same parent.
Formally, this model requires several changes in
the design of the hierarchical phrase-based system.
1. To facilitate topological ordering of function
words, the hierarchical phrases must be sub-
categorized with function words. Taking Xb
in Fig. 2 as a case in point, subcategorization
using function words would yield:
6
Xb(4 ?{) ? Xc4 Xd({) (3)
The subcategorization (indicated by the
information in parentheses following the
nonterminal) propagates the function word
4(are) of Xb to the higher level structure to-
gether with the function word {(of) of Xd.
This propagation process generalizes to other
rules by maintaining the ordering of the func-
tion words according to their appearance in
the source sentence. Note that the subcate-
gorized nonterminals often resemble genuine
syntactic categories, for instance X({) can
frequently be interpreted as a noun phrase.
2. To facilitate the computation of the domi-
nance relationship, the coindexing in syn-
chronized rules (indicated by the ? symbol
in Eq. 1) must be expanded to include infor-
mation not only about the nonterminal corre-
spondences but also about the alignment of
the lexical items. For example, adding lexi-
cal alignment information to rule Xd would
yield:
Xd ? ?X1{2?3, inventions3 of2 X1?
(4)
6
The target language side is concealed for clarity.
327
The computation of the dominance relation-
ship using this alignment information will be
discussed in detail in the next section.
Again takingXb in Fig. 2 as a case in point, the
dominance feature takes the following form:
fdom(Xb) ? dom(d(4,{)|4, {)) (5)
dom(d(YL, YR)|YL, YR)) (6)
where the probability of4 ?{ is estimated ac-
cording to the probability of d(4,{).
In practice, both 4(are) and {(of) may ap-
pear together in one same rule. In such a case, a
dominance score is not calculated since the topo-
logical ordering of the two function words is un-
ambiguous. Hence, in our implementation, a
dominance score is only calculated at the points
where the topological ordering of the hierarchical
phrases needs to be resolved, i.e. the two function
words always come from two different hierarchi-
cal phrases.
5 Parameter Estimation
Learning the dominance model involves extract-
ing d values for every pair of neighboring func-
tion words in the training bitext. Such statistics
are not directly observable in parallel corpora, so
estimation is needed. Our estimation method is
based on two facts: (1) the topological ordering
of hierarchical phrases is tightly coupled with the
span of the hierarchical phrases, and (2) the span
of a hierarchical phrase at a higher level is al-
ways a superset of the span of all other hierarchical
phrases at the lower level of its substructure. Thus,
to establish soft estimates of dominance counts,
we utilize alignment information available in the
rule together with the consistent alignment heuris-
tic (Och and Ney, 2004) traditionally used to guess
phrase alignments.
Specically, we dene the span of a function
word as a maximal, consistent alignment in the
source language that either starts from or ends
with the function word. (Requiring that spans be
maximal ensures their uniqueness.) We will re-
fer to such spans as Maximal Consistent Align-
ments (MCA). Note that each function word has
two such Maximal Consistent Alignments: one
that ends with the function word (MCAR)and an-
other that starts from the function word (MCAL).
Y ? Y ?? left- right- dont- nei-
First First Care ther
Z (and) 4 (are) 0.11 0.16 0.68 0.05
4 (are) { (of) 0.57 0.15 0.06 0.22
Table 1: The distribution of the dominance values
of the function words involved in Fig. 1. The value
with the highest probability is in bold.
Given two function words Y ? and Y ??, with Y ?
preceding Y ??, we dene the value of d by exam-
ining the MCAs of the two function words.
d(Y ?, Y ??) =?
?????
?????
leftFirst, Y ? 6? MCAR(Y ??) ? Y ??? MCAL(Y ?)
rightFirst, Y ?? MCAR(Y ??) ? Y ?? 6? MCAL(Y ?)
dontCare, Y ?? MCAR(Y ??) ? Y ??? MCAL(Y ?)
neither, Y ? 6? MCAR(Y ??) ? Y ?? 6? MCAL(Y ?)
(6)
Fig. 4a illustrates the leftFirst dominance value
where the intersection of the MCAs contains only
the second function word ({(of)). Fig. 4b illus-
trates the dontCare value, where the intersection
contains both function words. Similarly, rightFirst
and neither are represented by an intersection that
contains only Y ?, or by an empty intersection, re-
spectively. Once all the d values are counted, the
pairwise dominance model of neighboring func-
tion words can be estimated simply from counts
using maximum likelihood. Table 1 illustrates es-
timated dominance values that correctly resolve
the topological ordering for our running example.
6 Experimental Setup
We tested the effect of introducing the pairwise
dominance model into hierarchical phrase-based
translation on Chinese-to-English and Arabic-to-
English translation tasks, thus studying its effect
in two languages where the use of function words
differs signicantly. Following Setiawan et al
(2007), we identify function words as the N most
frequent words in the corpus, rather than identify-
ing them according to linguistic criteria; this ap-
proximation removes the need for any additional
language-specic resources. We report results
for N = 32, 64, 128, 256, 512, 1024, 2048.7 For
7
We observe that even N = 2048 represents less than
1.5% and 0.8% of the words in the Chinese and Arabic vo-
cabularies, respectively. The validity of the frequency-based
strategy, relative to linguistically-dened function words, is
discussed in Section 8
328
n
a
n
b
j
j
j
z
j
z
j
the last century
of
innovations
are
cell phones
and
computers
?
Z
C
? 4
?
-
 {

?
j
z
j
z
j
j
j
the last century
of
innovations
are
cell phones
and
computers
?
 Z
C
? 4
?
-
{

?
Figure 4: Illustrations for: a) the leftFirst value,
and b) the dontCare value. Thickly bordered
boxes are MCAs of the function words while solid
circles are the alignment points of the function
words. The gray boxes are the intersections of the
two MCAs.
all experiments, we report performance using the
BLEU score (Papineni et al, 2002), and we assess
statistical signicance using the standard boot-
strapping approach introduced by (Koehn, 2004).
Chinese-to-English experiments. We trained
the system on the NIST MT06 Eval corpus ex-
cluding the UN data (approximately 900K sen-
tence pairs). For the language model, we used a 5-
gram model with modied Kneser-Ney smoothing
(Kneser and Ney, 1995) trained on the English side
of our training data as well as portions of the Giga-
word v2 English corpus. We used the NIST MT03
test set as the development set for optimizing inter-
polation weights using minimum error rate train-
ing (MERT; (Och and Ney, 2002)). We carried out
evaluation of the systems on the NIST 2006 eval-
uation test (MT06) and the NIST 2008 evaluation
test (MT08). We segmented Chinese as a prepro-
cessing step using the Harbin segmenter (Zhao et
al., 2001).
Arabic-to-English experiments. We trained
the system on a subset of 950K sentence pairs
from the NIST MT08 training data, selected by
?subsampling? from the full training data using a
method proposed by Kishore Papineni (personal
communication). The subsampling algorithm se-
lects sentence pairs from the training data in a
way that seeks reasonable representation for all n-
grams appearing in the test set. For the language
model, we used a 5-gram model trained on the En-
glish portion of the whole training data plus por-
tions of the Gigaword v2 corpus. We used the
NIST MT03 test set as the development set for
optimizing the interpolation weights using MERT.
We carried out the evaluation of the systems on the
NIST 2006 evaluation set (MT06) and the NIST
2008 evaluation set (MT08). Arabic source text
was preprocessed by separating clitics, the de-
niteness marker, and the future tense marker from
their stems.
7 Experimental Results
Chinese-to-English experiments. Table 2 sum-
marizes the results of our Chinese-to-English ex-
periments. These results conrm that the pairwise
dominance model can signicantly increase per-
formance as measured by the BLEU score, with a
consistent pattern of results across the MT06 and
MT08 test sets. Modeling N = 32 drops the per-
formance marginally below baseline, suggesting
that perhaps there are not enough words for the
pairwise dominance model to work with. Dou-
bling the number of words (N = 64) produces
a small gain, and dening the pairwise dominance
model using N = 128 most frequent words pro-
duces a statistically signicant 1-point gain over
the baseline (p < 0.01). Larger values of N
yield statistically signicant performance above
the baseline, but without further improvements
over N = 128.
Arabic-to-English experiments. Table 3 sum-
marizes the results of our Arabic-to-English ex-
periments. This set of experiments shows a pat-
tern consistent with what we observed in Chinese-
to-English translation, again generally consistent
across MT06 and MT08 test sets although mod-
eling a small number of lexical items (N = 32)
brings a marginal improvement over the baseline.
In addition, we again nd that the pairwise dom-
inance model with N = 128 produces the most
signicant gain over the baseline in the MT06,
although, interestingly, modeling a much larger
number of lexical items (N = 2048) yields the
strongest improvement for the MT08 test set.
329
MT06 MT08
baseline 30.58 24.08
+dom(N = 32) 30.43 23.91
+dom(N = 64) 30.96 24.45
+dom(N = 128) 31.59 24.91
+dom(N = 256) 31.24 24.26
+dom(N = 512) 31.33 24.39
+dom(N = 1024) 31.22 24.79
+dom(N = 2048) 30.75 23.92
Table 2: Experimental results on Chinese-to-
English translation with the pairwise dominance
model (dom) of different N . The baseline (the
rst line) is the original hierarchical phrase-based
system. Statistically signicant results (p < 0.01)
over the baseline are in bold.
MT06 MT08
baseline 41.56 40.06
+dom(N = 32) 41.66 40.26
+dom(N = 64) 42.03 40.73
+dom(N = 128) 42.66 41.08
+dom(N = 256) 42.28 40.69
+dom(N = 512) 41.97 40.95
+dom(N = 1024) 42.05 40.55
+dom(N = 2048) 42.48 41.47
Table 3: Experimental results on Arabic-to-
English translation with the pairwise dominance
model (dom) of different N . The baseline (the
rst line) is the original hierarchical phrase-based
system. Statistically signicant results over the
baseline (p < 0.01) are in bold.
8 Discussion and Future Work
The results in both sets of experiments show con-
sistently that we have achieved a signicant gains
by modeling the topological ordering of function
words. When we visually inspect and compare
the outputs of our system with those of the base-
line, we observe that improved BLEU score often
corresponds to visible improvements in the sub-
jective translation quality. For example, the trans-
lations for the Chinese sentence ?<1 
?2 :3
?4 ?5 ?6 8?7 8 9 ?10 ?11 ?12
?13?, taken from Chinese MT06 test set, are as
follows (co-indexing subscripts represent recon-
structed word alignments):
? baseline: ?military1 intelligence2 un-
der observation8 in5 u.s.6 air raids7 :3 iran4
to9 how11 long12 ?13 ?
? +dom(N=128): ? military1 survey2 :3 how11
long12 iran4 under8 air strikes7 of the u.s6
can9 hold out10 ?13 ?
In addition to some lexical translation errors
(e.g. ?6 should be translated to U.S. Army),
the baseline system also makes mistakes in re-
ordering. The most obvious, perhaps, is its fail-
ure to capture the wh-movement involving the in-
terrogative word ?11 (how); this should move
to the beginning of the translated clause, consis-
tent with English wh-fronting as opposed to Chi-
nese wh in situ. The pairwise dominance model
helps, since the dominance value between the in-
terrogative word and its previous function word,
the modal verb 9(can) in the baseline system's
output, is neither, rather than rightFirst as in the
better translation.
The fact that performance tends to be best us-
ing a frequency threshold of N = 128 strikes
us as intuitively sensible, given what we know
about word frequency rankings.
8
In English,
for example, the most frequent 128 words in-
clude virtually all common conjunctions, deter-
miners, prepositions, auxiliaries, and comple-
mentizers ? the crucial elements of ?syntactic
glue? that characterize the types of linguistic
phrases and the ordering relationships between
them ? and a very small proportion of con-
tent words. Using Adam Kilgarriff's lemma-
tized frequency list from the British National Cor-
pus, http://www.kilgarriff.co.uk/bnc-readme.html,
the most frequent 128 words in English are heav-
ily dominated by determiners, ?functional? ad-
verbs like not and when, ?particle? adverbs like
up, prepositions, pronouns, and conjunctions, with
some arguably ?functional? auxiliary and light
verbs like be, have, do, give, make, take. Con-
tent words are generally limited to a small number
of frequent verbs like think and want and a very
small handful of frequent nouns. In contrast, ranks
129-256 are heavily dominated by the traditional
content-word categories, i.e. nouns, verbs, adjec-
tives and adverbs, with a small number of left-over
function words such as less frequent conjunctions
while, when, and although.
Consistent with these observations for English,
the empirical results for Chinese suggest that our
8
In fact, we initially simply choseN = 128 for our exper-
imentation, and then did runs with alternative N to conrm
our intuitions.
330
approximation of function words using word fre-
quency is reasonable. Using a list of approxi-
mately 900 linguistically identied function words
in Chinese extracted from (Howard, 2002), we ob-
serve that that the performance drops when in-
creasing N above 128 corresponds to a large in-
crease in the number of non-function words used
in the model. For example, with N = 2048, the
proportion of non-function words is 88%, com-
pared to 60% when N = 128.9
One natural extension of this work, therefore,
would be to tighten up our characterization of
function words, whether statistically, distribution-
ally, or simply using manually created resources
that exist for many languages. As a rst step, we
did a version of the Chinese-English experiment
using the list of approximately 900 genuine func-
tion words, testing on the Chinese MT06 set. Per-
haps surprisingly, translation performance, 30.90
BLEU, was around the level we obtained when
using frequency to approximate function words at
N = 64. However, we observe that many of
the words in the linguistically motivated function
word list are quite infrequent; this suggests that
data sparseness may be an additional factor worth
investigating.
Finally, although we believe there are strong
motivations for focusing on the role of function
words in reordering, there may well be value in
extending the dominance model to include content
categories. Verbs and many nouns have subcat-
egorization properties that may inuence phrase
ordering, for example, and this may turn out to ex-
plain the increase in Arabic-English performance
for N = 2048 using the MT08 test set. More gen-
erally, the approach we are taking can be viewed
as a way of selectively lexicalizing the automati-
cally extracted grammar, and there is a large range
of potentially interesting choices in how such lex-
icalization could be done.
9 Related Work
In the introduction, we discussed Chiang's (2005)
constituency feature, related ideas explored by
Marton and Resnik (2008) and Chiang et al
(2008), and the target-side variation investigated
by Zollman et al (2006). These methods differ
from each other mainly in terms of the specic lin-
9
We plan to do corresponding experimentation and anal-
ysis for Arabic once we identify a suitable list of manually
identied function words.
guistic knowledge being used and on which side
the constraints are applied.
Shen et al (2008) proposed to use lin-
guistic knowledge expressed in terms of a de-
pendency grammar, instead of a syntactic con-
stituency grammar. Villar et al (2008) attempted
to use syntactic constituency on both the source
and target languages in the same spirit as the con-
stituency feature, along with some simple pattern-
based heuristics ? an approach also investigated by
Iglesias et al (2009). Aiming at improving the se-
lection of derivations, Zhou et al (2008) proposed
prior derivation models utilizing syntactic annota-
tion of the source language, which can be seen as
smoothing the probabilities of hierarchical phrase
features.
A key point is that the model we have intro-
duced in this paper does not require the linguistic
supervision needed in most of this prior work. We
estimate the parameters of our model from parallel
text without any linguistic annotation. That said,
we would emphasize that our approach is, in fact,
motivated in linguistic terms by the role of func-
tion words in natural language syntax.
10 Conclusion
We have presented a pairwise dominance model
to address reordering issues that are not handled
particularly well by standard hierarchical phrase-
based modeling. In particular, the minimal lin-
guistic commitment in hierarchical phrase-based
models renders them susceptible to overgenera-
tion of reordering choices. Our proposal han-
dles the overgeneration problem by identifying
hierarchical phrases with function words and by
using function word relationships to incorporate
soft constraints on topological orderings. Our
experimental results demonstrate that introducing
the pairwise dominance model into hierarchical
phrase-based modeling improves performance sig-
nicantly in large-scale Chinese-to-English and
Arabic-to-English translation tasks.
Acknowledgments
This research was supported in part by the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-2-001. Any opinions, ndings, conclusions or
recommendations expressed in this paper are those
of the authors and do not necessarily reect the
view of the sponsors.
331
References
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the
2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 224?233, Honolulu,
Hawaii, October.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL'05), pages
263?270, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jiaying Howard. 2002. A Student Handbook for Chi-
nese Function Words. The Chinese University Press.
Gonzalo Iglesias, Adria de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule ltering by pattern
for efcient hierarchical translation. In Proceedings
of the 12th Conference of the European Chapter of
the Association of Computational Linguistics (to ap-
pear).
R. Kneser and H. Ney. 1995. Improved backing-
off for m-gram language modeling. In Proceed-
ings of IEEE International Conference on Acoustics,
Speech, and Signal Processing95, pages 181?184,
Detroit, MI, May.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 127?133,
Edmonton, Alberta, Canada, May. Association for
Computational Linguistics.
Philipp Koehn. 2004. Statistical signicance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395, Barcelona, Spain,
July.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of The 46th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1003?
1011, Columbus, Ohio, June.
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 295?302, Philadelphia,
Pennsylvania, USA, July.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li.
2007. Ordering phrases with function words. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 712?
719, Prague, Czech Republic, June.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of The 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 577?585, Columbus,
Ohio, June.
David Vilar, Daniel Stein, and Hermann Ney. 2008.
Analysing soft syntax features and heuristics for hi-
erarchical phrase based machine translation. Inter-
national Workshop on Spoken Language Translation
2008, pages 190?197, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404, Sep.
Tiejun Zhao, Yajuan Lv, Jianmin Yao, Hao Yu, Muyun
Yang, and Fang Liu. 2001. Increasing accuracy
of chinese segmentation with strategy of multi-step
processing. Journal of Chinese Information Pro-
cessing (Chinese Version), 1:13?18.
Bowen Zhou, Bing Xiang, Xiaodan Zhu, and Yuqing
Gao. 2008. Prior derivation models for formally
syntax-based translation using linguistically syntac-
tic parsing and tree kernels. In Proceedings of
the ACL-08: HLT Second Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages
19?27, Columbus, Ohio, June.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York City,
June.
332
