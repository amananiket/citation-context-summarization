Proceedings of the 12th Conference of the European Chapter of the ACL, pages 69?76,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Incremental Parsing with Parallel Multiple Context-Free Grammars
Krasimir Angelov
Chalmers University of Technology
Go?teborg, Sweden
krasimir@chalmers.se
Abstract
Parallel Multiple Context-Free Grammar
(PMCFG) is an extension of context-free
grammar for which the recognition problem is
still solvable in polynomial time. We describe
a new parsing algorithm that has the advantage
to be incremental and to support PMCFG
directly rather than the weaker MCFG formal-
ism. The algorithm is also top-down which
allows it to be used for grammar based word
prediction.
1 Introduction
Parallel Multiple Context-Free Grammar (PMCFG)
(Seki et al, 1991) is one of the grammar formalisms
that have been proposed for the syntax of natural lan-
guages. It is an extension of context-free grammar
(CFG) where the right hand side of the production rule
is a tuple of strings instead of only one string. Using tu-
ples the grammar can model discontinuous constituents
which makes it more powerful than context-free gram-
mar. In the same time PMCFG has the advantage to be
parseable in polynomial time which makes it attractive
from computational point of view.
A parsing algorithm is incremental if it reads the in-
put one token at the time and calculates all possible
consequences of the token, before the next token is
read. There is substantial evidence showing that hu-
mans process language in an incremental fashion which
makes the incremental algorithms attractive from cog-
nitive point of view.
If the algorithm is also top-down then it is possible
to predict the next word from the sequence of preced-
ing words using the grammar. This can be used for
example in text based dialog systems or text editors for
controlled language where the user might not be aware
of the grammar coverage. In this case the system can
suggest the possible continuations.
A restricted form of PMCFG that is still stronger
than CFG is Multiple Context-Free Grammar (MCFG).
In Seki and Kato (2008) it has been shown that
MCFG is equivalent to string-based Linear Context-
Free Rewriting Systems and Finite-Copying Tree
Transducers and it is stronger than Tree Adjoining
Grammars (Joshi and Schabes, 1997). Efficient recog-
nition and parsing algorithms for MCFG have been de-
scribed in Nakanishi et al (1997), Ljunglo?f (2004) and
Burden and Ljunglo?f (2005). They can be used with
PMCFG also but it has to be approximated with over-
generating MCFG and post processing is needed to fil-
ter out the spurious parsing trees.
We present a parsing algorithm that is incremental,
top-down and supports PMCFG directly. The algo-
rithm exploits a view of PMCFG as an infinite context-
free grammar where new context-free categories and
productions are generated during parsing. It is trivial to
turn the algorithm into statistical by attaching probabil-
ities to each rule.
In Ljunglo?f (2004) it has been shown that the Gram-
matical Framework (GF) formalism (Ranta, 2004) is
equivalent to PMCFG. The algorithm was implemented
as part of the GF interpreter and was evaluated with the
resource grammar library (Ranta, 2008) which is the
largest collection of grammars written in this formal-
ism. The incrementality was used to build a help sys-
tem which suggests the next possible words to the user.
Section 2 gives a formal definition of PMCFG. In
section 3 the procedure for ?linearization? i.e. the
derivation of string from syntax tree is defined. The
definition is needed for better understanding of the for-
mal proofs in the paper. The algorithm introduction
starts with informal description of the idea in section
4 and after that the formal rules are given in section
5. The implementation details are outlined in section 6
and after that there are some comments on the evalua-
tion in section 7. Section 8 gives a conclusion.
2 PMCFG definition
Definition 1 A parallel multiple context-free grammar
is an 8-tuple G = (N,T, F, P, S, d, r, a) where:
? N is a finite set of categories and a positive integer
d(A) called dimension is given for each A ? N .
? T is a finite set of terminal symbols which is dis-
joint with N .
? F is a finite set of functions where the arity a(f)
and the dimensions r(f) and di(f) (1 ? i ?
a(f)) are given for every f ? F . For every posi-
tive integer d, (T ?)d denote the set of all d-tuples
69
of strings over T . Each function f ? F is a to-
tal mapping from (T ?)d1(f) ? (T ?)d2(f) ? ? ? ? ?
(T ?)da(f)(f) to (T ?)r(f), defined as:
f := (?1, ?2, . . . , ?r(f))
Here ?i is a sequence of terminals and ?k; l?
pairs, where 1 ? k ? a(f) is called argument
index and 1 ? l ? dk(f) is called constituent
index.
? P is a finite set of productions of the form:
A? f [A1, A2, . . . , Aa(f)]
where A ? N is called result category,
A1, A2, . . . , Aa(f) ? N are called argument cat-
egories and f ? F is the function symbol. For
the production to be well formed the conditions
di(f) = d(Ai) (1 ? i ? a(f)) and r(f) = d(A)
must hold.
? S is the start category and d(S) = 1.
We use the same definition of PMCFG as is used by
Seki and Kato (2008) and Seki et al (1993) with the
minor difference that they use variable names like xkl
while we use ?k; l? to refer to the function arguments.
As an example we will use the anbncn language:
S ? c[N ]
N ? s[N ]
N ? z[]
c := (?1; 1? ?1; 2? ?1; 3?)
s := (a ?1; 1?, b ?1; 2?, c ?1; 3?)
z := (, , )
Here the dimensions are d(S) = 1 and d(N) = 3 and
the arities are a(c) = a(s) = 1 and a(z) = 0.  is the
empty string.
3 Derivation
The derivation of a string in PMCFG is a two-step pro-
cess. First we have to build a syntax tree of a category
S and after that to linearize this tree to string. The defi-
nition of a syntax tree is recursive:
Definition 2 (f t1 . . . ta(f)) is a tree of category A if
ti is a tree of category Bi and there is a production:
A? f [B1 . . . Ba(f)]
The abstract notation for ?t is a tree of category A?
is t : A. When a(f) = 0 then the tree does not have
children and the node is called leaf.
The linearization is bottom-up. The functions in the
leaves do not have arguments so the tuples in their defi-
nitions already contain constant strings. If the function
has arguments then they have to be linearized and the
results combined. Formally this can be defined as a
function L applied to the syntax tree:
L(f t1 t2 . . . ta(f)) = (x1, x2 . . . xr(f))
where xi = K(L(t1),L(t2) . . .L(ta(f))) ?i
and f := (?1, ?2 . . . ?r(f)) ? F
The function uses a helper function K which takes the
already linearized arguments and a sequence ?i of ter-
minals and ?k; l? pairs and returns a string. The string
is produced by simple substitution of each ?k; l? with
the string for constituent l from argument k:
K ? (?1?k1; l1??2?k2; l2? . . . ?n) = ?1?k1l1?2?k2l2 . . . ?n
where ?i ? T ?. The recursion in L terminates when a
leaf is reached.
In the example anbncn language the function z does
not have arguments and it corresponds to the base case
when n = 0. Every application of s over another tree
t : N increases n by one. For example the syntax tree
(s (s z)) will produce the tuple (aa, bb, cc). Finally the
application of c combines all elements in the tuple in
a single string i.e. c (s (s z)) will produce the string
aabbcc.
4 The Idea
Although PMCFG is not context-free it can be approx-
imated with an overgenerating context-free grammar.
The problem with this approach is that the parser pro-
duces many spurious parse trees that have to be filtered
out. A direct parsing algorithm for PMCFG should
avoid this and a careful look at the difference between
PMCFG and CFG gives an idea. The context-free ap-
proximation of anbncn is the language a?b?c? with
grammar:
S ? ABC
A?  | aA
B ?  | bB
C ?  | cC
The string ?aabbcc? is in the language and it can be
derived with the following steps:
S
? ABC
? aABC
? aaABC
? aaBC
? aabBC
? aabbBC
? aabbC
? aabbcC
? aabbccC
? aabbcc
70
The grammar is only an approximation because there
is no enforcement that we will use only equal number
of reductions for A, B and C. This can be guaranteed
if we replace B and C with new categories B? and C ?
after the derivation of A:
B? ? bB?? C ? ? cC ??
B?? ? bB??? C ?? ? cC ???
B??? ?  C ??? ? 
In this case the only possible derivation from aaB?C ?
is aabbcc.
The PMCFG parser presented in this paper works
like context-free parser, except that during the parsing
it generates fresh categories and rules which are spe-
cializations of the originals. The newly generated rules
are always versions of already existing rules where
some category is replaced with new more specialized
category. The generation of specialized categories pre-
vents the parser from recognizing phrases that are oth-
erwise withing the scope of the context-free approxi-
mation of the original grammar.
5 Parsing
The algorithm is described as a deductive process in
the style of (Shieber et al, 1995). The process derives
a set of items where each item is a statement about the
grammatical status of some substring in the input.
The inference rules are in natural deduction style:
X1 . . . Xn
Y
< side conditions on X1, . . . , Xn >
where the premises Xi are some items and Y is the
derived item. We assume that w1 . . . wn is the input
string.
5.1 Deduction Rules
The deduction system deals with three types of items:
active, passive and production items.
Productions In Shieber?s deduction systems the
grammar is a constant and the existence of a given pro-
duction is specified as a side condition. In our case the
grammar is incrementally extended at runtime, so the
set of productions is part of the deduction set. The pro-
ductions from the original grammar are axioms and are
included in the initial deduction set.
Active Items The active items represent the partial
parsing result:
[kjA? f [ ~B]; l : ? ? ?] , j ? k
The interpretation is that there is a function f with a
corresponding production:
A? f [ ~B]
f := (?1, . . . ?l?1, ??, . . . ?r(f))
such that the tree (f t1 . . . ta(f)) will produce the sub-
string wj+1 . . . wk as a prefix in constituent l for any
INITIAL PREDICT
S ? f [ ~B]
[00S ? f [ ~B]; 1 : ??]
S - start category, ? = rhs(f, 1)
PREDICT
Bd ? g[~C] [
k
jA? f [ ~B]; l : ? ? ?d; r? ?]
[kkBd ? g[~C]; r : ??]
? = rhs(g, r)
SCAN
[kjA? f [ ~B]; l : ? ? s ?]
[k+1j A? f [ ~B]; l : ? s ? ?]
s = wk+1
COMPLETE
[kjA? f [ ~B]; l : ??]
N ? f [ ~B] [kjA; l;N ]
N = (A, l, j, k)
COMBINE
[ujA? f [ ~B]; l : ? ? ?d; r? ?] [
k
uBd; r;N ]
[kjA? f [ ~B{d := N}]; l : ? ?d; r? ? ?]
Figure 1: Deduction Rules
sequence of arguments ti : Bi. The sequence ? is the
part that produced the substring:
K(L(t1),L(t2) . . .L(ta(f))) ? = wj+1 . . . wk
and ? is the part that is not processed yet.
Passive Items The passive items are of the form:
[kjA; l;N ] , j ? k
and state that there exists at least one production:
A? f [ ~B]
f := (?1, ?2, . . . ?r(f))
and a tree (f t1 . . . ta(f)) : A such that the constituent
with index l in the linearization of the tree is equal to
wj+1 . . . wk. Contrary to the active items in the passive
the whole constituent is matched:
K(L(t1),L(t2) . . .L(ta(f))) ?l = wj+1 . . . wk
Each time when we complete an active item, a pas-
sive item is created and at the same time we cre-
ate a new category N which accumulates all produc-
tions forA that produce thewj+1 . . . wk substring from
constituent l. All trees of category N must produce
wj+1 . . . wk in the constituent l.
There are six inference rules (see figure 1).
The INITIAL PREDICT rule derives one item spanning
the 0 ? 0 range for each production with the start cat-
egory S on the left hand side. The rhs(f, l) function
returns the constituent with index l of function f .
In the PREDICT rule, for each active item with dot be-
fore a ?d; r? pair and for each production for Bd, a new
active item is derived where the dot is in the beginning
of constituent r in g.
When the dot is before some terminal s and s is equal
to the current terminal wk then the SCAN rule derives a
new item where the dot is moved to the next position.
71
When the dot is at the end of an active item then it
is converted to passive item in the COMPLETE rule. The
category N in the passive item is a fresh category cre-
ated for each unique (A, l, j, k) quadruple. A new pro-
duction is derived for N which has the same function
and arguments as in the active item.
The item in the premise of COMPLETE was at some
point predicted in PREDICT from some other item. The
COMBINE rule will later replace the occurence A in the
original item (the premise of PREDICT) with the special-
ization N .
The COMBINE rule has two premises: one active item
and one passive. The passive item starts from position
u and the only inference rule that can derive items with
different start positions is PREDICT. Also the passive
item must have been predicted from active item where
the dot is before ?d; r?, the category for argument num-
ber d must have been Bd and the item ends at u. The
active item in the premise of COMBINE is such an item
so it was one of the items used to predict the passive
one. This means that we can move the dot after ?d; r?
and the d-th argument is replaced with its specialization
N .
If the string ? contains another reference to the d-th
argument then the next time when it has to be predicted
the rule PREDICT will generate active items, only for
those productions that were successfully used to parse
the previous constituents. If a context-free approxima-
tion was used this would have been equivalent to unifi-
cation of the redundant subtrees. Instead this is done at
runtime which also reduces the search space.
The parsing is successful if we had derived the
[n0S; 1;S
?] item, where n is the length of the text, S is
the start category and S? is the newly created category.
The parser is incremental because all active items
span up to position k and the only way to move to the
next position is the SCAN rule where a new symbol from
the input is consumed.
5.2 Soundness
The parsing system is sound if every derivable item rep-
resents a valid grammatical statement under the inter-
pretation given to every type of item.
The derivation in INITIAL PREDICT and PREDICT is
sound because the item is derived from existing pro-
duction and the string before the dot is empty so:
K ?  = 
The rationale for SCAN is that if
K ? ? = wj?1 . . . wk
and s = wk+1 then
K ? (? s) = wj?1 . . . wk+1
If the item in the premise is valid then it is based on
existing production and function and so will be the item
in the consequent.
In the COMPLETE rule the dot is at the end of the
string. This means that wj+1 . . . wk will be not just
a prefix in constituent l of the linearization but the full
string. This is exactly what is required in the semantics
of the passive item. The passive item is derived from
a valid active item so there is at least one production
for A. The category N is unique for each (A, l, j, k)
quadruple so it uniquely identifies the passive item in
which it is placed. There might be many productions
that can produce the passive item but all of them should
be able to generate wj+1 . . . wk and they are exactly
the productions that are added to N . From all this ar-
guments it follows that COMPLETE is sound.
The COMBINE rule is sound because from the active
item in the premise we know that:
K ? ? = wj+1 . . . wu
for every context ? built from the trees:
t1 : B1; t2 : B2; . . . ta(f) : Ba(f)
From the passive item we know that every production
forN produces thewu+1 . . . wk in r. From that follows
that
K ?? (??d; r?) = wj+1 . . . wk
where ?? is the same as ? except that Bd is replaced
withN . Note that the last conclusion will not hold if we
were using the original context because Bd is a more
general category and can contain productions that does
not derive wu+1 . . . wk.
5.3 Completeness
The parsing system is complete if it derives an item
for every valid grammatical statement. In our case we
have to prove that for every possible parse tree the cor-
responding items will be derived.
The proof for completeness requires the following
lemma:
Lemma 1 For every possible syntax tree
(f t1 . . . ta(f)) : A
with linearization
L(ft1 . . . ta(f)) = (x1, x2 . . . xd(A))
where xl = wj+1 . . . wk, the system will derive an item
[kjA; l;A
?] if the item [kjA ? f [ ~B]; l : ??l] was pre-
dicted before that. We assume that the function defini-
tion is:
f := (?1, ?2 . . . ?r(f))
The proof is by induction on the depth of the tree.
If the tree has only one level then the function f does
not have arguments and from the linearization defini-
tion and from the premise in the lemma it follows that
?l = wj+1 . . . wk. From the active item in the lemma
72
by applying iteratively the SCAN rule and finally the
COMPLETE rule the system will derive the requested
item.
If the tree has subtrees then we assume that the
lemma is true for every subtree and we prove it for the
whole tree. We know that
K ? ?l = wj+1 . . . wk
Since the function K does simple substitution it is pos-
sible for each ?d; s? pair in ?l to find a new range in the
input string j??k? such that the lemma to be applicable
for the corresponding subtree td : Bd. The terminals in
?l will be processed by the SCAN rule. Rule PREDICT
will generate the active items required for the subtrees
and the COMBINE rule will consume the produced pas-
sive items. Finally the COMPLETE rule will derive the
requested item for the whole tree.
From the lemma we can prove the completeness of
the parsing system. For every possible tree t : S such
that L(t) = (w1 . . . wn) we have to prove that the
[n0S; 1;S
?] item will be derived. Since the top-level
function of the tree must be from production for S the
INITIAL PREDICT rule will generate the active item in
the premise of the lemma. From this and from the as-
sumptions for t it follows that the requested passive
item will be derived.
5.4 Complexity
The algorithm is very similar to the Earley (1970) algo-
rithm for context-free grammars. The similarity is even
more apparent when the inference rules in this paper
are compared to the inference rules for the Earley al-
gorithm presented in Shieber et al (1995) and Ljunglo?f
(2004). This suggests that the space and time complex-
ity of the PMCFG parser should be similar to the com-
plexity of the Earley parser which is O(n2) for space
and O(n3) for time. However we generate new cate-
gories and productions at runtime and this have to be
taken into account.
Let theP(j) function be the maximal number of pro-
ductions generated from the beginning up to the state
where the parser has just consumed terminal number
j. P(j) is also the upper limit for the number of cat-
egories created because in the worst case there will be
only one production for each new category.
The active items have two variables that directly de-
pend on the input size - the start index j and the end
index k. If an item starts at position j then there are
(n ? j + 1) possible values for k because j ? k ? n.
The item also contains a production and there are P(j)
possible choices for it. In total there are:
n?
j=0
(n? j + 1)P(j)
possible choices for one active item. The possibilities
for all other variables are only a constant factor. The
P(j) function is monotonic because the algorithm only
adds new productions and never removes. From that
follows the inequality:
n?
j=0
(n? j + 1)P(j) ? P(n)
n?
i=0
(n? j + 1)
which gives the approximation for the upper limit:
P(n)
n(n+ 1)
2
The same result applies to the passive items. The only
difference is that the passive items have only a category
instead of a full production. However the upper limit
for the number of categories is the same. Finally the
upper limit for the total number of active, passive and
production items is:
P(n)(n2 + n+ 1)
The expression for P(n) is grammar dependent but
we can estimate that it is polynomial because the set
of productions corresponds to the compact representa-
tion of all parse trees in the context-free approximation
of the grammar. The exponent however is grammar de-
pendent. From this we can expect that asymptotic space
complexity will be O(ne) where e is some parameter
for the grammar. This is consistent with the results in
Nakanishi et al (1997) and Ljunglo?f (2004) where the
exponent also depends on the grammar.
The time complexity is proportional to the number
of items and the time needed to derive one item. The
time is dominated by the most complex rule which in
this algorithm is COMBINE. All variables that depend
on the input size are present both in the premises and
in the consequent except u. There are n possible values
for u so the time complexity is O(ne+1).
5.5 Tree Extraction
If the parsing is successful we need a way to extract the
syntax trees. Everything that we need is already in the
set of newly generated productions. If the goal item is
[n0S; 0;S
?] then every tree t of category S? that can be
constructed is a syntax tree for the input sentence (see
definition 2 in section 3 again).
Note that the grammar can be erasing; i.e., there
might be productions like this:
S ? f [B1, B2, B3]
f := (?1; 1??3; 1?)
There are three arguments but only two of them are
used. When the string is parsed this will generate a
new specialized production:
S? ? f [B?1, B2, B
?
3]
Here S,B1 and B3 are specialized to S?, B?1 and B
?
3
but the B2 category is still the same. This is correct
73
because actually any subtree for the second argument
will produce the same result. Despite this it is some-
times useful to know which parts of the tree were used
and which were not. In the GF interpreter such un-
used branches are replaced by meta variables. In this
case the tree extractor should check whether the cate-
gory also exists in the original set of categories N in
the grammar.
Just like with the context-free grammars the parsing
algorithm is polynomial but the chart can contain ex-
ponential or even infinite number of trees. Despite this
the chart is a compact finite representation of the set of
trees.
6 Implementation
Every implementation requires a careful design of the
data structures in the parser. For efficient access the set
of items is split into four subsets: A, Sj , C and P. A
is the agenda i.e. the set of active items that have to be
analyzed. Sj contains items for which the dot is before
an argument reference and which span up to position j.
C is the set of possible continuations i.e. a set of items
for which the dot is just after a terminal. P is the set
of productions. In addition the set F is used internally
for the generatation of fresh categories. The sets C,
Sj and F are used as association maps. They contain
associations like k 7? v where k is the key and v is the
value. All maps except F can contain more than one
value for one and the same key.
The pseudocode of the implementation is given in
figure 2. There are two procedures Init and Compute.
Init computes the initial values of S, P and A. The
initial agenda A is the set of all items that can be pre-
dicted from the start category S (INITIAL PREDICT rule).
Compute consumes items from the current agenda
and applies the SCAN, PREDICT, COMBINE or COMPLETE
rule. The case statement matches the current item
against the patterns of the rules and selects the proper
rule. The PREDICT and COMBINE rules have two
premises so they are used in two places. In both cases
one of the premises is related to the current item and a
loop is needed to find item matching the other premis.
The passive items are not independent entities but
are just the combination of key and value in the set F.
Only the start position of every item is kept because the
end position for the interesting passive items is always
the current position and the active items are either in
the agenda if they end at the current position or they
are in the Sj set if they end at position j. The active
items also keep only the dot position in the constituent
because the constituent definition can be retrieved from
the grammar. For this reason the runtime representation
of the items is [j;A ? f [ ~B]; l; p] where j is the start
position of the item and p is the dot position inside the
constituent.
The Compute function returns the updated S and P
sets and the set of possible continuations C. The set of
continuations is a map indexed by a terminal and the
Language Productions Constituents
Bulgarian 3516 75296
English 1165 8290
German 8078 21201
Swedish 1496 8793
Table 1: GF Resource Grammar Library size in number
of PMCFG productions and discontinuous constituents
0
200
400
600
800
1000
1200
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
Number of Tokens
ms
German Bulgarian Swedish English
Figure 3: Parser performance in miliseconds per token
values are active items. The parser computes the set of
continuations at each step and if the current terminal is
one of the keys the set of values for it is taken as an
agenda for the next step.
7 Evaluation
The algorithm was evaluated with four languages from
the GF resource grammar library (Ranta, 2008): Bul-
garian, English, German and Swedish. These gram-
mars are not primarily intended for parsing but as a
resource from which smaller domain dependent gram-
mars are derived for every application. Despite this, the
resource grammar library is a good benchmark for the
parser because these are the biggest GF grammars.
The compiler converts a grammar written in the
high-level GF language to a low-level PMCFG gram-
mar which the parser can use directly. The sizes of
the grammars in terms of number of productions and
number of unique discontinuous constituents are given
on table 1. The number of constituents roughly cor-
responds to the number of productions in the context-
free approximation of the grammar. The parser per-
formance in terms of miliseconds per token is shown in
figure 3. In the evaluation 34272 sentences were parsed
and the average time for parsing a given number of to-
kens is drawn in the chart. As it can be seen, although
the theoretical complexity is polynomial, the real-time
performance for practically interesting grammars tends
to be linear.
8 Conclusion
The algorithm has proven useful in the GF system. It
accomplished the initial goal to provide suggestions
74
procedure Init() {
k = 0
Si = ?, for every i
P = the set of productions P in the grammar
A = ?
forall S ? f [ ~B] ? P do // INITIAL PREDICT
A = A+ [0;S ? f [ ~B]; 1; 0]
return (S,P,A)
}
procedure Compute(k, (S,P,A)) {
C = ?
F = ?
while A 6= ? do {
let x ? A, x ? [j;A? f [ ~B]; l; p]
A = A? x
case the dot in x is {
before s ? T ? C = C+ (s 7? [j;A? f [ ~B]; l; p+ 1]) // SCAN
before ?d; r? ? if ((Bd, r) 7? (x, d)) 6? Sk then {
Sk = Sk + ((Bd, r) 7? (x, d))
forall Bd ? g[~C] ? P do // PREDICT
A = A+ [k;Bd ? g[~C]; r; 0]
}
forall (k;Bd, r) 7? N ? F do // COMBINE
A = A+ [j;A? f [ ~B{d := N}]; l; p+ 1]
at the end ? if ?N.((j, A, l) 7? N ? F) then {
forall (N, r) 7? (x?, d?) ? Sk do // PREDICT
A = A+ [k;N ? f [ ~B]; r; 0]
} else {
generate fresh N // COMPLETE
F = F+ ((j, A, l) 7? N)
forall (A, l) 7? ([j?;A? ? f ?[ ~B?]; l?; p?], d) ? Sj do // COMBINE
A = A+ [j?;A? ? f ?[ ~B?{d := N}]; l?; p? + 1]
}
P = P+ (N ? f [ ~B])
}
}
return (S,P,C)
}
Figure 2: Pseudocode of the parser implementation
75
in text based dialog systems and in editors for con-
trolled languages. Additionally the algorithm has prop-
erties that were not envisaged in the beginning. It
works with PMCFG directly rather that by approxima-
tion with MCFG or some other weaker formalism.
Since the Linear Context-Free Rewriting Systems,
Finite-Copying Tree Transducers and Tree Adjoining
Grammars can be converted to PMCFG, the algorithm
presented in this paper can be used with the converted
grammar. The approach to represent context-dependent
grammar as infinite context-free grammar might be ap-
plicable to other formalisms as well. This will make it
very attractive in applications where some of the other
formalisms are already in use.
References
Ha?kan Burden and Peter Ljunglo?f. 2005. Parsing
linear context-free rewriting systems. In Proceed-
ings of the Ninth International Workshop on Parsing
Technologies (IWPT), pages 11?17, October.
Jay Earley. 1970. An efficient context-free parsing al-
gorithm. Commun. ACM, 13(2):94?102.
Aravind Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Grzegorz Rozenberg and
Arto Salomaa, editors, Handbook of Formal Lan-
guages. Vol 3: Beyond Words, chapter 2, pages 69?
123. Springer-Verlag, Berlin/Heidelberg/New York.
Peter Ljunglo?f. 2004. Expressivity and Complexity of
the Grammatical Framework. Ph.D. thesis, Depart-
ment of Computer Science, Gothenburg University
and Chalmers University of Technology, November.
Ryuichi Nakanishi, Keita Takada, and Hiroyuki Seki.
1997. An Efficient Recognition Algorithm for Mul-
tiple ContextFree Languages. In Fifth Meeting
on Mathematics of Language. The Association for
Mathematics of Language, August.
Aarne Ranta. 2004. Grammatical Framework: A
Type-Theoretical Grammar Formalism. Journal of
Functional Programming, 14(2):145?189, March.
Aarne Ranta. 2008. GF Resource Grammar Library.
digitalgrammars.com/gf/lib/.
Hiroyuki Seki and Yuki Kato. 2008. On the Genera-
tive Power of Multiple Context-Free Grammars and
Macro Grammars. IEICE-Transactions on Info and
Systems, E91-D(2):209?221.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On multiple context-
free grammars. Theoretical Computer Science,
88(2):191?229, October.
Hiroyuki Seki, Ryuichi Nakanishi, Yuichi Kaji,
Sachiko Ando, and Tadao Kasami. 1993. Par-
allel Multiple Context-Free Grammars, Finite-State
Translation Systems, and Polynomial-Time Recog-
nizable Subclasses of Lexical-Functional Grammars.
In 31st Annual Meeting of the Association for Com-
putational Linguistics, pages 130?140. Ohio State
University, Association for Computational Linguis-
tics, June.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and Implementation of
Deductive Parsing. Journal of Logic Programming,
24(1&2):3?36.
76
