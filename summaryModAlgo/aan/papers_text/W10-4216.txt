Feature Selection for Fluency Ranking
Danie?l de Kok
University of Groningen
d.j.a.de.kok@rug.nl
Abstract
Fluency rankers are used in modern sentence
generation systems to pick sentences that are
not just grammatical, but also fluent. It has
been shown that feature-based models, such as
maximum entropy models, work well for this
task.
Since maximum entropy models allow for in-
corporation of arbitrary real-valued features,
it is often attractive to create very general
feature templates, that create a huge num-
ber of features. To select the most discrim-
inative features, feature selection can be ap-
plied. In this paper we compare three fea-
ture selection methods: frequency-based se-
lection, a generalization of maximum entropy
feature selection for ranking tasks with real-
valued features, and a new selection method
based on feature value correlation. We show
that the often-used frequency-based selection
performs badly compared to maximum en-
tropy feature selection, and that models with a
few hundred well-picked features are compet-
itive to models with no feature selection ap-
plied. In the experiments described in this pa-
per, we compressed a model of approximately
490.000 features to 1.000 features.
1 Introduction
As shown previously, maximum entropy models
have proven to be viable for fluency ranking (Nakan-
ishi et al, 2005; Velldal and Oepen, 2006; Velldal,
2008). The basic principle of maximum entropy
models is to minimize assumptions, while impos-
ing constraints such that the expected feature value
is equal to the observed feature value in the train-
ing data. In its canonical form, the probability of a
certain event (y) occurring in the context (x) is a log-
linear combination of features and feature weights,
whereZ(x) is a normalization over all events in con-
text x (Berger et al, 1996):
p(y|x) =
1
Z(x)
exp
n?
i=1
?ifi (1)
The training process estimates optimal feature
weights, given the constraints and the principle of
maximum entropy. In fluency ranking the input (e.g.
a dependency structure) is a context, and a realiza-
tion of that input is an event within that context.
Features can be hand-crafted or generated auto-
matically using very general feature templates. For
example, if we apply a template rule that enumer-
ates the rules used to construct a derivation tree to
the partial tree in figure 1 the rule(max xp(np)) and
rule(np det n) features will be created.
Figure 1: Partial derivation tree for the noun phrase de
adviezen (the advices).
To achieve high accuracy in fluency ranking
quickly, it is attractive to capture as much of the lan-
guage generation process as possible. For instance,
in sentence realization, one could extract nearly ev-
ery aspect of a derivation tree as a feature using very
general templates. This path is followed in recent
work, such as Velldal (2008). The advantage of this
approach is that it requires little human labor, and
generally gives good ranking performance. How-
ever, the generality of templates leads to huge mod-
els in terms of number of features. For instance, the
model that we will discuss contains about 490,000
features when no feature selection is applied. Such
models are very opaque, giving very little under-
standing of good discriminators for fluency ranking,
and the size of the models may also be inconvenient.
To make such models more compact and transpar-
ent, feature selection can be applied.
In this paper we make the following contribu-
tions: we modify a maximum entropy feature selec-
tion method for ranking tasks; we introduce a new
feature selection method based on statistical corre-
lation of features; we compare the performance of
the preceding feature selection methods, plus a com-
monly used frequency-based method; and we give
an analysis of the most effective features for fluency
ranking.
2 Feature Selection
2.1 Introduction
Feature selection is a process that tries to extract
S ? F from a set of features F , such that the model
using S performs comparably to the model using
F . Such a compression of a feature set can be ob-
tained if there are features: that occur sporadically;
that correlate strongly with other features (features
that show the same behavior within events and con-
texts); or have values with little or no correlation to
the classification or ranking.
Features that do have no correlation to the classi-
fication can be removed from the model. For a set
of highly-correlating features, one feature can be se-
lected to represent the whole group.
Initially it may seem attractive to perform fluency
selection by training a model on all features, select-
ing features with relatively high weights. However,
if features overlap, weight mass will usually be di-
vided over these features. For instance, suppose that
f1 alone has a weight of 0.5 in a given model. If
we retrain the model, after adding the features f2..f5
that behave identically to f1, the weight may be dis-
tributed evenly between f1..f5, giving each feature
the weight 0.1.
In the following sections, we will give a short
overview of previous research in feature selection,
and will then proceed to give a more detailed de-
scription of three feature selection methods.
2.2 Background
Feature selection can be seen as model selection,
where the best model of all models that can be
formed using a set of features should be selected.
Madigan and Raftery (1994) propose an method for
model selection aptly named Occam?s window. This
method excludes models that do not perform com-
petitively to other models or that do not perform bet-
ter than one of its submodels. Although this method
is conceptually firm, it is nearly infeasable to apply
it with the number of features used in fluency rank-
ing. Berger et al (1996) propose a selection method
that iteratively builds a maximum entropy model,
adding features that improve the model. We modify
this method for ranking tasks in section 2.5. Ratna-
parkhi (1999) uses a simple frequency-based cutoff,
where features that occur infrequently are excluded.
We discuss a variant of this selection criterium in
section 2.3. Perkins et al (2003) describe an ap-
proach where feature selection is applied as a part
of model parameter estimation. They rely on the
fact that `1 regularizers have a tendency to force a
subset of weights to zero. However, such integrated
approaches rely on parameter tuning to get the re-
quested number of features.
In the fluency ranking literature, the use of a fre-
quency cut-off (Velldal and Oepen, 2006) and `1
regularization (Cahill et al, 2007) is prevalent. We
are not aware of any detailed studies that compare
feature selection methods for fluency ranking.
2.3 Frequency-based Selection
In frequency-based selection we follow Malouf and
Van Noord (2004), and count for each feature f the
number of inputs where there are at least two realiza-
tions y1, y2, such that f(y1) 6= f(y2). We then use
the first N features with the most frequent changes
from the resulting feature frequency list.
Veldall (2008) also experiments with this selec-
tion method, and suggests to apply frequency-based
selection to fluency ranking models that will be dis-
tributed to the public (for compactness? sake). In
the variant he and Malouf and Van Noord (2004)
discuss, all features that change within more than n
contexts are included in the model.
2.4 Correlation-based Selection
While frequency-based selection helps selecting fea-
tures that are discriminative, it cannot account for
feature overlap. Discriminative features that have a
strong correlation to features that were selected pre-
viously may still be added.
To detect overlap, we calculate the correlation of a
candidate feature and exclude the feature if it shows
a high correlation with features selected previously.
To estimate Pearson?s correlation of two features, we
calculate the sample correlation coefficient,
rf1,f2 =
?
x?X,y?Y (f1(x, y)? f?1)(f2(x, y)? f?2)
(n? 1)sf1sf2
(2)
where f?x is the average feature value of fx, and
sfx is the sample standard deviation of fx.
Of course, correlation can only indicate overlap,
and is in itself not enough to find effective features.
In our experiments with correlation-based selection
we used frequency-based selection as described in
2.3, to make an initial ranking of feature effective-
ness.
2.5 Maximum Entropy Feature Selection
Correlation-based selection can detect overlap, how-
ever, there is yet another spurious type of feature
that may reduce its effectiveness. Features with rel-
atively noisy values may contribute less than their
frequency of change may seem to indicate. For in-
stance, consider a feature that returns a completely
random value for every context. Not only does this
feature change very often, its correlation with other
features will also be weak. Such a feature may seem
attractive from the point of view of a frequency or
correlation-based method, but is useless in practice.
To account for both problems, we have to measure
the effectiveness of features in terms of how much
their addition to the model can improve prediction
of the training sample. Or in other words: does the
log-likelihood of the training data increase?
We have modified the Selective Gain Com-
putation (SGC) algorithm described by Zhou et
al. (2003) for ranking tasks rather than classification
tasks. This method builds upon the maximum en-
tropy feature selection method described by Berger
et al (1996). In this method features are added iter-
atively to a model that is initially uniform. During
each step, the feature that provides the highest gain
as a result of being added to the model, is selected
and added to the model.
In maximum entropy modeling, the weights of the
features in a model are optimized simultaneously.
However, optimizing the weights of the features in
model pS,f for every candidate feature f is compu-
tationally intractable. As a simplification, it is as-
sumed that the weights of features that are already
in the model are not affected by the addition of a
feature f . As a result, the optimal weight ? of f can
be found using a simple line search method.
However, as Zhou et al (2003) note, there is still
an inefficiency in that the weight of every candidate
feature is recalculated during every selection step.
They observe that gains of remaining candidate fea-
tures rarely increase as the result of adding a fea-
ture. If it is assumed that this never happens, a list
of candidate features ordered by gain can be kept.
To account for the fact that the topmost feature in
that list may have lost its effectiveness as the result
of a previous addition of a feature to the model, the
gain of the topmost feature is recalculated and rein-
serted into the list according to its new gain. When
the topmost feature retains its position, it is selected
and added to the model.
Since we use feature selection with features that
are not binary, and for a ranking task, we modified
the recursive forms of the model to:
sum?S?f (y|x) = sumS(y|x) ? e
?f(y) (3)
Z?S?f (x) = ZS(x)?
?
y
sumS(y|x)
+
?
y
sumS?f (y|x) (4)
Another issue that needs to be dealt with is the
calculation of context and event probabilities. In the
literature two approaches are prevalent. The first ap-
proach divides the probability mass uniformly over
contexts, and the probability of events within a con-
text is proportional to the event score (Osborne,
2000):
p(x) =
1
|X|
(5)
p(y|x) =
p(x)
( score(x,y)P
y score(x,y)
)
(6)
where |X| is the number of contexts. The sec-
ond approach puts more emphasis on the contexts
that contain relatively many events with high scores,
by making the context probability dependent on the
scores of events within that context (Malouf and van
Noord, 2004):
p(x) =
?
y score(x, y)
?
y?X score(x, y)
(7)
In our experiments, the second definition of con-
text probability outperformed the first by such a
wide margin, that we only used the second defini-
tion in the experiments described in this paper.
2.6 A Note on Overlap Detection
Although maximum-entropy based feature-selection
may be worthwhile in itself, the technique can also
be used during feature engineering to find overlap-
ping features. In the selection method of Berger et
al. (1996), the weight and gain of each candidate fea-
ture is re-estimated during each selection step. We
can exploit the changes in gains to detect overlap be-
tween a selected feature fn, and the candidates for
fn+1. If the gain of a feature changed drastically in
the selection of fn+1 compared to that of fn, this
feature has overlap with fn.
To determine which features had a drastic change
in gain, we determine whether the change has a sig-
nificance with a confidence interval of 99% after
normalization. The normalized gain change is cal-
culated in the following manner as described in al-
gorithm 1.
Algorithm 1 Calculation of the normalized gain
delta
?Gf ? Gf,n ?Gf,n?1
if ?Gf ? 0.0 then
?Gf,norm ?
?Gf
Gf ,n
else
?Gf,norm ?
?Gf
Gf,n?1
end if
3 Experimental Setup
3.1 Task
We evaluated the feature selection methods in con-
junction with a sentence realizer for Dutch. Sen-
tences are realized with a chart generator for the
Alpino wide-coverage grammar and lexicon (Bouma
et al, 2001). As the input of the chart genera-
tor, we use abstract dependency structures, which
are dependency structures leaving out information
such as word order. During generation, we store the
compressed derivation trees and associated (HPSG-
inspired) attribute-value structures for every real-
ization of an abstract dependency structure. We
then use feature templates to extract features from
the derivation trees. Two classes of features (and
templates) can be distinguished output features that
model the output of a process and construction fea-
tures that model the process that constructs the out-
put.
3.1.1 Output Features
Currently, there are two output features, both rep-
resenting auxiliary distributions (Johnson and Rie-
zler, 2000): a word trigram model and a part-of-
speech trigram model. The part-of-speech tag set
consists of the Alpino part of speech tags. Both
models are trained on newspaper articles, consist-
ing of 110 million words, from the Twente Nieuws
Corpus1.
The probability of unknown trigrams is estimated
using linear interpolation smoothing (Brants, 2000).
Unknown word probabilities are determined with
Laplacian smoothing.
1http://wwwhome.cs.utwente.nl/druid/
TwNC/TwNC-main.html
3.1.2 Construction Features
The construction feature templates consist of tem-
plates that are used for parse disambiguation, and
templates that are specifically targeted at generation.
The parse disambiguation features are used in the
Alpino parser for Dutch, and model various linguis-
tic phenomena that can indicate preferred readings.
The following aspects of a realization are described
by parse disambiguation features:
? Topicalization of (non-)NPs and subjects.
? Use of long-distance/local dependencies.
? Orderings in the middle field.
? Identifiers of grammar rules used to build the
derivation tree.
? Parent-daughter combinations.
Output features for parse disambiguation, such
as features describing dependency triples, were not
used. Additionally, we use most of the templates de-
scribed by Velldal (2008):
? Local derivation subtrees with optional grand-
parenting, with a maximum of three parents.
? Local derivation subtrees with back-off and
optional grand-parenting, with a maximum of
three parents.
? Binned word domination frequencies of the
daughters of a node.
? Binned standard deviation of word domination
of node daughters.
3.2 Data
The training and evaluation data was constructed by
parsing 11764 sentences of 5-25 tokens, that were
randomly selected from the (unannotated) Dutch
Wikipedia of August 20082, with the wide-coverage
Alpino parser. For every sentence, the best parse ac-
cording to the disambiguation component was ex-
tracted and considered to be correct. The Alpino
system achieves a concept accuracy of around 90%
on common Dutch corpora (Van Noord, 2007). The
2http://ilps.science.uva.nl/WikiXML/
original sentence is considered to be the best realiza-
tion of the abstract dependency structure of the best
parse.
We then used the Alpino chart generator to con-
struct derivation trees that realize the abstract de-
pendency structure of the best parse. The result-
ing derivation trees, including attribute-value struc-
tures associated with each node, are compressed and
stored in a derivation treebank. Training and testing
data was then obtained by extracting features from
derivation trees stored in the derivation treebank.
At this time, the realizations are also scored using
the General Text Matcher method (GTM) (Melamed
et al, 2003), by comparing them to the original
sentence. We have previously experimented with
ROUGE-N scores, which gave rise to similar results.
However, it is shown that GTM shows the highest
correlation with human judgments (Cahill, 2009).
3.3 Methodology
To evaluate the feature selection methods, we first
train models for each selection method in three
steps: 1. For each abstract dependency structure in
the training data 100 realizations (and corresponding
features) are randomly selected. 2. Feature selection
is applied, and the N -best features according to the
selection method are extracted. 3. A maximum en-
tropy model is trained using the TADM3 software,
with a `2 prior of 0.001, and using the N -best fea-
tures.
We used 5884 training instances (abstract depen-
dency trees, and scored realizations) to train the
model. The maximum entropy selection method was
used with a weight convergence threshold of 1e?6.
Correlation is considered to be strong enough for
overlap in the correlation-based method when two
features have a correlation coefficient of rf1,f2 ?
0.9
Each model is then evaluated using 5880 held-
out evaluation instances, where we select only in-
stances with 5 or more realizations (4184 instances),
to avoid trivial ranking cases. For every instance,
we select the realization that is the closest to the
original sentence to be the correct realization4. We
then calculate the fraction of instances for which the
3http://tadm.sourceforge.net/
4We follow this approach, because the original sentence is
not always exactly reproduced by the generator.
model picked the correct sentence. Of course, this is
a fairly strict evaluation, since there may be multiple
equally fluent sentences.
4 Results
4.1 Comparing the Candidates
Since each feature selection method that we evalu-
ated gives us a ranked list of features, we can train
models for an increasing number of features. We
have followed this approach, and created models for
each method, using 100 to 5000 features with a step
size of 100 features. Figure 2 shows the accuracy
for all selection methods after N features. We have
also added the line that indicates the accuracy that
is obtained when a model is trained with all features
(490667 features).
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0  1000  2000  3000  4000  5000
Bes
t ma
tch 
accu
racy
Features
allmaxentcutoffcorr
Figure 2: Accuracy of maximum entropy, correlation-
based, and frequency-based selection methods after se-
lecting N features (N ? 5000), with increments of 100
features.
In this graph we can see two interesting phenom-
ena. First of all, only a very small number of fea-
tures is required to perform this task almost as well
as a model with all extracted features. Secondly, the
maximum entropy feature selection model is able
to select the most effective features quickly - fewer
than 1000 features are necessary to achieve a rela-
tively high accuracy.
As expected, the frequency-based method fared
worse than maximum entropy selection. Initially
some very useful features, such as the n-gram
models are selected, but improvement of accuracy
quickly stagnates. We expect this to be caused by
overlap of newly selected features with features that
were initially selected. Even after selecting 5000
features, this method does not reach the same accu-
racy as the maximum entropy selection method had
after selecting only a few hundred features.
The correlation-based selection method fares bet-
ter than the frequency-based method without over-
lap detection. This clearly shows that feature over-
lap is a problem. However, the correlation-based
method does not achieve good accuracy as quickly
as the maximum entropy selection method. There
are three possible explanations for this. First, there
may be noisy features that are frequent, and since
they show no overlap with selected features they are
good candidates according to the correlation-based
method. Second, less frequent features that overlap
with a frequent feature in a subset of contexts may
show a low correlation. Third, some less frequent
features may still be very discriminative for the con-
texts where they appear, while more frequent fea-
tures may just be a small indicator for a sentence
to be fluent or non-fluent. It is possible to refine
the correlation-based method to deal with the second
class of problems. However, the lack of performance
of the correlation-based method makes this unattrac-
tive - during every selection step a candidate feature
needs to be compared with all previously selected
features, rather than some abstraction of them.
Table 1 shows the peak accuracies when select-
ing up to 5000 features with the feature selection
methods described. Accuracy scores of the random
selection baseline, the n-gram models, and a model
trained on all features are included for comparison.
The random selection baseline picks a realizations
randomly. The n-gram models are the very same
n-gram models that were used as auxiliary distribu-
tions in the feature-based models. The combined
word/tag n-gram model was created by training a
model with both n-gram models as the only fea-
tures. We also list a variation of the frequency-based
method often used in other work (such as Velldal
(2008) and Malouf and Van Noord (2004)), where
there is a fixed frequency threshold (here 4), rather
than using the first N most frequently changing fea-
tures.
Besides confirming the observation that feature
selection can compress models very well, this table
shows that the popular method of using a frequency
cutoff, still gives a lot of opportunity for compress-
ing the model further. In practice, it seems best to
plot a graph as shown in figure 2, choose an accept-
able accuracy, and to use the (number of) features
that can provide that accuracy.
Method Features Accuracy
Random 0 0.0778
Tag n-gram 1 0.2039
Word n-gram 1 0.2799
Word/tag n-gram 2 0.2908
All 490667 0.4220
Fixed cutoff (4) 90103 0.4181
Frequency 4600 0.4029
Correlation 4700 0.4172
Maxent 4300 0.4201
Table 1: Peak accuracies for the maximum entropy,
correlation-based, and frequency-based selection meth-
ods when selecting up to 5000 features. Accuracies for
random, n-gram and full models are included for com-
parison.
4.2 Overlap in Frequency-based Selection
As we argued in section 2.5, the primary disadvan-
tage of the frequency-based selection is that it cannot
account for correlation between features. In the ex-
treme case, we could have two very distinctive fea-
tures f1 and f2 that behave exactly the same in any
event. While adding f2 after adding f1 does not im-
prove the model, frequency-based selection cannot
detect this. To support this argumentation empiri-
cally, we analyzed the first 100 selected features to
find good examples of this overlap.
Initially, the frequency-based selection chooses
three distinctive features that are also selected by
the maximum entropy selection method: the two n-
gram language models, and a preference for topical-
ized NP subjects. After that, features that indicate
whether the vp arg v(np) rule was used change very
frequently within a context. However, this aspect
of the parse tree is embodied in 13 successively se-
lected features. Due to the generality of the feature
templates, there are multiple templates to capture the
use of this grammar rule: through local derivation
trees (with optional grandparenting), back-off for lo-
cal derivation trees, and the features that calculate
lexical node dominance.
Another example of such overlap in the first
100 features is in features modeling the use of the
non wh topicalization(np) rule. Features containing
this rule identifier are used 30 times in sequence,
where it occurs in local derivation subtrees (with
varying amounts of context), back-off local deriva-
tion subtrees, lexical node domination, or as a grand-
parent of another local derivation subtree.
In the first 100 features, there were many overlap-
ping features, and we expect that this also is the case
for more infrequent features.
4.3 Effective Features
The maximum entropy selection method shows that
only a small number of features is necessary to per-
form fluency ranking (section 4.1). The first fea-
tures that were selected in maximum entropy selec-
tion can give us good insight of what features are
important for fluency ranking. Table 2 shows the 10
topmost features as returned by the maximum en-
tropy selection. The weights shown in this table, are
those given by the selection method, and their sign
indicates whether the feature was characteristic of a
fluent sentence (+) or a non-fluent sentence (?).
As expected (see table 1) the n-gram models are
a very important predictor for fluency. The only
surprise here may be that the overlap between both
n-gram models is small enough to have both mod-
els as a prominent feature. While the tag n-gram
model is a worse predictor than the word n-gram
model, we expect that the tag n-gram model is espe-
cially useful for estimating fluency of phrases with
word sequences that are unknown to the word n-
gram model.
The next feature that was selected,
r2(vp arg v(pred),2,vproj vc), indicates that
the rule vp arg v(pred) was used with a vproj vc
node as its second daughter. This combination
occurs when the predicative complement is placed
after the copula, for instance as in Amsterdam is de
hoofdstad van Nederland (Amsterdam is the capital
of The Netherlands), rather than De hoofdstad
van Nederland is Amsterdam (The capital of The
Netherlands is Amsterdam).
The feature s1(non subj np topic) and its neg-
ative weight indicates that realizations with non-
topicalized NP subjects are dispreferred. In Dutch,
non-topicalized NP subjects arise in the OVS word-
order, such as in de soep eet Jan (the soup eats Jan).
While this is legal, SVO word-order is clearly pre-
ferred (Jan eet de soep).
The next selected feature (ldsb(vc vb,vb v,
[vproj vc,vp arg v(pp)])) is also related to top-
icalization: it usually indicates a preference for
prepositional complements that are not topicalized.
For instance, dit zorgde voor veel verdeeldheid
(this caused lots of discord) is preferred over the
PP-topicalized voor veel verdeeldheid zorgde dit
(lots of discord caused this).
ldsb(n n pps,pp p arg(np),[]) gives preference
PP-ordering in conjuncts where the PP modifier fol-
lows the head. For instance, the conjunct groepen
van bestaan of khandas (planes of existance or khan-
das) is preferred by this feature over van bestaan
groepen of khandas (of existence planes or khan-
das).
The next feature (lds dl(mod2,[pp p arg(np)],
[1],[non wh topicalization(modifier)])) forms an
exception to the dispreference of topicalization of
PPs. If we have a PP that modifies a copula in a
subject-predicate structure, topicalization of the PP
can make the realization more fluent. For instance,
volgens Williamson is dit de synthese (according to
Williamson is this the synthesis) is considered more
fluent than dit is de synthese volgens Williamson
(this is the synthesis according to Williamson).
The final three features deal with punctuation.
Since punctuation is very prevalent in Wikipedia
texts due to the amount of definitions and clarifi-
cations, punctuation-related features are common.
Note that the last two lds dl features may seem to
be overlapping, they are not: they use different fre-
quency bins for word domination.
5 Conclusions and Future Work
Our conclusion after performing experiments with
feature selection is twofold. First, fluency models
can be compressed enormously by applying feature
selection, without losing much in terms of accuracy.
Second, we only need a small number of targeted
features to perform fluency ranking.
The maximum entropy feature selection method
Weight Name
0.012 ngram lm
0.009 ngram tag
0.087 r2(vp arg v(pred),2,vproj vc)
-0.094 s1(non subj np topic)
0.090 ldsb(vc vb,vb v,
[vproj vc,vp arg v(pp)])
0.083 ldsb(n n pps,pp p arg(np),[])
0.067 lds dl(mod2,[pp p arg(np)],[1],
[non wh topicalization(modifier)])
0.251 lds dl(start start ligg streep,
[top start xp,punct(ligg streep),
top start xp],[0,0,1],[top start])
0.186 lds dl(start start ligg streep,
[top start xp,punct(ligg streep),
top start xp],[0,0,2],[top start])
0.132 r2(n n modroot(haak),5,l)
Table 2: The first 10 features returned by maximum en-
tropy feature selection, including the weights estimated
by this feature selection method.
shows a high accuracy after selecting just a few fea-
tures. The commonly used frequency-based selec-
tion method fares far worse, and requires addition
of many more features to achieve the same perfor-
mance as the maximum entropy method. By exper-
imenting with a correlation-based selection method
that uses the frequency method to make an initial
ordering of features, but skips features that show a
high correlation with previously selected features,
we have shown that the ineffectiveness of frequency-
based selection can be attributed partly to feature
overlap. However, the maximum entropy method
was still more effective in our experiments.
In the future, we hope to evaluate the same tech-
niques to parse disambiguation. We also plan to
compare the feature selection methods described in
this paper to selection by imposing a `1 prior.
The feature selection methods described in this
paper are usable for feature sets devised for ranking
and classification tasks, especially when huge sets of
automatically extracted features are used. An open
source implementation of the methods described in
this paper is available5, and is optimized to work on
large data and feature sets.
5http://danieldk.eu/Code/FeatureSqueeze/
References
A.L. Berger, V.J.D. Pietra, and S.A.D. Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational linguistics, 22(1):71.
G. Bouma, G. Van Noord, and R. Malouf. 2001. Alpino:
Wide-coverage computational analysis of Dutch. In
Computational Linguistics in the Netherlands 2000.
Selected Papers from the 11th CLIN Meeting.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In Proceedings of the Sixth Applied Natural Language
Processing (ANLP-2000), Seattle, WA.
A. Cahill, M. Forst, and C. Rohrer. 2007. Stochas-
tic realisation ranking for a free word order language.
In ENLG ?07: Proceedings of the Eleventh European
Workshop on Natural Language Generation, pages
17?24, Morristown, NJ, USA. Association for Com-
putational Linguistics.
A. Cahill. 2009. Correlating Human and Automatic
Evaluation of a German Surface Realiser. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 97?100.
M. Johnson and S. Riezler. 2000. Exploiting auxiliary
distributions in stochastic unification-based grammars.
In Proceedings of the 1st North American chapter of
the Association for Computational Linguistics confer-
ence, pages 154?161, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
D. Madigan and A.E. Raftery. 1994. Model selection and
accounting for model uncertainty in graphical models
using Occam?s window. Journal of the American Sta-
tistical Association, 89(428):1535?1546.
R. Malouf and G. van Noord. 2004. Wide cover-
age parsing with stochastic attribute value grammars.
In IJCNLP-04 Workshop: Beyond shallow analyses -
Formalisms and statistical modeling for deep analy-
ses. JST CREST, March.
I. D. Melamed, R. Green, and J. P. Turian. 2003. Pre-
cision and recall of machine translation. In HLT-
NAACL.
H. Nakanishi, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic models for disambiguation of an hpsg-based chart
generator. In Parsing ?05: Proceedings of the Ninth
International Workshop on Parsing Technology, pages
93?102, Morristown, NJ, USA. Association for Com-
putational Linguistics.
M. Osborne. 2000. Estimation of stochastic attribute-
value grammars using an informative sample. In Pro-
ceedings of the 18th conference on Computational lin-
guistics, pages 586?592, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
S. Perkins, K. Lacker, and J. Theiler. 2003. Grafting:
Fast, incremental feature selection by gradient descent
in function space. The Journal of Machine Learning
Research, 3:1333?1356.
A. Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1):151?175.
G. Van Noord. 2007. Using self-trained bilexical pref-
erences to improve disambiguation accuracy. In Pro-
ceedings of the 10th International Conference on Pars-
ing Technologies, pages 1?10. Association for Compu-
tational Linguistics.
E. Velldal and S. Oepen. 2006. Statistical ranking in
tactical generation. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 517?525. Association for Compu-
tational Linguistics.
E. Velldal. 2008. Empirical Realization Ranking. Ph.D.
thesis, University of Oslo, Department of Informatics.
Y. Zhou, F. Weng, L. Wu, and H. Schmidt. 2003. A
fast algorithm for feature selection in conditional max-
imum entropy modeling.
