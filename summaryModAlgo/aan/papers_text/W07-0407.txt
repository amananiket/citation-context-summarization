Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 49?56,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Discriminative word alignment by learning the alignment structure
and syntactic divergence between a language pair
Sriram Venkatapathy1
Language Technologies Research
Centre, IIIT -Hyderabad
Hyderabad - 500019, India.
sriram@research.iiit.ac.in
Aravind K. Joshi
Department of Computer and
Information Science and Institute for
Research in Cognitive Science,
University of Pennsylvania, PA, USA.
joshi@linc.cis.upenn.edu
Abstract
Discriminative approaches for word align-
ment have gained popularity in recent
years because of the flexibility that they
offer for using a large variety of features
and combining information from various
sources. But, the models proposed in the
past have not been able to make much use
of features that capture the likelihood of an
alignment structure (the set of alignment
links) and the syntactic divergence be-
tween sentences in the parallel text. This is
primarily because of the limitation of their
search techniques. In this paper, we pro-
pose a generic discriminative re-ranking
approach for word alignment which allows
us to make use of structural features effec-
tively. These features are particularly use-
ful for language pairs with high structural
divergence (like English-Hindi, English-
Japanese). We have shown that by us-
ing the structural features, we have ob-
tained a decrease of 2.3% in the absolute
value of alignment error rate (AER). When
we add the cooccurence probabilities ob-
tained from IBM model-4 to our features,
we achieved the best AER (50.50) for the
English-Hindi parallel corpus.
1 Introduction
In this paper, we propose a discriminative re-
ranking approach for word alignment which al-
lows us to make use of structural features effec-
tively. The alignment algorithm first generates
11Part of the work was done at Institute for Research
in Cognitive Science (IRCS), University of Pennsylvania,
Philadelphia, PA 19104, USA, when he was visiting IRCS
as a Visiting Scholar, February to December, 2006.
a list of k-best alignments using local features.
Then it re-ranks this list of k-best alignments us-
ing global features which consider the entire align-
ment structure (set of alignment links) and the syn-
tactic divergence that exists between the sentence
pair. Use of structural information associated with
the alignment can be particularly helpful for lan-
guage pairs for which a large amount of unsuper-
vised data is not available to measure accurately
the word cooccurence values but which do have a
small set of supervised data to learn the structure
and divergence across the language pair. We have
tested our model on the English-Hindi language
pair. Here is an example of an alignment between
English-Hindi which shows the complexity of the
alignment task for this language pair.
Figure 1: An example of an alignment between an
English and a Hindi sentence
To learn the weights associated with the param-
eters used in our model, we have used a learning
framework called MIRA (The Margin Infused Re-
laxed Algorithm) (McDonald et al, 2005; Cram-
mer and Singer, 2003). This is an online learning
algorithm which looks at one sentence pair at a
time and compares the k-best predictions of the
alignment algorithm with the gold alignment to
update the parameter weights appropriately.
In the past, popular approaches for doing word
alignment have largely been generative (Och and
Ney, 2003; Vogel et al, 1996). In the past cou-
ple of years, the discriminative models for doing
word alignment have gained popularity because of
49
the flexibility they offer in using a large variety of
features and in combining information from vari-
ous sources.
(Taskar et al, 2005) cast the problem of align-
ment as a maximum weight bipartite matching
problem, where nodes correspond to the words
in the two sentences. The link between a pair
of words, (ep,hq) is associated with a score
(score(ep,hq)) reflecting the desirability of the ex-
istence of the link. The matching problem is
solved by formulating it as a linear programming
problem. The parameter estimation is done within
the framework of large margin estimation by re-
ducing the problem to a quadratic program (QP).
The main limitation of this work is that the fea-
tures considered are local to the alignment links
joining pairs of words. The score of an align-
ment is the sum of scores of individual alignment
links measured independently i.e., it is assumed
that there is no dependence between the align-
ment links. (Lacoste-Julien et al, 2006) extend
the above approach to include features for fertil-
ity and first-order correlation between alignment
links of consecutive words in the source sentence.
They solve this by formulating the problem as a
quadratic assignment problem (QAP). But, even
this algorithm cannot include more general fea-
tures over the entire alignment. In contrast to the
above two approaches, our approach does not im-
pose any constraints on the feature space except
for fertility (?1) of words in the source language.
In our approach, we model the one-to-one and
many-to-one links between the source sentence
and target sentence. The many-to-many alignment
links are inferred in the post-processing stage us-
ing simple generic rules. Another positive aspect
of our approach is the application of MIRA. It, be-
ing an online approach, converges fast and still re-
tains the generalizing capability of the large mar-
gin approach.
(Moore, 2005) has proposed an approach which
does not impose any restrictions on the form of
model features. But, the search technique has cer-
tain heuristic procedures dependent on the types
of features used. For example, there is little vari-
ation in the alignment search between the LLR
(Log-likelihood ratio) based model and the CLP
(Conditional-Link Probability) based model. LLR
and CLP are the word association statistics used
in Moore?s work (Moore, 2005). In contrast to
the above approach, our search technique is more
general. It achieves this by breaking the search
into two steps, first by using local features to get
the k-best alignments and then by using struc-
tural features to re-rank the list. Also, by using
all the k-best alignments for updating the parame-
ters through MIRA, it is possible to model the en-
tire inference algorithm but in Moore?s work, only
the best alignment is used to update the weights
of parameters. (Fraser and Marcu, 2006) have
proposed an algorithm for doing word alignment
which applies a discriminative step at every iter-
ation of the traditional Expectation-Maximization
algorithm used in IBM models. This model still
relies on the generative story and achieves only a
limited freedom in choosing the features. (Blun-
som and Cohn, 2006) do word alignment by com-
bining features using conditional random fields.
Even though their approach allows one to include
overlapping features while training a discrimina-
tive model, it still does not allow us to use fea-
tures that capture information of the entire align-
ment structure.
In Section 2, we describe the alignment search
in detail. Section 3 describes the features that
we have considered in our paper. Section 4 talks
about the Parameter optimization. In Section 5,
we present the results of our experiments. Section
6 contains the conclusion and our proposed future
work.
2 Alignment Search
The goal of the word alignment algorithm is to link
words in the source language with words in the tar-
get language to get the alignments structure. The
best alignment structure between a source sen-
tence and a target sentence can be predicted by
considering three kinds of information, (1) Prop-
erties of alignment links taken independently, (2)
Properties of the entire alignment structure taken
as a unit, and (3) The syntactic divergence between
the source sentence and the target sentence, given
the alignment structure. Using the set of alignment
links, the syntactic structure of the source sentence
is first projected onto the target language to ob-
serve the divergence.
Let ep and hq denote the source and target
words respectively. Let n be the number of words
in source sentence and m be the number of words
in target sentence. Let S be the source sentence
and T be the target sentence.
50
2.1 Populate the Beam
The task in this step is to obtain the k-best candi-
date alignment structures using the local features.
The local features mainly contain the cooccurence
information between a source and a target word
and are independent of other alignment links in
the sentence pair. Let the local feature vector be
denoted as fL(ep, hq). The score of a particular
alignment link is computed by taking a dot prod-
uct of the weight vector W with the local feature
vector of the alignment link. More formally, the
local score of an alignment link is
scoreL(ep, hq) = W.fL(ep, hq)
The total score of an alignment structure is com-
puted by adding the scores of individual alignment
links present in the alignment. Hence, the score of
an alignment structure a? is,
scoreLa(a?, S, T ) =
?
(ep,hq)?a?
scoreL(ep, hq)
We have proposed a dynamic programming al-
gorithm of worst case complexity O(nm2 + nk2)
to compute the k-best alignments. First, the local
score of each source word with every target word
is computed and stored in local beams associated
with the source words. The local beams corre-
sponding to all the source words are sorted and the
top-k alignment links in each beam are retained.
This operation has the worst-case complexity of
O(nm2).
Now, the goal is to get the k-best alignments in
the global beam. The global beam initially con-
tains no alignments. The k best alignment links of
the first source word e0 are added to the global
beam. To add the alignment links of the next
source word to the global beam, the k2 (if k < m)
combinations of the alignments in the global beam
and alignments links in the local beam are taken
and the best k are retained in the global beam.
If k > m, then the total combinations taken are
mk. This is repeated till the entries in all the lo-
cal beams are considered, the overall worst case
complexity being O(nk2) (or O(nmk) if k > m).
2.2 Reorder the beam
We now have the k-best alignments using the local
features from the last step. We then use global fea-
tures to reorder the beam. The global features look
at the properties of the entire alignment structure
instead of the alignment links locally.
Let the global feature vector be represented as
fG(a?). The global score is defined as the dot prod-
uct of the weight vector and the global feature vec-
tor.
scoreG(a?) = W.fG(a?)
The overall score is calculated by adding the local
score and the global score.
score(a?) = scoreLa(a?) + scoreG(a?)
The beam is now sorted based on the overall scores
of each alignment. The alignment at the top of
the beam is the best possible alignment between
source sentence and the target sentence.
2.3 Post-processing
The previous two steps produce alignment struc-
tures which contain one-to-one and many-to-one
links. In this step, the goal is to extend the best
alignment structure obtained in the previous step
to include the other alignments links of one-to-
many and many-to-many types.
The majority of the links between the source
sentence and the target sentence are one-to-one.
Some of the cases where this is not true are the in-
stances of idioms, alignment of verb groups where
auxiliaries do not correspond to each other, the
alignment of case-markers etc. Except for the
cases of idioms in target language, most of the
many-to-many links between a source and target
sentences can be inferred from the instances of
one-to-one and many-to-one links using three lan-
guage language specific rules (Hindi in our case)
to handle the above cases. Figure 1, Figure 2 and
Figure 3 depict the three such cases where many-
to-many alignments can be inferred. The align-
ments present at the left are those which can be
predicted by our alignment model. The alignments
on the right side are those which can be inferred in
the post-processing stage.
.....  are  playing ......
....... khel rahe hain 
.....  are  playing ......
....... khel rahe hain 
    
   (play  cont  be)
Figure 2: Inferring the many-to-many alignments
of verb and auxiliaries
After applying the language specific rules, the
dependency structure of the source sentence is tra-
versed to ensure the consistency of the alignment
51
John  ne  ....
John ..........
John  ne  ....
John ..........
Figure 3: Inferring the one-to-many alignment to
case-markers in Hindi
... kicked the bucket 
..........  mara gaya
... kicked the bucket 
..........  mara gaya
 (die   go?light verb)
Figure 4: Inferring many-to-many alignment for
source idioms
structure. If there is a dependency link between
two source words eo and ep, where eo is the head
and ep is the modifier and if eo and ep are linked
to one or more common target word(s), it is log-
ical to imagine that the alignment should be ex-
tended such that both eo and ep are linked to the
same set of target words. For example, in Figure 4,
new alignment link is first formed between ?kick?
and ?gayA? using the language specific rule, and
as ?kick? and ?bucket? are both linked to ?mara?,
?bucket? is also now linked to ?gayA?. Similarity,
?the? is linked to both ?mara? and ?gayA?. Hence,
the rules are applied by traversing through the de-
pendency tree associated with the source sentence
words in depth-first order. The dependency parser
used by us was developed by (Shen, 2006). The
following summarizes this step,
? Let w be the next word considered in the dependency
tree, let pw be the parent of w.
? If w and pw are linked to one or more common
word(s) in target language, align w to all target
words which are aligned to pw.
? Else, Use the target-specific rules (if they match)
to extend the alignments of w.
? Recursively consider all the children of w
3 Parameters
As the number of training examples is small, we
chose to use features (both local and structural)
which are generic. Some of the features which we
used in this experiment are as follows:
3.1 Local features (FL)
The local features which we consider are mainly
co-occurrence features. These features estimate
the likelihood of a source word aligning to a tar-
get word based on the co-occurrence information
obtained from a large sentence aligned corpora1.
3.1.1 DiceWords
Dice Coefficient of the source word and the tar-
get word (Taskar et al, 2005).
DCoeff(ep, hq) = 2 ? Count(ep, hq)Count(ep) + Count(hq)
where Count(ep, hq) is the number of times the
word hq was present in the translation of sentences
containing the word ep in the parallel corpus.
3.1.2 DiceRoots
Dice Coefficient of the lemmatized forms of the
source and target words. It is important to consider
this feature for language pairs which do not have a
large unsupervised sentence aligned corpora. Co-
occurrence information can be learnt better after
we lemmatize the words.
3.1.3 Dict
This feature tests whether there exists a dictio-
nary entry from the source word ep to the target
word hq. For English-Hindi, we used a medium-
coverage dictionary (25000 words) available from
IIIT - Hyderabad, India 2.
3.1.4 Null POS
These parameters measures the likelihood of a
source word with a particular part of speech tag3 to
be aligned to no word (Null) on the target language
side. This feature was extremely useful because
it models the cooccurence information of words
with nulls which is not captured by the features
DiceWords and DiceRoots. Here are some of the
features of this type with extreme estimated pa-
rameter weights.
3.2 Lemmatized word pairs
The word pairs themselves are a good indicator
of whether an alignment link exists between the
word pair or not. Also, taking word-pairs as fea-
ture helps in the alignment of some of the most
common words in both the languages. A variation
of this feature was used by (Moore, 2005) in his
paper.
150K sentence pairs originally collected as part of TIDES
MT project and later refined at IIIT-Hyderabad, India.
2http://ltrc.iiit.ac.in/onlineServices/Dictionaries/Dict Frame.html
3We have limited the number of POS tags by considering
only the first alphabets of Penn Tags as our POS tag cate-
gories
52
Param. weight Param. weight
Null ? 0.2737 null C -0.7030
Null U 0.1969 null D -0.6914
Null L 0.1814 null V -0.6360
Null . 0.0383 null N -0.5600
Null : 0.0055 null I -0.4839
Table 1: Top Five Features each with Maximum
and Minimum weights
Other parameters like the relative distance be-
tween the source word ep and the target word hq,
RelDist(ep, hq) = abs(j/|e| ? k/|h|), which are
mentioned as important features in the previous
literature, did not perform well for the English-
Hindi language pair. This is because of the pre-
dominant word-order variation between the sen-
tences of English and Hindi (Refer Figure 1).
3.3 Structural Features (FG)
The global features are used to model the prop-
erties of the entire alignment structure taken as a
unit, between the source and the target sentence.
In doing so, we have attempted to exploit the syn-
tactic information available on both the source and
the target sides of the corpus. The syntactic infor-
mation on the target side is obtained by projecting
the syntactic information of the source using the
alignment links. Some of the features which we
have used in our work are in the following subsec-
tion.
3.3.1 Overlap
This feature considers the instances in a sen-
tence pair where a source word links to a target
word which is a participant in more than one align-
ment links (has a fertility greater than one). This
feature is used to encourage the source words to
be linked to different words in the target language.
For example, we would prefer the alignment in
Figure 6 when compared to the alignment in Fig-
ure 5 even before looking at the actual words. This
parameter captures such prior information about
the alignment structure.
Figure 5: Alignment where many source words are
linked to one target word
Figure 6: Alignment where the source words are
aligned to many different target words
Formally, it is defined as
Overlap(a?) =
?
hq?T,Fert(hq)>1 Fert
2(hq)
?
h?T Fert(h)
where T is the Hindi sentence. ? Fert2(hq) is
measured in the numerator so that a more uniform
distribution of target word fertilities be favored in
comparison to others. The weight of overlap as
estimated by our model is -6.1306 which indicates
the alignments having a low overlap value are pre-
ferred.
3.3.2 NullPercent
This feature measures the percentage of words
in target language sentence which are not aligned
to any word in the source language sentence. It is
defined as
NullPercent =
|hq|hq?T,Fertility(hq)==0
|h|h?T
3.3.3 Direction DepPair
The following feature attempts to capture the
first order interdependence between the alignment
links of pairs of source sentence words which are
connected by dependency relations. One way in
which such an interdependence can be measured
is by noting the order of the target sentence words
linked to the child and parent of a source sentence
dependency relation. Figures 7, 8 and 9 depict
the various possibilities. The words in the source
sentence are represented using their part-of-speech
tags. These part-of-speech tags are also projected
onto the target words. In the figures p is the parent
and c is the part-of-speech of the child.
p c
c p
Figure 7: Target word linked to a child precedes
the target word linked to a parent
53
p c
p c
Figure 8: Target word linked to a parent precedes
the target word linked to a child
p c
p c
Figure 9: Parent and the child are both linked to
same target word
The situation in Figure 9 is an indicator that the
parent and child dependency pair might be part or
whole of a multi-word expression on the source
side. This feature thus captures the divergence be-
tween the source sentence dependency structure
and the target language dependency structure (in-
duced by taking the alignment as a constraint).
Hence, in the test data, the alignments which do
not express this divergence between the depen-
dency trees are penalized. For example, the align-
ment in Figure 10 will be heavily penalized by
the model during re-ranking step primarily for two
reasons, 1) The word aligned to the preposition
?of? does not precede the word aligned to the noun
?king? and 2) The word aligned to the preposition
?to? does not succeed the word aligned to the noun
?king?.
......... to the king of Rajastan .......
......  Rajastan  ke   Raja  ko   ..........
( Rajastan   of    King   to  )
Figure 10: A simple example of an alignment
that would be penalized by the feature Direc-
tion DepPair
3.3.4 Direction Bigram
This feature is a variation of the previous fea-
ture. In the previous feature, the dependency pair
on the source side was projected to the target side
to observe the divergence of the dependency pair.
In this feature, we take a bigram instead of a de-
pendency pair and observe its order in the target
side. This feature is equivalent to the first-order
features used in the related work.
There are three possibilities here, (1) The words
of the bigram maintain their order when projected
onto the target words, (2) The words of the bigram
are reversed when projected, (3) Both the words
are linked to the same word of the target sentence.
4 Online large margin training
For parameter optimization, we have used an on-
line large margin algorithm called MIRA (Mc-
Donald et al, 2005) (Crammer and Singer, 2003).
We will briefly describe the training algorithm that
we have used. Our training set is a set of English-
Hindi word aligned parallel corpus. Let the num-
ber of sentence pairs in the training data be t. We
have {Sr, Tr, a?r} for training where r ? t is the
index number of the sentence pair {Sr, Tr} in the
training set and a?r is the gold alignment for the
pair {Sr, Tr}. Let W be the weight vector which
has to be learnt, Wi be the weight vector after the
end of ith update. To avoid over-fitting, W is ob-
tained by averaging over all the weight vectors Wi.
A generic large margin algorithm is defined
follows for the training instances {Sr, Tr, a?r},
Initialize W0, W , i
for p = 1 to Iterations do
for r = 1 to t do
Get K-Best predictions ?r = {a1, a2...ak}
for the training example (Sr, Tr, a?r)
using the current model W i and applying
step 1 and 2 of section 4. Compute W i+1
by updating W i based on
(Sr, Tr, a?r, ?r).
i = i + 1
W = W + W i+1
W = WIterations?m
end for
end for
The goal of MIRA is to minimize the change in
W i such that the score of the gold alignment a? ex-
ceeds the score of each of the predictions in ? by a
margin which is equal to the number of mistakes in
the predictions when compared to the gold align-
ment. One could choose a different loss function
which assigns greater penalty for certain kinds of
mistakes when compared to others.
Step 4 (Get K-Best predictions) in the algo-
54
rithm mentioned above can be substituted by the
following optimization problem,
minimize ?(W i+1 ? W i)?
s.t. ?k, score(a?r, Sr, Tr)? score(aq,k, Sr, Tr)
>= Mistakes(ak, a?r, Sr, Tr)
For optimization of the parameters, ideally, we
need to consider all the possible predictions and
assign margin constraints based on every predic-
tion. But, here the number of such classes is ex-
ponential and therefore we restrict ourselves to the
k ? best predictions.
We estimate the parameters in two steps. In the
first step, we estimate only the weights of the lo-
cal parameters. After that, we keep the weights
of local parameters constant and then estimate the
weights of global parameters. It is important to
decouple the parameter estimation to two steps.
We also experimented estimating the parameters
in one stage but as expected, it had an adverse
impact on the parameter weights of local features
which resulted in generation of poor k-best list af-
ter the first step while testing.
5 Experiments and Results
5.1 Data
We have used English-Hindi unsupervised data of
50000 sentence pairs4. This data was used to ob-
tain the cooccurence statistics such as DiceWords
and DiceRoots which we used in our model. This
data was also used to obtain the predictions of
GIZA++ (Implements the IBM models and the
HMM model). We take the alignments of GIZA++
as baseline and evaluate our model for the English-
Hindi language pair.
The supervised training data which is used to
estimate the parameters consists of 4252 sentence
pairs. The development data consists of 100 sen-
tence pairs and the test data consists of 100 sen-
tence pairs. This supervised data was obtained
from IRCS, University of Pennsylvania. For train-
ing our model, we need to convert the many-to-
many alignments in the corpus to one-to-one or
may-to-one alignments. This is done by applying
inverse operations of those performed during the
post-processing step (section 2.3).
4Originally collected as part of TIDES MT project and
later refined at IIIT-Hyderabad, India.
5.2 Experiments
We first obtain the predictions of GIZA++ to ob-
tain the baseline accuracies. GIZA++ was run in
four different modes 1) English to Hindi, 2) Hindi
to English, 3) English to Hindi where the words in
both the languages are lemmatized and 4) Hindi to
English where the words are lemmatized. We then
take the intersections of the predictions run from
both the directions (English to Hindi and Hindi to
English). Table 2 contains the results of experi-
ments with GIZA++. As the recall of the align-
ment links of the intersection is very low for this
dataset, further refinements of the alignments as
suggested by (Och and Ney, 2003) were not per-
formed.
Mode Prec. Rec. F-meas. AER
Normal: Eng-Hin 47.57 40.87 43.96 56.04
Normal: Hin-Eng 47.97 38.50 42.72 57.28
Normal: Inter. 88.71 27.52 42.01 57.99
Lemma.: Eng-Hin 53.60 44.58 48.67 51.33
Lemma.: Hin-Eng 53.83 42.68 47.61 52.39
Lemma.: Inter. 86.14 32.80 47.51 52.49
Table 2: GIZA++ Results
In Table 3, we observe that the best result
(51.33) is obtained when GIZA++ is run after lem-
matizing the words on the both sides of the unsu-
pervised corpus. The best results obtained without
lemmatizing is 56.04 when GIZA++ is run from
English to Hindi.
The table 4 summarizes the results when we
used only the local features in our model.
Features Prec. Rec. F-meas. AER
DiceRoots 41.49 38.71 40.05 59.95
+ DiceWords
+ Null POS 42.82 38.29 40.43 59.57
+ Dict. 43.94 39.30 41.49 58.51
+ Word pairs 46.27 41.07 43.52 56.48
Table 3: Results using local features
We now add the global features. While esti-
mating the parameter weights associated with the
global features, we keep the weights of local fea-
tures constant. We choose the appropriate beam
size as 50 after testing with several values on the
development set. We observed that the beam sizes
(between 10 and 100) did not affect the alignment
error rates very much.
55
Features Prec. Rec. F-meas. AER
Local feats. 46.27 41.07 43.52 56.48
Local feats. 48.17 42.76 45.30 54.70
+ Overlap
Local feats. 47.93 42.55 45.08 54.92
+ Direc. Deppair
Local feats. 48.31 42.89 45.44 54.56
+ Direc. Bigram
Local feats. 48.81 43.31 45.90 54.10
+ All Global feats.
Table 4: Results after adding global features
We see that by adding global features, we ob-
tained an absolute increase of about 2.3 AER sug-
gesting the usefulness of structural features which
we considered. Also, the new AER is much better
than that obtained by GIZA++ run without lem-
matizing the words.
We now add the IBM Model-4 parameters (co-
occurrence probabilities between source and tar-
get words) obtained using GIZA++ and our fea-
tures, and observe the results (Table 6). We can
see that structural features resulted in a significant
decrease in AER. Also, the AER that we obtained
is slightly better than the best AER obtained by the
GIZA++ models.
Features Prec. Rec. F-meas. AER
IBM Model-4 Pars. 48.85 43.98 46.29 52.71
+ LocalFeats
IBM Model-4 Pars. 48.95 50.06 49.50 50.50
+ All feats.
Table 5: Results after combining IBM model-4 pa-
rameters with our features
6 Conclusion and Future Work
In this paper, we have proposed a discriminative
re-ranking approach for word alignment which al-
lows us to make use of structural features effec-
tively. We have shown that by using the structural
features, we have obtained a decrease of 2.3% in
the absolute value of alignment error rate (AER).
When we combine the prediction of IBM model-4
with our features, we have achieved an AER which
is slightly better than the best AER of GIZA++
for the English-Hindi parallel corpus (a language
pair with significant structural divergences). We
expect to get large improvements when we add
more number of relevant local and structural fea-
tures. We also plan to design an appropriate de-
pendency based decoder for machine translation
to make good use of the parameters estimated by
our model.
References
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Proceedings of the 21st COLING and 44th Annual
Meeting of the ACL, Sydney, Australia, July. ACL.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. In
Journal of Machine Learning Research.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. In
Proceedings of the 21st COLING and 44th Annual
Meeting of the ACL, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael I. Jordan. 2006. Word alignment via
quadratic assignment. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 112?119, New York City,
USA, June. Association for Computational Linguis-
tics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-project dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523?530, Vancouver,
British Columbia, Canada, October. Association of
Computational Linguistics.
Robert C. Moore. 2005. A discriminative frame-
work for bilingual word alignment. In Proceedings
of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 81?88, Vancouver, British
Columbia, Canada, October. Association of Compu-
tational Linguistics.
F. Och and H. Ney. 2003. A systematic comparisoin
of various statistical alignment models. In Compu-
tational Linguistics.
Libin Shen. 2006. Statistical LTAG Parsing. Ph.D.
thesis.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative machine approach to word
alignment. In Proceedings of HLT-EMNLP, pages
73?80, Vancouver, British Columbia, Canada, Octo-
ber. Association of Computational Linguistics.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th International
Conference on Computational Linguistics.
56
