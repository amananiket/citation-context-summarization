The Complexity of Recognition 
of Linguistically Adequate Dependency Grammars 
Peter Neuhaus 
Norbert Br i i ker  
Computat iona l  L inguist ics Research Group 
Freiburg University,  Fr iedr ichstrage 50 
D-79098 Freiburg, Germany 
email: { neuhaus,nobi } @coling.uni-freiburg.de 
Abstract 
Results of computational complexity exist for 
a wide range of phrase structure-based gram- 
mar formalisms, while there is an apparent 
lack of such results for dependency-based for-
malisms. We here adapt a result on the com- 
plexity of ID/LP-grammars tothe dependency 
framework. Contrary to previous tudies on 
heavily restricted ependency grammars, we 
prove that recognition (and thus, parsing) of 
linguistically adequate dependency grammars 
is~A/T'-complete. 
1 Introduction 
The introduction of dependency grammar (DG) into 
modern linguistics is marked by Tesni~re (1959). His 
conception addressed idactic goals and, thus, did not 
aim at formal precision, but rather at an intuitive un- 
derstanding of semantically motivated ependency re- 
lations. An early formalization was given by Gaifman 
(1965), who showed the generative capacity of DG to be 
(weakly) equivalent to standard context-free grammars. 
Given this equivalence, interest in DG as a linguistic 
framework diminished considerably, although many de- 
pendency grammarians view Gaifman's conception as an 
unfortunate one (cf. Section 2). To our knowledge, there 
has been no other formal study of DG.This is reflected 
by a recent study (Lombardo & Lesmo, 1996), which 
applies the Earley parsing technique (Earley, 1970) to 
DG, and thereby achieves cubic time complexity for the 
analysis of DG. In their discussion, Lombardo & Lesmo 
express their hope that slight increases in generative ca- 
pacity will correspond toequally slight increases incom- 
putational complexity. It is this claim that we challenge 
here. 
After motivating non-projective analyses for DG, we 
investigate various variants of DG and identify the sep- 
aration of dominance and precedence as a major part of 
current DG theorizing. Thus, no current variant of DG 
(not even Tesni~re's original formulation) is compatible 
with Gaifman' s conception, which seems to be motivated 
by formal considerations only (viz., the proof of equiva- 
lence). Section 3 advances our proposal, which cleanly 
separates dominance and precedence r lations. This is il- 
lustrated in the fourth section, where we give a simple en- 
coding of an A/P-complete problem in a discontinuous 
DG. Our proof of A/79-completeness, however, does not 
rely on discontinuity, but only requires unordered trees. 
It is adapted from a similar proof for unordered context- 
free grammars (UCFGs) by Barton (1985). 
2 Versions of Dependency Grammar 
The growing interest in the dependency concept (which 
roughly corresponds to the O-roles of GB, subcatego- 
rization in HPSG, and the so-called omain of locality 
of TAG) again raises the issue whether non-lexical cat- 
egories are necessary for linguistic analysis. After re- 
viewing several proposals in this section, we argue in the 
next section that word order - -  the description of which 
is the most prominent difference between PSGs and DGs 
- -  can adequately be described without reference to non- 
lexical categories. 
Standard PSG trees are projective, i.e., no branches 
cross when the terminal nodes are projected onto the 
input string. In contrast o PSG approaches, DG re- 
quires non-projective analyses. As DGs are restricted 
to lexical nodes, one cannot, e.g., describe the so-called 
unbounded ependencies without giving up projectiv- 
ity. First, the categorial approach employing partial con- 
stituents (Huck, 1988; Hepple, 1990) is not available, 
since there are no phrasal categories. Second, the coin- 
dexing (Haegeman, 1994) or structure-sharing (Pollard 
& Sag, 1994) approaches are not available, since there 
are no empty categories. 
Consider the extracted NP in "Beans, I know John 
likes" (cf. also to Fig.1 in Section 3). A projective tree 
would require "Beans" to be connected to either "I" or 
"know" - none of which is conceptually directly related 
to "Beans". It is "likes" that determines syntactic fea- 
337 
tures of "Beans" and which provides a semantic role for 
it. The only connection between "know" and "Beans" is 
that the finite verb allows the extraction of "Beans", thus 
defining order restrictions for the NP. This has led some 
DG variants to adopt a general graph structure with mul- 
tiple heads instead of trees. We will refer to DGs allow- 
ing non-projective analyses as discontinuous DGs. 
Tesni~re (1959) devised a bipartite grammar theory 
which consists of a dependency omponent and a trans- 
lation component (' translation' used in a technical sense 
denoting a change of category and grammatical func- 
tion). The dependency omponent defines four main cat- 
egories and possible dependencies between them. What 
is of interest here is that there is no mentioning of order 
in TesniSre's work. Some practitioneers of DG have al- 
lowed word order as a marker for translation, but they do 
not prohibit non-projective trees. 
Gaifman (1965) designed his DG entirely analogous 
to context-free phrase structure grammars. Each word 
is associated with a category, which functions like the 
non-terminals in CFG. He then defines the following rule 
format for dependency grammars: 
(1) X (Y , , . . .  , Y~, ,, Y~+I, . . . ,  Y,,) 
This rule states that a word of category X governs words 
of category Y1,... , Yn which occur in the given order. 
The head (the word of category X) must occur between 
the i-th and the (i + 1)-th modifier. The rule can be 
viewed as an ordered tree of depth one with node labels. 
Trees are combined through the identification of the root 
of one tree with a leaf of identical category of another 
tree. This formalization is restricted to projective trees 
with a completely specified order of sister nodes. As we 
have argued above, such a formulation cannot capture se- 
mantically motivated ependencies. 
2.1 Current Dependency Grammars 
Today's DGs differ considerably from Gaifman's con- 
ception, and we will very briefly sketch various order de- 
scriptions, showing that DGs generally dissociate dom- 
inance and precedence by some mechanism. All vari- 
ants share, however, the rejection of phrasal nodes (al- 
though phrasal features are sometimes allowed) and the 
introduction of edge labels (to distinguish different de- 
pendency relations). 
Meaning-Text Theory (Mer 5uk, 1988) assumes seven 
strata of representation. The rules mapping from the un- 
ordered ependency trees of surface-syntactic represen- 
tations onto the annotated lexeme sequences of deep- 
morphological representations include global ordering 
rules which allow discontinuities. These rules have not 
yet been formally specified (Mel' 5uk & Pertsov, 1987, 
p. 187f), but see the proposal by Rambow & Joshi (1994). 
Word Grammar (Hudson, 1990) is based on general 
graphs. The ordering of two linked words is specified to- 
gether with their dependency relation, as in the proposi- 
tion "object of verb succeeds it". Extraction is analyzed 
by establishing another dependency, visitor, between the 
verb and the extractee, which is required to precede the 
verb, as in "visitor of verb precedes it". Resulting incon- 
sistencies, e.g. in case of an extracted object, are not 
resolved, however. 
Lexicase (Starosta, 1988; 1992) employs complex fea- 
ture structures to represent lexical and syntactic enti- 
ties. Its word order description is much like that of 
Word Grammar (at least at some level of abstraction), 
and shares the above inconsistency. 
Dependency Unification Grammar (Hellwig, 1988) 
defines a tree-like data structure for the representation f 
syntactic analyses. Using morphosyntactic features with 
special interpretations, a word defines abstract positions 
into which modifiers are mapped. Partial orderings and 
even discontinuities can thus be described by allowing a 
modifier to occupy a position defined by some transitive 
head. The approach cannot restrict discontinuities prop- 
erly, however. 
Slot Grammar (McCord, 1990) employs a number of 
rule types, some of which are exclusively concerned with 
precedence. So-called head/slot and slot/slot ordering 
rules describe the precedence in projective trees, refer- 
ring to arbitrary predicates over head and modifiers. Ex- 
tractions (i.e., discontinuities) are merely handled by a 
mechanism built into the parser. 
This brief overview of current DG flavors shows that 
various mechanisms (global rules, general graphs, proce- 
dural means) are generally employed to lift the limitation 
to projective trees. Our own approach presented below 
improves on these proposals because it allows the lexi- 
calized and declarative formulation of precedence con- 
straints. The necessity of non-projective analyses in DG 
results from examples like "Beans, 1 know John likes" 
and the restriction to lexical nodes which prohibits gap- 
threading and other mechanisms tied to phrasal cate- 
gories. 
3 A Dependency Grammar with Word 
Order Domains 
We now sketch a minimal DG that incorporates only 
word classes and word order as descriptional dimensions. 
The separation of dominance and precedence presented 
here grew out of our work on German, and retains the lo- 
cal flavor of dependency specification, while at the same 
time covering arbitrary discontinuities. It is based on a 
(modal) logic with model-theoretic interpretation, which 
is presented in more detail in (Br~ker, 1997). 
338 
f  know 
/ /~ , , ,@x i ~ e s  ~ ) 
I 
dl d2 
Figure 1: Word order domains in "Beans, I know John 
likes" 
3.1 Order Specification 
Our initial observation is that DG cannot use binary 
precedence constraints as PSG does. Since DG analyses 
are hierarchically flatter, binary precedence constraints 
result in inconsistencies, asthe analyses of Word Gram- 
mar and Lexicase illustrate. In PSG, on the other hand, 
the phrasal hierarchy separates the scope of precedence 
restrictions. This effect is achieved in our approach by 
defining word order domains as sets of words, where 
precedence restrictions apply only to words within the 
same domain. Each word defines a sequence of order do- 
mains, into which the word and its modifiers are placed. 
Several restrictions are placed on domains. First, 
the domain sequence must mirror the precedence of the 
words included, i.e., words in a prior domain must pre- 
cede all words in a subsequent domain. Second, the order 
domains must be hierarchically ordered by set inclusion, 
i.e., be projective. Third, a domain (e.g., dl in Fig.l) 
can be constrained to contain at most one partial depen- 
dency tree. l We will write singleton domains as "_" ,  
while other domains are represented by " - " .  The prece- 
dence of words within domains is described by binary 
precedence r strictions, which must be locally satisfied 
in the domain with which they are associated. Consid- 
ering Fig. 1 again, a precedence r striction for "likes" to 
precede its object has no effect, since the two are in dif- 
ferent domains. The precedence constraints are formu- 
lated as a binary relation "~" over dependency labels, 
including the special symbol "self" denoting the head. 
Discontinuities can easily be characterized, since a word 
may be contained in any domain of (nearly) any of its 
transitive heads. If a domain of its direct head contains 
the modifier, a continuous dependency results. If, how- 
ever, a modifier is placed in a domain of some transitive 
head (as "Beans" in Fig. 1), discontinuities occur. Bound- 
ing effects on discontinuities are described by specifying 
that certain dependencies may not be crossed. 2 For the 
tFor details, cf. (Br6ker, 1997). 
2German data exist that cannot be captured by the (more 
common) bounding of discontinuities by nodes of a certain 
purpose of this paper, we need not formally introduce the 
bounding condition, though. 
A sample domain structure is given in Fig.l, with two 
domains dl and d2 associated with the governing verb 
"know" (solid) and one with the embedded verb "likes" 
(dashed). dl may contain only one partial dependency 
tree, the extracted phrase, d2 contains the rest of the sen- 
tence. Both domains are described by (2), where the do- 
main sequence is represented as "<<". d2 contains two 
precedence r strictions which require that "know" (rep- 
resented by self) must follow the subject (first precedence 
constraint) and precede the object (second precedence 
constraint). 
(2) __ { } << ----. { (subject -.< self), (self --< object)} 
3.2 Formal Description 
The following notation is used in the proof. A lexicon 
Lez maps words from an alphabet E to word classes, 
which in turn are associated with valencies and domain 
sequences. The set C of word classes is hierarchically 
ordered by a subclass relation 
(3) i saccCxC 
A word w of class c inherits the valencies (and domain 
sequence) from c, which are accessed by 
(4) w.valencies 
A valency (b, d, c) describes a possible dependency re- 
lation by specifying a flag b indicating whether the de- 
pendency may be discontinuous, the dependency name d 
(a symbol), and the word class c E C of the modifier. A 
word h may govern a word m in dependency d if h de- 
fines a valency (b, d, c) such that (m isao c) and m can 
consistently be inserted into a domain of h (for b = - )  
or a domain of a transitive head of h (for b = +). This 
condition is written as 
(5) governs(h,d,m) 
A DG is thus characterized by 
(6) G = (Lex, C, isac, E) 
The language L(G) includes any sequence of words 
for which a dependency tree can be constructed such that 
for each word h governing a word m in dependency d,
governs(h, d, m) holds. The modifier of h in dependency 
d is accessed by 
(7) h.mod(d) 
category. 
339 
4 The complexity of DG Recognition 
Lombardo & Lesmo (1996, p.728) convey their hope that 
increasing the flexibility of their conception of DG will 
" . . .  imply the restructuring of some parts of the rec- 
ognizer, with a plausible increment of the complexity". 
We will show that adding a little (linguistically required) 
flexibility might well render ecognition A/P-complete. 
To prove this, we will encode the vertex cover problem, 
which is known to be A/P-complete, in a DG. 
4.1 Encoding the Vertex Cover Problem in 
Discontinuous DG 
A vertex cover of a finite graph is a subset of its ver- 
tices such that (at least) one end point of every edge is 
a member of that set. The vertex cover problem is to 
decide whether for a given graph there exists a vertex 
cover with at most k elements. The problem is known to 
be A/7~-complete (Garey & Johnson, 1983, pp.53-56). 
Fig. 2 gives a simple example where {c, d} is a vertex 
cover. 
a b 
X 
d 
Figure 2: Simple graph with vertex cover {c, d}. 
A straightforward encoding of a solution in the DG 
formalism introduced in Section 3 defines a root word 
s of class S with k valencies for words of class O. O 
has IWl subclasses denoting the nodes of the graph. An 
edge is represented by two linked words (one for each 
end point) with the governing word corresponding to 
the node included in the vertex cover. The subordinated 
word is assigned the class R, while the governing word 
is assigned the subclass of O denoting the node it repre- 
sents. The latter word classes define a valency for words 
of class R (for the other end point) and a possibly discon- 
tinuous valency for another word of the identical class 
(representing the end point of another edge which is in- 
cluded in the vertex cover). This encoding is summarized 
in Table 1. 
The input string contains an initial s and for each edge 
the words representing its end points, e.g. "saccdadb- 
dcb" for our example. If the grammar allows the con- 
struction of a complete dependency tree (cf. Fig. 3 for 
one solution), this encodes a solution of the vertex cover 
problem. 
$ 
% 
I l l l l l l l l l  b 
I l t l l l l l l l  I 
I I I I I I I I I I  I 
$ac  c da  dbdc  b 
Figure 3: Encoding a solution to the vertex cover prob- 
lem from Fig. 2. 
4.2 Formal Proof using Continuous DG 
The encoding outlined above uses non-projective trees, 
i.e., crossing dependencies. In anticipation of counter 
arguments such as that the presented ependency gram- 
mar was just too powerful, we will present he proof us- 
ing only one feature supplied by most DG formalisms, 
namely the free order of modifiers with respect o their 
head. Thus, modifiers must be inserted into an order do- 
main of their head (i.e., no + mark in valencies). This 
version of the proof uses a slightly more complicated en- 
coding of the vertex cover problem and resembles the 
proof by Barton (1985). 
Definition 1 (Measure) 
Let II ? II be a measure for the encoded input length of a 
computational problem. We require that if S is a set or 
string and k E N then ISl > k implies IlSll ___ Ilkll and 
that for any tuple I1("" , z , . .  ")11 - Ilzll holds. < 
Definition 2 (Vertex Cover Problem) 
A possible instance of the vertex cover problem is a triple 
(V, E, k) where (V, E) is a finite graph and IvI > k 
N. The vertex cover problem is the set VC of all in- 
stances (V, E, k) for which there exists a subset V' C_ V 
and a function f : E ---> V I such that IV'l <_ k and 
V(Vm,Vn) E E:  f((vm,Vn)) E {Vm,Vn}. <1 
Definition 3 (DG recognition problem) 
A possible instance of the DG recognition problem is a 
tuple (G, a) where G = (Lex, C, i sac,  ~) is a depen- 
dency grammar as defined in Section 3 and a E E +. The 
DG recognition problem DGR consists of all instances 
(G, a) such that a E L(G). <1 
For an algorithm to decide the VC problem consider a
data structure representing the vertices of the graph (e.g., 
a set). We separate the elements of this data structure 
340 
classes valencies order domain 
S {(- ,  markl,O), (- ,  mark2,0)} --{(self-~ mark1), (mark1 -.< mark2)} 
A isac 0 {(- ,  unmrk, R), (+, same, A)} ={(unmrk -K same), (self -4 same)} 
B isac O {(- ,  unmrk, R), (+, same, B)} ={(unmrk --< same), (self -.< same)} 
(7 isac O {(- ,  unmrk, R), (+, same, C)} ~{(unmrk --4 same), (self -4 same)} 
D isac O {(- ,  unmrk, R), (+, same, D)} -{(unmrk --.< same), (self -~ same)} 
R {} --{} 
\[ word \[ classes I 
s {s} 
a {A,R} 
b {B,R} 
c {C,R} 
d {D,R} 
Table 1: Word classes and lexicon for vertex cover problem from Fig. 2 
into the (maximal) vertex cover set and its complement 
set. Hence, one end point of every edge is assigned to 
the vertex cover (i.e., it is marked). Since (at most) all 
IEI edges might share a common vertex, the data struc- 
ture has to be a multiset which contains IEI copies of 
each vertex. Thus, marking the IVI - k complement ver- 
tices actually requires marking IVI - k times IE\[ iden- 
tical vertices. This will leave (k - 1) * IEI unmarked 
vertices in the input structure. To achieve this algorithm 
through recognition of a dependency grammar, the mark- 
ing process will be encoded as the filling of appropriate 
valencies of a word s by words representing the vertices. 
Before we prove that this encoding can be generated in 
polynomial time we show that: 
Lemma 1 
The DG recognition problem is in the complexity class 
Alp. \[\] 
Let G = (Lex, C, isac,  Z) and a E \]E +. We give 
a nondeterministic algorithm for deciding whether a = 
(Sl- . -  sn) is in L(G). Let H be an empty set initially: 
1. Repeat until IHI = Iol 
(a) i. For every Si E O r choose a lexicon entry 
ci E Lex(si). 
ii. From the ci choose one word as the head 
h0. 
iii. Let H := {ho} and M := {cili E 
\[1, IOrl\]} \ H. 
(b) Repeat until M = 0: 
i. Choose a head h E H and a valency 
(b, d, c) E h.valencies and a modifier m E 
M. 
ii. If governs(h, d, m) holds then establish the 
dependency relation between h and the m, 
and add m to the set H. 
iii. Remove m from M. 
The algorithm obviously is (nondeterministically) 
polynomial in the length of the input. Given that 
(G, g) E DGR, a dependency tree covering the whole 
input exists and the algorithm will be able to guess the 
dependents of every head correctly. If, conversely, the 
algorithm halts for some input (G, or), then there neces- 
sarily must be a dependency tree rooted in ho completely 
covering a. Thus, (G, a) E DGR. \[\] 
Lemma 2 
Let (V, E, k) be a possible instance of the vertex cover 
problem. Then a grammar G(V, E, k) and an input 
a(V, E, k) can be constructed in time polynomial in 
II (v, E, k)II such that 
(V, E, k) E VC ?:::::v (G(V, E, k), a(V, E, k)) E DGR 
\[\] 
For the proof, we first define the encoding and show 
that it can be constructed in polynomial time. Then we 
proceed showing that the equivalence claim holds. The 
set of classes is G =aef {S, R, U} U {Hdi e \[1, IEI\]} U 
{U~, ?1i e \[1, IVI\]}. In the isac hierarchy the classes Ui 
share the superclass U, the classes V~ the superclass R. 
Valencies are defined for the classes according to Table 2. 
Furthermore, we define E =dee {S} U {vii/ E \[1, IVl\]}. 
The lexicon Lex associates words with classes as given 
in Table 2. 
We set 
G(V, E, k) =clef ( Lex, C, i sac,  ~) 
and 
a(V, E, k) =def s Vl ' ' "  V l " ' "  y IV \ [  " " " VlV ~ 
IEI IEI 
For an example, cf. Fig. 4 which shows a dependency 
tree for the instance of the vertex cover problem from 
Fig. 2. The two dependencies Ul and u2 represent the 
complement of the vertex cover. 
It is easily seen 3 that \[\[(G(V,E,k),a(V,E,k))\[\[ is 
polynomial in \[\[V\[\[, \[\[E\[\[ and k. From \[El _> k and Def- 
inition 1 it follows that H(V,E,k)\[I >_ \[IE\]\[ _> \]\[k\[\[ _> k. 
3The construction requires 2 ? \[V\[ + \[El + 3 word classes, 
IV\[ + 1 terminals in at most \[El + 2 readings each. S defines 
IV\[ + k ? IE\[ - k valencies, Ui defines \[E\[ - 1 valencies. The 
length of a is IV\[ ? \[E\[ + 1. 
341 
word class valencies 
Vvi ? V Vi isac R { } 
Vvi ? V Ui isac U {( - ,  rz, V/),--. , ( - ,  rlEl_l, V/)} 
Vei E E Hi {} 
S {(-, u,, u ) , . . . ,  ( - ,  u,v,_,, v) ,  
( - ,  hi, H i ) , - ' - ,  ( - ,  hie I, HIEI), 
( - ,  n,  R), ? ? ? , ( - ,  r(k-,)l~l, R)}  
I order I 
={ } word \] 
={ } "i 
-{}  
-{}  
word classes 
{U.~}U{Hjl3vm,v. ? v :  
ej = (vm, v,,)^ 
s {s} 
Table 2: Word classes and lexicon to encode vertex cover problem 
$ 
aaaa  bbbb 
Figure 4: Encoding a solution to the vertex cover prob- 
lem from Fig. 2. 
Hence, the construction of (G(V, E, k), a(V, E, k)) can 
be done in worst-case time polynomial in II(V,E,k)ll. 
We next show the equivalence of the two problems. 
Assume (V, E, k) ? VC: Then there exists a subset 
V' C_ V and a function f : E --+ V' such that IV'l <_ k 
and V(vm,v,~) ? E : f((vm,vn)) ? {(vm,Vn)}. A 
dependency tree for a(V, E, k) is constructed by: 
1. For every ei ? E, one word f(ei) is assigned class 
Hi and governed by s in valency hi. 
2. For each vi ? V \ V', IEI - I words vi are assigned 
class R and governed by the remaining copy of vi 
in reading Ui through valencies rl to rlEl_l. 
3. The vi in reading Ui are governed by s through the 
valencies uj (j ? \[1, IWl - k\]). 
4. (k - 1) ? IEI words remain in a. These receive 
reading R and are governed by s in valencies r~ (j ? 
\[1, (k - 1)IEI\]). 
The dependency tree rooted in s covers the whole in- 
put a(V, E, k). Since G(V, E, k) does not give any fur- 
ther restrictions this implies a( V, E, k) ? L ( G ( V, E, k ) ) 
and, thus, (G(V, E, k), a(V, E, k)) ? DGR. 
Conversely assume (G(V, E, k), a(V, E, k)) ? DGR: 
Then a(V, E, k) ? L(G(V, E, k)) holds, i.e., there ex- 
ists a dependency tree that covers the whole input. Since 
s cannot be governed in any valency, it follows that s 
must be the root. The instance s of S has IEI valencies 
of class H, (k -  1) * \[E I valencies of class R, and IWl - k 
valencies of class U, whose instances in turn have IE I -  1 
valencies of class R. This sums up to IEI * IVl potential 
dependents, which is the number of terminals in a be- 
sides s. Thus, all valencies are actually filled. We define 
a subset Vo C_ V by Vo =def {V E VI3i e \[1, IYl - k\] 
8.mod(ul) = v}. I.e., 
(1) IVol = IV I -  k 
The dependents ofs in valencies hl are from the set V' 
Vo. We define a function f : E --+ V \ Vo by f(ei) =def 
s.mod(hi) for all ei E E. By construction f(ei) is an 
end point of edge ei, i.e. 
(2) V(v,,,,v,d e E:  f((v,.,,,v,4,) e {v,,,,v,.,} 
We define a subset V' C V by V' =def {f(e)le ? E}. 
Thus 
(3) Ve ? E :  f(e) ? V' 
By construction of V' and by (1) it follows 
(4) IV'l < IY l -  IVol = k 
From (2), (3), and (4) we induce (V, E, k) ? VC. ? 
Theorem 3
The DG recognition problem is in the complexity class 
Af l)C. \[\] 
The Af:P-completeness of the DG recognition problem 
follows directly from lemmata 1and 2. ? 
5 Conc lus ion  
We have shown that current DG theorizing exhibits a 
feature not contained in previous formal studies of DG, 
namely the independent specification of dominance and 
precedence constraints. This feature leads to a A/'7% 
complete recognition problem. The necessity of this ex- 
tension approved by most current DGs relates to the fact 
that DG must directly characterize dependencies which 
in PSG are captured by a projective structure and addi- 
tional processes such as coindexing or structure sharing 
(most easily seen in treatments of so-called unbounded 
342 
dependencies). The dissociation of tree structure and 
linear order, as we have done in Section 3, nevertheless 
seems to be a promising approach for PSG as well; see a 
very similar proposal for HPSG (Reape, 1989). 
The .N'79-completeness result also holds for the dis- 
continuous DG presented in Section 3. This DG can 
characterize at least some context-sensitive languages 
such as anbnc n, i.e., the increase in complexity corre- 
sponds to an increase of generative capacity. We conjec- 
ture that, provided a proper formalization ofthe other DG 
versions presented in Section 2, their .A/P-completeness 
can be similarly shown. With respect to parser design, 
this result implies that the well known polynomial time 
complexity of chart- or tabular-based parsing techniques 
cannot be achieved for these DG formalisms in gen- 
eral. This is the reason why the PARSETALK text under- 
standing system (Neuhaus & Hahn, 1996) utilizes pecial 
heuristics in a heterogeneous chart- and backtracking- 
based parsing approach. 
References 
Barton, Jr., G. E. (1985). On the complexity of ID/LP 
parsing. Computational Linguistics, 11(4):205- 
218. 
Br6ker, N. (1997). Eine Dependenzgrammatik 
zur Kopplung heterogener Wissenssysteme auf 
modaUogischer Basis, (Dissertation). Freiburg, 
DE: Philosophische Fakult~it, Albert-Ludwigs- 
Universit~it. 
Earley, J. (1970). An efficient context-free parsing algo- 
rithm. Communications of the ACM, 13(2):94-102. 
Gaifman, H. (1965). Dependency s stems and phrase- 
structure systems. Information & Control, 8:304-- 
337. 
Garey, M. R. & D. S. Johnson (1983). Computers 
and Intractability: A Guide to the Theory of NP- 
completeness (2. ed.). New York, NY: Freeman. 
Haegeman, L. (1994). Introduction to Government and 
Binding. Oxford, UK: Basil Blackwell. 
Hellwig, E (1988). Chart parsing according to the slot 
and filler principle. In Proc. of the 12 th Int. Conf. 
on Computational Linguistics. Budapest, HU, 22- 
27Aug 1988, Vol. 1, pp. 242-244. 
Hepple, M. (1990). Word order and obliqueness in cat- 
egorial grammar. In G. Barry & G. Morill (Eds.), 
Studies in categorial grammar, pp. 47--64. Edin- 
burgh, UK: Edinburgh University Press. 
Huck, G. (1988). Phrasal verbs and the categories of 
postponement. In R. Oehrle, E. Bach & D. Wheeler 
(Eds.), Categorial Grammars and Natural Lan- 
guage Structures, pp. 249-263. Studies in Linguis- 
tics and Philosophy 32. Dordrecht, NL: D. Reidel. 
Hudson, R. (1990). English Word Grammar. Oxford, 
UK: Basil Blackwell. 
Lombardo, V. & L. Lesmo (1996). An earley-type r cog- 
nizer for dependency grammar. In Proc. of the 16 th 
Int. Conf. on Computational Linguistics. Copen- 
hagen, DK, 5-9 Aug 1996, Vol. 2, pp. 723-728. 
McCord, M. (1990). Slot grammar: A system for simpler 
construction of practical natural language gram- 
mars. In R. Studer (Ed.), Natural Language and 
Logic, pp. 118-145. Berlin, Heidelberg: Springer. 
Mer ~uk, I. (1988). Dependency S ntax: Theory and 
Practice. New York, NY: SUNY State University 
Press of New York. 
Mel' 6uk, I. & N. Pertsov (1987). Surface Syntax of En- 
glish: A Formal Model within the MTT Framework. 
Amsterdam, NL: John Benjamins. 
Neuhaus, R & U. Hahn (1996). Restricted parallelism in 
object-oriented l xical parsing. In Proc. of the 16 th 
Int. Conf. on Computational Linguistics. Copen- 
hagen, DK, 5-9 Aug 1996, pp. 502-507. 
Pollard, C. & I. Sag (1994). Head-Driven Phrase Struc- 
ture Grammar. Chicago, IL: University of Chicago 
Press. 
Rambow, O. & A. Joshi (1994). A formal ook at DGs 
and PSGs, with consideration of word-order phe- 
nomena. In L. Wanner (Ed.), Current Issues in 
Meaning-Text-Theory. London: Pinter. 
Reape, M. (I 989). A logical treatment ofsemi-free word 
order and discontinuous constituents. In Proc. of the 
27 th Annual Meeting of the Association for Compu- 
tational Linguistics. Vancouver, BC, 1989, pp. 103- 
110. 
Starosta, S. (1988). The Case for Lexicase. London: 
Pinter. 
Starosta, S. (1992). Lexicase revisited. Department of 
Linguistics, University of Hawaii. 
Tesni~re, L. ((1969) 1959). Elements de Syntaxe Struc- 
turale (2. ed.). Paris, FR: Klincksieck. 
343 
