Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 496?504,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Multi-Class Confidence Weighted Algorithms
Koby Crammer
?
?
Department of Computer
and Information Science
University of Pennsylvania
Philadelphia, PA 19104
{crammer,kulesza}@cis.upenn.edu
Mark Dredze
?
Alex Kulesza
?
?
Human Language Technology
Center of Excellence
Johns Hopkins University
Baltimore, MD 21211
mdredze@cs.jhu.edu
Abstract
The recently introduced online
confidence-weighted (CW) learning
algorithm for binary classification per-
forms well on many binary NLP tasks.
However, for multi-class problems CW
learning updates and inference cannot
be computed analytically or solved as
convex optimization problems as they are
in the binary case. We derive learning
algorithms for the multi-class CW setting
and provide extensive evaluation using
nine NLP datasets, including three derived
from the recently released New York
Times corpus. Our best algorithm out-
performs state-of-the-art online and batch
methods on eight of the nine tasks. We
also show that the confidence information
maintained during learning yields useful
probabilistic information at test time.
1 Introduction
Online learning algorithms such as the Perceptron
process one example at a time, yielding simple and
fast updates. They generally make few statisti-
cal assumptions about the data and are often used
for natural language problems, where high dimen-
sional feature representations, e.g., bags-of-words,
demand efficiency. Most online algorithms, how-
ever, do not take into account the unique properties
of such data, where many features are extremely
rare and a few are very frequent.
Dredze, Crammer and Pereira (Dredze et al,
2008; Crammer et al, 2008) recently introduced
confidence weighted (CW) online learning for bi-
nary prediction problems. CW learning explicitly
models classifier weight uncertainty using a multi-
variate Gaussian distribution over weight vectors.
The learner makes online updates based on its con-
fidence in the current parameters, making larger
changes in the weights of infrequently observed
features. Empirical evaluation has demonstrated
the advantages of this approach for a number of bi-
nary natural language processing (NLP) problems.
In this work, we develop and test multi-class
confidence weighted online learning algorithms.
For binary problems, the update rule is a sim-
ple convex optimization problem and inference
is analytically computable. However, neither is
true in the multi-class setting. We discuss sev-
eral efficient online learning updates. These up-
date rules can involve one, some, or all of the
competing (incorrect) labels. We then perform an
extensive evaluation of our algorithms using nine
multi-class NLP classification problems, includ-
ing three derived from the recently released New
York Times corpus (Sandhaus, 2008). To the best
of our knowledge, this is the first learning evalua-
tion on these data. Our best algorithm outperforms
state-of-the-art online algorithms and batch algo-
rithms on eight of the nine datasets.
Surprisingly, we find that a simple algorithm in
which updates consider only a single competing
label often performs as well as or better than multi-
constraint variants if it makes multiple passes over
the data. This is especially promising for large
datasets, where the efficiency of the update can
be important. In the true online setting, where
only one iteration is possible, multi-constraint al-
gorithms yield better performance.
Finally, we demonstrate that the label distribu-
tions induced by the Gaussian parameter distribu-
tions resulting from our methods have interesting
properties, such as higher entropy, compared to
those from maximum entropy models. Improved
label distributions may be useful in a variety of
learning settings.
2 Problem Setting
In the multi-class setting, instances from an input
space X take labels from a finite set Y , |Y| = K.
496
We use a standard approach (Collins, 2002) for
generalizing binary classification and assume a
feature function f(x, y) ? R
d
mapping instances
x ? X and labels y ? Y into a common space.
We work in the online framework, where learn-
ing is performed in rounds. On each round the
learner receives an input x
i
, makes a prediction y?
i
according to its current rule, and then learns the
true label y
i
. The learner uses the new example
(x
i
, y
i
) to modify its prediction rule. Its goal is to
minimize the total number of rounds with incor-
rect predictions, |{i : y
i
6= y?
i
}|.
In this work we focus on linear models parame-
terized by weightsw and utilizing prediction func-
tions of the form h
w
(x) = arg max
z
w ? f(x, z).
Note that since we can choose f(x, y) to be the
vectorized Cartesian product of an input feature
function g(x) and y, this setup generalizes the use
of unique weight vectors for each element of Y .
3 Confidence Weighted Learning
Dredze, Crammer, and Pereira (2008) introduced
online confidence weighted (CW) learning for bi-
nary classification, where X = R
d
and Y =
{?1}. Rather than using a single parameter vec-
tor w, CW maintains a distribution over param-
eters N (?,?), where N (?,?) the multivariate
normal distribution with mean ? ? R
d
and co-
variance matrix ? ? R
d?d
. Given an input in-
stance x, a Gibbs classifier draws a weight vector
w from the distribution and then makes a predic-
tion according to the sign of w ? x.
This prediction rule is robust if the example
is classified correctly with high-probability, that
is, for some confidence parameter .5 ? ? < 1,
Pr
w
[y (w ? x) ? 0] ? ?. To learn a binary CW
classifier in the online framework, the robustness
property is enforced at each iteration while mak-
ing a minimal update to the parameter distribution
in the KL sense:
(?
i+1
,?
i+1
) =
arg min
?,?
D
KL
(N (?,?) ?N (?
i
,?
i
))
s.t. Pr
w
[y
i
(w ? x
i
) ? 0] ? ? (1)
Dredze et al (2008) showed that this optimization
can be solved in closed form, yielding the updates
?
i+1
= ?
i
+ ?
i
?
i
x
i
(2)
?
i+1
=
(
?
?1
i
+ ?
i
x
i
x
T
i
)
?1
(3)
for appropriate ?
i
and ?
i
.
For prediction, they use the Bayesian rule
y? = arg max
z?{?1}
Pr
w?N (?,?)
[z (x ?w) ? 0] ,
which for binary labels is equivalent to using the
mean parameters directly, y? = sign (? ? x).
4 Multi-Class Confidence Weighted
Learning
As in the binary case, we maintain a distribution
over weight vectors w ? N (?,?). Given an in-
put instance x, a Gibbs classifier draws a weight
vector w ? N (?,?) and then predicts the label
with the maximal score, arg max
z
(w ? f(x, z)).
As in the binary case, we use this prediction rule
to define a robustness condition and corresponding
learning updates.
We generalize the robustness condition used in
Crammer et al (2008). Following the update on
round i, we require that the ith instance is correctly
labeled with probability at least ? < 1. Among the
distributions that satisfy this condition, we choose
the one that has the minimal KL distance from the
current distribution. This yields the update
(?
i+1
,?
i+1
) = (4)
arg min
?,?
D
KL
(N (?,?) ?N (?
i
,?
i
))
s.t. Pr [y
i
|x
i
,?,?] ? ? ,
where
Pr [y |x,?,?] =
Pr
w?N (?,?)
[
y = arg max
z?Y
(w ? f(x, z))
]
.
Due to the max operator in the constraint, this op-
timization is not convex when K > 2, and it does
not permit a closed form solution. We therefore
develop approximations that can be solved effi-
ciently. We define the following set of events for a
general input x:
A
r,s
(x)
def
= {w : w ? f(x, r) ? w ? f(x, s)}
B
r
(x)
def
= {w : w ? f(x, r) ? w ? f(x, s) ?s}
=
?
s 6=r
A
r,s
(x)
We assume the probability that w ? f(x, r) =
w ? f(x, s) for some s 6= r is zero, which
497
holds for non-trivial distribution parameters and
feature vectors. We rewrite the prediction y? =
arg max
r
Pr [B
r
(x)], and the constraint from
Eq. (4) becomes
Pr [B
y
i
(x)] ? ? . (5)
We focus now on approximating the event B
y
i
(x)
in terms of events A
y
i
,r
. We rely on the fact that
the level sets of Pr [A
y
i
,r
] are convex in ? and
?. This leads to convex constraints of the form
Pr [A
y
i
,r
] ? ?.
Outer Bound: Since B
r
(x) ? A
r,s
(x), it holds
trivially that Pr [B
y
i
(x)] ? ? ? Pr [A
y
i
,r
] ?
?,?r 6= y
i
. Thus we can replace the constraint
Pr [B
y
i
(x)] ? ? with Pr [A
y
i
,r
] ? ? to achieve an
outer bound. We can simultaneously apply all of
the pairwise constraints to achieve a tighter bound:
Pr [A
y
i
,r
] ? ? ?r 6= y
i
This yields a convex approximation to Eq. (4) that
may improve the objective value at the cost of
violating the constraint. In the context of learn-
ing, this means that the new parameter distribu-
tion will be close to the previous one, but may not
achieve the desired confidence on the current ex-
ample. This makes the updates more conservative.
Inner Bound: We can also consider an inner
bound. Note that B
y
i
(x)
c
= (?
r
A
y
i
,r
(x))
c
=
?
r
A
y
i
,r
(x)
c
, thus the constraint Pr [B
y
i
(x)] ? ?
is equivalent to
Pr [?
r
A
y
i
,r
(x)
c
] ? 1? ? ,
and by the union bound, this follows whenever
?
r
Pr [A
y
i
,r
(x)
c
] ? 1? ? .
We can achieve this by choosing non-negative
?
r
? 0,
?
r
?
r
= 1, and constraining
Pr [A
y
i
,r
(x)] ? 1? (1? ?) ?
r
for r 6= y
i
.
This formulation yields an inner bound on the
original constraint, guaranteeing its satisfaction
while possibly increasing the objective. In the
context of learning, this is a more aggressive up-
date, ensuring that the current example is robustly
classified even if doing so requires a larger change
to the parameter distribution.
Algorithm 1 Multi-Class CW Online Algorithm
Input: Confidence parameter ?
Feature function f(x, y) ? R
d
Initialize: ?
1
= 0 , ?
1
= I
for i = 1, 2 . . . do
Receive x
i
? X
Predict ranking of labels y?
1
, y?
2
, . . .
Receive y
i
? Y
Set ?
i+1
,?
i+1
by approximately solving
Eq. (4) using one of the following:
Single-constraint update (Sec. 5.1)
Exact many-constraint update (Sec. 5.2)
Seq. many-constraint approx. (Sec. 5.2)
Parallel many-constraint approx. (Sec. 5.2)
end for
Output: Final ? and ?
Discussion: The two approximations are quite
similar in form. Both replace the constraint
Pr [B
y
i
(x)] ? ? with one or more constraints of
the form
Pr [A
y
i
,r
(x)] ? ?
r
. (6)
To achieve an outer bound we choose ?
r
= ? for
any set of r 6= y
i
. To achieve an inner bound we
use all K ? 1 possible constraints, setting ?
r
=
1 ? (1? ?) ?
r
for suitable ?
r
. A simple choice is
?
r
= 1/(K ? 1).
In practice, ? is a learning parameter whose
value will be optimized for each task. In this case,
the outer bound (when all constraints are included)
and inner bound (when ?
r
= 1/(K ? 1)) can be
seen as equivalent, since for any fixed value of
?
(in)
for the inner bound we can choose
?
(out)
= 1?
1? ?
(in)
K ? 1
,
for the outer bound and the resulting ?
r
will be
equal. By optimizing ? we automatically tune the
approximation to achieve the best compromise be-
tween the inner and outer bounds. In the follow-
ing, we will therefore assume ?
r
= ?.
5 Online Updates
Our algorithms are online and process examples
one at a time. Pseudo-code for our approach is
given in algorithm 1. We approximate the pre-
diction step by ranking each label y according
to the score given by the mean weight vector,
? ? f(x
i
, y). Although this approach is Bayes op-
timal for binary problems (Dredze et al, 2008),
498
it is an approximation in general. We note that
more accurate inference can be performed in the
multi-class case by sampling weight vectors from
the distribution N (?,?) or selecting labels sen-
sitive to the variance of prediction; however, in
our experiments this did not improve performance
and required significantly more computation. We
therefore proceed with this simple and effective
approximation.
The update rule is given by an approximation
of the type described in Sec. 4. All that remains
is to choose the constraint set and solve the opti-
mization efficiently. We discuss several schemes
for minimizing KL divergence subject to one or
more constraints of the form Pr [A
y
i
,r
(x)] ? ?.
We start with a single constraint.
5.1 Single-Constraint Updates
The simplest approach is to select the single con-
straint Pr [A
y
i
,r
(x)] ? ? corresponding to the
highest-ranking label r 6= y
i
. This ensures that,
following the update, the true label is more likely
to be predicted than the label that was its closest
competitor. We refer to this as the k = 1 update.
Whenever we have only a single constraint, we
can reduce the optimization to one of the closed-
form CW updates used for binary classification.
Several have been proposed, based on linear ap-
proximations (Dredze et al, 2008) and exact for-
mulations (Crammer et al, 2008). For simplicity,
we use the Variance method from Dredze et al
(2008), which did well in our initial evaluations.
This method leads to the following update rules.
Note that in practice ? is projected to a diagonal
matrix as part of the update; this is necessary due
to the large number of features that we use.
?
i+1
= ?
i
+ ?
i
?
i
g
i,y
i
,r
(7)
?
i+1
=
(
?
?1
i
+ 2?
i
?g
i,y
i
,r
g
>
i,y
i
,r
)
?1
(8)
g
i,y
i
,r
= f(x
i
, y
i
)? f (x
i
, r) ? = ?
?1
(?)
The scale ?
i
is given by max(?
i
, 0), where ?
i
is
equal to
?(1 + 2?m
i
) +
?
(1 + 2?m
i
)
2
? 8?(m
i
? ?v
i
)
4?v
i
and
m
i
= ?
i
? g
i,y
i
,r
v
i
= g
>
i,y
i
,r
?
i
g
i,y
i
,r
.
These rules derive directly from Dredze et al
(2008) or Figure 1 in Crammer et al (2008); we
simply substitute y
i
= 1 and x
i
= g
i,y
i
,r
.
5.2 Many-Constraints Updates
A more accurate approximation can be obtained
by selecting multiple constraints. Analogously, we
choose the k ? K?1 constraints corresponding to
the labels r
1
, . . . , r
k
6= y
i
that achieve the highest
predicted ranks. The resulting optimization is con-
vex and can be solved by a standard Hildreth-like
algorithm (Censor & Zenios, 1997). We refer to
this update as Exact. However, Exact is expen-
sive to compute, and tends to over-fit in practice
(Sec. 6.2). We propose several approximate alter-
natives.
Sequential Update: The Hildreth algorithm it-
erates over the constraints, updating with respect
to each until convergence is reached. We approxi-
mate this solution by making only a single pass:
? Set ?
i,0
= ?
i
and ?
i,0
= ?
i
.
? For j = 1, . . . , k, set (?
i,j
,?
i,j
) to the solu-
tion of the following optimization:
min
?,?
D
KL
(
N (?,?) ?N
(
?
i,j?1
,?
i,j?1
))
s.t. Pr
[
A
y
i
,r
j
(x)
]
? ?
? Set ?
i+1
= ?
i,k
and ?
i+1
= ?
i,k
.
Parallel Update: As an alternative to the Hil-
dreth algorithm, we consider the simultaneous al-
gorithm of Iusem and Pierro (1987), which finds
an exact solution by iterating over the constraints
in parallel. As above, we approximate the exact
solution by performing only one iteration. The
process is as follows.
? For j = 1, . . . , k, set (?
i,j
,?
i,j
) to the solu-
tion of the following optimization:
min
?,?
D
KL
(N (?,?) ?N (?
i
,?
i
))
s.t. Pr
[
A
y
i
,r
j
(x)
]
? ?
? Let ? be a vector, ?
j
?0 ,
?
j
?
j
=1.
? Set ?
i+1
=
?
j
?
j
?
i,j
, ?
?1
i+1
=
?
j
?
j
?
?1
i,j
.
In practice we set ?
j
= 1/k for all j.
6 Experiments
6.1 Datasets
Following the approach of Dredze et al (2008),
we evaluate using five natural language classifica-
tion tasks over nine datasets that vary in difficulty,
size, and label/feature counts. See Table 1 for an
overview. Brief descriptions follow.
499
Task Instances Features Labels Bal.
20 News 18,828 252,115 20 Y
Amazon 7 13,580 686,724 7 Y
Amazon 3 7,000 494,481 3 Y
Enron A 3,000 13,559 10 N
Enron B 3,000 18,065 10 N
NYTD 10,000 108,671 26 N
NYTO 10,000 108,671 34 N
NYTS 10,000 114,316 20 N
Reuters 4,000 23,699 4 N
Table 1: A summary of the nine datasets, includ-
ing the number of instances, features, and labels,
and whether the numbers of examples in each class
are balanced.
Amazon Amazon product reviews. Using the
data of Dredze et al (2008), we created two do-
main classification datasets from seven product
types (apparel, books, dvds, electronics, kitchen,
music, video). Amazon 7 includes all seven prod-
uct types and Amazon 3 includes books, dvds, and
music. Feature extraction follows Blitzer et al
(2007) (bigram features and counts).
20 Newsgroups Approximately 20,000 news-
group messages, partitioned across 20 different
newsgroups.
1
This dataset is a popular choice for
binary and multi-class text classification as well as
unsupervised clustering. We represent each mes-
sage as a binary bag-of-words.
Enron Automatic sorting of emails into fold-
ers.
2
We selected two users with many email
folders and messages: farmer-d (Enron A) and
kaminski-v (Enron B). We used the ten largest
folders for each user, excluding non-archival email
folders such as ?inbox,? ?deleted items,? and ?dis-
cussion threads.? Emails were represented as bi-
nary bags-of-words with stop-words removed.
NY Times To the best of our knowledge we are
the first to evaluate machine learning methods on
the New York Times corpus. The corpus con-
tains 1.8 million articles that appeared from 1987
to 2007 (Sandhaus, 2008). In addition to being
one of the largest collections of raw news text,
it is possibly the largest collection of publicly re-
leased annotated news text, and therefore an ideal
corpus for large scale NLP tasks. Among other
annotations, each article is labeled with the desk
that produced the story (Financial, Sports, etc.)
(NYTD), the online section to which the article was
1
http://people.csail.mit.edu/jrennie/20Newsgroups/
2
http://www.cs.cmu.edu/?enron/
Task Sequential Parallel Exact
20 News 92.16 91.41 88.08
Amazon 7 77.98 78.35 77.92
Amazon 3 93.54 93.81 93.00
Enron A 82.40 81.30 77.07
Enron B 71.80 72.13 68.00
NYTD 83.43 81.43 80.92
NYTO 82.02 78.67 80.60
NYTS 52.96 54.78 51.62
Reuters 93.60 93.97 93.47
Table 2: A comparison of k = ? updates. While
the two approximations (sequential and parallel)
are roughly the same, the exact solution over-fits.
posted (NYTO), and the section in which the arti-
cle was printed (NYTS). Articles were represented
as bags-of-words with feature counts (stop-words
removed).
Reuters Over 800,000 manually categorized
newswire stories (RCV1-v2/ LYRL2004). Each
article contains one or more labels describing its
general topic, industry, and region. We performed
topic classification with the four general topics:
corporate, economic, government, and markets.
Details on document preparation and feature ex-
traction are given by Lewis et al (2004).
6.2 Evaluations
We first set out to compare the three update ap-
proaches proposed in Sec. 5.2: an exact solution
and two approximations (sequential and parallel).
Results (Table 2) show that the two approxima-
tions perform similarly. For every experiment the
CW parameter ? and the number of iterations (up
to 10) were optimized using a single randomized
iteration. However, sequential converges faster,
needing an average of 4.33 iterations compared to
7.56 for parallel across all datasets. Therefore, we
select sequential for our subsequent experiments.
The exact method performs poorly, displaying
the lowest performance on almost every dataset.
This is unsurprising given similar results for bi-
nary CW learning Dredze et al (2008), where ex-
act updates were shown to over-fit but converged
after a single iteration of training. Similarly, our
exact implementation converges after an average
of 1.25 iterations, much faster than either of the
approximations. However, this rapid convergence
appears to come at the expense of accuracy. Fig. 1
shows the accuracy on Amazon 7 test data after
each training iteration. While both sequential and
parallel improve with several iterations, exact de-
500
1 2 3 4 5Training Iterations
77.0
77.5
78.0
78.5
Tes
t Ac
cura
cy
K=1Sequential K=5Sequential K=AllParallel K=AllExact K=All
Figure 1: Accuracy on test data after each iteration
on the Amazon 7 dataset.
grades after the first iteration, suggesting that it
may over-fit to the training data. The approxima-
tions appear to smooth learning and produce better
performance in the long run.
6.3 Relaxing Many-Constraints
While enforcing many constraints may seem op-
timal, there are advantages to pruning the con-
straints as well. It may be time consuming to en-
force dozens or hundreds of constraints for tasks
with many labels. Structured prediction tasks of-
ten involve exponentially many constraints, mak-
ing pruning mandatory. Furthermore, many real
world datasets, especially in NLP, are noisy, and
enforcing too many constraints can lead to over-
fitting. Therefore, we consider the impact of re-
ducing the constraint set in terms of both reducing
run-time and improving accuracy.
We compared using all constraints (k = ?)
with using 5 constraints (k = 5) for the sequential
update method (Table 3). First, we observe that
k = 5 performs better than k =? on nearly every
dataset: fewer constraints help avoid over-fitting
and once again, simpler is better. Additionally,
k = 5 converges faster than k = ? in an average
of 2.22 iterations compared with 4.33 iterations.
Therefore, reducing the number of constraints im-
proves both speed and accuracy. In comparing
k = 5 with the further reduced k = 1 results, we
observe the latter improves on seven of the nine
methods. This surprising result suggests that CW
learning can perform well even without consid-
ering more than a single constraint per example.
However, k = 1 exceeds the performance of mul-
tiple constraints only through repeated training it-
erations. k = 5 CW learning converges faster ?
2.22 iterations compared with 6.67 for k = 1 ? a
desirable property in many resource restricted set-
tings. (In the true online setting, only a single it-
eration may be possible.) Fig. 1 plots the perfor-
mance of k = 1 and k = 5 CW on test data after
each training iteration. While k = 1 does better
in the long run, it lags behind k = 5 for several
iterations. In fact, after a single training iteration,
k = 5 outperforms k = 1 on eight out of nine
datasets. Thus, there is again a tradeoff between
faster convergence (k = 5) and increased accuracy
(k = 1). While the k = 5 update takes longer per
iteration, the time required for the approximate so-
lutions grows only linearly in the number of con-
straints. The evaluation in Fig. 1 required 3 sec-
onds for the first iteration of k = 1, 10 seconds
for k = 5 and 11 seconds for one iteration of all
7 constraints. These differences are insignificant
compared to the cost of performing multiple itera-
tions over a large dataset. We note that, while both
approximate methods took about the same amount
of time, the exact solution took over 4 minutes for
its first iteration.
Finally, we compare CW methods with sev-
eral baselines in Table 3. Online baselines in-
clude Top-1 Perceptron (Collins, 2002), Top-1
Passive-Aggressive (PA), and k-best PA (Cram-
mer & Singer, 2003; McDonald et al, 2004).
Batch algorithms include Maximum Entropy (de-
fault configuration in McCallum (2002)) and sup-
port vector machines (LibSVM (Chang & Lin,
2001) for one-against-one classification and multi-
class (MC) (Crammer & Singer, 2001)). Classifier
parameters (C for PA/SVM and maxent?s Gaus-
sian prior) and number of iterations (up to 10) for
the online methods were optimized using a sin-
gle randomized iteration. On eight of the nine
datasets, CW improves over all baselines. In gen-
eral, CW provides faster and more accurate multi-
class predictions.
7 Error and Probabilistic Output
Our focus so far has been on accuracy and speed.
However, there are other important considerations
for selecting learning algorithms. Maximum en-
tropy and other probabilistic classification algo-
rithms are sometimes favored for their probabil-
ity scores, which can be useful for integration
with other learning systems. However, practition-
501
PA CW SVM
Task Perceptron K=1 K=5 K=1 K=5 K=? 1 vs. 1 MC Maxent
20 News 81.07 88.59 88.60 ??92.90 ??92.78 ??92.16 85.18 90.33 88.94
Amazon 7 74.93 76.55 76.72 ??78.70 ??78.04 ??77.98 75.11 76.60 76.40
Amazon 3 92.26 92.47 93.29 ?94.01 ??94.29 93.54 92.83 93.60 93.60
Enron A 74.23 79.27 80.77 ??83.83 ?82.23 ?82.40 80.23 82.60 82.80
Enron B 66.30 69.93 68.90 ??73.57 ??72.27 ??71.80 65.97 71.87 69.47
NYTD 80.67 83.12 81.31 ??84.57 ?83.94 83.43 82.95 82.00 83.54
NYTO 78.47 81.93 81.22 ?82.72 ?82.55 82.02 82.13 81.01 82.53
NYTS 50.80 56.19 55.04 54.67 54.26 52.96 55.81 56.74 53.82
Reuters 92.10 93.12 93.30 93.60 93.67 93.60 92.97 93.32 93.40
Table 3: A comparison of CW learning (k = 1, 5,? with sequential updates) with several baseline
algorithms. CW learning achieves the best performance eight out of nine times. Statistical significance
(McNemar) is measured against all baselines (? indicates 0.05 and ?? 0.001) or against online baselines
(? indicates 0.05 and ?? 0.001).
0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.7528
29
30
31
32
33
entropy
error
MC CWMaxEnt
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90
200400
600800
10001200
Bin lower threshold
Number
 of exam
ples per
 bin
MaxEntMC CW
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90
2
4
68
1012
Bin lower threshold
Test err
or in bin
MaxEntMC CW
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90
0.10.2
0.30.4
0.50.6
0.70.8
Bin lower threshold
Test err
or given
 bin
MaxEntMC CW
Figure 2: First panel: Error versus prediction entropy on Enron B. As CW converges (right to left) error
and entropy are reduced. Second panel: Number of test examples per prediction probability bin. The
red bars correspond to maxent and the blue bars to CW, with increasing numbers of epochs from left
to right. Third panel: The contribution of each bin to the total test error. Fourth panel: Test error
conditioned on prediction probability.
ers have observed that maxent probabilities can
have low entropy and be unreliable for estimating
prediction confidence (Malkin & Bilmes, 2008).
Since CW also produces label probabilities ? and
does so in a conceptually distinct way ? we in-
vestigate in this section some empirical properties
of the label distributions induced by CW?s param-
eter distributions and compare them with those of
maxent.
We trained maxent and CW k = 1 classi-
fiers on the Enron B dataset, optimizing parame-
ters as before (maxent?s Gaussian prior and CW?s
?). We estimated the label distributions from our
CW classifiers after each iteration and on every
test example x by Gibbs sampling weight vec-
tors w ? N (?,?), and for each label y count-
ing the fraction of weight vectors for which y =
arg max
z
w ? f(x, z). Normalizing these counts
yields the label distributions Pr [y|x]. We denote
by y? the predicted label for a given x, and refer to
Pr [y?|x] as the prediction probability.
The leftmost panel of Fig. 2 plots each
method?s prediction error against the nor-
malized entropy of the label distribution
?
(
1
m
?
i
?
z
Pr [z|x
i
] log (Pr [z|x
i
])
)
/ log(K).
Each CW iteration (moving from right to left in
the plot) reduces both error and entropy. From our
maxent results we make the common observation
that maxent distributions have (ironically) low
entropy. In contrast, while CW accuracy exceeds
maxent after its second iteration, normalized
entropy remains high. Higher entropy suggests
a distribution over labels that is less peaked and
potentially more informative than those from
maxent. We found that the average probability
assigned to a correct prediction was 0.75 for
CW versus 0.83 for maxent and for an incorrect
prediction was 0.44 for CW versus 0.56 for
maxent.
Next, we investigate how these probabilities
relate to label accuracy. In the remaining pan-
els, we binned examples according to their pre-
diction probabilities Pr [y?|x] = max
y
Pr [y|x].
The second panel of Fig. 2 shows the numbers
of test examples with Pr [y?|x] ? [?, ? + 0.1) for
? = 0.1, 0.2 . . . 0.9. (Note that since there are 10
502
classes in this problem, we must have Pr [y?|x] ?
0.1.) The red (leftmost) bar corresponds to the
maximum entropy classifier, and the blue bars cor-
respond, from left to right, to CW after each suc-
cessive training epoch.
From the plot we observe that the maxent classi-
fier assigns prediction probability greater than 0.9
to more than 1,200 test examples out of 3,000.
Only 50 examples predicted by maxent fall in the
lowest bin, and the rest of examples are distributed
nearly uniformly across the remaining bins. The
large number of examples with very high predic-
tion probability explains the low entropy observed
for the maximum entropy classifier.
In contrast, the CW classifier shows the oppo-
site behavior after one epoch of training (the left-
most blue bar), assigning low prediction probabil-
ity (less than 0.3) to more than 1,200 examples
and prediction probability of at least 0.9 to only
100 examples. As CW makes additional passes
over the training data, its prediction confidence
increases and shifts toward more peaked distribu-
tions. After seven epochs fewer than 100 examples
have low prediction probability and almost 1,000
have high prediction probability. Nonetheless, we
note that this distribution is still less skewed than
that of the maximum entropy classifier.
Given the frequency of high probability maxent
predictions, it seems likely that many of the high
probability maxent labels will be wrong. This is
demonstrated in the third panel, which shows the
contribution of each bin to the total test error. Each
bar reflects the number of mistakes per bin divided
by the size of the complete test set (3,000). Thus,
the sum of the heights of the corresponding bars
in each bin is proportional to test error. Much of
the error of the maxent classifier comes not only
from the low-probability bins, due to their inac-
curacy, but also from the highest bin, due to its
very high population. In contrast, the CW clas-
sifiers see very little error contribution from the
high-probability bins. As training progresses, we
see again that the CW classifiers move in the direc-
tion of the maxent classifier but remain essentially
unimodal.
Finally, the rightmost panel shows the condi-
tional test error given bin identity, or the fraction
of test examples from each bin where the predic-
tion was incorrect. This is the pointwise ratio be-
tween corresponding values of the previous two
histograms. For both methods, there is a monoton-
ically decreasing trend in error as prediction prob-
ability increases; that is, the higher the value of
the prediction probability, the more likely that the
prediction it provides is correct. As CW is trained,
we see an increase in the conditional test error, yet
the overall error decreases (not shown). This sug-
gests that as CW is trained and its overall accuracy
improves, there are more examples with high pre-
diction probability, and the cost for this is a rela-
tive increase in the conditional test error per bin.
The maxent classifier produces an extremely large
number of test examples with very high prediction
probabilities, which yields relatively high condi-
tional test error. In nearly all cases, the conditional
error values for the CW classifiers are smaller than
the corresponding values for maximum entropy.
These observations suggest that CW assigns prob-
abilities more conservatively than maxent does,
and that the (fewer) high confidence predictions it
makes are of a higher quality. This is a potentially
valuable property, e.g., for system combination.
8 Conclusion
We have proposed a series of approximations for
multi-class confidence weighted learning, where
the simple analytical solutions of binary CW
learning do not apply. Our best CW method out-
performs online and batch baselines on eight of
nine NLP tasks, and is highly scalable due to the
use of a single optimization constraint. Alterna-
tively, our multi-constraint algorithms provide im-
proved performance for systems that can afford
only a single pass through the training data, as in
the true online setting. This result stands in con-
trast to previously observed behaviors in non-CW
settings (McDonald et al, 2004). Additionally, we
found improvements in both label entropy and ac-
curacy as compared to a maximum entropy clas-
sifier. We plan to extend these ideas to structured
problems with exponentially many labels and de-
velop methods that efficiently model label correla-
tions. An implementation of CW multi-class algo-
rithms is available upon request from the authors.
References
Blitzer, J., Dredze, M., & Pereira, F. (2007).
Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment
classification. Association for Computational
Linguistics (ACL).
503
Censor, Y., & Zenios, S. (1997). Parallel opti-
mization: Theory, algorithms, and applications.
Oxford University Press, New York, NY, USA.
Chang, C.-C., & Lin, C.-J. (2001). LIBSVM: a
library for support vector machines. Software
available at http://www.csie.ntu.edu.
tw/
?
cjlin/libsvm.
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models: Theory and ex-
periments with perceptron algorithms. Empir-
ical Methods in Natural Language Processing
(EMNLP).
Crammer, K., Dredze, M., & Pereira, F. (2008).
Exact confidence-weighted learning. Advances
in Neural Information Processing Systems 22.
Crammer, K., & Singer, Y. (2001). On the al-
gorithmic implementation of multiclass kernel-
based vector machines. Jornal of Machine
Learning Research, 2, 265?292.
Crammer, K., & Singer, Y. (2003). Ultraconserva-
tive online algorithms for multiclass problems.
Jornal of Machine Learning Research (JMLR),
3, 951?991.
Dredze, M., Crammer, K., & Pereira, F. (2008).
Confidence-weighted linear classification. In-
ternational Conference on Machine Learning
(ICML).
Iusem, A., & Pierro, A. D. (1987). A simultaneous
iterative method for computing projections on
polyhedra. SIAM J. Control and Optimization,
25.
Lewis, D. D., Yang, Y., Rose, T. G., & Li, F.
(2004). Rcv1: A new benchmark collection for
text categorization research. Journal of Machine
Learning Research (JMLR), 5, 361?397.
Malkin, J., & Bilmes, J. (2008). Ratio semi-
definite classifiers. IEEE Int. Conf. on Acous-
tics, Speech, and Signal Processing.
McCallum, A. (2002). MALLET: A machine
learning for language toolkit. http://
mallet.cs.umass.edu.
McDonald, R., Crammer, K., & Pereira, F. (2004).
Large margin online learning algorithms for
scalable structured classification. NIPS Work-
shop on Structured Outputs.
Sandhaus, E. (2008). The new york times an-
notated corpus. Linguistic Data Consortium,
Philadelphia.
504
