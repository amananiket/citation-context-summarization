Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 259?267,
Beijing, August 2010
Generating Learner-Like Morphological Errors in Russian
Markus Dickinson
Indiana University
md7@indiana.edu
Abstract
To speed up the process of categorizing
learner errors and obtaining data for lan-
guages which lack error-annotated data,
we describe a linguistically-informed
method for generating learner-like mor-
phological errors, focusing on Russian.
We outline a procedure to select likely er-
rors, relying on guiding stem and suffix
combinations from a segmented lexicon to
match particular error categories and rely-
ing on grammatical information from the
original context.
1 Introduction
Work on detecting grammatical errors in the lan-
guage of non-native speakers covers a range of
errors, but it has largely focused on syntax in
a small number of languages (e.g., Vandeven-
ter Faltin, 2003; Tetreault and Chodorow, 2008).
In more morphologically-rich languages, learn-
ers naturally make many errors in morphology
(Dickinson and Herring, 2008). Yet for many lan-
guages, there is a major bottleneck in system de-
velopment: there are not enough error-annotated
learner corpora which can be mined to discover
the nature of learner errors, let alne enough data
to train or evaluate a system. Our perspective is
that one can speed up the process of determin-
ing the nature of learner errors via semi-automatic
means, by generating plausible errors.
We set out to generate linguistically-plausible
morphological errors for Russian, a language with
rich inflections. Generating learner-like errors has
practical and theoretical benefits. First, there is
the issue of obtaining training data; as Foster and
Andersen (2009) state, ?The ideal situation for a
grammatical error detection system is one where a
large amount of labelled positive and negative ev-
idence is available.? Generated errors can bridge
this gap by creating realistic negative evidence
(see also Rozovskaya and Roth, 2010). As for
evaluation data, generated errors have at least one
advantage over real errors, in that we know pre-
cisely what the correct form is supposed to be, a
problem for real learner data (e.g., Boyd, 2010).
By starting with a coarse error taxonomy, gen-
erating errors can improve categorization. Gener-
ated errors provide data for an expert?e.g., a lan-
guage teacher?to search through, expanding the
taxonomy with new error types or subtypes and/or
deprecating error types which are unlikely. Given
the lack of real learner data, this has the potential
to speed up error categorization and subsequent
system development. Furthermore, error genera-
tion techniques can be re-used, adjusting the er-
rors for different learner levels, first languages,
and so forth.
The error generation process can benefit by us-
ing linguistic properties to mimic learner varia-
tions. This can lead to more realistic errors, a ben-
efit for machine learning (Foster and Andersen,
2009), and can also provide feedback for the lin-
guistic representation used to generate errors by,
e.g., demonstrating under which linguistic condi-
tions certain error types are generated and under
which they are not.
We are specifically interested in generating
Russian morphological errors. To do this, we need
a knowledge base representing Russian morphol-
ogy, allowing us to manipulate linguistic proper-
ties. After outlining the coarse error taxonomy
259
(section 2), we discuss enriching a part-of-speech
(POS) tagger lexicon with segmentation informa-
tion (section 3). We then describe the steps in er-
ror generation (section 4), highlighting decisions
which provide insight for the analysis of learner
language, and show the impact on POS tagging in
section 5.
2 Error taxonomy
Russian is an inflecting language with relatively
free word order, meaning that morphological syn-
tactic properties are often encoded by affixes. In
(1a), for example, the verb ?????? needs a suf-
fix to indicate person and number, and ?? is the
third person singular form.1 By contrast, (1b) il-
lustrates a paradigm error: the suffix ?? is third
singular, but not the correct one. Generating such
a form requires having access to individual mor-
phemes and their linguistic properties.
(1) a. ??????+??
begin-3s
[nachina+et]
b. *??????+??
begin-3s
[nachina+it]
(diff. verb paradigm)
This error is categorized as a suffix error in fig-
ure 1, expanding the taxonomy in Dickinson and
Herring (2008). Stem errors are similarly catego-
rized, with Semantic errors defined with respect
to a particular context (e.g., using a different stem
than required by an activity).
For formation errors (#3), one needs to know
how stems relate. For instance, some verbs
change their form depending on the suffix, as in
(2). In (2c), the stem and suffix are morpholog-
ically compatible, just not a valid combination.
One needs to know that ??? is a variant of ???.
(2) a. ???+??
can-3p
[mog+ut]
b. ???+??
can-3s
[mozh+et]
c. *???+??
can-3p
[mozh+ut] (#3)
(wrong formation)
Using a basic lexicon without such knowledge,
it is hard to tell formation errors apart from lex-
1For examples, we write the Cyrillic form and include a
Roman transliteration (SEV 1362-78) for ease of reading.
0. Correct: The word is well-formed.
1. Stem errors:
(a) Stem spelling error
(b) Semantic error
2. Suffix errors:
(a) Suffix spelling error
(b) Lexicon error:
i. Derivation error: The wrong POS is
used (e.g., a noun as a verb).
ii. Inherency error: The ending is for a
different subclass (e.g., inanimate as
an animate noun).
(c) Paradigm error: The ending is from the
wrong paradigm.
3. Formation errors: The stem does not follow
appropriate spelling/sound change rules.
4. Syntactic errors: The form is correct, but
used in an in appropriate syntactic context
(e.g., nominative case in a dative context)
? Lexicon incompleteness: The form may be
possible, but is not attested.
Figure 1: Error taxonomy
icon incompleteness (see section 4.2.2). If ??-
??? (2c) is generated and is not in the lexicon,
we do not know whether it is misformed or simply
unattested. In this paper, we group together such
cases, since this allows for a simpler and more
quickly-derivable lexicon.
We have added syntactic errors, whereas Dick-
inson and Herring (2008) focused on strictly mor-
phological errors. Learners make syntactic errors
(e.g., Rubinstein, 1995; Rosengrant, 1987), and
when creating errors, a well-formed word may re-
sult. In the future, syntactic errors can be subdi-
vided (Boyd, 2010).
This classification is of possible errors, making
no claim about the actual distribution of learner
errors, and does not delve into issues such as
errors stemming from first language interference
(Rubinstein, 1995). Generating errors from the
possible types allows one to investigate which
types are plausible in which contexts.
260
It should be noted that we focus on inflec-
tional morphology in Russian, meaning that we
focus on suffixes. Prefixes are rarely used in Rus-
sian as inflectional markers; for example, prefixes
mark semantically-relevant properties for verbs of
motion. The choice of prefix is thus related to
the overall word choice, an issue discussed under
Random stem generation in section 4.2.4.
3 Enriching a POS lexicon
To create errors, we need a segmented lexicon
with morphological information, as in (3). Here,
the word ???? (mogu, ?I am able to?) is split into
stem and suffix, with corresponding POS tags.2
(3) a. ???,Vm-----a-p,?,Vmip1s-a-p
b. ???,Vm-----a-p,??,Vmip3s-a-p
c. ???,Vm-----a-p,NULL,Vmis-sma-p
The freely-available POS lexicon from Sharoff
et al (2008), specifically the file for the POS
tagger TnT (Brants, 2000), contains full words
(239,889 unique forms), with frequency informa-
tion. Working with such a rich database, we only
need segmentation, providing a quickly-obtained
lexicon (cf. five years for a German lexicon in
Geyken and Hanneforth, 2005).
In the future, one could switch to a different
tagset, such as that in Hana and Feldman (2010),
which includes reflexivity, animacy, and aspect
features. One could also expand the lexicon, by
adapting algorithms for analyzing unknown words
(e.g., Mikheev, 1997), as suggested by Feldman
and Hana (2010). Still, our lexicon continues the
trend of linking traditional categories used for tag-
ging with deeper analyses (Sharoff et al, 2008;
Hana and Feldman, 2010).3
3.1 Finding segments/morphemes
We use a set of hand-crafted rules to segment
words into morphemes, of the form: if the tag is x
and the word ends with y, make y the suffix. Such
rules are easily and quickly derivable from a text-
book listing of paradigms. For certain exceptional
2POS tags are from the compositional tagset in
Sharoff et al (2008). A full description is at: http://
corpus.leeds.ac.uk/mocky/msd-ru.html.
3This lexicon now includes lemma information, but each
word is not segmented (Erjavec, 2010).
cases, we write word-specific rules. Additionally,
we remove word, tag pairs indicating punctuation
or non-words (PUNC, SENT, -).
One could use a sophisticated method for lem-
matizing words (e.g., Chew et al, 2008; Schone
and Jurafsky, 2001), but we would likely have
to clean the lexicon later; as Feldman and Hana
(2010) point out, it is difficult to automatically
guess the entries for a word, without POS in-
formation. Essentially, we write precise rules to
specify part of the Russian system of suffixes; the
lexicon then provides the stems for free.
We use the lexicon for generating errors, but
it should be compatible with analysis. Thus, we
focus on suffixes for beginning and intermediate
learners. We can easily prune or add to the rule
set later. From an analysis perspective, we need to
specify that certain grammatical properties are in
a tag (see below), as an analyzer is to support the
provision of feedback. Since the rules are freely
available,4 changing these criteria for other pur-
poses is straightforward.
3.1.1 Segmentation rules
We have written 1112 general morphology
rules and 59 rules for the numerals ?one? through
?four,? based on the Nachalo textbooks (Ervin
et al, 1997). A rule is simply a tag, suffix pair.
For example, in (4), Ncmsay (Noun, common,
masculine, singular, accusative, animate [yes])
words should end in either ? (a) or ? (ya).
(4) a. Ncmsay, ?
b. Ncmsay, ?
A program consults this list and segments a
word appropriately, requiring at least one charac-
ter in the stem. In the case where multiple suffixes
match (e.g., ??? (eni) and ? (i) for singular neuter
locative nouns), the longer one is chosen, as it is
unambiguously correct.
We add information in 101 of the 1112
rules. All numerals, for instance, are tagged as
Mc-s (Numeral, cardinal, [unspecified gender],
singular). The tagset in theory includes properties
such as case; they just were not marked (see foot-
note 6, though). Based on the ending, we add all
4http://cl.indiana.edu/
?boltundevelopment/
261
possible analyses. Using an optional output tag,
in (5), Mc-s could be genitive (g), locative (l),
or dative (d) when it ends in ? (i). These rules
increase ambiguity, but are necessary for learner
feedback.
(5) a. Mc-s, ?, Mc-sg
b. Mc-s, ?, Mc-sl
c. Mc-s, ?, Mc-sd
In applying the rules, we generate stem tags, en-
coding properties constant across suffixes. Based
on the word?s tag (e.g., Ncmsay, cf. (4)) a stem
is given a more basic tag (e.g., Ncm--y).
3.2 Lexicon statistics
To be flexible for future use, we have only en-
riched 90% of the words (248,014), removing ev-
ery 10th word. Using the set of 1112 rules results
in a lexicon with 190,450 analyses, where analy-
ses are as in (3). For these 190,450 analyses, there
are 117 suffix forms (e.g., ?, ya) corresponding to
808 suffix analyses (e.g., <?, Ncmsay>). On av-
erage 3.6 suffix tags are observed with each stem-
tag pair, but 22.2 tags are compatible, indicating
incomplete paradigms.
4 Generating errors
4.1 Basic procedure
Taking the morpheme-based lexicon, we generate
errors by randomly combining morphemes into
full forms. Such randomness must be constrained,
taking into account what types of errors are likely
to occur.
The procedure is given in figure 2 and de-
tailed in the following sections. First, we use the
contextually-determined POS tag to restrict the
space of possibilities. Secondly, given that ran-
dom combinations of a stem and a suffix can result
in many unlikely errors, we guide the combina-
tions, using a loose notion of likelihood to ensure
that the errors fall into a reasonable distribution.
After examining the generated errors, one could
restrict the errors even further. Thirdly, we com-
pare the stem and suffix to determine the possible
types of errors. A full form may have several dif-
ferent interpretations, and thus, lastly, we select
the best interpretation(s).
1. Determine POS properties of the word to be
generated (section 4.2.1).
2. Generate a full-form, via guided random
stem and suffix combination (section 4.2.4).
3. Determine possible error analyses for the full
form (section 4.2.2).
4. Select the error type(s) from among multiple
possible interpretations (section 4.2.3).
Figure 2: Error generation procedure
By trying to determine the best error type in
step 4, the generation process can provide in-
sight into error analysis. This is important, given
that suffixes are highly ambiguous; for example,
?? (-oj) has at least 6 different uses for adjec-
tives. Analysis is not simply generation in reverse,
though. Importantly, error generation relies upon
the context POS tag for the intended form, for
the whole process. To morphologically analyze
the corrupted data, one has to POS tag corrupted
forms (see section 5).
4.2 Corruption
We use a corpus of 5 million words automatically
tagged by TnT (Brants, 2000) and freely avail-
able online (Sharoff et al, 2008).5 Because we
want to make linguistically-informed corruptions,
we corrupt only the words we have information
for, identifying the words in the corpus which are
found in the lexicon with the appropriate POS
tag.6 We also select only words which have in-
flectional morphology: nouns, verbs, adjectives,
pronouns, and numerals.7
4.2.1 Determining word properties (step 1)
We use the POS tag to restrict the properties of
a word, regardless of how exactly we corrupt it.
Either the stem and its tag or the suffix and its tag
5See http://corpus.leeds.ac.uk/mocky/.
6We downloaded the TnT lexicon in 2008, but the corpus
in 2009; although no versions are listed on the website, there
are some discrepancies in the tags used (e.g., numeral tags
now have more information). To accommodate, we use a
looser match for determining whether a tag is known, namely
checking whether the tags are compatible. In the future, one
can tweak the rules to match the newer lexicon.
7Adverbs inflect for comparative forms, but we do not
consider them here.
262
can be used as an invariant, to guide the gener-
ated form (section 4.2.4). In (6a), for instance, the
adjective (Af) stem or plural instrumental suffix
(Afp-pif) can be used as the basis for genera-
tion.
(6) a. Original: ?????? (serymi, ?gray?)
7? ???/Af+???/Afp-pif
b. Corrupted: ???+?? (seroj)
The error type is defined in terms of the original
word?s POS tag. For example, when we generate a
correctly-formed word, as in (6b), it is a syntactic
error if it does not match this POS tag.
4.2.2 Determining error types (step 3)
Before discussing word corruption in step 2
(section 4.2.4), we need to discuss how error types
are determined (this section) and how to han-
dle multiple possibilities (section 4.2.3), as these
steps help guide step 2. After creating a corrupted
word, we elucidate all possible interpretations in
step 3 by comparing each suffix analysis with the
stem. If the stem and suffix form a legitimate
word (in the wrong context), it is a syntactic er-
ror. Incompatible features means a derivation or
inherency error, depending upon which features
are incompatible. If the features are compati-
ble, but there is no attested form, it is either a
paradigm error?if we know of a different suffix
with the same grammatical features?or a forma-
tion/incompleteness issue, if not.
This is a crude morphological analyzer (cf.
Dickinson and Herring, 2008), but bases its anal-
yses on what is known about the invariant part of
the original word. If we use ??? (ymi) from (6a)
as an invariant, for instance, we know to treat it as
a plural instrumental adjective ending, regardless
of any other possible interpretations, because that
is how it was used in this context.
4.2.3 Selecting the error type (step 4)
Corrupted forms may have many possible anal-
yses. For example, in (6b), the suffix ?? (oj)
has been randomly attached to the stem ??? (ser).
With the stem fixed as an adjective, the suf-
fix could be a feminine locative adjective (syn-
tactic error), a masculine nominative adjective
(paradigm error), or an instrumental feminine
noun (derivation error). Given what learners are
likely to do, we can use some heuristics to restrict
the set of possible error types.
First, we hypothesize that a correctly-formed
word is more likely a correct form than a mis-
formed word. This means that correct words
and syntactic errors?correctly-formed words in
the wrong context?have priority over other error
types. For (6b), for instance, the syntactic error
outranks the paradigm and derivation errors.
Secondly, we hypothesize that a contextually-
appropriate word, even if misformed, is
more likely the correct interpretation than a
contextually-inappropriate word. When we have
cases where there is: a) a correctly-formed word
not matching the context (a syntactic error), and
b) a malformed word which matches the context
(e.g., a paradigm error), we list both possibilities.
Finally, derivation errors seem less likely than
the others (a point confirmed by native speakers),
giving them lower priority. Given these heuristics,
not only can we rule out error types after gener-
ating new forms, but we can also split the error
generation process into different steps.
4.2.4 Corrupting selected words (step 2)
Using these heuristics, we take a known word
and generate errors based on a series of choices.
For each choice, we randomly generate a num-
ber between 0 and 1 and choose based on a given
threshold. Thresholds should be reset when more
is known about error frequency, and more deci-
sions added as error subtypes are added.
Decision #1: Correct forms The first choice is
whether to corrupt the word or not. Currently, the
threshold is set at 0.5. If we corrupt the word, we
continue on to the next decision.
Decision #2: Syntactic errors We can either
generate a syntactic or a morphological error. On
the assumption that syntactic errors are more com-
mon, we currently set a threshold of 0.7, generat-
ing syntactic errors 70% of the time and morpho-
logical form errors 30% of the time.
To generate a correct form used incorrectly, we
extract the stem from the word and randomly se-
lect a new suffix. We keep selecting a suffix until
263
we obtain a valid form.8 An example is given in
(7): the original (7a) is a plural instrumental ad-
jective, unspecified for gender; in (7b), it is singu-
lar nominative feminine.
(7) a. ??????
gray
Afp-pif
???????
eyes
Ncmpin
.
.
SENT
b. ?????
Afpfsnf
???????
Ncmpin
.
SENT
One might consider ensuring that each error
differs from the original in only one property. Or
one might want to co-vary errors, such that, in
this case, the adjective and noun both change from
instrumental to nominative. While this is eas-
ily accomplished algorithmically, we do not know
whether learners obey these constraints. Generat-
ing errors in a relatively unbounded way can help
pinpoint these types of constraints.
While the form in (7b) is unambiguous, syntac-
tic errors can have more than one possible analy-
sis. In (8), for instance, this word could be cor-
rupted with an -?? (-oj) ending, indicating fem-
inine singular genitive, instrumental, or locative.
We include all possible forms.
(8) ?????
Afpfsg.Afpfsi.Afpfsl
???????
Ncmpin
.
SENT
Likewise, considering the heuristics in sec-
tion 4.2.3, generating a syntactic error may lead
to a form which may be contextually-appropriate.
Consider (9): in (9a), the verb-preposition com-
bination requires an accusative (Ncnsan). By
changing -? to -?, we generate a form which could
be locative case (Ncnsln, type #4) or, since -
? can be an accusative marker, a misformed ac-
cusative with the incorrect paradigm (#2c). We
list both possibilities.
(9) a. . . . ???????
. . . (he) looked
. . . Vmis-sma-p
?
into
Sp-a
????
the sky
Ncnsan
b. . . . ?
. . . Sp-a
????
Ncnsan+2c.Ncnsln+4
Syntactic errors obviously conflate many dif-
ferent error types. The taxonomy for German
8We ensure that we do not generate the original form, so
that the new form is contextually-inappropriate.
from Boyd (2010), for example, includes selec-
tion, agreement, and word order errors. Our syn-
tactic errors are either selection (e.g., wrong case
as object of preposition) or agreement errors (e.g.,
subject-verb disagreement in number). However,
without accurate syntactic information, we cannot
divvy up the error space as precisely. With the
POS information, we can at least sort errors based
on the ways in which they vary from the original
(e.g., incorrect case).
Finally, if no syntactic error can be derived, we
revert to the correct form. This happens when the
lexicon contains only one form for a given stem.
Without changing the stem, we cannot generate a
new form which is verifiably correct.
Decision #3: Morphological errors The next
decision is: should we generate a true morpholog-
ical error or a spelling error? We currently bias
this by setting a 0.9 threshold. The process for
generating morphological errors (0.9) is described
in the next few sections, after which spelling er-
rors (0.1) are described. Surely, 10% is an un-
derestimate of the amount of spelling errors (cf.
Rosengrant, 1987); however, for refining a mor-
phological error taxonomy, biasing towards mor-
phological errors is appropriate.
Decision #4: Invariant morphemes When cre-
ating a context-dependent morphological error,
we have to ask what the unit, or morpheme, is
upon which the full form is dependent. The final
choice is thus to select whether we keep the stem
analysis constant and randomize the suffix or keep
the suffix and randomize the stem. Consider that
the stem is the locus of a word?s semantic proper-
ties, and the (inflectional) suffix reflects syntactic
properties. If we change the stem of a word, we
completely change the semantics (error type #1b).
Changing the suffix, on the other hand, creates a
morphological error with the same basic seman-
tics. We thus currently randomly generate a suffix
90% of the time.
Random suffix generation Randomly attach-
ing a suffix to a fixed stem is the same procedure
used above to generate syntactic errors. Here,
however, we force the form to be incorrect, not
allowing syntactic errors. If attaching a suffix re-
264
sults in a correct form (contextually-appropriate
or not), we re-select a random suffix.
Similarly, the intention is to generate inherency
(#2bii), paradigm (#2c), and formation (#3) errors
(or lexicon incompleteness). All of these seem
to be more likely than derivation (#2bi) errors, as
discussed in section 4.2.3. If we allow any suffix
to combine, we will overwhelmingly find deriva-
tion errors. As pointed out in Dickinson and Her-
ring (2008), such errors can arise when a learner
takes a Russian noun, e.g., ??? (dush, ?shower?)
and attempts to use it as a verb, as in English, e.g.,
???? (dushu) with first person singular morphol-
ogy. In such cases, we have the wrong stem be-
ing used with a contextually-appropriate ending.
Derviation errors are thus best served with ran-
dom stem selection, as described in the next sec-
tion. To rule out derivation errors, we only keep
suffix analyses which have the same major POS as
the stem.
For some stems, particular types of errors are
impossible to generate. a) Inherency errors do not
occur for underspecified stems, as happens with
adjectives. For example, ???- (nov-, ?new?) is an
adjective stem which is compatible with any ad-
jective ending. b) Paradigm errors cannot occur
for words whose suffixes in the lexicon have no al-
ternate forms; for instance, there is only one way
to realize a third singular nominative pronoun. c)
Lexicon incompleteness cannot be posited for a
word with a complete paradigm. These facts show
that the generated error types are biased, depend-
ing upon the POS and the completeness of the lex-
icon.
Random stem generation Keeping the suffix
fixed and randomly selecting a stem ties the gen-
erated form to the syntactic context, but changes
the semantics. Thus, these generated errors are
firstly semantic errors (#1b), featuring stems in-
appropriate for the context, in addition to having
some other morphological error. The fact that,
given a context, we have to generate two errors
lends weight to the idea that these are less likely.
A randomly-generated stem will most likely
be of a different POS class than the suffix, re-
sulting in a derivation error (#2bi). Further, as
with all morphological errors, we restrict the gen-
erated word not to be a correctly-formed word,
and we do not allow the stem or the suffix to be
closed class items. It makes little sense to put
noun inflections on a preposition, for example,
and derivation errors involve open class words.9
Spelling errors For spelling errors, we create an
error simply by randomly inserting, deleting, or
substituting a single character in the word.10 This
will either be a stem (#1a) or a suffix (#2a) error. It
is worth noting that since we know the process of
creating this error, we are able to compartmental-
ize spelling errors from morphological ones. An
error analyzer, however, will have a harder time
distinguishing them.
5 Tagging the corpus
Figure 3 presents the distribution of error types
generated, where Word refers to the number of
words with a particular error type, as opposed to
the count of error type+POS pairs, as each word
can have more than one POS for an error type (cf.
(9b)). For the 780,924 corrupted words, there are
2.67 error type+POS pairs per corrupted word. In-
herency (#2bii) errors in particular have many tags
per word, since the same suffix can have multiple
similar deviations from the original (cf. (8)). Fig-
ure 3 shows that we have generated roughly the
distribution we wanted, based on our initial ideas
of linguisic plausibility.
Type Word POS Type Word POS
1a 19,661 19,661 1b-2bi 11,772 11,772
2a 6,560 6,560 1b-2bii 5,529 5,529
2bii 150,710 749,292 1b-2c 279 279
2c 94,211 94,211 1b-3+ 1,770 1,770
4 524,269 721,051
3+ 83,763 208,208 1b-all 19,350 19,350
Figure 3: Distribution of generated errors
Without an error detection system, it is hard to
gauge the impact of the error generation process.
Although it is not a true evaluation of the error
generation process, as a first step, we test a POS
9Learners often misuse, e.g., prepositions, but these er-
rors do not affect morphology. Future work should examine
the relation between word choice and derivation errors, in-
cluding changes in prefixes.
10One could base spelling errors on known or assumed
phonological confusions (cf. Hovermale and Martin, 2008).
265
tagger against the newly-created data. This helps
test the difficulty of tagging corrupted forms, a
needed step in the process of analyzing learner
language. Note that for providing feedback, it
seems desirable to have the POS tagger match
the tag of the corrupted form. This is a different
goal than developing POS taggers which are ro-
bust to noise (e.g., Bigert et al, 2003), where the
tag should be of the original word.
To POS tag, we use the HMM tagger TnT
(Brants, 2000) with the model from http://
corpus.leeds.ac.uk/mocky/. The re-
sults on the generated data are in figure 4, using
a lenient measure of accuracy: a POS tag is cor-
rect if it matches any of the tags for the hypoth-
esized error types. The best performance is for
uncorrupted known words,11 but notable is that,
out of the box, the tagger obtains 79% precision
on corrupted words when compared to the gener-
ated tags, but is strongly divergent from the orig-
inal (no longer correct) tags. Given that 67%
(524,269780,924 ) of words have a syntactic error?i.e., awell-formed word in the wrong context?this in-
dicates that the tagger is likely relying on the form
in the lexicon more than the context.
Gold Tags
Original Error # words
Corrupted 3.8% 79.0% 780,924
Unchanged:
Known 92.1% 92.1% 965,280
Unknown 81.9% 81.9% 3,484,909
Overall 72.1% 83.4% 5,231,113
Figure 4: POS tagging results, comparing tagger
output to Original tags and Error tags
It is difficult to break down the results for cor-
rupted words by error type, since many words are
ambiguous between several different error types,
and each interpretation may have a different POS
tag. Still, we can say that words which are syn-
tactic errors have the best tagging accuracy. Of
the 524,269 words which may be syntactic er-
rors, TnT matches a tag in 96.1% of cases. Suffix
spelling errors are particularly in need of improve-
11Known here refers to being in the enriched lexicon, as
these are the cases we specificaly did not corrupt.
ment: only 17.3% of these words are correctly
tagged (compared to 62% for stem spelling er-
rors). With an ill-formed suffix, the tagger simply
does not have reliable information. To improve
tagging for morphological errors, one should in-
vestigate which linguistic properties are being in-
correctly tagged (cf. sub-tagging in Hana et al,
2004) and what roles distributional, morphologi-
cal, or lexicon cues should play in tagging learner
language (see also D??az-Negrillo et al, 2010).
6 Conclusions and Outlook
We have developed a general method for gener-
ating learner-like morphological errors, and we
have demonstrated how to do this for Russian.
While many insights are useful for doing error
analysis (including our results for POS tagging
the resulting corpus), generation proceeds from
knowing grammatical properties of the original
word. Generating errors based on linguistic prop-
erties has the potential to speed up the process of
categorizing learner errors, in addition to creating
realistic data for machine learning systems. As a
side effect, we also added segmentation to a wide-
coverage POS lexicon.
There are several directions to pursue. The
most immediate step is to properly evaluate the
quality of generated errors. Based on this analysis,
one can refine the taxonomy of errors, and thereby
generate even more realistic errors in a future iter-
ation. Additionally, building from the initial POS
tagging results, one can work on generally analyz-
ing the morphology of learner language, includ-
ing teasing apart what information a POS tagger
needs to examine and dealing with multiple hy-
potheses (Dickinson and Herring, 2008).
Acknowledgements
I would like to thank Josh Herring, Anna Feld-
man, Jennifer Foster, and three anonymous re-
viewers for useful comments on this work.
266
References
Bigert, Johnny, Ola Knutsson and Jonas Sjo?bergh
(2003). Automatic Evaluation of Robustness
and Degradation in Tagging and Parsing. In
Proceedings of RANLP-2003. Borovets, Bul-
garia, pp. 51?57.
Boyd, Adriane (2010). EAGLE: an Error-
Annotated Corpus of Beginning Learner Ger-
man. In Proceedings of LREC-10. Malta.
Brants, Thorsten (2000). TnT ? A Statistical Part-
of-Speech Tagger. In Proceedings of ANLP-00.
Seattle, WA, pp. 224?231.
Chew, Peter A., Brett W. Bader and Ahmed Abde-
lali (2008). Latent Morpho-Semantic Analysis:
Multilingual Information Retrieval with Char-
acter N-Grams and Mutual Information. In Pro-
ceedings of Coling 2008. Manchester, pp. 129?
136.
D??az-Negrillo, Ana, Detmar Meurers, Salvador
Valera and Holger Wunsch (2010). Towards
interlanguage POS annotation for effective
learner corpora in SLA and FLT. Language Fo-
rum .
Dickinson, Markus and Joshua Herring (2008).
Developing Online ICALL Exercises for Rus-
sian. In The 3rd Workshop on Innovative Use
of NLP for Building Educational Applications.
Columbus, OH, pp. 1?9.
Erjavec, Tomaz? (2010). MULTEXT-East Ver-
sion 4: Multilingual Morphosyntactic Specifi-
cations, Lexicons and Corpora. In Proceedings
of LREC-10. Malta.
Ervin, Gerard L., Sophia Lubensky and Donald K.
Jarvis (1997). Nachalo: When in Russia . . . .
New York: McGraw-Hill.
Feldman, Anna and Jirka Hana (2010). A
Resource-light Approach to Morpho-syntactic
Tagging. Amsterdam: Rodopi.
Foster, Jennifer and Oistein Andersen (2009).
GenERRate: Generating Errors for Use in
Grammatical Error Detection. In The 4th Work-
shop on Innovative Use of NLP for Building Ed-
ucational Applications. Boulder, CO, pp. 82?
90.
Geyken, Alexander and Thomas Hanneforth
(2005). TAGH: A Complete Morphology for
German Based on Weighted Finite State Au-
tomata. In FSMNLP 2005. Springer, pp. 55?66.
Hana, Jirka and Anna Feldman (2010). A Posi-
tional Tagset for Russian. In Proceedings of
LREC-10. Malta.
Hana, Jirka, Anna Feldman and Chris Brew
(2004). A Resource-light Approach to Russian
Morphology: Tagging Russian using Czech
resources. In Proceedings of EMNLP-04.
Barcelona, Spain.
Hovermale, DJ and Scott Martin (2008). Devel-
oping an Annotation Scheme for ELL Spelling
Errors. In Proceedings of MCLC-5 (Midwest
Computational Linguistics Colloquium). East
Lansing, MI.
Mikheev, Andrei (1997). Automatic Rule Induc-
tion for Unknown-Word Guessing. Computa-
tional Linguistics 23(3), 405?423.
Rosengrant, Sandra F. (1987). Error Patterns in
Written Russian. The Modern Language Jour-
nal 71(2), 138?145.
Rozovskaya, Alla and Dan Roth (2010). Training
Paradigms for Correcting Errors in Grammar
and Usage. In Proceedings of HLT-NAACL-10.
Los Angeles, California, pp. 154?162.
Rubinstein, George (1995). On Case Errors Made
in Oral Speech by American Learners of Rus-
sian. Slavic and East European Journal 39(3),
408?429.
Schone, Patrick and Daniel Jurafsky (2001).
Knowledge-Free Induction of Inflectional Mor-
phologies. In Proceedings of NAACL-01. Pitts-
burgh, PA.
Sharoff, Serge, Mikhail Kopotev, Tomaz? Erjavec,
Anna Feldman and Dagmar Divjak (2008). De-
signing and evaluating Russian tagsets. In Pro-
ceedings of LREC-08. Marrakech.
Tetreault, Joel and Martin Chodorow (2008). The
Ups and Downs of Preposition Error Detection
in ESL Writing. In Proceedings of COLING-
08. Manchester.
Vandeventer Faltin, Anne (2003). Syntactic error
diagnosis in the context of computer assisted
language learning. The`se de doctorat, Univer-
site? de Gene`ve, Gene`ve.
267
