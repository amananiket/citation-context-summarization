Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 400?409,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Natural Language Generation with Tree Conditional Random Fields
Wei Lu
1
, Hwee Tou Ng
1,2
, Wee Sun Lee
1,2
1
Singapore-MIT Alliance
2
Department of Computer Science
National University of Singapore
luwei@nus.edu.sg
{nght,leews}@comp.nus.edu.sg
Abstract
This paper presents an effective method
for generating natural language sentences
from their underlying meaning represen-
tations. The method is built on top of
a hybrid tree representation that jointly
encodes both the meaning representation
as well as the natural language in a tree
structure. By using a tree conditional
random field on top of the hybrid tree
representation, we are able to explicitly
model phrase-level dependencies amongst
neighboring natural language phrases and
meaning representation components in a
simple and natural way. We show that
the additional dependencies captured by
the tree conditional random field allows it
to perform better than directly inverting a
previously developed hybrid tree semantic
parser. Furthermore, we demonstrate that
the model performs better than a previ-
ous state-of-the-art natural language gen-
eration model. Experiments are performed
on two benchmark corpora with standard
automatic evaluation metrics.
1 Introduction
One of the ultimate goals in the field of natural lan-
guage processing (NLP) is to enable computers to
converse with humans through human languages.
To achieve this goal, two important issues need
to be studied. First, it is important for comput-
ers to capture the meaning of a natural language
sentence in a meaning representation. Second,
computers should be able to produce a human-
understandable natural language sentence from its
meaning representation. These two tasks are re-
ferred to as semantic parsing and natural language
generation (NLG), respectively.
In this paper, we use corpus-based statistical
methods for constructing a natural language gener-
ation system. Given a set of pairs, where each pair
consists of a natural language (NL) sentence and
its formal meaning representation (MR), a learn-
ing method induces an algorithm that can be used
for performing language generation from other
previously unseen meaning representations.
A crucial question in any natural language pro-
cessing system is the representation used. Mean-
ing representations can be in the form of a tree
structure. In Lu et al (2008), we introduced a
hybrid tree framework together with a probabilis-
tic generative model to tackle semantic parsing,
where tree structured meaning representations are
used. The hybrid tree gives a natural joint tree rep-
resentation of a natural language sentence and its
meaning representation.
A joint generative model for natural language
and its meaning representation, such as that used
in Lu et al (2008) has several advantages over var-
ious previous approaches designed for semantic
parsing. First, unlike most previous approaches,
the generative approach models a simultaneous
generation process for both NL and MR. One el-
egant property of such a joint generative model
is that it allows the modeling of both semantic
parsing and natural language generation within the
same process. Second, the generative process pro-
ceeds as a recursive top-down Markov process in
a way that takes advantage of the tree structure
of the MR. The hybrid tree generative model pro-
posed in Lu et al (2008) was shown to give state-
of-the-art accuracy in semantic parsing on bench-
mark corpora.
While semantic parsing with hybrid trees has
been studied in Lu et al (2008), its inverse task
? NLG with hybrid trees ? has not yet been ex-
plored. We believe that the properties that make
the hybrid trees effective for semantic parsing also
make them effective for NLG. In this paper, we de-
velop systems for the generation task by building
400
on top of the generative model introduced in Lu et
al. (2008) (referred to as the LNLZ08 system).
We first present a baseline model by directly
?inverting? the LNLZ08 system, where an NL sen-
tence is generated word by word. We call this
model the direct inversion model. This model is
unable to model some long range global depen-
dencies over the entire NL sentence to be gener-
ated. To tackle several weaknesses exhibited by
the baseline model, we next introduce an alterna-
tive, novel model that performs generation at the
phrase level. Motivated by conditional random
fields (CRF) (Lafferty et al, 2001), a different pa-
rameterization of the conditional probability of the
hybrid tree that enables the model to encode some
longer range dependencies amongst phrases and
MRs is used. This novel model is referred to as
the tree CRF-based model.
Evaluation results for both models are pre-
sented, through which we demonstrate that the tree
CRF-based model performs better than the direct
inversion model. We also compare the tree CRF-
based model against the previous state-of-the-art
model of Wong and Mooney (2007). Further-
more, we evaluate our model on a dataset anno-
tated with several natural languages other than En-
glish (Japanese, Spanish, and Turkish). Evalua-
tion results show that our proposed tree CRF-based
model outperforms the previous model.
2 Related Work
There have been substantial earlier research ef-
forts on investigating methods for transforming
MR to their corresponding NL sentences. Most
of the recent systems tackled the problem through
the architecture of chart generation introduced by
Kay (1996). Examples of such systems include
the chart generator for Head-Driven Phrase Struc-
ture Grammar (HPSG) (Carroll et al, 1999; Car-
roll and Oepen, 2005; Nakanishi et al, 2005), and
more recently for Combinatory Categorial Gram-
mar (CCG) (White and Baldridge, 2003; White,
2004). However, most of these systems only fo-
cused on surface realization (inflection and order-
ing of NL words) and ignored lexical selection
(learning the mappings from MR domain concepts
to NL words).
The recent work by Wong and Mooney (2007)
explored methods for generation by inverting a
system originally designed for semantic pars-
ing. They introduced a system named WASP
?1
that employed techniques from statistical ma-
chine translation using Synchronous Context-Free
Grammar (SCFG) (Aho and Ullman, 1972). The
system took in a linearized MR tree as input, and
translated it into a natural language sentence as
output. Unlike most previous systems, their sys-
tem integrated both lexical selection and surface
realization in a single framework. The perfor-
mance of the system was enhanced by incorpo-
rating models borrowed from PHARAOH (Koehn,
2004). Experiments show that this new hybrid
system named WASP
?1
++ gives state-of-the-art
accuracies and outperforms the direct translation
model obtained from PHARAOH, when evaluated
on two corpora. We will compare our system?s
performance against that of WASP
?1
++ in Sec-
tion 5.
3 The Hybrid Tree Framework and the
LNLZ08 System
QUERY : answer(RIVER)
RIVER : longest(RIVER)
RIVER : exclude(RIVER
1
RIVER
2
)
RIVER : river(all) RIVER : traverse(STATE)
STATE : stateid(STATENAME)
STATENAME : texas
what is the longest river that
does not run through texas
Figure 1: An example MR paired with its NL sen-
tence.
Following most previous works in this
area (Kate et al, 2005; Ge and Mooney, 2005;
Kate and Mooney, 2006; Wong and Mooney,
2006; Lu et al, 2008), we consider MRs in the
form of tree structures. An example MR and
its corresponding natural language sentence are
shown in Figure 1. The MR is a tree consisting
of nodes called MR productions. For example,
the node ?QUERY : answer(RIVER)? is one MR
production. Each MR production consists of a
semantic category (?QUERY?), a function symbol
(?answer?) which can be optionally omitted, as
well as an argument list which possibly contains
401
QUERY : answer(RIVER)
RIVER : longest(RIVER)
RIVER : exclude(RIVER
1
RIVER
2
)
RIVER : traverse(STATE)
STATE : stateid(STATENAME)
STATENAME : texas
texas
run through
that does notRIVER : river(all)
river
the longest
what is
Figure 2: One possible hybrid tree T
1
child semantic categories (?RIVER?).
Now we give a brief overview of the hybrid tree
framework and the LNLZ08 system that was pre-
sented in Lu et al (2008). The training corpus re-
quired by the LNLZ08 system contains example
pairs d
(i)
= (
?
m
(i)
,
?
w
(i)
) for i = 1 . . . N , where
each
?
m
(i)
is an MR, and each
?
w
(i)
is an NL sen-
tence. The system makes the assumption that the
entire training corpus is generated from an under-
lying generative model, which is specified by the
parameter set ?.
The parameter set ? includes the following: the
MR model parameter ?(m
j
|m
i
, arg
k
) which mod-
els the generation of an MR production m
j
from
its parent MR production m
i
as its k-th child, the
emission parameter ?(t|m
i
,?) that is responsible
for generation of an NL word or a semantic cate-
gory t from the MR production m
i
(the parent of
t) under the context ? (such as the token to the left
of the current token), and the pattern parameter
?(r|m
i
), which models the selection of a hybrid
pattern r that defines globally how the NL words
and semantic categories are interleaved given a
parent MR production m
i
. All these parameters
are estimated from the corpus during the training
phase. The list of possible hybrid patterns is given
in Table 1 (at most two child semantic categories
are allowed ? MR productions with more child se-
mantic categories are transformed into those with
two).
In the table, m refers to the MR production, the
symbol w denotes an NL word sequence and is
optional if it appears inside []. The symbol Y and
Z refer to the first and second semantic category
under the MR production m respectively.
# RHS Hybrid Pattern # Patterns
0 m ? w 1
1 m ? [w]Y[w] 4
2
m ? [w]Y[w]Z[w] 8
m ? [w]Z[w]Y[w] 8
Table 1: The list of possible hybrid patterns, [] de-
notes optional
The generative process recursively creates MR
productions as well as NL words at each gen-
eration step in a top-down manner. This pro-
cess results in a hybrid tree for each MR-NL
pair. The list of children under each MR pro-
duction in the hybrid tree forms a hybrid se-
quence. One example hybrid tree for the MR-
NL pair given in Figure 1 is shown in Figure 2.
In this hybrid tree T
1
, the list of children under
the production RIVER : longest(RIVER) forms
the hybrid sequence ?the longest RIVER :
exclude(RIVER
1
RIVER
2
)?. The yield of the hy-
brid tree is exactly the NL sentence. The MR can
also be recovered from the hybrid tree by record-
ing all the internal nodes of the tree, subject to the
reordering operation required by the hybrid pat-
tern.
To illustrate, consider the generation of the hy-
brid tree T
1
shown in Figure 2. The model first
generates an MR production from its parent MR
production (empty as the MR production is the
root in the MR). Next, it selects a hybrid pattern
m ? wY from the predefined list of hybrid pat-
terns, which puts a constraint on the set of all al-
lowable hybrid sequences that can be generated:
the hybrid sequence must be an NL word sequence
402
followed by a semantic category. Finally, actual
NL words and semantic categories are generated
from the parent MR production. Now the genera-
tion for one level is complete, and the above pro-
cess repeats at the newly generated MR produc-
tions, until the complete NL sentence and MR are
both generated.
Mathematically, the above generative process
yields the following formula that models the joint
probability for the MR-NL pair, assuming the con-
text ? for the emission parameter is the preceding
word or semantic category (i.e., the bigram model
is assumed, as discussed in Lu et al (2008)):
p
(
T
1
(
?
w,
?
m)
)
= ?(QUERY : answer(RIVER)|?, arg
1
)
??(m ? wY|QUERY : answer(RIVER))
??(what|QUERY : answer(RIVER),BEGIN)
??(is|QUERY : answer(RIVER),what)
??(RIVER|QUERY : answer(RIVER),is)
??(END|QUERY : answer(RIVER),RIVER)
??(RIVER : longest(RIVER)|
QUERY : answer(RIVER), arg
1
)? . . . (1)
where T
1
(
?
w,
?
m) denotes the hybrid tree T
1
which
contains the NL sentence
?
w and MR
?
m.
For each MR-NL pair in the training set, there
can be potentially many possible hybrid trees asso-
ciated with the pair. However, the correct hybrid
tree is completely unknown during training. The
correct hybrid tree is therefore treated as a hidden
variable. An efficient inside-outside style algo-
rithm (Baker, 1979) coupled with further dynamic
programming techniques is used for efficient pa-
rameter estimation.
During the testing phase, the system makes use
of the learned model parameters to determine the
most probable hybrid tree given a new natural lan-
guage sentence. The MR contained in that hybrid
tree is the output of the system. Dynamic pro-
gramming techniques similar to those of training
are also employed for efficient decoding.
The generative model used in the LNLZ08 sys-
tem has a natural symmetry, allowing for easy
transformation from NL to MR, as well as from
MR to NL. This provides the starting point for our
work in ?inverting? the LNLZ08 system to gener-
ate natural language sentences from the underly-
ing meaning representations.
4 Generation with Hybrid Trees
The task of generating NL sentences from MRs
can be defined as follows. Given a training cor-
pus consisting of MRs paired with their NL sen-
tences, one needs to develop algorithms that learn
how to effectively ?paraphrase? MRs with natu-
ral language sentences. During testing, the sys-
tem should be able to output the most probable NL
?paraphrase? for a given new MR.
The LNLZ08 system models p(T (
?
w,
?
m)), the
joint generative process for the hybrid tree con-
taining both NL and MR. This term can be rewrit-
ten in the following way:
p(T (
?
w,
?
m)) = p(
?
m)? p (T (
?
w,
?
m)|
?
m) (2)
In other words, we reach an alternative view of
the joint generative process as follows. We choose
to generate the complete MR
?
m first. Given
?
m, we
generate hybrid sequences below each of its MR
production, which gives us a complete hybrid tree
T (
?
w,
?
m). The NL sentence
?
w can be constructed
from this hybrid tree exactly.
We define an operation yield(T ) which returns
the NL sentence as the yield of the hybrid tree T .
Given an MR
?
m, we find the most probable NL
sentence
?
w
?
as follows:
?
w
?
= yield
(
argmax
T
p(T |
?
m)
)
(3)
In other words, we first find the most probable
hybrid tree T that contains the provided MR
?
m.
Next we return the yield of T as the most probable
NL sentence.
Different assumptions can be made in the pro-
cess of finding the most probable hybrid tree. We
first describe a simple model which is a direct in-
version of the LNLZ08 system. This model, as a
baseline model, generates a complete NL sentence
word by word. Next, a more sophisticated model
that exploits NL phrase-level dependencies is built
that tackles some weaknesses of the simple base-
line model.
4.1 Direct Inversion Model
Assume that a pre-order traversal of the
MR
?
m gives us the list of MR productions
m
1
,m
2
, . . . ,m
S
, where S is the number of MR
productions in
?
m. Based on the independence
assumption made by the LNLZ08 system, each
MR production independently generates a hybrid
403
sequence. Denote the hybrid sequence gener-
ated under the MR production m
s
as h
s
, for
s = 1, . . . , S. We call the list of hybrid sequences
h = ?h
1
, h
2
, . . . , h
S
? a hybrid sequence list
associated with this particular MR. Thus, our goal
is to find the optimal hybrid sequence list h
?
for
the given MR
?
m, which is formulated as follows:
h
?
= ?h
?
1
, . . . , h
?
S
? = argmax
h
1
,...,h
S
S
?
s=1
p(h
s
|m
s
) (4)
The optimal hybrid sequence list defines the op-
timal hybrid tree whose yield gives the optimal NL
sentence.
Due to the strong independence assumption in-
troduced by the LNLZ08 system, the hybrid tree
generation process is in fact highly decompos-
able. Optimization of the hybrid sequence list
?h
1
, . . . , h
S
? can be performed individually since
they are independent of one another. Thus, math-
ematically, for s = 1, . . . , S, we have:
h
?
s
= argmax
h
s
p(h
s
|m
s
) (5)
The LNLZ08 system presented three models for
the task of transforming NL to MR. In this in-
verse task, for generation of a hybrid sequence,
we choose to use the bigram model (model II). We
choose this model mainly due to its stronger abil-
ity in modeling dependencies between adjacent
NL words, which we believe to be quite important
in this NL generation task. With the bigram model
assumption, the optimal hybrid sequence that can
be generated from each MR production is defined
as follows:
h
?
s
= argmax
h
s
p(h
s
|m
s
)
= argmax
h
s
{
?(r|m
s
)?
|h
s
|+1
?
j=1
?(t
j
|m
s
, t
j?1
)
}
(6)
where t
i
is either an NL word or a semantic cat-
egory with t
0
? BEGIN and t
|h
s
|+1
? END, and
r is the hybrid pattern that matches the hybrid se-
quence h
s
, which is equivalent to t
1
, . . . , t
|h
s
|
.
Equivalently, we can view the problem in the
log-space:
h
?
s
= argmin
h
s
{
? log ?(r|m
s
)
+
|h
s
|+1
?
j=1
? log ?(t
j
|m
s
, t
j?1
)
}
(7)
Note the term ? log ?(r|m
s
) is a constant for
a particular MR production m
s
and a particu-
lar hybrid pattern r. This search problem can
be equivalently cast as the shortest path problem
which can be solved efficiently with Dijkstra?s al-
gorithm (Cormen et al, 2001). We define a set
of states. Each state represents a single NL word
or a semantic category, including the special sym-
bols BEGIN and END. A directed path between
two different states t
u
and t
v
is associated with
a distance measure ? log ?(t
v
|m
s
, t
u
), which is
non-negative. The task now is to find the short-
est path between BEGIN and END
1
. The sequence
of words appearing in this path is simply the most
probable hybrid sequence under this MR produc-
tion m
s
. We build this model by directly inverting
the LNLZ08 system, and this model is therefore
referred to as the direct inversion model.
A major weakness of this baseline model is that
it encodes strong independence assumptions dur-
ing the hybrid tree generation process. Though
shown to be effective in the task of transform-
ing NL to MR, such independence assumptions
may introduce difficulties in this NLG task. For
example, consider the MR shown in Figure 1.
The generation steps of the hybrid sequences
from the two adjacent MR productions QUERY :
answer(RIVER) and RIVER : longest(RIVER)
are completely independent of each other. This
may harm the fluency of the generated NL sen-
tence, especially when a transition from one hy-
brid sequence to another is required. In fact, due
to such an independence assumption, the model
always generates the same hybrid sequence from
the same MR production, regardless of its context
such as parent or child MR productions. Such a
limitation points to the importance of better uti-
lizing the tree structure of the MR for this NLG
task. Furthermore, due to the bigram assumption,
the model is unable to capture longer range depen-
dencies amongst the words or semantic categories
in each hybrid sequence.
To tackle the above issues, we explore ways of
relaxing various assumptions, which leads to an
1
In addition, we should make sure that the generated hy-
brid sequence t
0
. . . t
|h
s
|+1
is a valid hybrid sequence that
comply with the hybrid pattern r. For example, the MR
production STATE : loc 1(RIVER) can generate the follow-
ing hybrid sequence ?BEGIN have RIVER END? but not
this hybrid sequence ?BEGIN have END?. This can be
achieved by finding the shortest path from BEGIN to RIVER,
which then gets concatenated to the shortest path from RIVER
to END.
404
QUERY : answer(RIVER)
RIVER : longest(RIVER)
RIVER : exclude(RIVER
1
RIVER
2
)
RIVER : river(all) RIVER : traverse(STATE)
STATE : stateid(STATENAME)
STATENAME : texas
what is RIVER
1
the longest RIVER
1
RIVER
1
that does not RIVER
2
river run through STATE
1
STATENAME
1
texas
Figure 3: An MR (left) and its associated hybrid sequences (right)
alternative model as discussed next.
4.2 Tree CRF-Based Model
Based on the belief that using known phrases usu-
ally leads to better fluency in the NLG task (Wong
and Mooney, 2007), we explore methods for gen-
erating an NL sentence at phrase level rather than
at word level. This is done by generating hybrid
sequences as complete objects, rather than sequen-
tially one word or semantic category at a time,
from MR productions.
We assume that each MR production can gen-
erate a complete hybrid sequence below it from a
finite set of possible hybrid sequences. Each such
hybrid sequence is called a candidate hybrid se-
quence associated with that particular MR produc-
tion. Given a set of candidate hybrid sequences as-
sociated with each MR production, the generation
task is to find the optimal hybrid sequence list h
?
for a given MR
?
m:
h
?
= argmax
h
p(h|
?
m) (8)
Figure 3 shows a complete MR, as well as a
possible tree that contains hybrid sequences as-
sociated with the MR productions. For exam-
ple, in the figure the MR production RIVER :
traverse(STATE) is associated with the hybrid se-
quence run through STATE
1
. Each MR pro-
duction can be associated with potentially many
different hybrid sequences. The task is to deter-
mine the most probable list of hybrid sequences as
the ones appearing on the right of Figure 3, one for
each MR production.
To make better use of the tree structure of MR,
we take the approach of modeling the conditional
distribution using a log-linear model. Following
the conditional random fields (CRF) framework
(Lafferty et al, 2001), we can define the probabil-
ity of the hybrid sequence list given the complete
MR
?
m, as follows:
p(h|
?
m) =
1
Z(
?
m)
exp
(
?
i?V
?
k
?
k
g
k
(h
i
,
?
m, i)
+
?
(i,j)?E
?
k
?
k
f
k
(h
i
, h
j
,
?
m, i, j)
)
(9)
where V is the set of all the vertices in the tree, and
E is the set of the edges in the tree, consisting of
parent-child pairs. The function Z(
?
m) is the nor-
malization function. Note that the dependencies
among the features here form a tree, unlike the se-
quence models used in Lafferty et al (2001). The
function f
k
(h
i
, h
j
,
?
m, i, j) is a feature function of
the entire MR tree
?
m and the hybrid sequences at
vertex i and j. These features are usually referred
to as the edge features in the CRF framework. The
function g
k
(h
i
,
?
m, i) is a feature function of the
hybrid sequence at vertex i and the entire MR tree.
These features are usually referred to as the vertex
features. The parameters ?
k
and ?
k
are learned
from the training data.
In this task, we are given only MR-NL pairs
and do not have the hybrid tree corresponding to
each MR as training data. Now we describe how
the set of candidate hybrid sequences for each MR
production is obtained as well as how the train-
ing data for this model is constructed. After the
joint generative model is learned as done in Lu et
al. (2008), we first use a Viterbi algorithm to find
the optimal hybrid tree for each MR-NL pair in
the training set. From each optimal hybrid tree,
we extract the hybrid sequence h
i
below each MR
production m
i
. Using this process on the train-
ing MR-NL pairs, we can obtain a set of candidate
405
hybrid sequences that can be associated with each
MR production. The optimal hybrid tree generated
by the Viterbi algorithm in this way is considered
the ?correct? hybrid tree for theMR-NL pair and is
used as training data. While this does not provide
hand-labeled training data, we believe the hybrid
trees generated this way form a high quality train-
ing set as both the MR and NL are available when
Viterbi decoding is performed, guaranteeing that
the generated hybrid tree has the correct yield.
There exist several advantages of such a model
over the simple generative model. First, this model
allows features that specifically model the depen-
dencies between neighboring hybrid sequences in
the tree to be used. In addition, the model can effi-
ciently capture long range dependencies between
MR productions and hybrid sequences since each
hybrid sequence is allowed to depend on the entire
MR tree.
For features, we employ four types of simple
features, as presented below. Note that the first
three types of features are vertex features, and the
last are edge features. Examples are given based
on Figure 3. All the features are indicator func-
tions, i.e., a feature takes value 1 if a certain com-
bination is present, and 0 otherwise. The last three
features explicitly encode information from the
tree structure of MR.
Hybrid sequence features : one hybrid sequence
together with the associated MR production.
For example:
g
1
: ?run through STATE
1
,
RIVER : traverse(STATE)? ;
Two-level hybrid sequence features : one hy-
brid sequence, its associated MR production,
and the parent MR production. For example:
g
2
: ?run through STATE
1
,
RIVER : traverse(STATE),
RIVER : exclude(RIVER
1
,RIVER
2
)? ;
Three-level hybrid sequence features : one hy-
brid sequence, its associated MR production,
the parent MR production, and the grandpar-
ent MR production. For example:
g
3
: ?run through STATE
1
,
RIVER : traverse(STATE),
RIVER : exclude(RIVER
1
,RIVER
2
),
RIVER : longest(RIVER)? ;
Adjacent hybrid sequence features : two adja-
cent hybrid sequences, together with their as-
sociated MR productions. For example:
f
1
: ?run through STATE
1
,
RIVER
1
that does not RIVER
2
,
RIVER : traverse(STATE),
RIVER : exclude(RIVER
1
,RIVER
2
)? .
For training, we use the feature forest model
(Miyao and Tsujii, 2008), which was originally
designed as an efficient algorithm for solving max-
imum entropy models for data with complex struc-
tures. The model enables efficient training over
packed trees that potentially represent exponen-
tial number of trees. The tree conditional random
fields model can be effectively represented using
the feature forest model. The model has also been
successfully applied to the HPSG parsing task.
To train the model, we run the Viterbi algorithm
on the trained LNLZ08 model and perform convex
optimization using the feature forest model. The
LNLZ08 model is trained using an EM algorithm
with time complexity O(MN
3
D) per EM itera-
tion, where M and N are respectively the maxi-
mum number of MR productions and NL words
for each MR-NL pair, and D is the number of
training instances. The time complexity of the
Viterbi algorithm is also O(MN
3
D). For training
the feature forest, we use the Amis toolkit (Miyao
and Tsujii, 2002) which utilizes the GIS algorithm.
The time complexity for each iteration of the GIS
algorithm is O(MK
2
D), where K is the maxi-
mum number of candidate hybrid sequences asso-
ciated with each MR production. Finally, the time
complexity for generating a natural language sen-
tence from a particular MR is O(MK
2
).
5 Experiments
In this section, we present the results of our sys-
tems when evaluated on two standard benchmark
corpora. The first corpus is GEOQUERY, which
contains Prolog-based MRs that can be used to
query a US geographic database (Kate et al,
2005). Our task for this domain is to generate
NL sentences from the formal queries. The second
corpus is ROBOCUP. This domain contains MRs
which are instructions written in a formal language
called CLANG. Our task for this domain is to gen-
erate NL sentences from the coaching advice writ-
ten in CLANG.
406
GEOQUERY (880) ROBOCUP (300)
BLEU NIST BLEU NIST
Direct inversion model 0.3973 5.5466 0.5468 6.6738
Tree CRF-based model 0.5733 6.7459 0.6220 6.9845
Table 2: Results of automatic evaluation of both models (bold type indicates the best performing system).
GEOQUERY (880) ROBOCUP (300)
BLEU NIST BLEU NIST
WASP
?1
++ 0.5370 6.4808 0.6022 6.8976
Tree CRF-based model 0.5733 6.7459 0.6220 6.9845
Table 3: Results of automatic evaluation of our tree CRF-based model and WASP
?1
++.
English Japanese Spanish Turkish
BLEU NIST BLEU NIST BLEU NIST BLEU NIST
WASP
?1
++ 0.6035 5.7133 0.6585 4.6648 0.6175 5.7293 0.4824 4.3283
Tree CRF-based model 0.6265 5.8907 0.6788 4.8486 0.6382 5.8488 0.5096 4.5033
Table 4: Results on the GEOQUERY-250 corpus with 4 natural languages.
The GEOQUERY domain contains 880 in-
stances, while the ROBOCUP domain contains 300
instances. The average NL sentence length for the
two corpora are 7.57 and 22.52 respectively. Fol-
lowing the evaluation methodology of Wong and
Mooney (2007), we performed 4 runs of the stan-
dard 10-fold cross validation and report the aver-
aged performance in this section using the stan-
dard automatic evaluation metric BLEU (Papineni
et al, 2002) and NIST (Doddington, 2002)
2
. The
BLEU and NIST scores of the WASP
?1
++ sys-
tem reported in this section are obtained from
the published paper of Wong and Mooney (2007).
Note that to make our experimental results directly
comparable to Wong and Mooney (2007), we used
the identical training and test data splits for the 4
runs of 10-fold cross validation used by Wong and
Mooney (2007) on both corpora.
Our system has the advantage of always pro-
ducing an NL sentence given any input MR, even
if there exist unseen MR productions in the input
MR. We can achieve this by simply skipping those
unseen MR productions during the generation pro-
cess. However, in order to make a fair comparison
against WASP
?1
++, which can only generate NL
sentences for 97% of the input MRs, we also do
not generate any NL sentence in the case of ob-
serving an unseen MR production. All the evalu-
ations discussed in this section follow this evalu-
2
We used the official evaluation script (version 11b) pro-
vided by http://www.nist.gov/.
ation methodology, but we notice that empirically
our system is able to achieve higher BLEU/NIST
scores if we allow generation for those MRs that
include unseen MR productions.
5.1 Comparison between the two models
We compare the performance of our two models
in Table 2. From the table, we observe that the
tree CRF-based model outperforms the direct in-
version model on both domains. This validates
our earlier belief that some long range dependen-
cies are important for the generation task. In ad-
dition, while the direct inversion model performs
reasonably well on the ROBOCUP domain, it per-
forms substantially worse on the GEOQUERY do-
main where the sentence length is shorter. We note
that the evaluation metrics are strongly correlated
with the cumulative matching n-grams between
the output and the reference sentence (n ranges
from 1 to 4 for BLEU, and 1 to 5 for NIST). The
direct inversion model fails to capture the transi-
tional behavior from one phrase to another, which
makes it more vulnerable to n-gram mismatch, es-
pecially when evaluated on the GEOQUERY cor-
pus where phrase-to-phrase transitions are more
frequent. On the other hand, the tree CRF-based
model does not suffer from this problem, mainly
due to its ability to model such dependencies be-
tween neighboring phrases. Sample outputs from
the two models are shown in Figure 4.
407
Reference: what is the largest state bordering texas
Direct inversion model: what the largest states border texas
Tree CRF-based model: what is the largest state that borders texas
Reference: if DR2C7 is true then players 2 , 3 , 7 and 8
should pass to player 4
Direct inversion model: if DR2C7 , then players 2 , 3 7 and 8 should
ball to player 4
Tree CRF-based model: if the condition DR2C7 is true then players 2 ,
3 , 7 and 8 should pass to player 4
Figure 4: Sample outputs from the two models, for GEOQUERY domain (top) and ROBOCUP domain
(bottom) respectively.
5.2 Comparison with previous model
We also compare the performance of our tree CRF-
based model against the previous state-of-the-art
system WASP
?1
++ in Table 3. Our tree CRF-based
model achieves better performance on both cor-
pora. We are unable to carry out statistical sig-
nificance tests since the detailed BLEU and NIST
scores of the cross validation runs of WASP
?1
++
as reported in the published paper of Wong and
Mooney (2007) are not available.
The results confirm our earlier discussions: the
dependencies between the generated NL words
are important and need to be properly modeled.
The WASP
?1
++ system uses a log-linear model
which incorporates two major techniques to at-
tempt to model such dependencies. First, a back-
off language model is used to capture dependen-
cies at adjacent word level. Second, a technique
that merges smaller translation rules into a single
rigid rule is used to capture dependencies at phrase
level (Wong, 2007). In contrast, the proposed tree
CRF-based model is able to explicitly and flexibly
exploit phrase-level features that model dependen-
cies between adjacent phrases. In fact, with the
hybrid tree framework, the better treatment of the
tree structure of MR enables us to model some cru-
cial dependencies between the complete MR tree
and generated NL phrases. We believe that this
property plays an important role in improving the
quality of the generated sentences in terms of flu-
ency, which is assessed by the evaluation metrics.
Furthermore, WASP
?1
++ employs minimum
error rate training (Och, 2003) to directly optimize
the evaluation metrics. We have not done so but
still obtain better performance. In future, we plan
to explore ways to directly optimize the evaluation
metrics in our system.
5.3 Experiments on different languages
Following the work of Wong and Mooney (2007),
we also evaluated our system?s performance on
a subset of the GEOQUERY corpus with 250 in-
stances, where sentences of 4 natural languages
(English, Japanese, Spanish, and Turkish) are
available. The evaluation results are shown in Ta-
ble 4. Our tree CRF-based model achieves better
performance on this task compared to WASP
?1
++.
We are again unable to conduct statistical signifi-
cance tests for the same reason reported earlier.
6 Conclusions
In this paper, we presented two novel models for
the task of generating natural language sentences
from given meaning representations, under a hy-
brid tree framework. We first built a simple di-
rect inversion model as a baseline. Next, to ad-
dress the limitations associated with the direct in-
version model, a tree CRF-based model was in-
troduced. We evaluated both models on standard
benchmark corpora. Evaluation results show that
the tree CRF-based model performs better than the
direct inversion model, and that the tree CRF-based
model also outperforms WASP
?1
++, which was a
previous state-of-the-art system reported in the lit-
erature.
Acknowledgments
The authors would like to thank Seung-Hoon Na
for his suggestions on the presentation of this pa-
per, Yuk Wah Wong for answering various ques-
tions related to the WASP
?1
++ system, and the
anonymous reviewers for their thoughtful com-
ments on this work.
408
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation and Compiling.
Prentice-Hall, Englewood Clis, NJ.
James K. Baker. 1979. Trainable grammars for speech
recognition. In Proceedings of the Spring Confer-
ence of the Acoustical Society of America, pages
547?550, Boston, MA, June.
John Carroll and Stephan Oepen. 2005. High ef-
ficiency realization for a wide-coverage unification
grammar. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing
(IJCNLP 2005), pages 165?176.
John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznanski. 1999. An efficient chart generator
for (semi-) lexicalist grammars. In Proceedings of
the 7th European Workshop on Natural Language
Generation (EWNLG 1999), pages 86?95.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to
Algorithms (Second Edition). MIT Press.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the 2nd In-
ternational Conference on Human Language Tech-
nology Research (HLT 2002), pages 138?145.
Ruifang Ge and Raymond J. Mooney. 2005. A statis-
tical semantic parser that integrates syntax and se-
mantics. In Proceedings of the 9th Conference on
Computational Natural Language Learning (CoNLL
2005), pages 9?16.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting of the Association for Computational Lin-
guistics (COLING/ACL 2006), pages 913?920.
Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to for-
mal languages. In Proceedings of the 20th National
Conference on Artificial Intelligence (AAAI 2005),
pages 1062?1068.
Martin Kay. 1996. Chart generation. In Proceedings
of the 34th Annual Meeting of the Association for
Computational Linguistics (ACL 1996), pages 200?
204.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of the 6th Conference
of the Association for Machine Translation in the
Americas (AMTA 2004), pages 115?124.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the 18th Inter-
national Conference on Machine Learning (ICML
2001), pages 282?289.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2008), pages 783?792.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proceed-
ings of the 2nd International Conference on Human
Language Technology Research (HLT 2002), pages
292?297.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic models for disambiguation of an
HPSG-based chart generator. In Proceedings of the
9th International Workshop on Parsing Technologies
(IWPT 2005), volume 5, pages 93?102.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2003), pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2002), pages 311?318.
Michael White and Jason Baldridge. 2003. Adapting
chart realization to CCG. In Proceedings of the 9th
European Workshop on Natural Language Genera-
tion (EWNLG 2003), pages 119?126.
Michael White. 2004. Reining in CCG chart realiza-
tion. In Proceeding of the 3rd International Confer-
ence on Natural Language Generation (INLG 2004),
pages 182?191.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics (HLT/NAACL 2006), pages 439?446.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Generation by inverting a semantic parser that uses
statistical machine translation. In Proceedings of
the Human Language Technology Conference of
the North American Chapter of the Association
for Computational Linguistics (NAACL/HLT 2007),
pages 172?179.
Yuk Wah Wong. 2007. Learning for Semantic Parsing
and Natural Language Generation Using Statistical
Machine Translation Techniques. Ph.D. thesis, The
University of Texas at Austin.
409
