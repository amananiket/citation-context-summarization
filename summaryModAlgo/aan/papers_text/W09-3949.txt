Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 333?336,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Simultaneous dialogue act segmentation and labelling using lexical and
syntactic features
Ramon Granell, Stephen Pulman
Oxford University Computing Laboratory,
Wolfson Building, Parks Road,
Oxford, OX1 3QD, England
ramg@comlab.ox.ac.uk
sgp@clg.ox.ac.uk
Carlos-D. Mart??nez-Hinarejos
Instituto Tecnolo?gico de Informa?tica,
Universidad Polite?cnica de Valencia,
Camino de Vera, s/n, 46022, Valencia, Spain
cmartine@dsic.upv.es
Abstract
Segmentation of utterances and annotation
as dialogue acts can be helpful for sev-
eral modules of dialogue systems. In this
work, we study a statistical machine learn-
ing model to perform these tasks simulta-
neously using lexical features and incorpo-
rating deterministic syntactic restrictions.
There is a slight improvement in both seg-
mentation and labelling due to these re-
strictions.
1 Introduction
Dialogue acts (DA) are linguistic abstractions that
are commonly accepted and employed by the the
dialogue community. In the framework of dia-
logue systems, they can be helpful to identify and
model user intentions and system answers by the
dialogue manager. Furthermore, in other dialogue
modules such as the automatic speech recognizer
or speech synthesiser, DA information may be also
used to increase their performance.
Many researchers have studied automatic DA
labelling using different techniques. However, in
most of this work it is common to assume that the
dialogue turns are already segmented into separate
utterances, where each utterance corresponds to
just one DA label, as in (Stolcke et al(2000); Ji
and Bilmes (2005); Webb et al(2005)). This is
not a realistic situation because the segmentation
of turns into utterances is not a trivial problem.
There have been many previous approaches to
segmentation of turns prior to DA labelling, be-
ginning with (Stolcke and Shriberg (1996)). Typ-
ically some combination of words and part of
speech (POS) tags is used to predict segmentation
boundaries. In this work we make use of a sta-
tistical model to solve both the DA labelling task
and the segmentation task simultaneously, follow-
ing (Ang et al(2005); Mart??nez-Hinarejos et al
(2006)). Our aim is to see whether going beyond
the word n-gram models can improve accuracy,
using syntactic information (constituent structure)
obtained from the dialogue transcriptions. We ex-
amine whether this information can improve the
segmentation of the dialogue turns into DA seg-
ments. Intuitively, it seems logical to believe that
most of these segments must coincide with partic-
ular syntactic structures, and that segment bound-
aries would respect constituent boundaries.
2 Dialogue data
The dialogue corpus used to perform the exper-
iments is the Switchboard database (SWBD). It
consists of human-human conversations by tele-
phone about generic topics. There are 1155 5-
minute conversations, comprising approximately
205000 utterances and 1.4 million words. The size
of the vocabulary is approximately 22000 words.
All this corpus has been manually annotated at
the dialogue act level using the SWBD-DAMSL
scheme, (Jurafsky et al(1997)), consisting of 42
different labels. Every dialogue turn was manu-
ally segmented into utterances. The average num-
ber of segments (utterances) per dialogue turn is
1.78 with a standard deviation of 1.41. Each utter-
ance was assigned one SWBD-DAMSL label (see
Figure 1).
3 Syntactic analysis of DA segments
An initial analysis of the syntactic structures of the
dialogue data was performed to study their possi-
ble relevance for DA segmentation.
333
- $LAUGH he waits until it gets about seventeen below up here $SEG and then he calls us , $SEG
sd sd
- he waits until it gets about seventeen below up here and then he calls us .
Figure 1: The first row is an original segmented dialogue turn, where the $SEG label indicates the end
of a DA segment. The second row contains the corresponding DA label for each segment, where ?sd?
corresponds to the SWBD-DAMSL label of Statement non-opinion. The third row is the input for the
parser.
3.1 Parsing of spontaneous dialogues
One of the main problems we face when we try
to syntactically analyse a corpus transcribed from
spontaneous speech by different people such as
SWBD corpus, is the inconsistency of annotation
conventions for spontaneous speech phenomena
and punctuation marks. This can be problematic
for parsers, as they work at the sentence level.
Some of the dialogue turns of the SWBD corpus
are not transcribed using consistent punctuation
conventions. We therefore carried out some pre-
processing so that all turns end with proper punc-
tuation marks. Additionally, the non-verbal labels
(e.g. $LAUGH, $OVERLAP, $SEG, ...) are re-
moved. In Figure 1 there is an example of this
process.
The Stanford Parser, (Klein and Manning
(2003)) was used for the syntactic analysis of the
transcriptions of SWBD dialogues. The English
grammar used to train the parser is based on the
standard LDC Penn Treebank WSJ training sec-
tions 2-21. Is is important to remark that the nature
of the training corpus (journalistic style reports)
is different from the transcriptions of spontaneous
speech conversations. We would therefore expect
a decrease in accuracy. As output of the parsing
process, a tree that contains syntactic structures
was provided (e.g. see Figure 2).
3.2 Syntactic features and segmentation
As we are interested in studying the coincidence
of syntactic structures with DA segments, we will
select two general features for each word (see Fig-
ure 3):
? Most general syntactic category that starts
with a word, (MGSS), i.e., the root of the cur-
rent subtree of the syntactic analysis, (e.g. in
Figure 2, ?CC? is the MGSS of the first word
of the second segment, ?and?).
? Most general syntactic category that ends
with a word, (MGSE), i.e., the root of the
(ROOT
(S (: -)
(S
(NP (PRP he))
(VP (VBZ waits)
(SBAR (IN until)
(S
(NP (PRP it))
(VP (VBZ gets)
(PP (IN about)
(NP (NN seventeen)))
(PP (IN below)
(ADVP (RB up) (RB here))))))))
(CC and)
(S
(ADVP (RB then))
(NP (PRP he))
(VP (VBZ calls)
(NP (PRP us))))
(. .)))
Figure 2: Example of the syntactic analysis of the
dialogue turn that appears in Figure 1.
subtree of the syntactic analysis that ends
with that word, (e.g. in Figure 2, ?S? is
the MGSE of last word of the first segment,
?here?).
Using these features, we have analysed the syn-
tactic categories of boundary words of segments.
Particularly, it seems interesting to studyMGSE of
last word of the segment and MGSS of first word
of the segment, because it indicates which syntac-
tic structure ends before the segment boundary and
which one starts after it. As there is always the be-
ginning of a segment with the first word of the turn
and the end of a segment with the last word of the
turn, we are ignoring these for the analysis, be-
cause we are looking for intra-turn segments. Re-
sults of this analysis can be seen in Table 1.
4 The model
The statistical model used to DA label and
segment the dialogues is extensively explained
in (Mart??nez-Hinarejos (2008)). Basically, it is
334
ROOT+-+: $LAUGH S+he+NP VP+waits+VBZ SBAR+until+IN S+it+NP VP+gets+VBZ
PP+about+IN NP+seventeen+PP PP+below+IN ADVP+up+RB RB+here+S $SEG
CC+and+CC S+then+ADVP NP+he+NP VP+calls+VBZ NP+us+S .+.+ROOT $SEG
Figure 3: For each word of the example turn of Figure 1, MGSS (item before the word) and MGSE (item
after the word) are obtained from the tree of Figure 2. Non-verbal labels were reincorporated.
MGSE MGSS
Occ % Cat Occ % Cat
33516 37.1 , 30318 33.5 ROOT
30640 33.9 ROOT 19988 22.1 CC
7801 8.6 : 13275 14.7 NP
7134 7.9 S 10187 11.3 S
2687 3.0 NP 3508 3.9 SBAR
2319 2.6 PRN 3421 3.8 ADVP
750 0.8 VP 2034 2.2 VP
531 0.6 ADVP 1957 2.2 INTJ
478 0.5 PP 1300 1.4 UH
465 0.5 RB 972 1.1 PP
4078 4.5 Other 3481 3.8 Other
Table 1: Occurrences and percentage of the syn-
tactic categories that correspond with the most fre-
quent MGSE of the last segment word (except last
segment) andMGSS of the first segment word (ex-
cept first segment).
based on a combination of a Hidden Markov
Model at lexical level and a Language Model (n-
gram) at DA level. The Viterbi algorithm is used
to find the most likely sequence of DA labels ac-
cording to the trained models. The segmentation
is obtained from the jumps between DAs of this
sequence.
The previous section has shown that the MGSE
and MGSS for the segments boundary words are
concentrated in a small set of categories (see Ta-
ble 1). Therefore, one quick and easy way to in-
corporate this information to the existing model is
to add some restrictions during the decoding pro-
cess, giving the model:
U? = argmax
U
max
r,sr1
r?
k=1
Pr(uk|u
k?1
k?n?1) ?
?Pr(W sksk?1+1|uk)?(xsk)
where U? is the sequence of DAs that we will get
from the annotation/segmentation process. The
search process produces a segmentation s =
(s0, s1, . . . , sr), that divides the word sequence
W into the segments W s1s0+1W
s2
s1+1 . . .W
sr
sr?1+1.
Each segment is assigned to a DA ui that forms
the DA sequence U = u1 . . . ur. xi corresponds
to the syntactic features of the i word that can be
MGSE, MGSS or both of them, and
?(xi) =
?
?
?
1 if xi ? X
0 otherwise
where X can be a subset of all the possible syn-
tactic categories that correspond to:
1. the most frequent MGSE of last segment
word, if x is MGSE.
2. the most frequent MGSS of first segment
word, if x is MGSS
3. the most frequent combinations of both pre-
vious sets.
It means that we will only allow a segment end-
ing when the MGSE of a word is in this set, or
a start of a segment when the MGSS of the fol-
lowing word is in the corresponding set or both
conditions at the same time.
5 Experiments and results
Ten cross-validation experiments were performed
for each model using, in each experiment a train-
ing partition composed of 1136 dialogues and
a test set of 19 dialogues, as in (Stolcke et al
(2000); Webb et al(2005); Mart??nez-Hinarejos
et al(2006)). The N-grams were obtained using
the SLM toolkit (Rosenfeld (1998)) with Good-
Turing discounting and the HMMs were trained
using the Baum-Welch algorithm. We use the fol-
lowing evaluation measures:
? To evaluate the labelling, we use the DA Er-
ror Rate (equivalent to Word Error Rate) and
the percentage of error labelling of whole
turns.
? For the segment evaluation, we only check
where the segments bounds are produced
(word position in the segment), making use
of F-score obtained from precision and recall.
335
The results from using different sizes for the set
X are shown for labelling performance in Tables 2
and 3, and F-score of the segmentation in Table 4.
Model/SizeX 5 10 20 All
MGSE 53.31 54.76 54.60 54.76
MGSS 53.35 52.76 54.92 54.76
Both 53.58 52.84 54.76 54.76
Table 2: DAER for models using MGSE, MGSS
and both features. SizeX indicates the size of the
set of most frequent categories accepted. Without
syntactic categories (baseline) we obtain a DAER
of 54.41.
Model/SizeX 5 10 20 All
MGSE 53.61 55.41 55.34 55.77
MGSS 53.61 53.32 55.63 55.77
Both 53.46 53.10 55.19 55.77
Table 3: Percentage of error of labelling of com-
plete turns for all the possible models. The base-
line value is 55.41.
Model / SizeX 5 10 20 All
MGSE 73.08 71.18 71.44 71.17
MGSS 73.60 73.72 71.44 71.17
Both 74.36 74.08 71.75 71.16
Table 4: F-score of segmentation. The baseline
value is 71.17.
6 Discussion and future work
In this work, we have used lexical and syntactic
features for labelling and segmenting DAs simul-
taneously. Syntactic features obtained automati-
cally were deterministically applied during the sta-
tistical decoding process. There is a slight im-
provement using syntactic information, obtaining
better results than reported in other work such
as (Mart??nez-Hinarejos et al(2006)). The F-score
of the segmentation improves 3% using the syn-
tactic features, however values are slightly worse
(2%) than results in (Stolcke and Shriberg (1996)).
As future work, we think that incorporating the
syntactic information in a non-deterministic way
might further improve the annotation and segmen-
tation scores. Furthermore, it is possible to make
use of additional information from the syntactic
structure, rather than just the boundary informa-
tion we are currently using. Finally, an evalua-
tion over different corpora must be done to check
both the performance of the proposed model and
the reusability of the syntactic sets.
Acknowledgments
This work was partially funded by the Compan-
ions project (http://www.companions-project.org)
sponsored by the European Commission as part of
the Information Society Technologies (IST) pro-
gramme under EC grant number IST-FP6-034434.
References
Ang J., Liu Y., Shriberg E. 2005. Automatic Dialog Act
Segmentation and Classification in Multiparty Meet-
ings. Proc. ICASSP, Philadelphia, USA, pp. 1061-
1064
Ji, G and Bilmes, J. 2005. Dialog act tagging using
graphical models. Proc. ICASSP, Philadelphia, USA
Jurafsky, D. Shriberg, E., Biasca, D. 1997. Switchboard
swbd-damsl shallow- discourse-function annotation
coders manual. Tech. Rep. 97-01, University of Col-
orado Institute of Cognitive Science
Klein D. and Manning, C. D. 2003. Accurate Unlex-
icalized Parsing. Proc. ACL, Sapporo, Japan, pp.
423-430
Mart??nez-Hinarejos, C. D., Granell, R., Bened??, J. M.
2006. Segmented and unsegmented dialogue-act
annotation with statistical dialogue models. Proc.
COLING/ACL Sydney, Australia, pp. 563-570
Mart??nez-Hinarejos, C. D., Bened??, J. M., Granell, R.
2008. Statistical framework for a spanish spoken
dialogue corpus. Speech Communication, vol. 50,
number 11-12, pp. 992-1008
Rosenfeld, R. 1998. The cmu-cambridge statistical
language modelling toolkit v2. Technical report,
Carnegie Mellon University
Stolcke, A. and Shriberg, E. 1996. Automatic linguis-
tic segmentation of conversational speech. Proc. of
ICSLP, Philadelphia, USA
Stolcke, A., Coccaro, N., Bates, R., Taylor, P., van
Ess-Dykema, C., Ries, K., Shriberg, E., Jurafsky,
D., Martin, R., Meteer, M. 2000. Dialogue act
modelling for automatic tagging and recognition
of conversational speech. Computational Linguistics
26 (3), 1-34
Webb, N., Hepple, M., Wilks, Y. 2005. Dialogue act
classification using intra-utterance features. Proc. of
the AAAI Workshop on Spoken Language Under-
standing. Pittsburgh, USA
336
