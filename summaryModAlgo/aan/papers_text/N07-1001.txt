Proceedings of NAACL HLT 2007, pages 1?8,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Exploiting acoustic and syntactic features for prosody labeling in
a maximum entropy framework
Vivek Rangarajan, Shrikanth Narayanan
Speech Analysis and Interpretation Laboratory
University of Southern California
Viterbi School of Electrical Engineering
vrangara@usc.edu,shri@sipi.usc.edu
Srinivas Bangalore
AT&T Research Labs
180 Park Avenue
Florham Park, NJ 07932, U.S.A.
srini@research.att.com
Abstract
In this paper we describe an automatic
prosody labeling framework that exploits
both language and speech information.
We model the syntactic-prosodic informa-
tion with a maximum entropy model that
achieves an accuracy of 85.2% and 91.5%
for pitch accent and boundary tone la-
beling on the Boston University Radio
News corpus. We model the acoustic-
prosodic stream with two different mod-
els, one a maximum entropy model and
the other a traditional HMM. We finally
couple the syntactic-prosodic and acoustic-
prosodic components to achieve signifi-
cantly improved pitch accent and bound-
ary tone classification accuracies of 86.0%
and 93.1% respectively. Similar experimen-
tal results are also reported on Boston Di-
rections corpus.
1 Introduction
Prosody refers to intonation, rhythm and lexical
stress patterns of spoken language that convey lin-
guistic and paralinguistic information such as em-
phasis, intent, attitude and emotion of a speaker.
Prosodic information associated with a unit of
speech, say, syllable, word, phrase or clause, influ-
ence all the segments of the unit in an utterance. In
this sense they are also referred to as suprasegmen-
tals (Lehiste, 1970). Prosody in general is highly
dependent on individual speaker style, gender, di-
alect and other phonological factors. The difficulty in
reliably characterizing suprasegmental information
present in speech has resulted in symbolic and para-
meteric prosody labeling standards like ToBI (Tones
and Break Indices) (Silverman et al, 1992) and Tilt
model (Taylor, 1998) respectively.
Prosody in spoken language can be characterized
through acoustic features or lexical features or both.
Acoustic correlates of duration, intensity and pitch,
like syllable nuclei duration, short time energy and
fundamental frequency (f0) are some acoustic fea-
tures that are perceived to confer prosodic promi-
nence or stress in English. Lexical features like parts-
of-speech, syllable nuclei identity, syllable stress of
neighboring words have also demonstrated high de-
gree of discriminatory evidence in prosody detection
tasks.
The interplay between acoustic and lexical fea-
tures in characterizing prosodic events has been suc-
cessfully exploited in text-to-speech synthesis (Bu-
lyko and Ostendorf, 2001; Ma et al, 2003), speech
recognition (Hasegawa-Johnson et al, 2005) and
speech understanding (Wightman and Ostendorf,
1994). Text-to-speech synthesis relies on lexical fea-
tures derived predominantly from the input text to
synthesize natural sounding speech with appropri-
ate prosody. In contrast, output of a typical auto-
matic speech recognition (ASR) system is noisy and
hence, the acoustic features are more useful in pre-
dicting prosody than the hypothesized lexical tran-
script which may be erroneous. Speech understand-
ing systems model both the lexical and acoustic fea-
tures at the output of an ASR to improve natural
language understanding. Another source of renewed
interest has come from spoken language translation
(No?th et al, 2000; Agu?ero et al, 2006). A pre-
requisite for all these applications is accurate prosody
detection, the topic of the present work.
In this paper, we describe our framework for build-
ing an automatic prosody labeler for English. We
report results on the Boston University (BU) Ra-
dio Speech Corpus (Ostendorf et al, 1995) and
Boston Directions Corpus (BDC) (Hirschberg and
Nakatani, 1996), two publicly available speech cor-
pora with manual ToBI annotations intended for ex-
periments in automatic prosody labeling. We con-
dition prosody not only on word strings and their
parts-of-speech but also on richer syntactic informa-
tion encapsulated in the form of Supertags (Banga-
lore and Joshi, 1999). We propose a maximum en-
tropy modeling framework for the syntactic features.
We model the acoustic-prosodic stream with two dif-
ferent models, a maximum entropy model and a more
traditional hidden markov model (HMM). In an au-
tomatic prosody labeling task, one is essentially try-
1
ing to predict the correct prosody label sequence for
a given utterance and a maximum entropy model of-
fers an elegant solution to this learning problem. The
framework is also robust in the selection of discrim-
inative features for the classification problem. So,
given a word sequence W = {w1, ? ? ? , wn} and a set
of acoustic-prosodic features A = {o1, ? ? ? , oT }, the
best prosodic label sequence L? = {l1, l2, ? ? ? , ln} is
obtained as follows,
L? = argmax
L
P (L|A,W ) (1)
= argmax
L
P (L|W ).P (A|L,W ) (2)
? argmax
L
P (L|?(W )).P (A|L,W ) (3)
where ?(W ) is the syntactic feature encoding of the
word sequence W . The first term in Equation (3)
corresponds to the probability obtained through our
maximum entropy syntactic model. The second term
in Equation (3), computed by an HMM corresponds
to the probability of the acoustic data stream which
is assumed to be dependent only on the prosodic la-
bel sequence.
The paper is organized as follows. In section 2
we describe related work in automatic prosody la-
beling followed by a description of the data used in
our experiments in section 3. We present prosody
prediction results from off-the-shelf synthesizers in
section 4. Section 5 details our proposed maximum
entropy syntactic-prosodic model for prosody label-
ing. In section 6, we describe our acoustic-prosodic
model and discuss our results in section 7. We finally
conclude in section 8 with directions for future work.
2 Related work
Automatic prosody labeling has been an active re-
search topic for over a decade. Wightman and Os-
tendorf (Wightman and Ostendorf, 1994) developed
a decision-tree algorithm for labeling prosodic pat-
terns. The algorithm detected phrasal prominence
and boundary tones at the syllable level. Bulyko
and Ostendorf (Bulyko and Ostendorf, 2001) used
a prosody prediction module to synthesize natural
speech with appropriate prosody. Verbmobil (No?th
et al, 2000) incorporated prosodic labeling into a
translation framework for improved linguistic analy-
sis and speech understanding.
Prosody has typically been represented either sym-
bolically, e.g., ToBI (Silverman et al, 1992) or
parametrically, e.g., Tilt Intonation Model (Tay-
lor, 1998). Parametric approaches either restrict
the variants of prosody by definition or automati-
cally learn prosodic patterns from data (Agu?ero et
al., 2006). The BU corpus is a widely used cor-
pus with symbolic representation of prosody. The
hand-labeled ToBI annotations make this an attrac-
tive corpus to perform prosody labeling experiments.
The main drawback of this corpus is that it com-
prises only read speech. Prosody labeling on sponta-
neous speech corpora like Boston Directions corpus
(BDC), Switchboard (SWBD) has garnered atten-
tion in (Hirschberg and Nakatani, 1996; Gregory and
Altun, 2004).
Automatic prosody labeling has been achieved
through various machine learning techniques, such
as decision trees (Hirschberg, 1993; Wightman and
Ostendorf, 1994; Ma et al, 2003), rule-based sys-
tems (Shimei and McKeown, 1999), bagging and
boosting on CART (Sun, 2002), hidden markov
models (Conkie et al, 1999), neural networks
(Hasegawa-Johnson et al, 2005),maximum-entropy
models (Brenier et al, 2005) and conditional ran-
dom fields (Gregory and Altun, 2004).
Prosody labeling of the BU corpus has been re-
ported in many studies (Hirschberg, 1993; Hasegawa-
Johnson et al, 2005; Ananthakrishnan and
Narayanan, 2005). Hirschberg (Hirschberg, 1993)
used a decision-tree based system that achieved
82.4% speaker dependent accent labeling accuracy
at the word level on the BU corpus using lexical fea-
tures. (Ross and Ostendorf, 1996) also used an ap-
proach similar to (Wightman and Ostendorf, 1994)
to predict prosody for a TTS system from lexical fea-
tures. Pitch accent accuracy at the word-level was
reported to be 82.5% and syllable-level accent accu-
racy was 80.2%. (Hasegawa-Johnson et al, 2005)
proposed a neural network based syntactic-prosodic
model and a gaussian mixture model based acoustic-
prosodic model to predict accent and boundary tones
on the BU corpus that achieved 84.2% accuracy in
accent prediction and 93.0% accuracy in intonational
boundary prediction. With syntactic information
alone they achieved 82.7% and 90.1% for accent and
boundary prediction, respectively. (Ananthakrish-
nan and Narayanan, 2005) modeled the acoustic-
prosodic information using a coupled hidden markov
model that modeled the asynchrony between the
acoustic streams. The pitch accent and boundary
tone detection accuracy at the syllable level were
75% and 88% respectively. Our proposed maximum
entropy syntactic model outperforms previous work.
On the BU corpus, with syntactic information alone
we achieve pitch accent and boundary tone accuracy
of 85.2% and 91.5% on the same training and test
sets used in (Chen et al, 2004; Hasegawa-Johnson
et al, 2005). Further, the coupled model with both
acoustic and syntactic information results in accura-
cies of 86.0% and 93.1% respectively. On the BDC
corpus, we achieve pitch accent and boundary tone
accuracies of 79.8% and 90.3%.
3 Data
The BU corpus consists of broadcast news stories in-
cluding original radio broadcasts and laboratory sim-
2
BU BDC
Corpus statistics f2b f1a m1b m2b h1 h2 h3 h4
# Utterances 165 69 72 51 10 9 9 9
# words (w/o punc) 12608 3681 5058 3608 2234 4127 1456 3008
# pitch accents 6874 2099 2706 2016 1006 1573 678 1333
# boundary tones (w IP) 3916 1059 1282 1023 498 727 361 333
# boundary tones (w/o IP) 2793 684 771 652 308 428 245 216
Table 1: BU and BDC dataset used in experiments
ulations recorded from seven FM radio announcers.
The corpus is annotated with orthographic transcrip-
tion, automatically generated and hand-corrected
part-of-speech tags and automatic phone alignments.
A subset of the corpus is also hand annotated with
ToBI labels. In particular, the experiments in this
paper are carried out on 4 speakers similar to (Chen
et al, 2004), 2 male and 2 female referred to here-
after asm1b, m2b, f1a and f2b. The BDC corpus is
made up of elicited monologues produced by subjects
who were instructed to perform a series of direction-
giving tasks. Both spontaneous and read versions of
the speech are available for four speakers h1, h2, h3
and h4 with hand-annotated ToBI labels and auto-
matic phone alignments, similar to the BU corpus.
Table 1 shows some of the statistics of the speakers
in the BU and BDC corpora.
In Table 1, the pitch accent and boundary tone
statistics are obtained by decomposing the ToBI la-
bels into binary classes using the mapping shown in
Table 2.
BU Labels Intermediate Mapping Coarse Mapping
H*,!H*
L* Single Accent
*,*?,X*? accent
H+!H*,L+H*,L+!H* Bitonal Accent
L*+!H,L*+H
L-L%,!H-L%,H-L%
H-H% Final Boundary tone
L-H%
%?,X%?,%H btone
L-,H-,!H- Intermediate Phrase (IP) boundary
-X?,-?
<,>,no label none none
Table 2: ToBI label mapping used in experiments
In all our prosody labeling experiments we adopt
a leave-one-out speaker validation similar to the
method in (Hasegawa-Johnson et al, 2005) for the
four speakers with data from one speaker for testing
and from the other three for training. For the BU
corpus, f2b speaker was always used in the training
set since it contains the most data. In addition to
performing experiments on all the utterances in BU
corpus, we also perform identical experiments on the
train and test sets reported in (Chen et al, 2004)
which is referred to as Hasegawa-Johnson et al set.
4 Baseline Experiments
We present three baseline experiments. One is sim-
ply based on chance where the majority class label is
predicted. The second is a baseline only for pitch ac-
cents derived from the lexical stress obtained through
look-up from a pronunciation lexicon labeled with
stress. Finally, the third and more concrete base-
line is obtained through prosody detection in current
speech synthesis systems.
4.1 Prosody labels derived from lexical
stress
Pitch accents are usually carried by the stressed syl-
lable in a particular word. Lexicons with phonetic
transcription and lexical stress are available in many
languages. Hence, one can use these lexical stress
markers within the syllables and evaluate the corre-
lation with pitch accents. Eventhough the lexicon
has a closed vocabulary, letter-to-sound rules can be
derived from it for unseen words. For each word car-
rying a pitch accent, we find the particular syllable
where the pitch accent occurs from the manual anno-
tation. For the same syllable, we predict pitch accent
based on the presence or absence of a lexical stress
marker in the phonetic transcription. The results are
presented in Table 3.
4.2 Prosody labeling with Festival and
AT&T Natural Voices R? Speech
Synthesizer
Festival (Black et al, 1998) and AT&T Natural
Voices R? (NV) speech synthesizer (att, ) are two
publicly available speech synthesizers that have a
prosody prediction module available. We performed
automatic prosody labeling using the two synthesiz-
ers to get a baseline.
4.2.1 AT&T Natural Voices R? Speech
Synthesizer
The AT&T NV R? speech synthesizer is a half
phone speech synthesizer. The toolkit accepts
an input text utterance and predicts appropriate
ToBI pitch accent and boundary tones for each of
3
Pitch accent Boundary tone
Corpus Speaker Set Prediction Module Chance Accuracy Chance Accuracy
Lexical stress 54.33 72.64 - -
Entire Set AT&T Natural Voices 54.33 81.51 81.14 89.10
Festival 54.33 69.55 81.14 89.54
Lexical stress 56.53 74.10 - -
BU Hasegawa-Johnson et al set AT&T Natural Voices 56.53 81.73 82.88 89.67
Festival 56.53 68.65 82.88 90.21
Lexical stress 57.60 67.42 - -
BDC Entire Set AT&T Natural Voices 57.60 68.49 88.90 84.90
Festival 57.60 64.94 88.90 85.17
Table 3: Classification results of pitch accents and boundary tones (in %) using Festival and AT&T NV R? synthesizer
the selected units (in this case, a pair of phones)
from the database. We reverse mapped the se-
lected half phone units to words, thus obtaining
the ToBI labels for each word in the input utter-
ance. The toolkit uses a rule-based procedure to
predict the ToBI labels from lexical information.
The pitch accent labels predicted by the toolkit are
Laccent  {H?,L?,none} and the boundary tones
are Lbtone  {L-L%,H-H%,L-H%,none}.
4.2.2 Festival Speech Synthesizer
Festival (Black et al, 1998) is an open-source unit
selection speech synthesizer. The toolkit includes
a CART-based prediction system that can predict
ToBI pitch accents and boundary tones for the input
text utterance. The pitch accent labels predicted by
the toolkit are Laccent  {H?,L+H?, !H?,none}
and the boundary tones are
Lbtone  {L-L%,H-H%,L-H%,none}. The
prosody labeling results obtained through both the
speech synthesis engines are presented in Table
3. The chance column in Table 3 is obtained by
predicting the most frequent label in the data set.
In the next sections, we describe our proposed
maximum entropy based syntactic model and HMM
based acoustic-prosodic model for automatic prosody
labeling.
5 Syntactic-prosodic Model
We propose a maximum entropy approach to model
the words, syntactic information and the prosodic
labels as a sequence. We model the prediction prob-
lem as a classification task as follows: given a se-
quence of words wi in a sentence W = {w1, ? ? ? , wn}
and a prosodic label vocabulary (li  L), we need
to predict the best prosodic label sequence L? =
{l1, l2, ? ? ? , ln}. We approximate the conditional
probability to be within a bounded n-gram context.
Thus,
L? = argmax
L
P (L|W,T, S) (4)
? argmax
L
n?
i
p(li|w
i+k
i?k, t
i+k
i?k, s
i+k
i?k) (5)
where W = {w1, ? ? ? , wn} is the word sequence and
T = {t1, ? ? ? , tn}, S = {s1, ? ? ? , sn} are the corre-
sponding part-of-speech and additional syntactic in-
formation sequences. The variable k controls the
context.
The BU corpus is automatically labeled (and
hand-corrected) with part-of-speech (POS) tags.
The POS inventory is the same as the Penn treebank
which includes 47 POS tags: 22 open class categories,
14 closed class categories and 11 punctuation labels.
We also automatically tagged the utterances using
the AT&T POS tagger. The POS tags were mapped
to function and content word categories 1 which was
added as a discrete feature. In addition to the POS
tags, we also annotate the utterance with Supertags
(Bangalore and Joshi, 1999). Supertags encapsulate
predicate-argument information in a local structure.
They are composed with each other using substi-
tution and adjunction operations of Tree-Adjoining
Grammars (TAGs) to derive a dependency analysis
of an utterance and its predicate-argument structure.
Even though there is a potential to exploit the de-
pendency structure between supertags and prosody
labels as demonstrated in (Hirschberg and Rambow,
2001), for this paper we use only the supertag labels.
Finally, we generate one feature vector (?) for
each word in the data set (with local contextual fea-
tures). The best prosodic label sequence is then,
L? = argmax
L
n?
i
P (li|?) (6)
To estimate the conditional distribution P (li|?) we
use the general technique of choosing the maximum
entropy (maxent) distribution that estimates the av-
erage of each feature over the training data (Berger
et al, 1996). This can be written in terms of Gibbs
distribution parameterized with weights ?, where V
is the size of the prosodic label set. Hence,
P (li|?) =
e?li .?
?V
l=1 e
?li .?
(7)
1function and content word features were obtained
through a look-up table based on POS
4
k=3
Corpus Speaker Set Syntactic features accent btone
correct POS tags 84.75 91.39
Entire Set AT&T POS + supertags 84.59 91.34
BU Joint Model (w AT&T POS + supertags) 84.60 91.36
correct POS tags 85.22 91.33
Hasegawa-Johnson et al set AT&T POS + supertags 84.95 91.21
Joint Model (w AT&T POS + supertags) 84.78 91.54
BDC Entire Set AT&T POS + supertags 79.81 90.28
Joint Model (w AT&T POS + supertags) 79.57 89.76
Table 4: Classification results (%) of pitch accents and boundary tones for different syntactic representation (k = 3)
We use the machine learning toolkit LLAMA
(Haffner, 2006) to estimate the conditional distribu-
tion using maxent. LLAMA encodes multiclass max-
ent as binary maxent to increase the training speed
and to scale the method to large data sets. Each of
the V classes in the label set L is encoded as a bit
vector such that, in the vector for class i, the ith bit
is one and all other bits are zero. Finally, V one-
versus-other binary classifiers are used as follows.
P (y|?) = 1? P (y?|?) =
e?y.?
e?y.? + e?y?.?
(8)
where ?y? is the parameter vector for the anti-label y?.
To compute P (li|?), we use the class independence
assumption and require that yi = 1 and for all j 6=
i, yj = 0.
P (li|?) = P (yi|?)
V?
j 6=i
P (yj |?) (9)
5.1 Joint Modeling of Accents and
Boundary Tones
Prosodic prominence and phrasing can also be
viewed as joint events occurring simultaneously. Pre-
vious work by (Wightman and Ostendorf, 1994) sug-
gests that a joint labeling approach may be more
beneficial in prosody labeling. In this scenario,
we treat each word to have one of the four labels
li  L = {accent-btone, accent-none, none-
btone, none-none}. We trained the classifier on
the joint labels and then computed the error rates for
individual classes. The results of prosody prediction
using the set of syntactic-prosodic features for k = 3
is shown in Table 4. The joint modeling approach
provides a marginal improvement in the boundary
tone prediction but is slightly worse for pitch accent
prediction.
5.2 Supertagger performance on
Intermediate Phrase boundaries
Perceptual experiments have indicated that inter-
annotator agreement for ToBI intermediate phrase
boundaries is very low compared to full-intonational
boundaries (Syrdal and McGory, 2000). Interme-
diate phrasing is important in TTS applications to
synthesize appropriate short pauses to make the ut-
terance sound natural. The significance of syntactic
features in the boundary tone prediction prompted
us to examine the effect of predicting intermediate
phrase boundaries in isolation. It is intuitive to ex-
pect supertags to perform well in this task as they
essentially form a local dependency analysis on an
utterance and provide an encoding of the syntactic
phrasal information. We performed this task as a
three way classification where li  L = {btone, ip,
none}. The results of the classifier on IPs is shown
in Table 5.
Model Syntactic features IP accuracy
correct POS tags 83.25
k=2 (bigram context) AT&T POS tags 83.32
supertags 83.37
correct POS tags 83.30
k=3 (trigram context) AT&T POS tags 83.46
supertags 83.74
Table 5: Accuracy (in %) obtained by leave-one out
speaker validation using IPs as a separate class on
entire speaker set
6 Acoustic-prosodic model
We propose two approaches to modeling the
acoustic-prosodic features for prosody prediction.
First, we propose a maximum entropy framework
similar to the syntactic model where we quantize
the acoustic features and model them as discrete
sequences. Second, we use a more traditional ap-
proach where we train continuous observation den-
sity HMMs to represent pitch accents and bound-
ary tones. We first describe the features used in the
acoustic modeling followed by a more detailed de-
scription of the acoustic-prosodic model.
6.1 Acoustic-prosodic features
The BU corpus contains the corresponding acoustic-
prosodic feature file for each utterance. The f0, RMS
energy (e) of the utterance along with features for
5
Pitch accent Boundary tone
Corpus Speaker Set Model Acoustics Acoustics+syntax Acoustics Acoustics+syntax
Entire Set Maxent acoustic model 80.09 84.53 84.10 91.56
HMM acoustic model 70.58 85.13 71.28 92.91
BU Hasegawa-Johnson et al set Maxent acoustic model 80.12 84.84 82.70 91.76
HMM acoustic model 71.42 86.01 73.43 93.09
BDC Entire Set Maxent acoustic model 74.51 78.64 83.53 90.49
Table 6: Classification results of pitch accents and boundary tones (in %) with acoustics only and acoustics+syntax
using both our models
distinction between voiced/unvoiced segment, cross-
correlation values at estimated f0 value and ratio of
first two cross correlation values are computed over
10 msec frame intervals. In our experiments, we use
these values rather than computing them explicitly
which is straightforward with most audio toolkits.
Both the energy and the f0 levels were normalized
with speaker specific means and variances. Delta
and acceleration coefficients were also computed for
each frame. The final feature vector is 6-dimensional
comprising of f0, ?f0, ?2f0, e, ?e, ?2e per frame.
6.2 Maximum Entropy acoustic-prosodic
model
We propose a maximum entropy modeling frame-
work to model the continuous acoustic-prosodic ob-
servation sequence as a discrete sequence through
the means of quantization. The quantized acoustic
stream is then used as a feature vector and the condi-
tional probabilities are approximated by an n-gram
model. This is equivalent to reducing the vocabu-
lary of the acoustic-prosodic features and hence of-
fers better estimates of the conditional probabilities.
Such an n-gram model of quantized continuous fea-
tures is similar to representing the set of features
with a linear fit as done in the tilt intonational model
(Taylor, 1998).
The quantized acoustic-prosodic feature stream is
modeled with a maxent acoustic-prosodic model sim-
ilar to the one described in section 5. Finally, we ap-
pend the syntactic and acoustic features to model the
combined stream with the maxent acoustic-syntactic
model, where the objective criterion for maximiza-
tion is Equation (1). The pitch accent and bound-
ary tone prediction accuracies for quantization per-
formed by considering only the first decimal place
is reported in Table 6. As expected, we found the
classification accuracy to drop with increasing num-
ber of bins used in the quantization due to the small
amount of training data.
6.3 HMM acoustic-prosodic model
We also investigated the traditional HMM approach
to model the high variability exhibited by the
acoustic-prosodic features. First, we trained sepa-
rate context independent single state Gaussian mix-
ture density HMMs for pitch accents and boundary
tones in a generative framework. The label sequence
was decoded using the viterbi algorithm. Next, we
trained HMMs with 3 state left-to-right topology
with uniform segmentation. The segmentations need
to be uniform due to lack of an acoustic-prosodic
model trained on the features pertinent to our task
to obtain forced segmentation.
The final label sequence using the maximum en-
tropy syntactic-prosodic model and the HMM based
acoustic-prosodic model was obtained by combin-
ing the syntactic and acoustic probabilities shown in
Equation (3). The syntactic-prosodic maxent model
outputs a posterior probability for each class per
word. We formed a lattice out of this structure and
composed it with the lattice generated by the HMM
acoustic-prosodic model. The best path was chosen
from the composed lattice through a Viterbi search.
The acoustic-prosodic probability P (A|L,W ) was
raised by a power of ? to adjust the weighting be-
tween the acoustic and syntactic model. The value of
? was chosen as 0.008 and 0.015 for pitch accent and
boundary tone respectively, by tuning on the train-
ing set. The results of the acoustic-prosodic model
and the coupled model are shown in Table 6.
7 Discussion
The baseline experiment with lexical stress obtained
from a pronunciation lexicon for prediction of pitch
accent yields substantially higher accuracy than
chance. This could be particularly useful in resource-
limited languages where prosody labels are usually
not available but one has access to a reasonable lex-
icon with lexical stress markers. Off-the-shelf speech
synthesizers like Festival and AT&T speech synthe-
sizer perform reasonably well in pitch accent and
boundary tone prediction. AT&T speech synthesizer
performs better than Festival in pitch accent predic-
tion and the latter performs better in boundary tone
prediction. This can be attributed to better rules
in the AT&T synthesizer for pitch accent prediction.
Boundary tones are usually highly correlated with
punctuation and Festival seems to capture this well.
However, both these synthesizers generate a high de-
6
gree of false alarms.
Our syntactic-prosodic maximum entropy model
proposed in section 5 outperforms previously re-
ported results on pitch accent and boundary tone
classification. Much of the gain comes from the ro-
bustness of the maximum entropy modeling in cap-
turing the uncertainty in the classification task. Con-
sidering the inter-annotator agreement for ToBI la-
bels is only about 81% for pitch accents and 93% for
boundary tones, the maximum entropy framework is
able to capture the uncertainty present in manual an-
notation. The supertag feature offers additional dis-
criminative information over the part-of-speech tags
(also as shown by (Hirschberg and Rambow, 2001).
The maximum entropy acoustic-prosodic model
discussed in section 6.2 performs reasonably well in
isolation. This is a simple method and the quantiza-
tion resolution can be adjusted based on the amount
of data available for training. However, the model
does not perform as well when combined with the
syntactic features. We conjecture that the gener-
alization provided by the acoustic HMM model is
complementary to that provided by the maximum
entropy model, resulting in better accuracy when
combined together as compared to that of a maxent-
based acoustic and syntactic model.
The weighted maximum entropy syntactic-
prosodic model and HMM acoustic-prosodic model
performs the best in pitch accent and boundary tone
classification. The classification accuracies are as
good as the inter-annotator agreement for the ToBI
labels. Our HMM acoustic-prosodic model is a gen-
erative model and does not assume the knowledge
of word boundaries in predicting the prosodic labels
as in most approaches (Hirschberg, 1993; Wightman
and Ostendorf, 1994; Hasegawa-Johnson et al,
2005). This makes it possible to have true parallel
prosody prediction during speech recognition. The
weighted approach also offers flexibility in prosody
labeling for either speech synthesis or speech recog-
nition. While the syntactic-prosodic model would
be more discriminative for speech synthesis, the
acoustic-prosodic model is more appropriate for
speech recognition.
8 Conclusions and Future Work
In this paper, we described a maximum entropy
modeling framework for automatic prosody label-
ing. We presented two schemes for prosody label-
ing that utilize the acoustic and syntactic informa-
tion from the input utterance, a maximum entropy
model that models the acoustic-syntactic informa-
tion as a sequence and the other that combines the
maximum entropy syntactic-prosodic model and a
HMM based acoustic-prosodic model. We also used
enriched syntactic information in the form of su-
pertags in addition to POS tags. The supertags
provide an improvement in both the pitch accent
and boundary tone classification. Especially, in the
case where the input utterance is automatically POS
tagged (and not hand-corrected), supertags provide
a marginal but definite improvement in prosody la-
beling. The maximum entropy syntactic-prosodic
model alone resulted in pitch accent and bound-
ary tone accuracies of 85.2% and 91.5% on training
and test sets identical to (Chen et al, 2004). As
far as we know, these are the best results on the
BU corpus using syntactic information alone and a
train-test split that does not contain the same speak-
ers. The acoustic-syntactic maximum entropy model
performs better than its syntactic-prosodic counter-
part for the boundary tone case but is slightly worse
for pitch accent scenario partly due to the approx-
imation involved in quantization. But these results
are still better than the baseline results from out-
of-the-box speech synthesizers. Finally, our com-
bined maximum entropy syntactic-prosodic model
and HMM acoustic-prosodic model performs the best
with pitch accent and boundary tone labeling accu-
racies of 86.0% and 93.1% respectively.
As a continuation of our work, we are incorpo-
rating our automatic prosody labeler in a speech-
to-speech translation framework. Typically, state-
of-the-art speech translation systems have a source
language recognizer followed by a machine transla-
tion system. The translated text is then synthesized
in the target language with prosody predicted from
text. In this process, some of the critical prosodic
information present in the source data is lost during
translation. With reliable prosody labeling in the
source language, one can transfer the prosody to the
target language (this is feasible for languages with
phrase level correspondence). The prosody labels by
themselves may or may not improve the translation
accuracy but they provide a framework where one
can obtain prosody labels in the target language from
the speech signal rather than depending on a lexical
prosody prediction module in the target language.
Acknowledgements
We would like to thank Vincent Goffin, Stephan
Kanthak, Patrick Haffner, Enrico Bocchieri for their
support with acoustic modeling tools. We are also
thankful to Alistair Conkie, Yeon-Jun Kim, Ann
Syrdal and Julia Hirschberg for their help and guid-
ance with the synthesis components and ToBI label-
ing standard.
References
P. D. Agu?ero, J. Adell, and A. Bonafonte. 2006.
Prosody generation for speech-to-speech transla-
7
tion. In Proceedings of ICASSP, Toulouse, France,
May.
S. Ananthakrishnan and S. Narayanan. 2005. An au-
tomatic prosody recognizer using a coupled multi-
stream acoustic model and a syntactic-prosodic
language model. In In Proceedings of ICASSP,
Philadelphia, PA, March.
AT&T Natural Voices speech synthesizer.
http://www.naturalvoices.att.com.
S. Bangalore and A. K. Joshi. 1999. Supertagging:
An approach to almost parsing. Computational
Linguistics, 25(2), June.
A. Berger, S. D. Pietra, and V. D. Pietra. 1996. A
maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39?
71.
A. W. Black, P. Taylor, and R. Caley.
1998. The Festival speech synthesis system.
http://festvox.org/festival.
J. M. Brenier, D. Cer, and D. Jurafsky. 2005. The
detection of emphatic words using acoustic and
lexical features. In In Proceedings of Eurospeech.
I. Bulyko and M. Ostendorf. 2001. Joint prosody
prediction and unit selection for concatenative
speech synthesis. In Proc. of ICASSP.
K. Chen, M. Hasegawa-Johnson, and A. Cohen.
2004. An automatic prosody labeling system using
ANN-based syntactic-prosodic model and GMM-
based acoustic-prosodic model. In Proceedings of
ICASSP.
A. Conkie, G. Riccardi, and R. C. Rose. 1999.
Prosody recognition from speech utterances using
acoustic and linguistic based models of prosodic
events. In Proc. Eurospeech, pages 523?526, Bu-
dapest, Hungary.
M. Gregory and Y. Altun. 2004. Using conditional
random fields to predict pitch accent in conver-
sational speech. In 42nd Annual Meeting of the
Association for Computational Linguistics (ACL).
P. Haffner. 2006. Scaling large margin classifiers for
spoken language understanding. Speech Commu-
nication, 48(iv):239?261.
M. Hasegawa-Johnson, K. Chen, J. Cole, S. Borys,
S. Kim, A. Cohen, T. Zhang, J. Choi, H. Kim,
T. Yoon, and S. Chavara. 2005. Simultaneous
recognition of words and prosody in the boston
university radio speech corpus. Speech Communi-
cation, 46:418?439.
J. Hirschberg and C. Nakatani. 1996. A prosodic
analysis of discourse segments in direction-giving
monologues. In Proceedings of the 34th confer-
ence on Association for Computational Linguis-
tics, pages 286?293.
J. Hirschberg and O. Rambow. 2001. Learning
prosodic features using a tree representation. In
Proceedings of Eurospeech, pages 1175?1180, Aal-
borg.
J. Hirschberg. 1993. Pitch accent in context: Pre-
dicting intonational prominence from text. Artifi-
cial Intelligence, 63(1-2).
I. Lehiste. 1970. Suprasegmentals. MIT Press, Cam-
bridge, MA.
X. Ma, W. Zhang, Q. Shi, W. Zhu, and L. Shen.
2003. Automatic prosody labeling using both
text and acoustic information. In Proceedings of
ICASSP, volume 1, pages 516?519, April.
E. No?th, A. Batliner, A. Kie?ling, R. Kompe, and
H. Niemann. 2000. VERBMOBIL: The use of
prosody in the linguistic components of a speech
understanding system. IEEE Transactions on
Speech and Audio processing, 8(5):519?532.
M. Ostendorf, P. J. Price, and S. Shattuck-Hufnagel.
1995. The Boston University Radio News Corpus.
Technical Report ECS-95-001, Boston University,
March.
K. Ross and M. Ostendorf. 1996. Prediction of ab-
stract prosodic labels for speech synthesis. Com-
puter Speech and Language, 10:155?185, Oct.
P. Shimei and K. McKeown. 1999. Word infor-
mativeness and automatic pitch accent modeling.
In In Proceedings of EMNLP/VLC, College Park,
Maryland.
K. Silverman, M. Beckman, J. Pitrelli, M. Osten-
dorf, C. Wightman, P. Price, J. Pierrehumbert,
and J. Hirschberg. 1992. ToBI: A standard for la-
beling English prosody. In Proceedings of ICSLP,
pages 867?870.
X. Sun. 2002. Pitch accent prediction using ensem-
ble machine learning. In Proc. of ICSLP.
A. K. Syrdal and J. McGory. 2000. Inter-transcriber
reliability of tobi prosodic labeling. In Proc. IC-
SLP, pages 235?238, Beijing, China.
P. Taylor. 1998. The tilt intonation model. In Proc.
ICSLP, volume 4, pages 1383?1386.
C. W. Wightman and M. Ostendorf. 1994. Auto-
matic labeling of prosodic patterns. IEEE Trans-
actions on Speech and Audio Processing, 2(3):469?
481.
8
