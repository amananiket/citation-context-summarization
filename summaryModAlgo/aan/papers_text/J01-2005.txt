Squibs and Discussions 
Nonminimal Derivations in Unification-based 
Parsing 
Noriko Tomuro* 
DePaul University 
Steven L. Lytinen t 
DePaul University 
Shieber's abstract parsing algorithm (Shieber 1992)for unification grammars is an extension of 
Earley's algorithm (Earley 1970)for context-free grammars to feature structures. In this paper, 
we show that, under certain conditions, Shieber ' salgorithm produces what we call a nonminimal 
derivation: aparse tree which contains additional features that are not in the licensing productions. 
While Shieber's definition of parse tree allows for such nonminimal derivations, we claim that they 
should be viewed as invalid. We describe the sources of the nonminimal derivation problem, and 
propose aprecise definition of minimal parse tree, as well as a modification to Shieber's algorithm 
which ensures minimality, although at some computational cost. 
1. Introduction 
Unification grammar is a term often used to describe a family of feature-based gram- 
mar formalisms, including GPSG (Gazdar et al 1985), PATR-II (Shieber 1986), DCG 
(Pereira and Warren 1980), and HPSG (Pollard and Sag 1994). In an effort to formalize 
the common elements of unification-style grammars, Shieber (1992) developed a logic 
for describing them, and used this logic to define an abstract parsing algorithm. The 
algorithm uses the same set of operations as Earley's (1970) algorithm for context-free 
grammars, but modified for unification grammars. 
In this paper, we show that, under certain conditions, Shieber's algorithm produces 
unintended, spurious parses in addition to the intended ones. We call these spurious 
parses nonminimal derivations (or nonminimal parse trees), because they contain 
extra features which are not in the productions that license the parse, aWe claim that 
such nonminimal derivations are invalid. The basis of our claim is that the unifica- 
tion operation as set union preserves minimality; thus any correct unification-based 
parsing algorithm should produce parses that contain all and only features from the 
licensing productions (i.e., minimal derivations or minimal parse trees). Nonminimal 
derivations are also undesirable in practice because, given a parse tree, we cannot ell 
whether a particular feature should be in the model or not unless we reconstruct the 
whole tree. 
Despite the nonminimal derivations, Shieber (1992) proved the correctness of his 
algorithm. As it turned out, his definition of parse tree, which his proof relied on, was 
* School of Computer  Science, Telecommunications and Information Systems, Chicago, IL 60604. E-mail: 
tomuro@cs.depaul.edu 
t School of Computer  Science, Telecommunications and Information Systems, Chicago, IL 60604. E-maih 
lytinen@cs.depaul.edu 
1 In this paper, we use "nonminimal  derivations" synonymous ly  with "nonminimal  parses". Normal ly 
the notions of derivation and parse tree are different. However, in this paper we focus on parse trees as 
the final result of derivation, thus we mean that a derivation is nonminimal  when its result is a 
nonminimal  parse, in contrast o a minimal  derivation which produces a minimal  parse. Unfortunately, 
formal definitions of min imal  and nonmin imal  derivations are outside the scope of this short paper; 
interested readers are encouraged to read Tomuro (1999). 
(~) 2001 Association for Computat ional  Linguistics 
Computational Linguistics Volume 27, Number 2 
((cat) - S 
/ (1 cat) =" NP 
.J (2 cat) -- VP 
P0 = (2, ~0 : \] (head) -- (2 head) 
/ / head subj) - (1 head} 
~, (head agr> - (1 head agr> 
((cat) - VP 
J <1 cat/----" V 
P2 : (1, q)2 : ~ (head) -- (1 head} > 
I, (head type) - intrans ,/ 
Figure 1 
Examples of productions. 
((cat) -- NP 
pl = ("John",~l : ~ (head agr pers) - 3rd } > 
~, (head agr num)-  singJ 
((cat) -- V "\] 
~l (head agr pers) - 3rd / P3 = ("sleeps", ,I~ 3 : \] (head agr num) - sing ) 
I, (head tense} - pres 
not constrain ing enough to disal low nonmin imal  derivat ions.  To solve this twofo ld  
prob lem,  we propose  an alternate def init ion of min ima l  parse tree for unif icat ion gram-  
mars,  and present  a modi f icat ion to Shieber 's  a lgor i thm which ensures minimality.  
It is impor tant  to note that the same spur ious  parses also occur in context-free 
parsing,  specif ically in Ear ley's  algor i thm. However ,  since the only in format ion a con- 
st ituent carries in context-free grammar  is the grammar  symbol ,  the spur ious  der iva-  
t ions only produce  exactly the same results as the normal  ones. When the a lgor i thm 
is extended to unif icat ion grammar ,  however ,  these spur ious  parses are a prob lem.  
2. Unification Grammar and Parse Trees 
Shieber (1992) defines a unif icat ion grammar  as a 3-tuple (G, P, p0), where  ~ is the 
vocabu lary  of the grammar ,  P is the set of product ions ,  and P0 E P is the start pro-  
duct ion.  G contains L, a set of labels  (feature names);  C, a set of constants (feature 
values); and W, a set of terminals. There are two k inds of product ions  in P: phrasal 
and lexical. A phrasa l  product ion  is a 2-tuple (a, ~),  where  a is the arity of the rule (the 
number  of r ight-hand-s ide \[RHS\] constituents),  and ~ is a logical formula.  Typically, 
q~ is a conjunct ion of equat ions of the form pl - p2 or pl -" c, where  pl, p2 E L* are 
paths,  and c E C. In an equat ion,  any  path  wh ich  begins wi th  an integer i (1 < i < a) 
represents the ith RHS const i tuent of the rule. 2 A lexical p roduct ion  is a 2-tuple (w, ~), 
where  w E W and q~ is the same as above,  except that there are no RHS constituents. 
F igure 1 shows some example  phrasa l  and lexical product ions  (P0 cor responds  to the 
context-free rule S --+ NP  VP and is the start product ion) .  Then a mode l  M relates to 
a formula  q~ by  a satisfaction relat ion ~ as usual  (M ~ ~), and when q~ is the formula  
in a product ion  p = (a, ~),  p is said to l icense M. 
Based on the logic above,  Shieber defines a parse tree and the language of a 
g rammar  expressed in his formal ism. To define a val id parse tree, he first def ines the 
set of possible parse trees I1 = Ui>_0 Hi for a g iven grammar  G, where  each Eli is def ined 
as follows: 
Definition 
A parse tree r is a mode l  that is a member  of the infinite un ion of sets of bounded-  
depth  parse trees FI = Ui_>0 I1i, where  each IIi is def ined as: 
2 Shieber (1992) also uses a path that begins with 0 for the left-hand-side (LHS) constituent of a rule. In 
this paper, we omit the 0 arcs and place the features of the LHS constituent directly at the root. This 
change does not affect the formalism for the purpose of this paper. 
278 
Tomuro and Lytinen Nonminimal Derivations 
. 
. 
rio is the set of models 7- for which there is a lexical production 
p = <w, q)) E G such that 7- ~ 4< 
I I i ( i  > 0) is the set of models 7- for which there is a phrasal production 
p = (a, q~) C G such that 7- ~ ~ and, for all 1 < i < a, 7-/{i) is defined and 
7-/<i} C Uj<iIIy. 
In the second condition, the extraction operator, denoted by / ,  retrieves the feature 
structure found at the end of a particular path; so for instance 7-/<1) retrieves the first 
subconstituent on the RHS of the production that licenses 7-. In the definition above, 
II0 contains all models that satisfy any lexical production in the grammar, while Hi 
contains all models that satisfy a phrasal production, and whose subconstituents are 
all i n  UjGi I\]j. 
To specify what constitutes a valid parse for a particular sentence, the next step is 
to define the yield of a parse tree. It is defined recursively as follows: if 7- is licensed by 
some lexical production p = {w, q~/, then the yield of 7- is w; or if 7- is licensed by some 
phrasal production {a, q~} and O~ 1 . . . . .  (X a are the yields of 7-/(1) . . . . .  7-/<a) respectively, 
then the yield of 7- is ~1 ...  %. 
Finally, Shieber defines a valid parse tree 7- c II for sentence Wl . . .  wn as follows: 
o 
2. 
The yield of 7- is Wl . . .  Wn 
7- is licensed by the start production po 
Notice that this definition allows extra features in a parse tree, because a parse tree 
7- is defined by the satisfaction relation (7- ~ ~), which allows the existence of features 
in the model that are not in the licensing production's formula. Given this definition, 
for any valid parse tree 7-, we can construct another parse tree 7-' by simply adding an 
arbitrary (nonnumeric) feature to any node in 7-. Such a parse tree T' is nonminimal 
because extra features are nonminimal with respect o the minimal features in the 
licensing productions. We will return to the issue of minimal and nonminimal parse 
trees in Section 4. 
3. The Abstract Parsing Algorithm 
Based on the logic described above, Shieber defines an abstract parsing algorithm as a 
set of four logical deduction rules. Each rule derives a new item, from previous items 
and/or  productions in the grammar. An item is a 5-tuple {i,j, p, M, d), where i and j are 
indices into the sentence and specify which words in the sentence have been used to 
construct the item; p is the production used to construct the item; M is a model; and d 
is the position of the "dot"; i.e., how many subconstituents in p have been completed 
so far. 
The logical rules of the abstract algorithm are shown in Figure 2. The Initial Item 
rule produces the first item, and is constructed from the start production P0. It spans 
none of the input (i and j are both 0), and its model is the minimal model (ram) of P0. 
The Prediction rule is essentially the top-down rewriting of the expectation (a 
subconstituent just after the dot) in a prior item. In this rule, the extraction of M/(d  + 
1 / retrieves the d + 1st submodel in M (i.e., expectation). The function p, which is 
left underspecified as a parameter in the abstract algorithm, filters out some features 
predefined in the various instantiations of the algorithm. Here, it is applied to the 
expectation, by which it effectively controls the top-down predictive power of the 
279 
Computational Linguistics Volume 27, Number 2 
INITIAL ITEM: {O,O, po, mm(~o),O) 
PREDICTION: 
SCANNING: 
li, j,p = la, ~l,M,d) 
(j,j, p', p(M/(d+l)) t3 mm(~'), 0) ' where d K a and p' = (a',O') ? P 
(i,j,p = (a, ~},M,d} 
{i,j+lip, M t_l (mm(~2') \ {d+l ) ) ,d+l}  ' where  d < a and  (wj+l, O'} ? P 
COMPLETION: li'j'P = la' ~l 'M'd) (j,k,p' = (a',/I~'),M',a' / where d < a 
I {i, kip, M El (M' \ {d+l) ,d+l) 
Figure 2 
Shieber's parsing operations. 
I0 = (O,O, po, mm(420),O) 
11 = (O, 1,po, Ml,1) 
12 = (1,1,p2,M2,0 I 
I3 = (1,2,p2,M3,1) 
I4 = (0, 2, p0, M4, 2) 
ag 5 ?yP? 
pers~ -n~ p mtrans 
3rd sing 
Figure 3 
Items produced in the parse of John sleeps, and the final parse. 
algorithm and provides flexibility to the instantiated algorithms. Then the expectation 
is unified with a production (~'), which can consistently rewrite it. By this operation, 
some features in the expectation may be propagated own in the production. 
The remaining two rules advance the dot in a prior item, by unifying the sub- 
constituent to the right of the dot with either a lexical item from the input string (the 
Scanning rule) or some other completed higher-level item (the Completion rule). Both 
rules perform the correct unification by utilizing the embedding operator (signified 
by \), which places a model M under a path p (M\p). 
We illustrate these operators with a simple step-by-step example parse. Consider 
the grammar that consists of the rules presented in Figure 1. Using this grammar, 
Figure 3 shows the parse of the sentence John sleeps. First, the Initial Item operator 
is applied, producing item I0, whose model is mm(~o). Next, the Scanning operator 
scans the word John, producing 11. The Prediction operator then produces 12. Next, 
the word sleeps is scanned (since the first subconstituent of the model in 12 is a V), 
producing 13. Finally, since the item in 13 is complete (d = 1, the arity of production 
p2), Completion is applied to items 11 and/3, producing 14. Model M4 is the final parse 
of the sentence. 
4. Nonminimal Derivations 
In Section 2, we noted that Shieber's definition of parse trees allows them to be non- 
minimal. We consider these to be invalid based on a principle that, since the unification 
operation as set union preserves minimality (as proved in Shieber, \[1992\]), repeated 
applications of unification using licensing productions hould result in parses that 
contain features only from those productions and nothing more. In this section, we 
280 
Tomuro and Lytinen Nonminimal Derivations 
((cat) - VP 
I (1 cat) -- VP 
p4 = (2,~4: { (2 cat} -- ADV } 
| (head) - (1 head) / 
( (head modified) - true ) 
Figure 4 
A phrasal production that results in a nonminimal derivation. 
I~ = (1,1,p4,M~, 0} 
/~' = (1,1, p2,M~r, 0) 
I~ = (1,2,p2,M~, 1) 
I~ = {0, 2, p0, M~, 2} 
M4 ~ ." 
cat~-43 
c S a l t  
NP nea~/ l~Vub~V t 
ag~.. 7se.t~pe~modified 
per~ n~ lntrans t}ue 
3rd sing 
Figure 5 
Nonminimal derivation of John sleeps. 
formally define minimal and nonminimal parse trees, and show an example in which 
nonminimal parse trees are produced by Shieber's algorithm. 
Our definition of minimal parse tree is to a large extent similar to Shieber's def- 
inition, but to ensure minimality, our definition uses the equality relation instead of 
D, and inductively specifies a minimal parse tree bottom-up. 
Definition 
Given a grammar G, a minimal parse tree r admitted by G is a model that is a member 
of the infinite union of sets of bounded-depth parse trees 11' = Oi>0 IIl, where each 
171 is defined as: 
. 
2. 
For each lexical production p = (w, ~b) E G, mm(~) E 11'o. 
For each phrasal production p = (a, ~} E G, let rl . . . . .  ra E Uj<i I1;. If 
r = mm(~) l i t1\(1) t3.. .  I lr l \(a}, then r E 1I;. 
It is obvious that 1I' is a subset of 17 in Shieber's definition. Then, a nonminimal parse 
tree is defined as a model that is a member of the difference of the two sets (II - 1I'). 3 
Here is a simple example in which a nonminimal parse is produced in Shieber's 
algorithm. Say that we add the production in Figure 4 to the grammar in the previous 
section. The intent of this production is to mark the verb with the feature modified if an 
adverb follows. Using this grammar, Shieber's algorithm will produce a nonminimal 
parse for the sentence John sleeps, in addition to the minimal parse shown in the 
previous section. 4 The nonminimal parse, shown in Figure 5, arises as follows: after 
scanning John, Prediction can produce items I~ and I~', first using production p4 (thus 
inserting /head modified} - true into the model), and then P2. Scanning the word 
3 Note that using subsumption (which we will discuss in Section 5) here does not work, for instance by 
saying "a model r"  is a nonminimal parse tree if r "  E 17 and there exists r '  E II such that r '  _< r"",  
because some r" 's  are minimal. See the example in Section 5. 
4 Here, we are assuming that the filtering function/9 is the identity function. 
281 
Computational Linguistics Volume 27, Number 2 
sleeps then produces I~ from I~ I. Completion then can be applied directly to 11 and 11 by 
skipping a completion using I~ and I~, thereby producing item I~. The feature modified 
remains in I~, even though an adverb was never encountered in the sentence. The 
final parse M~, shown in Figure 5, is clearly nonminimal according to our definition 
because of this feature. 
Note that the example grammar can be changed to prevent he nonminimal parse, 
by moving the feature modified off of the head path in ff~4 (i.e., (modified / - true 
instead of (head modified / - true), sHowever, the point of the example is not to argue 
whether or not well-designed grammars will produce erroneous parses. A formally 
defined parser (see the discussion below) should in principle produce correct parses 
regardless of the grammar used; otherwise, the grammar formalism (i.e., Shieber's logic 
for unification grammars) must be revised and properly constrained to allow only the 
kinds of productions with which the parser produces correct results. 
In general, nonminimal derivations may arise whenever two or more predictions 
that are not mutually exclusive can be produced at the same point in the sentence; 
i.e., two prediction items (i, i, p, M, 0 / and (i, i, p', M ~, 0 / are produced such that M 
M / and M and M ~ are unifiable. In the example, items 12 = (1,1, p2, M2, 0/ and I~ -- 
(1,1, P4, M~, 0) (as well as I2 and I~ ~ = (1,1, p2, M~ ~, 0/) are two such items. Since the two 
predictions did not have any conflicting features from the beginning, a situation may 
occur where a completion generated from one prediction can fill the other prediction 
without causing conflict. When this happens, features that were in the other prediction 
but not the original one become nonminimal in the resulting model. 
As to what causes nonminimal situations, we speculate that there are a number 
of possibilRies. First, nonminimal derivations occur when a prediction is filled by a 
complete item that was not generated from the prediction. This mismatch will not 
happen if parsing is done in one direction only (e.g. purely top-down or bottom-up 
parsing). Thus, the mixed-direction parsing strategy is a contributing factor. 
Second, wrong complete items are retrieved because Shieber's item-based algo- 
rithm makes all partial results available during parsing, as if they are kept in a global 
structure (such as a chart in chart parsing). But if the accessibility of items were some- 
how restricted, prediction-completion mismatch would not happen. In this respect, 
other chart-based algorithms for unification grammars which adopt mixed-direction 
parsing strategy, including head-corner parsing (van Noord 1997) and left-corner pars- 
ing (Alshawi 1992), are subject o the same problem. 
Third, extra features can only appear when the grammar contains rules which 
interact in a certain way (such as rules P2 and P4 above). If the grammar contained 
no such rules, or if p (the filtering function applied in Prediction) filtered out those 
features, even the prediction-completion mismatch would not produce nonminimal 
derivations. 
As we stated in the beginning of this section, we consider nonminimal parses to 
be invalid on the basis of minimality. It then immediately follows that any parsing 
algorithm that produces nonminimal parses is considered to be unsound; in particular, 
Shieber's algorithm is unsound. However, since nonminimal parse trees have the same 
yield as their minimal counterparts, his algorithm does indeed recognize xactly the 
language of a given grammar. So, Shieber's algorithm is sound as a recognizer, 6 but 
not as a transducer or parser (as in van Noord, \[1997\]) where the correctness of output 
models (i.e., parse trees) is critical. In other words, Shieber's algorithm is correct up to 
5 Note that adding (head modified) -- false to ~2 (VP --* V) or ~3 (sleeps) isnot feasible, because they 
cannot specify the modified feature at their level, 
6 In fact, Shieber hints at this: "The process of parsing (more properly, recognition)..." (Shieber 1992, 78). 
282 
Tomuro and Lytinen Nonminimal Derivations 
licensing, but incorrect on the basis of a stronger criteria of minimality. Thus, to guar-  
antee correctness based on minimality, we need another algorithm; such an a lgor i thm 
is exactly the solution to the nonmin imal  der ivat ion problem. 
5. Practical Techniques 
Before present ing our solution to the nonmin imal  der ivat ion problem, we discuss 
several possible practical techniques to get around the prob lem in implemented sys- 
tems. These are known techniques, which have been appl ied to solve other problems 
in unif ication-based systems. However ,  most  of them only offer partial solutions to 
the nonmin imal  derivat ion problem. First, whenever  Shieber's a lgor i thm produces a 
nonmin imal  derivation, it also produces a corresponding minimal  der ivat ion (Tomuro 
1999). Thus, one possible solution is to use subsumpt ion  to discard items that are more 
specific than any other items that are produced.  Subsumpt ion  has often been used in 
unif ication-based systems to pack items or models  (e.g., A lshawi  1992). However,  
s imple subsumpt ion  may filter out val id parses for some grammars,  thus sacrificing 
completeness. 7 
Another  possibil ity is to filter out problematic features in the Prediction step by 
using the funct ion p. However,  automatic detection of such features (i.e., automatic 
derivat ion of p) is undecidable for the same reason as the prediction nontermination 
problem (caused by left recursion) for unif ication grammars  (Shieber 1985). Manual  
detection is also problematic:  when a grammar  is large, part icular ly if semantic fea- 
tures are included, complete detection is nearly impossible. As for the techniques 
developed so far which (partially) solve predict ion nonterminat ion (e.g., Shieber 1985; 
Haas 1989; Samuelsson 1993), they do not apply  to nonmin imal  derivations because 
nonmin imal  derivations may arise wi thout  left recursion or recursion in genera l  s One 
way  is to define p to filter out all features except the context-free backbone of predic- 
tions. However ,  this severely restricts the range of possible instantiations of Shieber's 
algorithm. 9 
A third possibil ity is to manual ly  fix the grammar  so that nonmin imal  derivations 
do not occur, as we noted in Section 4. However ,  this approach is problematic for the 
same reason as the manua l  der ivat ion of p ment ioned above. 
6. Modified Algorithm 
Finally, we propose an algor i thm that does not produce nonmin imal  derivations. It is a 
modif icat ion of Shieber's a lgor i thm that incorporates parent pointers. Figure 6 shows 
7 For example, when there are two predictions M1 and M2 for category C and a production p where 
M1 : {<cat> -- C, <x> - a}, M2 : {<cat> - C, <y> - b}, and p = <1, {<cat> - C, <1 cat> "- D, <x> - a}> 
respectively, the resulting model M2 ~ = {<cat> - C, <1 cat> - D, <x> -- a, <y> -- b} will have strictly more 
information than the other esulting model MI' = {<cat> ~ C, <1 cat> - D, <x> - a}, although both 
models are minimal. 
8 We do not show any particular example here, but if we change the left-recursive VP rule in the earlier 
example to a non-left-recursive rule, for instance VP --* VP2 ADV, and add some rules, a nonrninimal 
parse will indeed arise. 
Note also that some (but not all) cases of prediction ontermination will produce nonminimal 
derivations. Those cases occur when there is a prediction for a category, and repeated applications of 
some left-recursive rule(s) generate predictions for the same category that are not mutually exclusive to 
the original prediction or each other. 
9 In head-corner parsing, Sikkel (1997) proposes the use of transitive features: features that propagate 
only through ead arcs. However, this method oes not solve nonminimal derivations either, because 
problematic features may be subfeatures of a head (such as the example case shown earlier), which will 
not be filtered. 
283 
Computational Linguistics Volume 27, Number 2 
INITIAL ITEM: 
PREDICTION: 
(id, nil, (O,O, po, mm( ~o),O) ' where id is a new symbol 
(id, pid, (i,j,p = (a, ~),M,d) ) 
(id', id, (j,j, p', p(M/ (d+l) ) U mm(~I,'), 0))' 
where id I is a new symbol, and d ( a and pl = (ar,~t) C P 
SCANNING: 
COMPLETION: 
(id, pid, (i,j,p = (a, ~),M,d) ) 
(id, pid, (i,j+l,p,M U mm( ~') \ (d+l),d+l)) ' where d< a and (wj+D ~') E P 
(id, pid,(i,j,p,M,d)) (id",id,(j,k,p',M',a')) where d < a 
(ia, pie, (i,k,p, UU (U' \ (d+l)),d+l)) ' 
Figure 6 
Shieber's parsing operations modified. 
the modified algorithm. In the figure, an item is represented by a nested 3-tuple, where 
the first argument is the self index, the second is the parent index/pointer, and the 
third is the old 5-tuple used in Shieber's original algorithm. A parent pointer, then, 
is set in Prediction--the r sulting item has the index of the antecedent item (id) as 
its parent. By generating a new symbol for the self index in every Prediction item 
(id'), parent pointers in those items are threaded to form a prediction path. Then in 
Completion, the parent pointer is used to restrict he antecedent items: the complete 
item (on the right) must have the prior expectation (on the left) as its parent (id), 
thereby ensuring a prediction path to be precisely restored. 
While this modified algorithm offers a complete solution on the level of logic, it 
has some undesirable implications in implemented systems. The most prominent one 
is that the parent pointer scheme makes implementation f memoizat ion rather diffi- 
cult. Normally, memoization is used to avoid storing duplicate items that are identical; 
however, in the modified algorithm, many items that are otherwise identical will have 
different parent pointers, thereby changing the polynomial time algorithm (O(n3); Ear- 
ley \[1970\]) to an exponential one. To avoid computational inefficiency, a way must be 
devised for items that are identical except for parent poInters to share information, 
especially models, and thus avoid the expense of duplicate identical unification opera- 
tions. One possibility is to represent the 5-tuple from Shieber's original algorithm by a 
separate structure and have an index to it in the new 3-tuple item. This way, not only 
can the items be shared, they can still be memoized in the usual way as well. Another 
possibility is to adopt an efficiency technique along the line of selective memoization 
(van Noord 1997). Implementation a d empirical analysis is our future research. 
Whatever the practical performance will turn out to be, it is important to note 
that the proposed algorithm is a formal solution that guarantees minimality for any 
grammar defined in Shieber's logic. Moreover the algorithm preserves the same gen- 
erality and flexibility as Shieber's: a mixed top-down, bottom-up arsing with the 
filtering function p to allow various instantiations of the algorithm to characterize 
their algorithms. 
References 
Alshawi, H., editor. 1992. The Core Language 
Engine. MIT Press. 
Earley, J. 1970. An efficient context-free 
parsing algorithm. Communications ofthe 
ACM, 13(2). 
Gazdar, G., E. Klein, G. Pullum, and I. Sag. 
1985. Generalized Phrase Structure Grammar. 
Blackwell Publishing. 
Haas, A. 1989. A parsing algorithm for 
unification grammar. Computational 
Linguistics, 15(4):219-232. 
284 
Tomuro and Lytinen Nonminimal Derivations 
Pereira, F. and D. Warren. 1980. Definite 
clause grammars for language analysis. 
Arti~'cial Intelligence, 13:231-278. 
Pollard, C. and I. Sag. 1994. Head-driven 
Phrase Structure Grammar. CSLI. University 
of Chicago Press. 
Samuelsson, C. 1993. Avoiding 
non-termination in unification grammars. 
In Natural Language Understanding and Logic 
Programming IV. 
Shieber, S. 1985. Using restriction to extend 
parsing algorithms for complex-feature- 
based formalisms. In Proceedings ofthe 23rd 
Annual Meeting, Association for 
Computational Linguistics. 
Shieber, S. 1986. An Introduction to 
UniX'cation-Based Approaches to Grammar. 
CSLI. University of Chicago Press. 
Shieber, S. 1992. Constraint-based Grammar 
Formalisms. MIT Press. 
Sikkel, K. 1997. Parsing Schemata. 
Springer?Verlag. 
Tomuro, N. 1999. Left-Corner Parsing 
Algorithm for UniX'cation Grammars. Ph.D. 
thesis, DePaul University. 
van Noord, G. 1997. An efficient 
implementation f the head-corner parser. 
Computational Linguistics, 23(3):425-456. 
285 
