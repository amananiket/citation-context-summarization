Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1348?1357,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Predicting Subjectivity in Multimodal Conversations
Gabriel Murray and Giuseppe Carenini
University of British Columbia
Vancouver, Canada
(gabrielm, carenini)@cs.ubc.ca
Abstract
In this research we aim to detect sub-
jective sentences in multimodal conversa-
tions. We introduce a novel technique
wherein subjective patterns are learned
from both labeled and unlabeled data, us-
ing n-gram word sequences with vary-
ing levels of lexical instantiation. Ap-
plying this technique to meeting speech
and email conversations, we gain signifi-
cant improvement over state-of-the-art ap-
proaches. Furthermore, we show that cou-
pling the pattern-based approach with fea-
tures that capture characteristics of gen-
eral conversation structure yields addi-
tional improvement.
1 Introduction
Conversations are rich in subjectivity. Conversa-
tion participants agree and disagree with one other,
argue for and against various proposals, and gen-
erally take turns expressing their private states.
Being able to separate these subjective utterances
from more objective utterances would greatly fa-
cilitate the analysis, mining and summarization of
a large number of conversations.
Two of the most prevalent conversational me-
dia are meetings and emails. Face-to-face meet-
ings enable numerous people to exchange a large
amount of information and opinions in a short pe-
riod of time, while emails allow for concise ex-
changes between potentially far-flung participants.
Meetings and emails can also feed into one an-
other, with face-to-face meetings occurring at reg-
ular intervals and emails continuing the conver-
sations in the interim. This poses several inter-
esting questions, such as whether subjective utter-
ances are more or less likely to be found in email
exchanges compared with meetings, and whether
the ratios of positive and negative subjective utter-
ances differ between the two modalities.
In this paper we describe a novel approach for
predicting subjectivity, and test it in two sets of
experiments on meetings and emails. Our ap-
proach combines a new general purpose method
for learning subjective patterns, with features that
capture basic characteristics of conversation struc-
ture across modalities. The subjective patterns are
essentially n-gram sequences with varying levels
of lexical instantiation, and we demonstrate how
they can be learned from both labeled and un-
labeled data. The conversation features capture
structural characteristics of multimodal conversa-
tions as well as participant information.
We test our approach in two sets of experi-
ments. The goal of the first set of experiments is to
discriminate subjective from non-subjective utter-
ances, comparing the novel approach to existing
state-of-the-art techniques. In the second set of
experiments, the goal is to discriminate positive-
subjective and negative-subjective utterances, es-
tablishing their polarity. In both sets of experi-
ments, we assess the impact of features relating
to conversation structure.
2 Related Research
Raaijmakers et al (2008) have approached
the problem of detecting subjectivity in meeting
speech by using a variety of multimodal features
such as prosodic features, word n-grams, charac-
ter n-grams and phoneme n-grams. For subjec-
tivity detection, they found that a combination of
all features was best, while prosodic features were
less useful for discriminating between positive and
negative utterances. They found character n-grams
to be particularly useful.
Riloff and Wiebe (2004) presented a method for
learning subjective extraction patterns from a large
amount of data, which takes subjective and non-
subjective text as input, and outputs significant
lexico-syntactic patterns. These patterns are based
on syntactic structure output by the Sundance shal-
1348
low dependency parser (Riloff and Phillips, 2004).
They are extracted by exhaustively applying syn-
tactic templates such as < subj > passive-verb
and active-verb < dobj > to a training cor-
pus, with an extracted pattern for every instan-
tiation of the syntactic template. These patterns
are scored according to probability of relevance
given the pattern and frequency of the pattern. Be-
cause these patterns are based on syntactic struc-
ture, they can represent subjective expressions that
are not fixed word sequences and would therefore
be missed by a simple n-gram approach.
Riloff et al (2006) explore feature subsumption
for opinion detection, where a given feature may
subsume another feature representationally if the
strings matched by the first feature include all of
the strings matched by the second feature. To give
their own example, the unigram happy subsumes
the bigram very happy. The first feature will be-
haviorally subsume the second if it representa-
tionally subsumes the second and has roughly the
same information gain, within an acceptable mar-
gin. They show that they can improve opinion
analysis results by modeling these relations and
reducing the feature set.
Our approach for learning subjective patterns
like Raaijmakers et al relies on n-grams, but like
Riloff et al moves beyond fixed sequences of
words by varying levels of lexical instantiation.
Yu and Hatzivassiloglou (2003) addressed three
challenges in the news article domain: discrimi-
nating between objective documents and subjec-
tive documents such as editorials, detecting sub-
jectivity at the sentence level, and determining po-
larity at the sentence level. They found that the
latter two tasks were substantially more difficult
than classification at the document level. Of par-
ticular relevance here is that they found that part-
of-speech (POS) features were especially useful
for assigning polarity scores, with adjectives, ad-
verbs and verbs comprising the best set of POS
tags. This work inspired us to look at generaliza-
tion of n-grams based on POS.
On the slightly different task of classifying the
intensity of opinions, Wilson et al (2006) em-
ployed several types of features including depen-
dency structures in which words can be backed off
to POS tags. They found that this feature class im-
proved the overall accuracy of their system.
Somasundaran et al (2007) investigated subjec-
tivity classification in meetings. Their findings in-
dicate that both lexical features (list of words and
expressions) and discourse features (dialogue acts
and adjacency pairs) can be beneficial. In the same
spirit, we effectively combine lexical patterns and
conversational features.
The approach to predicting subjectivity we
present in this paper is a novel contribution to the
field of opinion and sentiment analysis. Pang and
Lee (2008) give an overview of the state of the art,
discussing motivation, features, approaches and
available resources.
3 Subjectivity Detection
In this section we describe our approach to sub-
jectivity detection. We begin by describing how
to learn subjective n-gram patterns with varying
levels of lexical instantiation. We then describe a
set of features characterizing multimodal conver-
sation structure which can be used to supplement
the n-gram approach. Finally, we describe the
baseline subjectivity detection approaches used
for comparison.
3.1 Partially Instantiated N-Grams
Our approach to subjectivity detection and polar-
ity detection is to learn significant patterns that
correlate with the subjective and polar utterances.
These patterns are word trigrams, but with varying
levels of lexical instantiation, so that each unit of
the n-gram can be either a word or the word?s part-
of-speech (POS) tag. This contrasts, then, with
work such as that of Raaijmakers et al (2008)
who include trigram features in their experiments,
but where their learned trigrams are fully instanti-
ated. As an example, while they may learn that a
trigram really great idea is positive, we may addi-
tionally find that really great NN and RB great NN
are informative patterns, and these patterns may
sometimes be better cues than the fully instanti-
ated trigrams. To differentiate this approach from
the typical use of trigrams, we will refer to it as the
VIN (varying instantiation n-grams) method.
In some respects, our approach to subjectiv-
ity detection is similar to Riloff and Wiebe?s
work cited above, in the sense that their extrac-
tion patterns are partly instantiated. However,
the AutoSlog-TS approach relies on deriving syn-
tactic structure with the Sundance shallow parser
(Riloff and Phillips, 2004). We hypothesize that
our trigram approach may be more robust to dis-
fluent and fragmented meeting speech and emails
1349
1 2 3
really great idea
really great NN
really JJ idea
RB great idea
really JJ NN
RB great NN
RB JJ idea
RB JJ NN
Table 1: Sample Instantiation Set
on which syntactic parsers may perform poorly.
Also, our learned trigram patterns range from fully
instantiated to completely uninstantiated. For ex-
ample, we might find that the pattern RB JJ NN
is a very good indicator of subjective utterances
because it matches a variety of scenarios where
people are ascribing qualities to things, e.g. re-
ally bad movie, horribly overcooked steak. Notice
that we do not see our approach and AutoSlog-TS
as mutually exclusive, and indeed we demonstrate
through these experiments that they can be effec-
tively combined.
Our approach begins by running the Brill POS
tagger (Brill, 1992) over all sentences in a doc-
ument. We then extract all of the word trigrams
from the document, and represent each trigram us-
ing every possible instantiation. Because we are
working at the trigram level, and each unit of the
trigram can be a word or its POS tag there are
2
3
= 8 representations in each trigram?s instantia-
tion set. To continue the example from above, the
instantiation set for the trigram really great idea is
given in Table 1. As we scan down the instanti-
ation set, we can see that the level of abstraction
increases until it is completely uninstantiated. It is
this multilevel abstraction that we are hypothesiz-
ing will be useful for learning new subjective and
polar cues.
All trigrams are then scored according to their
prevalence in relevant versus irrelevant documents
(e.g. subjective vs. non-subjective sentences),
following the scoring methodology of Riloff and
Wiebe (2003). We calculate the conditional prob-
ability p(relevance|trigram) using the actual tri-
gram counts in relevant and irrelevant text. For
learning negative-subjective patterns, we treat all
negative sentences as the relevant text and the re-
mainder of the sentences as irrelevant text, and
conduct the same process for learning positive-
subjective patterns. We consider significant pat-
terns to be those where the conditional proba-
bility is greater than 0.65 and the pattern occurs
more than five times in the entire document set
(slightly higher than probability >= 0.60 and
frequency >= 2 used by Riloff and Wiebe
(2003)).
We possess a fairly small amount of conversa-
tional data annotated for subjectivity and polarity.
The AMI meeting corpus and BC3 email corpus
are described in more detail in Section 4.1. To ad-
dress this shortfall in annotated data, we take two
approaches to learning patterns. In the first, we
learn a set of patterns from the annotated conversa-
tion data. In the second approach, we complement
those patterns by learning additional patterns from
unannotated data that are typically overwhelm-
ingly subjective or objective in nature. We de-
scribe these two approaches here in turn.
3.1.1 Supervised Learning of Patterns from
Conversation Data
The first learning strategy is to apply the above-
described methods to the annotated conversation
data, learning the positive patterns by compar-
ing positive-subjective utterances to all other ut-
terances, and learning the negative patterns by
comparing the negative-subjective utterances to
all other utterances, using the described methods.
This results in 759 significant positive patterns and
67 significant negative patterns. This difference in
pattern numbers can be explained by negative ut-
terances being less common in the AMI meetings,
as noted by Wilson (2008). It may be that people
are less comfortable in expressing negative sen-
timents in face-to-face conversations, particularly
when the meeting participants do not know each
other well (in the AMI scenario meetings, many
participants were meeting each other for the first
time). But there may be a further explanation for
why we learn many more positive than negative
patterns. When conversation participants do ex-
press negative sentiments, they may couch those
sentiments in more euphemistic or guarded terms
compared with positive sentiments. Table 2 gives
examples of significant positive and negative pat-
terns learned from the labeled meeting data. The
last two rows in Table 2 show how two patterns
in the same instantiation set can have substantially
different probabilities.
1350
POS p(r|t) NEG p(r|t)
you MD change 1.0 VBD not RB 1.0
should VBP DT 1.0 doesn?t RB VB 0.875
very easy to 0.88 a bit JJ 0.66
we could VBP 0.78 think PRP might 0.66
NNS should VBP 0.71 be DT problem 0.71
PRP could do 0.66 doesn?t really VB 0.833
it could VBP 83 doesn?t RB VB 0.875
Table 2: Example Pos. and Neg. Patterns (AMI)
3.1.2 Unsupervised Learning of Patterns
from Blog Data
The second pattern learning strategy we take to
learning subjective patterns is to use a relevant,
but unannotated corpus. We focus on weblog
(blog) data for several reasons. First, blog posts
share many characteristics with both meetings and
emails: they are conversational, informal and the
language can be very ungrammatical. Second,
blog posts are known for being subjective; blog-
gers post on issues that are passionate to them, of-
fering arguments, opinions and invective. Third,
there is a huge amount of available blog data. But
because we do not possess blog data annotated
for subjectivity, we take the following approach
to learning subjective patterns from this data. We
work on the assumption that a great many blog
posts are inherently subjective, and that compar-
ing this data to inherently objective text such as
newswire articles, treating the latter as our irrele-
vant text, should lead to the detection of many new
subjective patterns and greatly increase our cover-
age. While the patterns learned will be noisy, we
hypothesize that the increased coverage will im-
prove our subjectivity detection overall.
For our blog data, we use the BLOG06 Corpus1
that was featured as training and testing data for
the Text Analysis Conference (TAC) 2008 track
on summarizing blog opinions. The portion used
totals approximately 4,000 documents on all man-
ner of topics. Treating that dataset as our rele-
vant, subjective data, we then learn the subjec-
tive trigrams by comparing with the irrelevant
TAC/DUC newswire data from the 2007 and 2008
update summarization tasks. To try to reduce the
amount of noise in our learned patterns, we set the
conditional probability threshold at 0.75 (vs. 0.65
for annotated data), and stipulate that all signif-
icant patterns must occur at least once in the ir-
relevant text. This last rule is meant to prevent
1http://ir.dcs.gla.ac.uk/test collections/blog06info.html
Pattern p(r|t)
can not VB 0.99
i can RB 0.99
i have not 0.98
do RB think 0.97
RB think that 0.95
RB agree with 0.95
IN PRP opinion 0.95
Table 3: Example Subjective Patterns (BLOG06)
us from learning completely blog-specific patterns
such as posted by NN or linked to DT. In the end,
more than 20,000 patterns were learned from the
blog data. While manual inspection does show
that many undesirable patterns were extracted,
among the highest-scoring patterns are many sen-
sible subjective trigrams such as those indicated in
Table 3.
This approach is similar in spirit to the work of
Biadsy et al (2008) on unsupervised biography
production. Without access to labeled biographi-
cal data, the authors chose to use sentences from
Wikipedia biographies as their positive set and
sentences from newswire articles as their negative
set, on the assumption that most of the Wikipedia
sentences would be relevant to biographies and
most of the newswire sentences would not.
3.2 Deriving VIN Features
For our machine learning experiments, we derive,
for each sentence, features indicating the presence
of the significant VIN patterns. Patterns are binned
according to their conditional probability range
(i.e., 0.65 <= p < 0.75, 0.75 <= p < 0.85,
0.85 <= p < 0.95, and 0.95 <= p). There are
three bins for the blog patterns, since the proba-
bility cutoff is 0.75 For each bin, there is a feature
indicating the count of its patterns in the given sen-
tence. When attempting to match these trigram
patterns to sentences, we allow up to two wild-
card lexical items between the trigram units. In
this way a sentence can match a learned pattern
even if the units of the n-gram are not contiguous
(Raaijmakers et al (2008) similarly include an n-
gram feature allowing such intervening material).
A key reason for counting the number of
matched patterns for each probability range as just
described, rather than including a feature for each
individual pattern, is to maintain the same level
of dimensionality in our machine learning exper-
iments when comparing the VIN approach to the
baseline approaches described in Section 3.4.
1351
3.3 Conversational Features
While we hypothesize that the general pur-
pose pattern-based approach described above will
greatly aid subjectivity and polarity detection, we
also recognize that there are many additional fea-
tures specific for characterizing multimodal con-
versations that may correlate well with subjectiv-
ity and polarity. Such features include structural
characteristics like the position of a sentence in a
turn and the position of a turn in the conversation,
and participant features relating to dominance or
leadership. For example, it may be that subjective
sentences are more likely to come at the end of a
conversation, or that a person who dominates the
conversation may utter more negative sentences.
We use the feature set provided by Murray and
Carenini (2008), which they used for automatic
summarization of conversations and which are
shown in Table 4. Many of the features are based
on so-called Sprob and Tprob term-weights, the
former of which weights words based on their dis-
tributions across conversation participants and the
latter of which similarly weights words based on
their distributions across conversation turns. Other
features include word entropy of the candidate
sentence, lexical cohesion of the sentence with the
greater conversation, and structural features indi-
cating position of the candidate sentence in the
turn and in the conversation, such as the elapsed
time since the beginning of the conversation.
3.4 Baseline Approaches
There are two baselines in particular to which
we are interested in comparing the VIN ap-
proach. As stated earlier, we are hypothesiz-
ing that the increasing levels of abstraction found
with partially instantiated trigrams will lead to im-
proved classification compared with using only
fully instantiated trigrams. To test this, we
also run the subjective/non-subjective and posi-
tive/negative experiments using only fully instan-
tiated trigrams. There are 71 such positive tri-
grams and 5 such negative trigrams learned from
the AMI data. There are just over 1200 fully in-
stantiated trigrams learned from the unannotated
BLOG06 data.
Believing that the current approach may offer
benefits over state-of-the-art pattern-based subjec-
tivity detection, we also implement the AutoSlog-
TS method of Riloff and Wiebe (2003) for extract-
ing subjective extraction patterns. In AutoSlog-
Feature ID Description
MXS max Sprob score
MNS mean Sprob score
SMS sum of Sprob scores
MXT max Tprob score
MNT mean Tprob score
SMT sum of Tprob scores
TLOC position in turn
CLOC position in conv.
SLEN word count, globally normalized
SLEN2 word count, locally normalized
TPOS1 time from beg. of conv. to turn
TPOS2 time from turn to end of conv.
DOM participant dominance in words
COS1 cosine of conv. splits, w/ Sprob
COS2 cosine of conv. splits, w/ Tprob
PENT entropy of conv. up to sentence
SENT entropy of conv. after the sentence
THISENT entropy of current sentence
PPAU time btwn. current and prior turn
SPAU time btwn. current and next turn
BEGAUTH is first participant (0/1)
CWS rough ClueWordScore (cohesion)
CENT1 cos. of sentence & conv., w/ Sprob
CENT2 cos. of sentence & conv., w/ Tprob
Table 4: Features Key
TS, once all of the patterns are extracted using
the Sundance parser, the scoring methodology is
much the same as desribed in Section 3.1. Con-
ditional probabilities are calculated by comparing
pattern occurrences in the relevant text with oc-
currences in all text, and we again use a thresh-
old of p >= 0.65 and frequency >= 5 for sig-
nificant patterns. For the BLOG06 data, we use
a probability cutoff of 0.75 as before. For deriv-
ing the features used in our machine learning ex-
periments, the patterns are similarly grouped ac-
cording to conditional probability. From the anno-
tated data, 48 patterns are learned in total, 46 pos-
itive and only 2 negative. From the BLOG06 data,
more than 3000 significant patterns are learned.
Among significant patterns learned from the AMI
corpus are < subj > BE good, change < dobj >,
< subj > agree and problem with < NP >.
To gauge the effectiveness of the various feature
types, for both sets of experiments we build multi-
ple models on a variety of feature combinations:
fully instantiated trigrams (TRIG), varying in-
stantiation n-grams (VIN), AutoSlog-TS (SLOG),
conversational structure features (CONV), and the
set of all features.
4 Experimental Setup
In this section we describe the corpora used, the
relevant subjectivity annotation, and the statistical
1352
classifiers employed.
4.1 Corpora
We use two annotated corpora for these experi-
ments. The AMI corpus (Carletta et al, 2005) con-
sists of meetings in which participants take part in
role-playing exercises concerning the design and
development of a remote control. Participants are
grouped in fours, and each group takes part in a
sequence of four meetings, bringing the remote
control from design to market. The four members
of the group are assigned roles of project man-
ager, industrial designer, user interface designer,
and marketing expert. In total there are 140 such
scenario meetings, with individual meetings rang-
ing from approximately 15 to 45 minutes.
The BC3 corpus (Ulrich et al, 2008) contains
email threads from the World Wide Web Consor-
tium (W3C) mailing list. The threads feature a va-
riety of topics such as web accessibility and plan-
ning face-to-face meetings. The annotated portion
of the mailing list consists of 40 threads.
4.2 Subjectivity Annotation
Wilson (2008) has annotated 20 AMI meetings for
a variety of subjective phenomena which fall into
the broad classes of subjective utterances, objec-
tive polar utterances and subjective questions. It
is this first class in which we are primarily in-
terested here. Two subclasses of subjective utter-
ances are positive subjective and negative subjec-
tive utterances. Such subjective utterances involve
the expression of a private state, such as a posi-
tive/negative opinion, positive/negative argument,
and agreement/disagreement. The 20 meetings
were labeled by a single annotator, though Wilson
(2008) did conduct a study of annotator agreement
on two meetings, reporting a ? of 0.56 for detect-
ing subjective utterances. Of the roughly 20,000
dialogue acts total in the 20 AMI meetings, nearly
4000 are labeled as positive-subjective and nearly
1300 as negative-subjective. For the first exper-
imental task, we consider the subjective class to
be the union of positive-subjective and negative-
subjective dialogue acts. For the second experi-
mental task, the goal is to discriminate positive-
subjective from negative-subjective.
For the BC3 emails, annotators were initially
asked to create extractive and abstractive sum-
maries of each thread, in addition to labeling a
variety of sentence-level phenomena, including
whether each sentence was subjective. In a second
round of annotations, three different annotators
were asked to go through all of the sentences pre-
viously labeled as subjective and indicate whether
each sentence was positive, negative, positive-
negative, or other. The definitions for positive and
negative subjectivity mirrored those given by Wil-
son (2008). For the purpose of these experiments,
we consider a sentence to be subjective if at least
two of the annotators labeled it as subjective, and
similarly consider a subjective sentence to be pos-
itive or negative if at least two annotators label it
as such. Using this majority vote labeling, 172
of 1800 sentences are considered subjective, with
44% of those labeled as positive-subjective and
37% as negative-subjective, showing that there is
much more of a balance between positive and neg-
ative sentiment in these email threads compared
with meeting speech (note that some subjective
sentences are not positive or negative). The ? for
labeling subjective sentences in the email corpus
is 0.32. The lower annotator agreement on emails
compared with meetings suggests that subjectiv-
ity in email text may be manifested more subtly or
conveyed somewhat amibiguously.
4.3 Classifier and Experimental Setup
For these experiments we use a maximum entropy
classifier using the liblinear toolkit2 (Fan et al,
2008). Feature subset selection is carried out by
calculating the F-statistic for each feature, ranking
the features according to the statistic, and train-
ing on increasingly smaller subsets of feature in
a cross-validation procedure, ultimately choosing
the feature set with the highest balanced accuracy
during cross-validation.
Because the annotated portions of our corpora
are fairly small (20 meetings, 40 email threads),
we employ a leave-one-out method for training
and testing rather than using dedicated training
and test sets. For the polarity labeling task ap-
plied to the BC3 corpus, we pool all of the sen-
tences and perform 10-fold cross-validation at the
sentence level.
4.4 Evaluation Metrics
We employ two sets of metrics for evaluating all
classifiers: precision/recall/f-measure and the re-
ceiver operator characteristic (ROC) curve. The
ROC curve plots the true-positive/false-positive
ratio while the posterior threshold is varied, and
2http://www.csie.ntu.edu.tw/ cjlin/liblinear/
1353
we report the area under the curve (AUROC) as the
measure of interest. Random performance would
feature an AUROC of approximately 0.5, while
perfect classification would yield an AUROC of
1. The advantage of the AUROC score compared
with precision/recall/f-measure is that it evaluates
a given classifier across all thresholds, indicating
the classifier?s overall discriminating power. This
metric is also known to be appropriate when class
distributions are skewed (Fawcett, 2003), as is our
case. For completeness we report both AUROC
and p/r/f, but our discussions focus primarily on
the AUROC comparisons.
5 Results
In this section we describe the experimental re-
sults, first for the subjective/non-subjective clas-
sification task, and subsequently for the positive-
negative classification task.
5.1 Subjective / Non-Subjective Classification
For the subjectivity detection task, the results on
the AMI and BC3 data closely mirrored each
other, with the VIN approach constituting a very
effective feature set, outperforming both baselines.
We report the results on meeting and emails in
turn.
5.1.1 AMI corpus
For the subjectivity task with the AMI corpus, we
first report the precision, recall and f-measure re-
sults in Table 5 where the various classifiers are
compared with a lower bound (LB) in which the
positive class is always predicted, leading to per-
fect recall. It can be seen that the novel systems
exhibit substantial improvement in precision and
f-measure over this lower-bound. While the VIN
approach yields the best precision scores, the full
feature set achieves the highest f-measure.
As shown in Figure 1, the average AUROC with
the VIN approach is 0.69, compared with 0.61 for
AutoSlog-TS, a significant difference according to
paired t-test (p<0.01). The VIN approach is also
significantly better than the standard fully instan-
tiated trigram pattern approach (p<0.01). This
latter result suggests that the increased level of
abstraction found in the varying instantiation n-
grams does improve performance.
The conversational features alone give compa-
rable performance to the VIN method (no signifi-
cant difference), and the best results are found us-
ing the full feature set, which gives an average AU-
Sys Precision Recall F-Measure
AMI Corpus
LB 26 100 41
Trig 25 63 36
Slog 39 48 43
VIN 41 58 48
Conv 36 73 49
All Feas 38 70 49
BC3 Corpus
LB 10 100 17
Trig 27 10 14
Slog 24 13 17
VIN 27 22 24
Conv 25 29 27
All Feas 33 34 33
Table 5: P/R/F Results, Subjectivity Task
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Trig - AMI
Trig - BC3
Slog - AMI
Slog - BC3
VIN - AMI
VIN - BC3
Conv - AMI
Conv - BC3
All Feas - AMI
All Feas - BC3
AU
RO
C
Figure 1: AUROCs on Subjectivity Task for AMI
and BC3 corpora
ROC of 0.71, a significant improvement over VIN
only (p<0.05).
5.1.2 BC3 corpus
For the subjectivity task with the BC3 corpus, the
best precision and f-measure scores are found by
combining all features, as displayed in Table 5.
The f-measure for the VIN approach is ten points
higher than for the standard trigram approach.
The average AUROC with the VIN approach is
0.77, compared with 0.70 for AutoSlog-TS (sig-
nificant at p<0.05). The varying instantiation ap-
proach is significantly better than the standard tri-
gram pattern approach (p<0.01), where the aver-
age AUROC is 0.66. We again find that conver-
sational features are very useful for this task, and
that the best overall results utilize the entire fea-
ture set. These results are displayed in Figure 1.
5.1.3 Impact of Blog Data
An interesting question is whether our use of the
BLOG06 data was worthwhile. We can measure
this by comparing the VIN AUROC results re-
1354
Sys Precision Recall F-Measure
AMI Corpus
LB 76 100 86
Trig 87 8 14
Slog 75 46 57
VIN 83 60 70
Conv 82 47 60
All Feas 83 56 67
BC3 Corpus
LB 54 100 70
Trig 50 84 63
Slog 58 56 57
VIN 53 84 65
Conv 63 80 71
All Feas 60 76 67
Table 6: P/R/F Results, Polarity Task
ported above with the VIN AUROC scores using
only the annotated data for learning the significant
patterns. The finding is that the blog data was
very helpful, as the VIN approach averages only
0.66 on the BC3 data and 0.63 on the AMI data
when the blog patterns are not used, both signif-
icantly lower (p<0.01). Figure 2 shows the ROC
curves for the VIN approach with and without blog
patterns applied to the AMI subjectivity detection
task, illustrating the impact of the unsupervised
pattern-learning strategy.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
TP
FP
VIN with Blog Patterns
VIN without Blog Patterns
chance level
Figure 2: Effect of Blog Patterns on AMI Subjec-
tivity Task
5.2 Positive / Negative Classification
For the polarity classification task, the results dif-
fer between the two corpora. We describe the re-
sults on meetings and emails in turn.
5.2.1 AMI corpus
The p/r/f results for the AMI polarity task are pre-
sented in Table 6, with the scores pertaining to
the positive-subjective class. The VIN classifier
and full features classifier achieve the highest pre-
cision, but the f-measures are below the lower-
bound.
Comparing AUROC results, the VIN approach
is again significantly better than AutoSlog-TS,
averaging 0.65 compared with 0.56, and signifi-
cantly better than the standard trigram approach
(p<0.01 in both cases). The results are dis-
played in Figure 3. The conversational features are
significantly less effective than the VIN features
(p<0.05), and the best overall results are found by
utilizing all features, with significant improvement
over VIN only at p<0.05 and significant improve-
ment over AutoSlog-TS only at p<0.01.
5.2.2 BC3 corpus
The results of the polarity task on the BC3 cor-
pus are markedly different from the other exper-
imental results. In this case, neither VIN nor
AutoSlog-TS are particularly good for discrimi-
nating between positive and negative sentences,
and the best strategy is to use features relating to
conversational structure. According to p/r/f (Ta-
ble 6), the only method outperforming the lower-
bound in terms of f-measure is the conversational
features classifier. According to AUROC scores
shown in Figure 3, the conversational features by
themselves are significantly better than the VIN
approach (p<0.01 for non-paired t-test). So for
emails, we are more likely to correctly classify
positive and negative sentence by looking at fea-
tures such as position in the turn and participant
dominance than by matching our learned patterns.
While we showed previously that pattern-based
approaches perform well for the subjectivity task
on this dataset, there was less success in using the
patterns to discern the polarity of email sentences.
We are again interested in whether the use of the
BLOG06 data was beneficial. For the BC3 data,
there is very little difference between the VIN ap-
proach with and without the blog patterns, as they
both perform poorly, but with the AMI corpus, the
blog patterns yield significant improvement in po-
larity classification, increasing from an average of
0.56 without the blog patterns to 0.65 with them
(p<0.01).
6 Discussion and Future Work
A key difference between the AMI and BC3 data
with regards to subjectivity is that negative ut-
terances are much more common in the BC3
email threads. Additionally, the pattern-based ap-
proaches fared worst in discriminating between
1355
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Trig - AMI
Trig - BC3
Slog - AMI
Slog - BC3
VIN - AMI
VIN - BC3
Conv - AMI
Conv - BC3
All Feas - AMI
All Feas - BC3
AU
RO
C
Figure 3: AUROCs on Polarity Task for AMI and
BC3 corpora
negative and positive utterances in that corpus.
Positive and negative email sentences are more
easily recognized via features relating to conver-
sation structure and participant status than through
the learned lexical patterns.
The use of patterns learned from unlabeled blog
data significantly improved performance. We are
currently developing further techniques for learn-
ing subjective and polar patterns from such raw,
natural text.
A potential area of improvement is to reduce the
feature set by eliminating some of the subjective
patterns. In Section 2, we briefly described the
work of Riloff et al (2006) on feature subsump-
tion relationships. In our case, in a VIN instantia-
tion set a given trigram instantiation subsumes the
less abstract instantiations in the set, so the most
abstract instantiation (i.e. completely uninstanti-
ated trigram) representationally subsumes the rest.
Eliminating some of the representationally sub-
sumed instantiations when they are also behav-
iorally subsumed may improve our results.
It is difficult to compare our results directly with
those of Raaijmakers et al (2008) as they used a
smaller set of AMI meetings for their experiments,
and because for the first experiment we consider
the subjective class to be the union of positive-
subjective and negative-subjective dialogue acts
whereas they additionally include subjective ques-
tions and dialogue acts expressing uncertainty.
These differences are reflected by the substantially
differing scores reported for majority-vote base-
lines on each task. However, their success with
character n-gram features suggests that we could
improve our system by incorporating a variety of
character features. Character n-grams were the
best single feature class in their experiments.
The VIN representation is a general one and
may hold promise for learning patterns relevant to
other interesting conversation phenomena such as
decision-making and action items. We plan to ap-
ply the methods described here to these other ap-
plications in the near future.
7 Conclusion
In this work we have shown that learning subjec-
tive trigrams with varying instantiation levels from
both annotated and raw data can improve subjec-
tivity detection and polarity labeling for meeting
speech and email threads. The novel pattern-based
approach was significantly better than standard tri-
grams for three of the four tasks, and was signif-
icantly better than a state-of-the-art syntactic ap-
proach for those same tasks. We also found that
features relating to conversational structure were
beneficial for all tasks, and particularly for polar-
ity labeling in email data. Interestingly, in three
out of four cases combining all the features pro-
duced the best performance.
References
F. Biadsy, J. Hirschberg, and E. Filatova. 2008. An un-
supervised approach to biography production using
wikipedia. In Proc. of ACL-HLT 2008, Columbus,
OH, USA.
E. Brill. 1992. A simple rule-based part of speech
tagger. In Proc. of DARPA Speech and Natural Lan-
guage Workshop, San Mateo, CA, USA, pages 112?
116.
J. Carletta, S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos,
W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, I. McCowan, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI meeting corpus: A pre-
announcement. In Proc. of MLMI 2005, Edinburgh,
UK, pages 28?39.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. Liblinear: A library for large lin-
ear classification. Journal of Machine Learning Re-
search, 9:1871?1874.
T. Fawcett. 2003. Roc graphs: Notes and practical
considerations for researchers.
G. Murray and G. Carenini. 2008. Summarizing spo-
ken and written conversations. In Proc. of EMNLP
2008, Honolulu, HI, USA.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Informa-
tion Retrieval, 1-2(2):1?135.
1356
S. Raaijmakers, K. Truong, and T. Wilson. 2008. Mul-
timodal subjectivity analysis of multiparty conversa-
tion. In Proc. of EMNLP 2008, Honolulu, HI, USA.
E. Riloff and W. Phillips. 2004. An introduction to the
sundance and autoslog systems.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proc. of EMNLP
2003, Sapporo, Japan.
E. Riloff, S. Patwardhan, and J. Wiebe. 2006. Fea-
ture subsumption for opinion analysis. In Proc. of
EMNLP 2006, Sydney, Australia.
S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In
Proc. of SIGDIAL 2007, Antwerp, Belgium.
J. Ulrich, G. Murray, and G. Carenini. 2008. A
publicly available annotated corpus for supervised
email summarization. In Proc. of AAAI EMAIL-
2008 Workshop, Chicago, USA.
T. Wilson, J. Wiebe, and R. Hwa. 2006. Recognizing
strong and weak opinion clauses. Computational In-
telligence, 22(2):73?99.
T. Wilson. 2008. Annotating subjective content in
meetings. In Proc. of LREC 2008, Marrakech, Mo-
rocco.
H. Yu and V. Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proc. of EMNLP 2003, Sapporo, Japan.
1357
