American Jo&nsl of Computational Linguistics Microf i che  61 
T O W A R D  A  " N A T U R A L "  L A N G U A G E  
Q U E S T I O N - A N S W E R I N G  F A C I L I T Y  
BILL D .  MAXWELL AND FRANCIS D. ~ U G G L E  
Department of Computer Science 
University of Kansas 
Lawrence 66045 
Maxwell is also associated with the Computation Center 
-Tuggle is also associated with the School of Business 
Copyright  @ 1977 
Associa  t l o n  f p r  Computational L i n g u i s t i c s  
This study describes the structure, implementation and potential 
of a simple computer program that understands and answers questions in 
a humanoid mannet. An emphasis bas been placed on the creation of an 
extendible memory structure--one capable of supposting conversation 
in normal, unrestricted ErIgli6h on a variety of topics. An attempt 
has also been made to find procedures that can easily and accurately 
determine the meaning of input text- A parser using a combination of 
syntax and semantics has been developed for understanding YES-NO 
questions, in particular, DO-type (DO, DID, DOES, etc.) questions. A 
third and major emphasis has been on the development of procedures 
to allow the program to converse, easily and "naturally1' with a human. 
This general gaql has been met by developing procedures that generate 
answers to DO-questions in a manner similar to the way a person might 
answer them. 
TABLE OF CONTENTS 
1. INTRODUCTION 
. . . . . . . . . . . . . . . . . . . . . . . . . . . .  
1.1 MIfiGOALS . . . . . . . . . . . . . . . . . . . . . . . . .  
1.2 SMLEDIALOG . . . . . . m e . . . . . . . . . . . . . . .  
. . . . . . . . . . . . . . . . . . . . . . . . . . .  1.3 REVJE\J 
1.3.) llemory Models . . 
1.3.2 L i n g u i s t i c  P a r s e r s  . . . 
1.3.3 Outbu tGenera t io r l  . = . . 
. . . . . . . . . . . . . . . . . . . . . .  4 . PARSER 
4 .1  PARSINGSTRATEGY . . . . . . . . . . . . . . .  
4.2 G M R  . . . . . . . . . . . . . . . . . . . .  
4.2.1 Acceptable  Inpu t  Forms . . . 
4.2.2 Semantics and Syntax . 9 
4.2.3 Pronouns. Ambiguity arid Undefined W o r k  
4.3 PARSING ALGORITHM . . . . . . . . . . . . . . .  
4.3.1 Preprocess ing  of  t h e  Inpu t  Text , . . .  
4.3.2 Determining Type of  Inpu t  . . . . . . .  
4.3.3 P a r s i n g  a DO-Question o r  Statement  . . .  
. . . . . . . . . . . . . . . . . . . . . . . . . .  5 1EYORY scARCHING 
. . . . . . . . . . . . . . . . . . . . . . . . . .  5.1 OVERVIEII 
. . . . . . . . . . . . . . . .  5.2 TIIE IlEIIORY MATCH STRUCTURE 
. . . . . . . . . . . . . . . . . .  5.2.1 General  S t r u c t u r e  
. . . . . . .  5.2.2 Actor.  Act and Objec t  Comparison R e s u l t s  
5.2.3 lJord l l o d i f i c a t i o n  R e s u l t s  . . . . . . . . . . . . . .  
5.2.4 Example S t r u c t u r e s  . . . . . . . . . . . . . . . . . .  
5.3 PlATCllING MEllORY . . . . . . . . . . . . . . . . . . . . . . . . .  
5.3.1 Bas ic  Algoritlun . . . . . . . . . . . . . . . . . . .  
5.3.2 S e l e c t i n g  S u i t a b l e  Actors  . . . . . . . . . . . . . .  
. 6 PRODUCTION OF OUTPUT . . . . . . . . . . . . . . . . . . . . . . .  
6.1 OVERVIEW . . . . . . . . . . . . . . . . . . . . . . . . . .  
. . . . . . . . . . . . . . . . .  6.2 THE OUTPUT PRODUCTION LIST 
6.3  PRODUCTION METHOD . . . . . . . . . . . . . . . . . . . . .  
. . . . . . . . . .  6.3.1 Responding t o  I n p u t  l o t  Understood 
. . . . . . . . . . . . .  6.3.2 Determining Mode of Response 
6.3.3 Producing a Nomal  Answer . . . . . . . . . . . . . .  
6.3.4 Flaking Output Grammatical . . . . . . . . . . . . . .  
I)ISCUSSIOI~ . . . . . . . . . . . . . . . . . .  
7 . 1  RESULTS . . . . . . . . . . . . . . . . .  
. . . . . . . . . .  7.1.1 Objectives Net 
7.1.2 Does the Program Really Understand? 
I *  2 LIMITATIONS . . . . . . . . . . . . . . . .  
7 .2 .1  Memory Model . . . . . . . . . . .  
7.2.2 Parser . . . . . . . . . . . . . .  
7 .2 .3  Output Production . . . . . . . . .  
7 . 3  IGNORED PROBLEMS . . . . . . . . . . . . .  
7.3.1 Intentions andPlotivations . . . .  
7.3.2 ilorman's Problems . . . . . . . . .  
1. INTRODUCTION 
There a r e  a number of e x t a n t  computer programs which i n t e r a c t  
i n t e l l i g e n t l y  wi th  human i n t e r r o g a t o r s ,  but  a l l  do s o  i n  a way w e  
cha rac t e r i ee  a s  "unnatural". By "unnatural ,"  we mean t h a t  they con- 
v e r s e  i n  a way s i g n i f i c a n t l y  un l ike  two normal a d u l t  humans do. For 
example, Winograd's SHRBLU [19] seems chi ld- l ike ,  and i t  sha res  w i th  t h e  
woods' moonrocks system [20] the  problem of being task-const rained.  
Weizenbaum's ELIZA [16] gives  the  appearance of be ing  vague, evas lve  and 
doubting. Colby's PARRY [s], of course,  is va r ious ly  paranoid. F i n a l l y ,  
t h e  C A I  systems, such as ca rbone l l ' s  SCHOLAR [ 4 ]  and Brown and Burton's 
SOPHIE [3], seem o f f i c i o u s l y  pedantic. I n  c o n t r a s t  t o  t hose  approaches, 
w e  wanted t o  cons t ruc t  a program a b l e  t o  c a r r y  on an i n - l l i g e n t  and 
"na tura l"  d ia logue  wi th  a human user .  
As an example of a "natural"  and a n  "unnatural" d ia logue ,  imagine 
two computer programs (CPs) conversing wi th  a human (H) i n  which each 
CP a l r eady  knows "THE BOY O~JNS A BALL." One CP is "natural"  (NCP); t h e  
o t h e r  is  "unnatural" (uCP) . (One may f i n d  fragments of responses from 
e s i s t i n g  programs i n  those  responses we have l a b e l l e d  a s  "UCP.") 
H: The b a l l  is red. 
U C t :  By ' t h e  b a l l , '  I assume you mean t h e  b a l l  t h e  boy owns. 
NCP: b.K. 
H: Does t h e  boy own a l a rge ,  round b a l l ?  
UCP: No. [or a t  b e s t ]  I don't  know. 
NCP: H e  owns a red  one. It probably i s  round and may be la rge .  
H: The boy l i v e s  on Main S t r e e t .  
UCP:. By ' t h e  boy', I assume you mean t h e  one who owns t h e  red  b a l l .  
NCP: O.K. 
H: l lhete does t h e  boy p l ay  with h i s  b a l l ?  
UCP : I don' t krtow. 
NCP: I don' t  know, but I assume near  h i s  res idence  on Main S t r e e t .  
H: A house is on Main S t r e e t .  
UCP: O.K. 
NCP: Is i t  t h e  house of t h e  boy who owns t h e  red b a l l ?  
H: It was,expensive t o  cons t ruc t .  
UCP: By 'it,' do youmean 
1. the  house 
2. Main S t r e e t ?  
NCP: 0 . K .  
H: The b a l l  broke a window. 
UCP: By ' t h e  Ball, ' 1 assume you mean the  red  one the  boy owns. BY 'a  
window,' I assume i r  is  a p a r t  of  t h e  house i n  Main S t r ee t .  
NCP: DPd t h e  boy throw t h e  b a l l ?  
H: Was t h e  window expensive t o  r e p a i r ?  
UCP: I don' t  know. 
NCP: S i ~ c e  t h e  house was expensive t o  cons t ruc t ,  I assume t h a t  i ts  
windows a r e  expensive t o  repa i r .  
From t h i s  s h o r t ,  s imple dialogue some of  t he  c h a r a c t e r i s t i c s  which 
d is t inguixh  "natural"  d ia logue  can be seen: "natural"  language under- 
s tanding processes  a r e  a b l e  t o  work with p a r t i a l  and overlapping 
information, a r e  a b l e  t o  r e t a i n  ambiguity u n t i l  disambigation i s  needed, 
a r e  a b l e  t o  perform "short" chains of deductions,  a r e  a b l e  t o  engage i n  
common s e n s e  reasoning, and a r e  a b l e  t o  pose r ec ip roca l  ques t ions  t o  
the human t o  confirm expectat ions.  
Reasons f o r  t r y i n g  t o  i n s t i l l  a c e r t a i n  degree of "humanness1' t o  
computer proBrams should be obvious--to f a c i l i t a t e  t h e i r  acceptance, t o  
extend t h e i r  use, and t o  make them more p l easan t  t o  dea l  with.  People 
w i l l  be much more w i l l i n g  r o  work with a computer program i f  i t  g ives  t h e  
appearance of being humanoid i t s e l f ,  whether t h e  ke rne l  p a r t  of  Che 
program concerns CAT, MIS, o r  whatever. 
- 7- 
We have developed a computer program c a l l e d  J I M M Y 3  which embodies 
some of t he  abave s e t  of c h a r a c t e r i s t i c s  of a "natural1'  language processing 
system s o  a s  t o  demonstrate i t s  f e a s i b i l i t y ,  usefu lness ,  and p o t e n t i a l  
power. The implementation has neces sa r i l y  been l a r g e l y  ad hoc, bu t  as 
Lindsay [ 6 ]  notes ,  t h i s  is no t  a l t oge the r  bad. Mewell [7] proper1.y 
records t h a t  t he re  is a t radeoff  between g e n e r a l i t y  and power. Like 
Lindsay, we d e l i b e r a t e l y  eschew t h e  general i n  favor  of t h e  spec i f i c .  
I n  t h e  i n t e r e s t s  of r e p l i c a b i l i t y  and e x t e n s i b i l i t y ,  we a l s o  provide a 
reasonably complete desc r ip t i on  of t he  inards  of JIMMY3. 
1.1 MAJOR GOALS 
The focus of t h i s  work has been on th ree  problems: a )  t h e  develop- 
ment ~f a memory s t r u c t u r e  u se fu l  f o r  engaging i n  a dialogue wi th  a person, 
b) t h e  development of procedures t h a t  can e a s i l y  and accura-tely determine 
the  meaning of i npu t  t e x t ,  and c )  t he  development of procedures f o r  t h e  
generat ion of output .  
A s  cont ras ted  with more conventional  models f o r  language understand- 
i ng  where input  is decoded i n  terms of concepts and then mapped i n t o  
memory, t h i s  model s t o r c s  su r f ace  s t r u c t u r e s  more o r  l e s s  i n t a c t  without  
any conversion. This  approach s i m p l i f i e s  both t h e  i npu t  processing and 
output  generat ion phases of t he  system. However, i t  does n e c e s s i t a t e  t h e  
use of complex memory matching procedures during t h e  answer-producing 
phase-of understanding. 
For parsing,  s eve ra l  ad hoc r u l e s  were developed f o r  applxing a 
met r ic  t o  measure "meaningfulness." Using t h i s  procedure, s e v e r a l  
d i f f e r e n t  i n t e r p r e t a t i o n s  of t h e  i npu t  a r e  examined. The one having t h e  
h ighes t  "score," as determined by t h e  met r ic ,  is  s e l e c t e d  a s  t h e  proper  
meaning. 
Most memorymodels and undez'standing systems do not  have e labora te  
. f a c i l i t i e s  f o r  t h e  c rea t ibn  of output t h a t  is i n t e r e s t i n g  o r  s t imula t ing  
to conversation.  Output production by t h i s  program approaches so lu t ions  
t o  t h r e e  re la fed  problems: a )  what information should be used i n  the  
9esponse t o  a given input ,  b)  how a response can be s t ruc tu red  so 
c e r t a i n  p a r t s  a r e  emphasized an$ c )  how B Tesponse can be made tp  
appear n a t u r a l  i n  the  sense of being l i k e  a s i m i l a r  remark a person 
might make. 
1.2 SAMPLE DIALOGUE 
To g e t  a f e e l i n g  f o r  t h e  types of responses t h e  currentJy Imple- 
mented program CBn produce, t h e  following s h o r t  dialogue is presented. 
Note t h a t  with only the  c a p a b i l i t y  for answering W-questions 
implemeqted, a continuing dialogue is  not very easy t o  obta in .  
Three e x p l i c i t  f a c t s  e x i s t e d  i n  remory a t  the time of t h i s  con- 
versa t ion.  They were: 
1 )  BRANDT -JELL OWNS A RED BOOK. 
2) BRANDT MAXWELL OWNS A BLUE BOOK. 
3) JIMMY3 KNOWS BRANDT MAXWELL. 
( t h i s  computer model has  been dubbed JIMElY3) 
The dialogue is  given i n  upper case  wi th  commentary i n  lower case. 
The j: and p: ( iden t i fy ing  JZMMY3 and person) were added liere f o r  
c l a r i t y .  
j: HI. WHO ARE YOU? 
JIMMY3 must f i n d  o u t  who it is t a l k i n g  with i n  orderr  
t o  t r a n s l a t e  the  pronoun "I" t o  f a c i l i t a t e  memory 
searching. 
p : BdANDT MAXWELL. 
OK. 
BR@DT@MAXWELL i s  r tcognized as a l e g i t i m a t e  name. 
The @ i n  BRANDT@EWLL is used t o  i n d i c a t e  the  
combining o f  two o r  more words t o  form a Separate  
ert t i ty.  I n  the  above example, t h e  names BRANDT 
and W E L L  a r e  recognized as f i r s t  and l a s t  names, 
respect ively ,  t h a t ,  when appearing together ,  a r e  
in te rp re ted  a s  t h e  f u l l  name, BRANDT@MAXWELL. 
W YOU KNOW M S N E ?  
YES, YOU. 
Pars ing is accomplished by matching inpu t  t o  
templates c o n s i s t i n g  of (ACTOR ACT OBJECT) 
descr ibing t h e  d i f f e r e n t  meaning senses  f o r  main 
verbs. Th i s  quest ion is parsed using t h e  (PERSON 
KNOW PERSON) d e f i n i t i o n  of KNOW. On i n p u t ,  YOU 
is t r a n s l a t e d  t o  JIEIMY3 (which has ISA PERSON a s  
one of i t s  proper t ies) .  
Memory is matched aga ins t  t h e  (JIMMY3 KNOW 
PERSON) p a t t e r n  which y i e l d s  the  match (JIEIMY3 
KNOWS BRANDT@MAxwELL)'. Output procedures then 
cdnvert t h e  match i n t o - t h e  given response. 
DO YOU KNOW BILL MAXWELL? 
NO. 
The search using (JIMMY3 KNOW BILLeMAXWELL) y i e l d s  
no match. Under o t h e r  circumstances a f a i l u r e  of 
t h j s  type would produce an answer of "I DON'T KNOW". 
However, t h e  a c t  KNOW is tagged s o  t h a t  "NO" is t h e  
proper response i f  a sea rch  of memory f a i l s  when 
t h e  t r i p l e  has  KNOW a s  i ts act(main verb). 
DO YOU THINK? 
r DON'T MOW. 
The input  was c o r r e c t l y  parsed a s  (PERSON THINK NIL) 
but  t h e  memory search found no matches. The 
cur ren t  model has no f a c i l i t i e s  f o r  i n t r o s p e c t i o n  
o t h e r  t h a n  the examination of t r i p l e s  i n  memory 
t h a t  d e a l  e x p l i c i t l y  wi th  JIMMY3. 
DO YOU KNOW ANYTHING9 
I DON'T UNDERSTAND THE USE OF KNOW. 
A t  t h e  t i m e  t h i s  d ia logue was produced, t h e  only mean- 
ing  of KNOW contained i n  memory was (PERSON KNOW PERSON). 
Therefore, t h e  reasonable pa r se  of (PERSON KNOW THING) 
was not found. 
p: DO I HAVE A BLUE ANI;EML BOOK? 
j: A BLUE BOOK BUT NOT AN ANIMAL BOOK: 
Some, bu t  no t  a l l ,  of t h e  modi f ie rs  on the  input  
were matched by a t r i p l e  i n  memory a s  ind ica ted  by 
t h e  response. Since not  a l l  modi f ie rs  matchkd, t h e  
ansQer is not  prefaced wi th  YES. 
p: DOES N O N E  HAVE A RED THING? 
j: YES. YOU OIJN A RED BOOK. 
liemory Ls searched using t h e  (PERSON 1WVE TIIING) 
p a t t e r n  which b e s t  matches (BRAEJDT@MAXWELL OWNS 
BOOK) . 9 comparing modi f ie rs  , (BRANDT@MAXWELL 
OWNS RED-BOOK) i s  s e l e c t e d  over  (BRANDT@MAXWELL 
orws BLUE-BOOK) . 
p: M I OWN A GREEN IjOOK? 
j : NO. YOU OWN A RED BOOK. 
S ince  GREEN arhd RED a r e  mutually exc lus ive  i n s t ances  
of COLOR, t h e  answer is  NO. (BRANDT@MAX\ELL OWN 
BLUE-BOOK) was a l s o  matched buQ fie t r i p l e  with 
RED HOOK was used t o  produce the  response s i n c e  it 
was found f i r s t  by t h e  memory search  rout ine .  
p: DO I HAVE AN QLD BOOK? 
j: YES. A RED ONE. I DON'T mow WHAT OLP MEANS. 
OLD is  ignored s in=  ~t i s  an unaefined word. Input  
is parsed a s  i f  i t  had been "DO I HAVE AN BOOK?" 
p: DOES BILL IfAXWELL O \ N  A W O K ?  
j: L DON'T KNOW. 
The f a c t  ( B I L L @ W J E L L  Oms BOOK) is  no t  present  
i n  memory. Nei ther  i s  any con t r ad i c t i on  of t h a t  f a c t .  
p: B Y E -  
j: GOODBYE . 
1.3 REVIEW 
The r e sea rch  involving memory models can b e  divided i n t o  two bas ic  
approaches. F i r s t  are models c rea ted  f o r  t h e  e x p l o r a a o n  o f  t heo r i e s  of 
human memory and f o r  t h e  t e s t i n g  of l i n g u i s t i c .  theor ies .  In  these  
systems, t h e  o t h e r  f e a t u r e s  of a  complete language understanding system 
assume a background p o s i t i o n  s i n c e  t h e  main emphasis is t h e  memory model 
i t s e l f .  Models which f a l l  i n t o  t h i s  c l a s s  a r e  t h e  works of Q u i l l i a n  [ll], 
Anderson and Bower (11, and Norman, Rumelhart and the  LNR research  
-11 - 
group [lo]. The second type of memory model is t h a t  developed a s  v a r t  
of a system which has  splpe component o t h e r  than memory a s  the  major 
emphasis. These models include ~ 5 n o g r a d ' s  blocks world L1-93, ~ c h a n k ' s  
Conceptml Dependency System C13, 143 and ~ o l b y ' s  A r t i f  l c i a l  Paranoia 
model [s]. 
The memory model developed by t h i s  cu r ren t  research does not  
correspond t o  any  xis sting model. It is  not based n n  case  grammar i n  
a s t r i c t  sense, but  s t o r e s  informat5on i n  a form more c lose ly  r e l a t e d  
t o  su r face  Qtructures u t i l i z i n g  only th ree  main components: a c t o r ,  
a c t  and object .  Although t h i s  development was in f luen ted  t o  a l imi ted  
ex ten t  by Anderson and Bower's model [ I ] ,  i t  was shaped I n  a c t u a l  design 
by t h e  pa r se r  Osed by Wilks (gce below) which a t tempts  t o  f i n d  meaning 
by searching f o r  t r i p l e s .  Wilks' work was a l s o  i n f l u e n t i a l  i n  the  
development oP the  parser.  
1.3.2 LINGUISTIC PARSERS 
I n  the  a r e a  of l i n g u i s t i c  parsers ,  t h e r e  a r e  cur ren t ly  four  
d i f f e r e n t  methods betng used. They a re :  a) semantic t r i p l e s ,  b) aug- 
mented t r a n s i t i o n  networks, c) procedures and d) pa t  t e r n  matching. 
These can be described most e a s i l y  by examining s p e c i f i c  examples 
of each. 
The parser  developed by Yorick Wilks [17, 181 is based on i d e n t i -  
fy ing t r i p l e s  composed of an a c t o r ,  an  a c t  and an object .  Input i s  
parsed by applying t r ia l  templates t o  the  input and i d e n t i f y i n g  
candidates t o  f i l l  t h e  t h r e e  s l o t s  i n  a template. The a s s o c i a t i o n  of 
an  input  word with a template s l o t  is made by consideratiom of t h e  
-12- 
semantic  r e l a t i o n s h i p  between t h e  i n p u t  word and t h e  requirements  of t h e  
s l o t ,  The c l o s e r  t h e  r e l a t i o n s h i p ,  t h e  b e t t e r  t h e  match, and t h e r e f o r e ,  
t h e  b e t t e r  t h e  parse .  The cho ice  between s e v e r a l  p o t e n t i a l  pa r s ings  
of an  i n p u t  s t r i n g  is detennined by a scheme for computing t h e  qemantic 
d e n s i t y  of t h e  pa r se  based on t h e  dumber and types  of matches t h a t  a r e  
obtainkd by f i l l i n g  t h e  s l o t s  i n  t h e  t r i p l e .  Assuming Engl i sh  i s  a 
redundant language, tbe par se  having t h e  g r e a t e s t  dens i ty  is t h e  one 
which con ta ins  t h e  most  i n t e rconnec t ions  and, t he re fn re ,  is  t h e  one to  
be  accepted as the  c o r r e c t  pa r se .  
An important  f e a t u r e  of Wilks' system is  t h a t  s y n t a c t i c  propert icis  
o f  words t a k e  a r o l e  secondary t o  t h a t  assumed by t h e  semantic  p rope r t i e s .  
Thus i t  is  p o s s i b l e  t o  determine t.he meaning of  i n p u t  which i s  ill-formed 
and grammatically i pco r rec t .  
Augmented t r a n s i t i o n  networks have been used f o r  &me t i m e  a s  a 
method f o r  parsing.  The p r i n c i p a l  system u t i l i z i n g  t h i s  method is t h e  
NASA l u n a r  rocks  system developed by Woods [20]. The p a r s e r  used by th  
LNR group [lo] is  of  similar design. While t h i s  technique is very  
flexible i n  a l lowing a l a r g e  grammar t o  be c o n s i s t e n t l y  modelled, most 
implementations of it  have been s t r i c t l y  s y n t a c t i c .  L i t t l e  a t tempt  has  
been made t o  inco rpora t e  semantic  knowledge i n t o  t h e  pars ing .  Without 
semantic  knowledge, and us ing  the s t r i c t  grammatical r u l e s  embedded i n  
t h e  t r a n s i t i o n  ne'tuorks, t h i s  approach is  r e a l l y  very b r i t t l e .  I t  is no t  
capable  of handl ing  ungrammatical i npu t  w i t h  much success.  
- i n -  
Clqsely r e l a t e d  t o  t h e  augmented t r a n s i t i o n  network approach is  t h e  
use of procedures t o  desc r ibe  a g r a m a r  a s  exemplif ied hy Winograd's pro- 
gram [lg] .  This  system is a l s o  predominately s y n t a c t i c  i n  nature.  A l l  
information about t h e  grammar is represented  i n  terms of  a c t u a l  t o u t i n e s  
whioh a r e  invoked during parsing.  
The input  ana lyze r  used i n  Colby's A r t i f i c i a l  Paranoia modbl. [ 5 ]  
is e s s e n t i a l l y  a p a t t e r n  matcher which uses  a few t r i c k s  t o  normalize 
a l l  i npu t  t o  s h o r t  s t r i n g s  whibb it hopes t o  recognize.  It i s  no t  r e a l l y  
e i t h e r  s y n t a c t i c a l l y  o r  semanr lca l ly  based bu t  depends mostly on t r ans -  
formations t o  reduce inpu t  t o  s imple,  empi r i ca l ly  de r ived ,  recognizable  
forms. The power of  t h i s  approach l i e s  i n  i t s  a b i l i t y  t g  accept  even 
t h e  most ungrammatical i npu t  and r e l a t e  i t  t c  sodeth ing  which is a l r eady  
known. Thus, t h e r e  is a given context  i n  which a l l  inpu t  is  in terpre ted--  
t he  con tex t  of what t h e  model knows and wants t o  cont inue  t a l k i n g  about. 
1.3.3 OUTPUT GENERATION 
Perhaps t h e  work on output  product ion  which has  most inf luenced t h e  
c u r r e n t  program i s  t h e  model by Colby [ 5 ] .  A s  descr ibed  i n  t h e  previous  
sec t ion ,  i n p u t  is recognized by reduct ion  t o  s imple  i d e n t i f i a b l e  p a t t e r n s  
which can b e  matched t o  p re s to red  s t r i n g s  i n  t h e  program's memory. Also 
inc luded i n  t h e  memory a r e  corresponding sets of s t r i n g s  which a r e  t o  be  
used as output .  Fixed responses a r e  s e l e c t e d  based on t h e  c u r r e n t  s t a t e  
of t h e  program's s e l f  model; t he  s t a t e  of i t s  mgdel of t h e  person and t h e  
previous conversat ion.  Th i s  technique g ives  t h e  appearance of a normal, 
humanlike dialogue. Thfs approach is success fu l  because conversa t ion  is  
always d i - r ec t ed - in  one p a r t i c u l a r ,  very  narrow, d i r e c t i o n  by t h e  way inpu t  
i s  understood. The context  is f i x e d  and must no t  d e v i a t e  from a s i n g l e  
2. PROGM1 OVERVIEW 
2.1 PROGRAEi STRUCTURE 
The l o g i c a l  s t r u c t u r e  f o r  t h e  program c o n s i s t s  of  c s s e n t i n l l y  fou r  
main s e c t i o n s  executed  i n  sequence wi th  each s t e p  producing informnt iou  
r e q u i r e d  by t h e  fo l lowing  s t e p .  This  des ign ,  w i th  minor v a r i a t i o n ,  i s  
c h a r a c t e r i s t i c  o f  many e x i s t i n g  programs f o r  language understanding.  
One no t a b l e  except ion  has  been the  SPEECH UNDERSTANDING PIIOJKCT [8] 
which advocates  t h e  use of p a r a l l e l  nodules  working s i~nu l tnneous ly  on 
rhe i n p u t ,  p a s s i n g  d a t a  f r e e l y  between r o u t i n e s ,  u n t i l  t h e  d e s i r e d  end 
r e s u l t  is reached.  
The fo l lowing  a lgo r i t hm d e s c r i b e s  how JIMEfY3 behaves q$ t h e  m B s t  
s u p e r f i c i a l  l e v e l .  
DO u n t i l  person i s  through t a lk ing :  
(1)  . Request u s e r  i npu t  and t r a n s l a t e  Engl i sh  words i n t o  i n t e r n a l  
. codes (?EMORY node numbers). 
(2)  . P a r s e  i n p u t  t o  c r e a t e  t h e  "best"  p a r s e  network(s) .  
(3) . Match each p a r s e  network wi th  s t r u c t u r e s  i n  memory t o  produce 
. t h e  "best"  match(es)  . 
. 
( 4 )  . Produce a  response  based on t h e  memory match(es) .  
END-DO 
To make t h e  p roces s ing  of  t e x t  more e f f i c i e h t ,  Engl i sh  words and 
punc tua t ion  a r e  t r a n s l a t e d  i n  s t e p  (1)  i n t o  node p o i n t e r s .  Undefined 
words a r e  changed t o  n u l l  p o i n t e r s .  
The p a r s e r  t hen  c r e a t e s  a p a r s e  network from which a network d e n s i t y  
is compu~ed. This d e n s i t y  is a measure of how w e l l  t h e  p a r t i c u l a r  p a r s e  
c a p t u r e s  t h e  meaning of t h e  i npu t .  The p a r s i n g  procedure is d r iven  by 
heur is t ics  t o  do a search of the most IikBly parse networks. In case of 
ambiguity, several  parse networks can be passed t o  the  !uemurymatch routine. 
I f  the input was a statement, the  information given by the netpork is 
stored i n  short  term membry (Sdi) .  If a question is asked, then the parse 
network is  marched agains t  event nodes i n  the memory s t ruc tu re  t o  f ind 
potent ia l  answers. A match score id computed f o r  each pat tern  match 
based on how well  the three  major components (ACTOR, ACT and OBJECT) and 
a l l  minor components (s ingle  modifiers and preposit ional  phrase modification) 
match, Only the c loses t  matches a r e  re ta ined fo r  use by the output 
g & e r a t ~ r .  Ambiguity i n  the parse, i f  i t  ex i s t s ,  is  resolved here by the 
se lec t ion of the bes t  match regardless of i ts  generating parse network. 
The response Is generated by procedures operating on the memory 
matches. The value of the match score determines generally what the  
response content should be.  The exact form of the response i s  determined 
by. theway the components of the input matched (or did not hatch) the  
malory pattern.  After  a form is decided on, the  response i s  made 
g ra rnmat ica lmi ihen  printed. Control is then returned to  the input 
routine and the w N e  sequence is  ready t o  be repeated. 
2.2 TI= RUNNING PROGRAM 
The environment, the fime-sharing system on the  Honeywell 66/60 
running GCOS a t  the University of Kansas, i n  which t h i s  program was 
developed and is  run dic ta ted  its form.. This machine poses severa l  
problems besides the  lack of su i t ab le  languages, the most ser ious  being 
the 25K word l i m i t  on the amount of memory an in te rac t ive  program 
can obtain. 
Since the re  a r e  no i n t e r a c t i v e  s t r f n g  ar l is t  processin2 languages 
ava i l ab le  on the system, only FORTRAN came c l o s e  t o  s a t i s f y i n g  t h e  
requirements of a language i n  which a natura l  langpage system could be 
r e a l i s t i c a l l y  implemented. What was wanted was a high l e v e l  language 
with overlay c a p a b i l i t i e s ,  s m a l l  but powerful I/o packages and the  
f a c i l i t y  f o r  independently compiled and t e s t e d  subrout ines  with u t i l i t i e s  
f o r  the  maintenance of subrout ine  l i b r a r i e s .  
The program now running cons i s t s  of over 130 subroutines w r i t t e n  i n  
a t ~ t a l  of about 13K l i n e s  of FORTRAN code. It runs i n  approximately 
20K (36-bit) words of memory when segmented i n t o  5 overlays.  An 
unlinked vers ion  i s  approximately 37K words i n  s ize .  The use  of core  
by various p a r t s  of t h e  program i s  given i n  Table 1. 
Responee time f o r  t h e  pragfam is  goad considering t h e  amount of 
overhead required because of memory cons t ra in t s .  Dialogue l i k e  t h a t  
given e a r l i e r  t akes  2-3 seconds between the l a s t  charac te r  of input  
u n t i l  t h e  answer s t a r t s  t o  P r j n t .  
TOTALS 
1 / 0  PACKAGE 
DATA 
SCRTCH ar ray  
MEMORY Paging a r e a  
WORDLIST a rea  
Miscellaneous 
PROGRAM 
Main l i n k  + support r o u t i n e s  10.2 
I n i t i a l i z a t i o n  & Setup 1.9 
Command processor 3.3 
Parsing 5-3 
Memory matching 6.2 
Output production 3.3 -------- 30.2 
Table 1: Storage a l l o c a t i o n  f o r  unlinked program i n  
thousands of Honeywell 66/60 words 
-17- 
2.3 DATA FILES 
Two data f i l e s ,  WORDLIST and MDIORY, a r e  required by the program. 
The WORDLIST f i l e  contains t he  tat representation fo r  a l l  words* 
punctuation and system commands along with the keys fo r  t ranslat ing tha t  
tex t  in to  memory node numbers. The MEliORY f i l e  is the col lect ion of a l l  
memory nodes. 
To aid i n  the recreation and continual updating of these f i l e s ,  the 
data ~ m t a i n e d  i n  them is present i n  a text  f i l e  which is maintained on- 
l i n e  using the time-sharing tex t  edi tor .  After changes to t h i s  primary 
f i le ,  are made, a program, separate from JIMMY3, t rans la tes  the tex t  code 
in to  the WORDLIST and 1lEMORY f i l e s .  A second program can unload the  
WORDLIST and MEMORY f i l e s  back to tex t  whennecessary. Currently the 
text  f i l e  contains a vocabulary of 387 en t r ies  (t~oltds, punctuation and 
commands), 22 ACT usages and 7 facts .  This information is encoded i n  
approximately 4000 l i n e s  of symbolic node representation. It, when 
loaded, creates a WORDLIST of 387 seoarate en t r ies  and a MMORY with 
approximately 800 memory nodes. 
3. MEMORY STRUCTURE 
The memory system fo r  JIMMY3 is an aggregation of four components: 
WORDLIST, MEMORY, temporary s t ructures  and STM. With the exception of the 
WORDLIST, each part  i s  a collection of one o r  more substructures consist- 
ing of nodes connected by relations.  
-18- 
3.1 COEPONENTS 
WORDLIST. The WORDLIST is an index i n t o  t h e  tEMORY component end 
cons i s t s  of r epresen ta t ions  f o r  a l l  u n i t s  of input  JIMMY3 is  t o  recognize. 
Items not  included i n  the  WORIILIST a r e  declared undefined by the  input 
decoding routines.  I n  add i t ion  t o  the  exact  t e x t  representa t ion,  a 
po in te r  t o  the  corresponding node i n  MEMORY is given. 
MEMORY. This is  t h e  model's long term memory--essentially a collec- 
t i o n  of i n t e r r e l a t e d  nodes. 
A node is  t h e  smal les t  packet of information i n  MISElORY t h a t  can Be 
referenced by a s i n g l e  po in te r  ( e i t h e r  a WORDLIST pointer  o r  a po in te r  
from another node.) The information contained i n  a node may descr ibe  o r  
modify a s i n g l e  word o r  symbol o r  may be a c o l l e c t i o n  of r e l a t i o n s  
connecting s e v e r a l  nodes i n t o  a more complex s t ruc tu re .  
information s to red  i n  nodes describing s i n g l e  words is  var ied and 
includes such items a s  p a r t  of speech, i n f l e c t i o n a l  v a r i a t i o n s  and subset  
o r  superset  names. Other types  of data ,  f o r  exampld, events and s t a t e -  
ments of f a c t ,  a r e  formed by nodes t h a t  contain polnter  s t r u c t u r e s  
r e l a t i n g  o ther  nodes i n  a predetermined fashion. 
TENPORARY STRUCTURES. There a r e  th ree  b a s i c  memory s t r u c t u r e s  of a 
t r a n s i e n t  na tu re  t h a t  can e x i s t  during t h e  processing required t o  deter-  
mine meaning and produce an answer. They are :  a )  the  pa rse  network, 
b) the  memory match s t r u c t u r e ,  and c) t h e  output production l is t .  
A parse  network is a smal l  co l l ec t ion  of nodes of t h e  same format, 
and connected i n  the  same fashion,  a s  the  nodes i n  MEMORY. Produced by 
the parser ,  i t  is  used t o  represent  the  meaning of input t ex t .  This net- 
-1 9- 
work canta ins  information about the  major components of the  input ,  i.e., 
what wards o r  phrases represent. the  ACTOR, ACT and OBJECT; i t  a l s o  conta ins  
i n f o v a t i o n  abwY dl modifying words and phrases Found i n  the  input.  
A memory match s t zuc tu re  is  produced by matching the parse network 
with MEMORY. It conta ins  comparison da ta  r e l a t i n g  t h e  corresponding 
components of the  input ( ~ a r s e  hetwork) and a subs t ruc tu re  i n  MEMORY. 
T ~ Q  output production l i s t  is used during the  examination of t h e  
memory match s t r u c t u r e s  t o  accumulate the  d i s c r e t e  components of t h e  
response t o  be made, i.e., the  words and punctuation f o r  t h e  answer t o  
the q u e s t f ~ n .  Each element i n  t h i s  f i s t  contains a po in te r  t o  a t e x t  
representa t ion f o r  the  word (or punctuation), its funct ion and i t s  
r e l a t i o n  t o  o the r  words i n  the  sentence. When the  output l ist  has  been 
formed, it  i s  passed t o  a p r i n t  rou t ine  which w r i t e s  the  answer t o  the  
terminal. 
STM. STM is t h e  s h o r t  term memory component f o r  the  program. It 
is e s s e n t i a l l y  a co l l ec t ion  a rea  f o r  parse  networks, memory match 
s t ruc tu res  and output production lists generated f o r  previous inpu t s  
and t h e i r  responses. 
3.2 RELATIONS 
Among the a t t r i b u t e s  used t o  r e l a t e  the  d i f f e r e n t  MEMORY and parse  
network nodes t o  each o the r  a r e  those  f o r  descr ib ing h ie ra rch ies  and 
chains. 
HIERARCHY. Hierarchies a r e  v e r t i c a l  s t r u c t u r e s  formed by r e l a t i n g  
nodes t o  one another using the  ISA a t t r i b u t e .  Set  (superse t ,  subset)  
r e l a t i o n s  a r e  implement& as hierarchies .  A t y p i c a l  example would be 
-20- 
t h e  path through t h e  nodes: BR,ANDT (ISA) BOY (ISA) PERSON which d i sp lays  
t h e  r e l a t i o n s  among t h e  t h r e e  nodes. 
I n  a hierarchy,  each node is connected only t o  a s i n g l e  node 
immediately be fore  and a f t e r  it. However, f o r  any given hierarchy,  i t  
is poss ib le  t h a t  t h e  node9 involved a r e  connected t o  o thers  both i n  and 
o u t  of t h e  hierarchy by connections independent of t h a t  p a r t i c u l a r  
h ierarchy s t r u c t u r e .  
T r a n s i t i v i t y  is a property of h i e r a r c h i e s  as implemented by t h e  
program. Therefore,  i n  t h e  example slbove, t h e  information, BRANDT (ISA) 
PERSON, is  i m p l i c i t  dn t h e  hierarchy.  It should be pointed our t h a t  i n  
t h e  forward l i n k  between BRANDT and BOY, t h e  connection is  n o t  one of 
set membership but  r a t h e r  one of subse t  as is the  r e l a t i o n  between BOY 
and PERSON. This  is  accomplished by the  concept of GENERIC nodes even 
f o r  s p e c i f i c  ins tances  o f ,  say, people. Therefore,  t h e  BRANDT i n  t h e  
hierarchy is a GENERIC node which w i l l ,  i n  tu rn ,  have under i t  an 
INSTANCE node of BRANDT which i s  r e l a t e d  by set  membership. The GENERIC 
BRANDT is a set of one element. 
CHAIN. The mechanism f o r  const ruct ing hor izon ta l  s t r u c t u r e s  is 
the chain. A s  contras ted t o  t h e  be fore  l ink-a f te r  l i n k  s t r u c t u r e  of 
t h e  hierarchy, t h e  chain allows t h e  c rea t ion  of a l i n e a r l y  ordered set 
of nodes each r e l a t i n g  t o  a common "root" node t o  which i t  i ss  attached.  
This allows t h e  c r e a t i o n  o f  s e t s  of nodes r e l a t e d  by a common property.  
For example, cha ins  e x i s t  i n  MEMORY tying toge ther  a l l  i n f l e c t i o n a l  
v a r i a t i o n s  of a word. Another example of a chain is t h e  l ist  of co lo r  
nodes: RED, BLUE and GREEN. A s ing ly  l inked list exdsts  through these  
nodes b u t  i n  addi t ion,  each of t h e  t h r e e  po in t s  back t o  t h e  node f o r  
COLOR--the node which po in t s  t o  t h e  f i r s t  c o l o r  i n  t h e  list. 
The more important l i n k s  cur ren t ly  used t o  form chains a r e  
described next. 
DEFN. The DEFN l i n k  i s  used t o  t i e  together a l l  nodes t h a t  
represent  d i f f e r e n t  d e f i n i t i o n s  for  a word. Thus, f o r  every symbol i n  
the  WORDLIST, the re  is a chain of nodes i n  MEMORY connected v i a  t h e  
DEFN l ink.  
. . 
' - [  U]ISA\ +j : -3 To next  nodc 
tcprcscnLing a 
s u b s e t  oE PERSON. 
S " " " '  "" 
I 
r IIMND'S 
I \ h  
............ 
Flgure 1 : H i  cra rcliv C t l r  RRANIIT ( I S A )  
BOY ( 1 S A )  PERSON 
INST. A chain emanates from each d e f i n i t i o n  of each word i n  MEMORY. 
These chains a r e  created w i t h  t h e  INST l ink  and represent  the set of a l l  
ins tances  of the  word given by t h e  roo t  node of t h e  chain. 
-22- 
ISA. This l i n k  is used t o  generate h ierarchies .  Actually created 
is  a chain with the  property t h a t  every node i n  the  chain is re la ted  by 
the  ISA l i n k  t o  the root  node. The root  node can be a member of 
another ISA chain, thus giving a mul t i level  l i~e ra rchy .  Figure 1 shows 
p a r t  of the hierarchy f o r  B W D T  (IsA) BOY (ISA) PERSON. 
ACTOR. This is used t o  connect a s e t  of ACTION nodes i n  which the  
root  node is used a s  the  ACTOR of a t r i p l e .  For example, the re  would be 
a chain through the  t r i p l e s  representing (BRANDT IUVE BOOK), (BRANDT LIKE 
MILK) and (BRANDT HAVE CAT). 
ACT. Used t o  form s e t s  of ACTION nodes which contain the  root  node 
as the  4CT of a t r i p l e .  
OBJECT. Similar t o  the  ACTOR and ACT l i n k s  except i t  chains through 
ACTION nodes which. have the root  node a s  the  OBJECJ of a t r i p l e .  
MODIFY. This i s  used t o  speci fy  a s e t  of nodes which a r e  modified 
by the  root  node. The type of t h i s  s ing le  word modification i s  p a r t i a l l y  
determined by the  p a r t  of speech of the  root  node and t h a t  of the  modified 
node. It i s  f u r t h e r  speci f ied  by the  hierarchy of t h e  root  node and the 
node being modified, For example, RED may be used t o  modify BOOK. I n  
terms of grammatical function, the  p a r t s  of speech specify t h i s  a s  an 
adject ive  modifying a noun. However, examination of the h ie ra rch ies  
d isc loses  t h a t  RED (ISA) COLOR which is  a property tha t  TIIINGk (BOOK (ISA) 
THING) can have. 
POS. This is  the  p a r t  of speech l ink.  
PREP. This l i n k  is used i n  a CONTEXT node t o  point  t o  the  preposit ion 
p a r t  of the  preposi t ional  phrase represented by the  node. 
-2 3- 
POBJ. The CONTEXT node a l so  contains the  reference t o  a preposi t ional  
object. 
CNTXT, This is the  l i n k  used t o  a t t ach  a CONTEXT node a s  a modifter 
of another node. The chain created by a CNTXT l i n k  represents  a l l  nodes 
modified by the  same CONTEXT node. 
INFLEC. Most words i n  MEMORY belong t o  an INFLEC chain. This is a 
s e t  of i n f l e c t i o n a l  va r ia t ions  of the  word. It brings together d i f f e r e n t  
forms which vary in tense,  number, person, e tc .  The I N n E C  chain through 
"I" would jo in  nodes f o r  I. m., MY, and MINE. A similar chain through 
OWN would l i n k  nodes f o r  OWN, OWNS, OWED and OWNING. 
ANTONYM. A l l  antonyms of a word a r e  chained together  using t h i s  
l ink.  The words i n  the  chain a r e  not  antonyms of each other.  
PARTS. This l i n k  is used i n  nodes t o  express the  sub-part, super- 
pa r t  re la t ionships .  
3.3 SUBSTRUCTURES 
Using the r e l a t i o n s  f o r  constructing h ie ra rch ies  and chains, var ious  
other ,  more complex, s t ruc tu res  can be created. These a r e  s t r u c t u r e s  
formed by t h e  coincidence of several  h ie ra rch ies  o r  chains passing through 
a s ing le  node, Of a l l  poss ible  subst ructures ,  t r i p l e s  (ACTION nodes), 
CONTEXT nodes and SEMANTIC MODIFICATION nodes a r e  of g rea tes t  importaae .  
TRIPLES. A t r i p l e  (ACTION node) is a node through which passes th ree  
separate chains, one each f o r  ACTOR, ACT, and OBJECT. These t r i p l e s  a r e  
used t o  specify events, f a c t s  and a s  semantic d e f i n i t i o n s  f o r  t h e  ACTS 
ii.e., as ranges f o r  acceptable candidates f o r  ACTORS and OBJECTS.) The 
chains f o r  ACTOR, ACT and OBJECT o r i g i n a t e  i n  nodes which represent those 
major components and continue through t h i s  node t o  where they merge i n  
d i f f e r e n t  combinYtions with s t i l l  o t h e r  chains  t o  form more t r i p l e s .  
Figure 2 show$ the  re levant  stsructure of a t r i p l e  represent ing "PERSON 
HAVE BOOKI~.  
CONTEXT NODES. A c l o s e  r e l a t i v e  of t h e  t r i p l e  is  the  context node. 
It too has chains passing through it determining i ts s t ruc tu re .  However, 
i t  has only two chains: those  fo r  PREP and POBJ. These spec i fy  a pre- 
pos i t ion  and i t s  object .  Nodes of t h i s  s o r t  a r e  used a s  modif iers  of 
s i n g l e  nodes, t r i p l e s  and o t h e r  context nodes. 
PERSON., . . . 
c3: 
. . 
"-:-r-i ACTOH [ 
:. ... . .. . ..: I 
I 
l o  n r x t  t r i p l e  u d n q  
l'1'.1?501\1 a s an ACTOI?. 
I-MVE. .. :,.. 
' YbI ACT[- lo  n e x t  t r l p l f ?  u s i n 7  
I-L$Vl: a s  an ACP. :'. . . . . . . . . . 
ro n o x t  t r i p l e  u ,'in3 
f 3 O O K  a .c a n  013J LC;-I . 
,,y IlOOK.. . . . . . 
I .  
I 
* .  .... 0 .  ...* 
Figure 2: A t r i p l e  represent ing (PERSON 
IlAVE BOOK). 
-25- 
SEMANTIC MODIFICATION NODES, These nodes a r e  not  s i m i l a r  t o  t r i p l e s  
o r  context  nodes e i t h e r  i n  design o r  i n  function. However, they a r e  one 
of t h e  major subs t ruc tures  appearing i n  MEMORY so  w i l l  b e  considered he re  
b r i e f ly .  These nodes o r  co l l ec t ions  of nodes a r e  t h e  d a t a  s t r i n g s  which 
a r e  used t o  d r ive  the  pa r se r  and i n  t h a t  capaci tyl  w i l l  be described l a t e r .  
Essen t i a l ly  they contain ordered lists of semantic ca t egor i e s  represent ing  
p o t e n t i a l  modif icat ion pa t t e rns  f o r  words, These lists a re  appl ied  by the  
parser  t o  determine semantical ly acceptable  word s t r i n g s  i n  the inpu t  t h a t  
can moaify o the r  word s t r ings .  For example, a noun would have a list 
describing the  types of  ad j ec t ives  t h a t  could modify it. Tor ad jec t ives ,  
t h e r e  would be a list of p o t e n t i a l  adverb types. Each d e f i n i t i o n  of every 
word i n  MEMORY t h a t  i s  t o  be recognized during pars ing  of input  must have 
at tached t o  i t  a semantic rnudification node. I n  cases  where many words 
have t h e  same node, t h e  s t r u c t u r e  f o r  semantic modif icat ion is  implemented 
as a chain th rough-a l l  words having t h e  same modif ica t ion  requirements. 
3.4 NODES 
Nodes i n  MEMORY are represented a s  f ixed-size blocks of contiguous d i s k  
o r  co re  loca t ions  and are the  smal les t  u n i t s  of MEMORY t h a t  can be refer -  
enced. Each node is composed of l i n k s  and v a r i a b l e  length  a t t r i b u t e s  which 
may be da t a  o r  po in t e r s  t o  o the r  nodes. A l l  t h e  space a l loca t ed  to  a node 
does n o t  have t o  be used. I n  f a c t ,  most nodes use only  p a r t  of t h e i r  
a l l oca t ed  space t o  conta in  a t t r i b u t e s ;  t he  remainder is empty (zero). The 
current  model uses a s i z e  of 8 words f o r  its nodes. (This s i z e  w a s  no t  
determined ebnpirically a s  t h e  optimum s i z e  but  was, ins tead ,  s e l ec t ed  
because of d i sk  hardware cons idera t ions , )  The f i r s t  word of  every node i n  
MEMORY is used f o r  bookkeeping and, t he re fo re ,  is no t  ava i l ab le  f o r  s t o r i n g  
-26- 
a t t r i bu te s .  Contained i n  t h e  f i r s t  word is  the  node number i t s e l f ,  an 
ind ica to r  of &he kind of node p lus  a poin ter  t h a t  gives The next  word of 
the  node ava i l ab le  f o r  use a s  a 3 Lnk o r  va r i ab le  length a t t r i b u t e .  
Whenever a s i n g l e  node has more a t t r i b u t e s  than i t  can conta-in, addi t ional  
8-word blocks a r e  a l loca ted  a s  extensions of the o r i g i n a l  node, These 
extensions are  connected i n  a chain t o  the o r i g i n a l  node by t h e  CONT l i n k  
and a r e  t ransparent  t o  a l l  t he  program except f o r  t he  most bas i c  MEMORY 
manipulation routines.  
There a r e  e igh t  d i f f e r e n t  kinds of nodes i n  memory, each with i t s  
own function. The Kinds a r e  TYPE, SIMPLE GENERIC, ACTION GENERIC, CONTUT 
GENERIC, SIlPLE.INSTANCE, ACTION INSTANCE, CONTEXT INSTANCE and SEMANTIC 
MODIFICATION. Each ha@ a d i f f e r e n t  purpose i n  t h e  representa t ion  of in- 
formation i n  MEMORY. Br ief ly ,  t h e i r  purposes a r e  a s  follows. The TYPE 
node is used a s  t h e  reference poin t  between the  WORDLIST. and MEMORY. The 
th ree  GENERIC nodes a r e  used t o  spec i fy  s y n t a c t i c  and semantic information 
associa ted  with words and subs t ruc tures  given by t h e  INSTANCE nodes. The 
INSTANCE nodes a r e  used t o  represent  ac tua l  ins tances  of words o r  fac ts .  
SEMANTIC MODIFICATION nodes a r e  used t o  cdntain information required by 
the  parser  t o  he lp  d i r e c t  i ts  s e l e c t i o n  of t he  modification pa t t e rns  during 
parsing of t h e  input. The o the r  d i s t i n c t i o n  made on node types is among 
SIMPLE, ACTION and CONTEXT. SIMPLE nodes reprebent  s ing le  words, ACTION 
nodes a r e  used t o  represent  t r i p l e s  and CONTEXT nodes a r e  used f o r  
preposf t i o n a l  phrases. 
TYPE. The 'TYPE nodes in MEMORY a r e  i n  a one-to-one correspondence with 
the  e n t r i e s  i n  t h e  WORDLIST and serve  as t he  reference nodes f o r  the WORDLIST 
poin ters  t o  MEMORY. Some a t t r i b u t e s  t h a t  may appear i n  a TYPE node a r e  DEFN 
-27- 
and POS. F i r s t  to appear i n  a TYPE node is an a t t r i b u t e  giving the  t e x t  
representa t ion f o r  t h e  symbol. An): rou t ines ,  sucb a s  t h e  output produc- 
t i o n  programs, can g e t  t h e  t e x t  r epresen ta t ion  f o r  p r i n t i n g  d i r e c t l y  from 
the TYPE nodes. This t e x t  is repeated here  s ince  the  symbol given ilP the  
WQRDLIST is i n  6 charac te r  chunks l inked together  -- a form not s u i t a b l e  
f o r  p r in t ing .  The second a t t r i b u t e  always present  is the  DEFN l i n k  used 
t o  chain together a l l  d e f i n i t i o n s  of t h e  symbol. Only words and 
punctuation w i l l  have non-null chains of d e f i n i t i o n s .  For words, t h e r e  
is a SIMPLE GENERIC node i n  the  DEFN rliilrn f o r  each d i f f e r e n t  word 
usage. For punctuation, t h e r e  is  a s i n g l e  SIMPLE GENERIC node chained 
t o  the TYPk node. System commands and s e t  names have a n u l l  Set  acr 
usages s i n c e  information of a more d e t a i l e d  na tu re  f o r  them is  not 
required; Figure 3 shows the  r e l a t i o n s h i p  between TYPE nodes and the  
WORDLIST and between TYPE- nodes and SIMPLE GEIJERIZ nodes. 
Among tne opt iona l  a t t r i b u t e s  used i n  a TYPE node is t h e  SYSSET l i n k  
used t o  chain together  a l l  TYPE nodes which name i t e m s  i n  t h e  set, An 
example of the use of SYSSET is f o r  p a r t  of speech. I n  t h e  TYPE node 
f o r  t h e  symbol i s  t h e  root  f o r  t h e  chain througli t h e  TYPE nodes f o r  
NOUN, PRONOUN, VERB, e t c .  
The' POS l i n k  is present  i n  the  TYPE nodes f o r  words t h a t  name t h e  
various grammatical p roper t i e s  (s ingular ,  nominal, e t c . )  and p a r t s  of 
speech (noun, e tc . ) .  
-28- 
MEMORY 
I - 1 
I -------- 
I 
I 
I 
1 BOOK 1- - * TYPE node 
f o r  ROOK. 
S LEPLE GENERIC 
node f o r  one 
usage of  BOOK. 
SIMPLE GENERIC 
node f o r  a second 
usage of BOOK. 
Figure 3: A segment o f  MEMORY showing t h e  kJa majur 
func t ions  of the  TYPE node. 
SIMPLE GENEKIC. These nodes are used t o  represen t  t h e  d i f f e r e n t  
usages f o r  words, i.e., 40 s e r v e  mainly a s  a source  f o r  semantic and 
s y n t a c t i c  information about a word. A l a r g e  v a r i e t y  of a t t r i b u t e s  can 
appear i n  a SIMPLE GENERIC node, such as DEFN, IRST, ISA, POS, SYSMOD, 
INFLEC, SYNONYM, ANTONYM, ACTOR, ACT, OBJECT, MGDIFY, and PARTS. O f  
these ,  only two a r e  required.  The DEFN l i n k  must be p resen t  t o  t i e  t h i s  
usage of t h e  word wi th  i ts TYPE node and t o  cont inue t h e  chain  t o  t h e  
nex t  usage, i f  any. A l s c ~ r e a u i r e d  is  t h e  part of  peech l ink ,  POS. 
Of the other  a t t r i b u t e s  tha t  can appear, two of the more important: nnes 
a r e  the  INST and ISA l inks .  The INST l i n k  is used t o  c rea te  the  chain 
of spec i f i c  instances of t h i s  word represented by SIMPLE INSTANCE nodes 
To create  h ierarchies  within the  s e t  of SIMPLE GENERIC nodes, the ISA 
l i n k  is  used. Although a c h  ISA l ink  belongs t o  a chain, t h e  presence of 
two ISA l inks  -- one a roo t  l ink,  t h e  other d $on l i h k  -- r e l a t e s  the  
current node t o  the  ohe immediately above it and t h e  ones below it. 
The ACTOR, ACT and OBJECT l inks  i n  a SIMPLE GENERIC node point t c  
ACTION GENERIC nodes that  use the  node a s  an ACTOR, ACT o r  OBJECT, The 
PREP and POBJ l inks ,  i f  present, point  t o  CONTEXT GENERIC nodes t h a t  use 
t h i s  node as  a preposit lop o r  a preposi t ional  object .  The INFLEC, MODIFY, 
PARTS, ANTONYM and SYNONYM l inks  poidt  to  o ther  SIMPLE GENERIC nodes tha t .  
a r e  re la ted  t o  the current  node i n  the  specif ied  manner. 
ACTION GENERIC. These nodes represent the  t r i p l e s  bsed t o  give the  
meanings f o r  ACTS. They ushally contain three  mandatory links- those f o r  
the ACTOR, ACT and OBJECT chhins. However, f o r  some ACTS, t he  OBJECT i s  
e i t h e r  not required o r  is optional. As an example of an ACTION GENERIC 
node, consider the  ACT "know1*. In  t h e  curreht MEMORY, i t  has three  t r i p l e s  
attacbed t o  it  giving the  verb senses of (PERSON OWN THING), (PERSON OWN 
ANIILZL) and (PERSON OWN SLAVE). 
Only two other a t t r i b u t e s  a re  allowed i n  an ACTION GENEKIC node. 
These a r e  the  MODIFY and CNTXT l i n k s  which are used t o  specify s i n g l e  word 
and preposi t ional  phrase modification of the  ACTION node. When these  Gwo 
links appear i n  a GENENC node, they r e f e r  t o  the po ten t i a l  types of 
m d d f  i ca t ion  t h a t  may occur? 
-30- 
CONTEXT GENERIC. Nodes of t h i s  kind always contain exactly three  
l inks:  PREP, POW and CmT. Since t h i s  node is used t o  specify po ten t i a l  
types of modification, the PREP and POBJ l i n k s  a r e  used t o  point  t o  
pa r t i cu la r  SIMPLE GENERIC nodes f o r  the  preposit ion and preposi t ional  
object. The CNTXT l i n k  is  used t o  t i e  t h i s  CONTEXT node t o  the ACTIOK 
GENERIC node i t  modifies. 
SIMPLE INSTANCE. A SIMPLE INSTANCE node is  present i n  MBIORY f o p  each 
d i s t i n c t  instance of each word tha t  has been used anywhere na an ACTOR, ACT&, 
OBJECT, modifier, etc.,  i n  the. representation of information by ACTION 
INSTANCE and CONTEXT INSTANCE nodes, T b r e  is only one mandatory l ink  in 
the  SIMPLE INSTANCE node, the  INST link.  However, there  a r e  usually several  
more se lected from the set of ABTOR, ACT, OBJECT, PREP, POBJ, and MODIFY 
depending on t h e  uses to  which t h i s   articular instance has been put. I n  
the  case of a l l  l i n k s  except MODIFY, t h e  l i n k  p o i ~ t s  t o  the  ACTION INSTMCE 
o r  CONTEXT INSTANCE nodes where the  current node is used. For MODIFY, 
however, i t  can show where t h i s  node modifies another o r  is modified by 
another depending on whether o r  not t h i s  is the  root  l ink.  
ACTION INSTANCE. A l l  f ac tua l  information within the  systm. is 
represented by ACTION INSTANCE nodes. Fhese nodes are  t r i p l e s  t h a t  bring 
together the re la t ions  between actual  INSTANCES of ACTORS, ACTS and 
OBJECTS plus  t h e i r  modiffcatioq. 
CONTEXT INSTAN&. The modification of ACTION INSTANCE nodes by 
preposit ional  phrases is specif ied  by the  use of PREP, POBJ and CNTXT l inks  
i n  nodes of t h i s  kind. 
SEMANTIC MODIFICATION. The s t r u c t u r a l  information necessary f o r  the  
parser t o  determine correct  forms of modification is given by var iable  
length a t t r i b u t e s  t h a t  can occur i n  t h i s  kind. of node. 
4. PARSER 
4.1 PARSING STRATEGY 
The data s t ruc tu re  used t o  drive the  parser is the  c r ip le  (ACTION 
GENERIC node) which spec i f i es  the semantics f o r  the  major components of 
the parse. By applying the t r i p l e  a s  a template t o  the input, the  ACTOR, 
ACT and OBJECT can be identif ied.  
A s  the  input is parsed, its meaning is converted' i n t o  a parse network 
and a network "score" is calculated Usually there  a r e  several  parse  
networks constructed from a s ing le  input representing d i f fe ren t  meanings of 
that  input. The b e s t  parse is tha t  one which has t h e  highest score from i t s  
parse network 
A parse network is created from nodes s imilar  i n  design and furrcbion t o  
those present i n  MEMORY. Like the MEMORY s t ructures ,  i t  is composed of 
INSTANCE nodes of a l l  kinds: SIMPLE, ACTION and CONTEXT (see Figure 4). 
These nodes a r e  connected t o  one another by the same kinds of a t t r i b u t e s ,  
e.g., the  ACTION INSTANCE node has l i n k s  t o  the ACTOR, ACT and OBJECT 
INSTANCE nodes, the  SIMPLE INSTANCE nodes contain MODIFY and CNTXT l inks  
to other SIMPLE nodes o r  CONTEXT nodes, respectively,  etc. Because the  
normal access paths to  INSTANCE nodes using DEFN and INST l inks  i n  TYPE and 
GENERIC nodes do not e x i s t  f o r  temporary nodes, the  nodes i n  a parse  net- 
work are kept track of by a system of pointers a s  shown along the r i g h t  $n 
Figure 4. The e n t i r e  network is referenced by the  pointer  word i n  the  upper 
r igh t  hand corner. 411 references i n  the  temporary nodes of the  parse nef- 
I 
i TEFlPORARY STORAGE 
I 
I i 
I 
I 
I 
I 
I 
I 
I 
I 
. .... I +.. \. 
i 
I 
I 
I 
: SG . .. ....... , I I 
I 
I 
I 
I : (BRANDTI :(. , ,%, ,J , , , :...........: I ! 
I 
I 
I 
I 
I 
I 
: SG I ......... I ! I 
I 
I 
I 
I 
: ( W E )  I :+- -- 1 - - 
............. I 
i 
I 
I 
I 
I :SG.... .....: I 
I i 
: I 
: (BOOK) I 
............. 
....... 
: 4 -  - - I - -  
i 
I 
I 
I 
I 
* SG.. : I I 
I 
I 
I - y e  : e :  
..... 
: (RED) I - -  - - -  - <+*]INST[ ] 
Figure 4: Parse network for "BRANDT HAVE RED BOOK." 
work t o  GENERIC nodes a r e  by l inks which point to nodes that  a r e  par t  of 
MEMORY. This relates  the input to specif ic  par ts  of memory as well as 
provides the required syntactic and semantic properties of the input 
wordg for  reference during other stages of parsing. When complete, the 
parse network is i n  a format ident ical  to similar s t ructures  i n  MlMORY. 
This is very important l a t e r  during the matching of input to MEMORY where 
compatibility between the two is necessary. 
4.2 GRAMMAR 
4.2.1 ACCEPTABLE INPUT FORMS 
The parser has been developed to  correctly handle restr ic ted forms 
of W;questions and declarative sentences. 
<DO> ::= DO o r  DID or  DOES 
: := (< lef t  modification>) ACTOR 
(<right modification>) 
: := (<context phrase>) (<adverb>) ACT 
(<adverb>) (<context phrase>) 
. .- (< lef t  modif ication>) OBJECT 
(<right modification>) 
c l e f t  modification> . ,- . a s t r i ng  of words, usually adjectives, nouns, 
adverbs acid determiners whichtgodify the item 
t o  the i r  right. 
::= one of a s e t  of prepositions specified by an 
a t t r i bu te  
::= a noun from a p a r t i c u l a r  semantic c l a s s  a s  
speci f ied  by an a t t r i b u t e  
: := essen t i a l ly  the  same as <r ight  modification, 
but i e  used t o  modify verbs 
::- an adverb from a set of pa r t i cu la r  words 
speci f ied  by an a t t r i b u t e  o r  an adverb plus 
its c l e f t  modification>. 
Table 2: Grammar f o r  DO-questions and statements. 
The th ree  components of <ACTOR>, <ACT> and <QBJECT> a r e  iden t i ca l  f o r  the 
DO-question and statement. The <DO> component is  found only i n  DO-questions. 
The question mark and period a r e  the only terminating punctuation symbols 
current ly  allowed. A l l  of these components a r e  expanded i n  Table 2 i n  a 
BNF-like format. 
Additional r e s t r i c t i o n s  currently imposed upon the input a r e  the 
following: 
1. No r e l a t i v e  clauses a r e  allowed. 
2. No compound u n i t s  (subject ,  verb, etc.)  a r e  allowed. 
Elements i n  Table 2 enclosed i n  < > a r e  non-terminal elements of the 
grammar. Such elements- enclosed i n  ( ) a r e  optional. 
I n  the de f in i t ion  of <ACTOR> and <OBJECT>, the l e f t  and r i g h t  modi- 
Eication is  t o  the  i m e d i a t e  left o r  r i g h t  of the  word being modified. I n  
the case of <ACT>, however, the <adverb> and <context phrase> modification 
can occur anywhere i n  the sentence. Usually the  s ing le  word modifiers a r e  
adjacent t o  the  ACT but do not have t o  be. The <context phrase>s usually 
appear at the  beginning of the  sentence o r  somewhere a f t e r  the  ACT. 
I n  the  case of t h e  l a s t  f i v e  d e f i n i t i o n s  i n  Table 2,  i.e., 
c l e f t  modification> through cadverb>, the re  a r e  r e s t r i c t i o n s  on the  
nodes which a r e  appl icable  at t h a t  point  i n  the  parsing. Some examples 
of t h e  types of modification allowed are :  
c l e f t  modification> of a noun a blue animal book 
c l e f t  modification> of an  adj  e c t i v e  a very blue sky 
< r i g h t  modification> of a noun a f r i end  of mlne 
i n  June 
t o  the  house 
yesterday 
not 
4.2.2 SEMANTICS AND SYNTAX 
Semantics and syntax have been in tegra ted  throughout the  parsing 
procedure so tha t  both work together i n  the se lec t ion  of appropr ia te  
words and phrases out  of which the  pa r se  network is constructed. 
When matching an ACTION GENEP.IC node t o  the  input,  syntax is  used f i r s t  
t o  i d e n t i f y  nouns as p o t e n t i a l  ACTORS and OBJECTS. Then the  semantic 
accep tab i l i ty  of each is v e r i f i e d  by comparing the word with the semantic 
c l a s s  spec i f i ed  by the  t r t p l e .  
Syntax is checked by a simple matching of the  p a r t  of speech. 
Proper word order within a grammatical subunit  IS maintained automat ica l l  
by the  way the  modification requirements a r e  s e t  up. A word is  deemed 
semantically acceptable i f  it matches the  semantic c l a s s  l i s t e d  as a 
requirement, o r  i f  a word upward i n  its hierarchy matches the semantic 
c l ass .  For example, suppose the  candidate is BRANDT and the  required 
semantic c l a s s  l a  PERSON. I n  the  hierarchy containing BRANDT w e  have 
BRANDT (ISA) BOY (ISA) PERSON. A t  t h a t  polnt  the re  is a match on PERSON, 
so  BRANDT would be semantically acceptable. 
-36- 
4.2.3 PRONOUNS, AMBIGUITY AND UNDEFINED WORDS 
The cur ren t ly  implemented vers ion  of t he  program provides f o r  only 
very simple treatment of pronouns, On input ,  "I" pronouns (I, Me, MY, etc.) 
are t r ans l a t ed  t o  the  name of  the  person talking.  "YOU" pronouns a r e  
t r ans l a t ed  t o  JIMMY3. Similarly,  on output ,  t he  person's name and JIMMY3 
are t r ans l a t ed  back t o  "YOU" and "I", respect ive ly .  
Ambiguity i e  not a problem i n  t h i s  model. If two parse networks 
have i d e n t i c a l  scores ,  ambiguity i s  resolved by the  memory matching scheme. 
A l l  networks are matched aga ins t  lremory i n  the  attempt t o  l o c a t e  an  answer. 
I f  s eve ra l  matches come ou t  e q u a l l y  l i k e l y ,  they can a l l  be  reported. 
This  simple-minded approach works w e l l  by r e l a t i n g  the  input  t o  what t he  
program knows. 
Throughout t h e  program, undefined words i n  the  input  are ignored. 
However, t h e i r  t e x t  representa t ions  are saved so they can be p r in t ed  
l a t e r  t o  help explain,  say, why the  program was unable t o  i n t e r p r e t  the  
input. A s  i l l u s t r a t e d  by one l i n e  of t h e  sample conversat ian i n  
s ec t ion  1, t h e  response t o  "DO I HAVE AN OLD BOOK?" was "YES. A RED ONE. 
I DON'T KNOW WllhT OLD MEANS." T11e input  was interpreted a s  i f  i t  had 
been "DO I HAVE AN BOOK?". 
4 .3  PARSING ALGORITHM 
There a r e  two operat ions performed on the  Input t e x t  before it 1s 
a c t u a l l y  parsed: a )  preprocessing of  t he  text I n  an  e f f o r t  t o  
11 
standardize" it and b) t h e  determination of t he  type of input  recelved 
so the  proper pars ing  technique can be selected.  
-37- 
4.3.1 PREPROCESSING OF THE: INPUT TEXT 
The f i r s t  operation perfonaed on the  input is the  t r ans la t ion  of 
each defined component t o  its TYPE node pointer  t o  MEMORY. Undefined 
input items a r e  converted t o  n u l l  pointers.  Af ter  the  t r ans la t ion  t o  
TYPE node pointers,  the  input i s  checked f o r  standard greet ings  or  
c l iches  tha t  usually e l i c i t  a standard response. Examples a r e  "IIELLO. 
HOW ARE YOU?", "HI", "GOODBYE", e tc .  
The second type of preprocessing is  the attempted reductioh of 
words and phrases t o  simpler forms. This approach can be used f o r  the  
reduction of slang, misspelled words, idioms, names, e t c .  Stoled i n  the 
TYPE nodes f o r  input items t h a t  can be reduced a r e  context s t r i n g s  i n  
which an item can occur p lus  its replacement form. 
4.3.2 DETERMINING TYPE OF INPUT 
This par t  of the parsing algorithm 2s where the kznd of input, i .e . ,  
DO-question, IS-question, Wh-question, declara t ive  statement, etc., is 
deter mined. 
Questions can be detected by t h e  presence of the  question mark a t  
t h e n  end. Discrimination of questions into classes  of YES-NO, o r  Wh- 
is  determined almost completely by the  f i r s t  word of the  question. The 
only questions not co r rec t ly  c l a s s i f l e d  by t h i s  approach a r e  those wi th  
inverted order, e.g., "IiE ASKED A QUESTION, DIDN'T HE?" and hypothetical  
questions, e.g. , "IF I ASK A QUESTION. WHAT WILL YOUR ANSWER BE?". 
Any input  t h a t  ends with a period and does not begln with a verb 18 
c l a s s i f l e d  a s  a statement, 
Text t h a t  does not  end with e i t h e r  a question mark o r  a perlod is xe- 
jected with a request t h a t  the person supply punctuation with his  input. 
4.3.3 PARSING A DO-QUESTION OR STATEMENT 
The DO-question and the  statement a r e  parsed i n  iden t i ca l  fashion 
a f t e r  the  DO-word which begins the  question is s t r ipped off  and the  f i n a l  
punctuation i a  thrown away. 
An exhaustive approach t o  parsing has been selected fo r  implementation 
ra the r  t h a t  one designed t o  use predict ion coupled w i t h  backup f a c i l i t i e s .  
This decision was made pa r t ly  on technical  grounds--the inherent  d i f f i c u l t y  
i n  implementing such a parser i n  FORTRAN. A more important consideration, 
though, was the a t t r ac t iveness  of working with the input as a whole using 
the  matching of templates (GENERIC ACTION nodes) r a the r  than parsing i n  a 
s e r i a l  fashion whereby components a r e  recognized i n  some left- to-right  
decoding process. 
The algorithm i n  Figure 5 describes how the  parser  works. 
DO f o r  a l l  possible combinations of usages of Lhe input words: 
. DO f o r  a l l  combinations of reasonable re fe ren t s  f o r  a l l  pronouns: 
. . 
. . DO f o r  each verb i n  the  input: 
. . .  
. . . DO f o r  a l l  GENERIC ACTION nodes for t h a t  verb: 
. a * .  
. . . . Find su i t ab le  ac to r  and object  candidates. 
. . . .  
. . . . DO f o r  a l l  combinat~ons of ac to r s  and objects: 
. . . . .  
. . . . . Create a parse network skeleton. 
. . . . . Add modification to  the  parse network. 
. . . . . Retam network i f  b e t t e r  than previous one. 
* . . . *  
. . . . END-DO 
. . . .  
. . END-W 
. . .  
. . END-DO 
. . 
. END-W 
. 
END-DO 
Figure 5: The parsing a l g a ~ i t h m  
Essential ly,  the  algorithm is s e t  up t o  produce a l l  possible parse 
networks using the known meanings of the  words. 
Steps (7), (8), and 1" a r e  t h e  hear t  of t h e  parsing scheme. For 
each t r i p l e  provided by s t ep  (6), a skeletson consist ing of an ACTION 
INSTANCE node and th ree  SIMPLE INSTANCE nodes is constructed a s  a temporary, 
data s t ructure .  The other  words i n  t h e  input a r e  then attached t o  i t  
according t o  the following modification scheme. 
1. Apply l e f t  modification t o  the  ACTOR, i.e., form a noun 
group tha t  consiste of the ACTOR plus a l l  i t s  modification 
tha t  l i e s  t o  i ts  immediate l e f t .  This includes a l l  adject ives  
and determiners. 
2. Apply l e f t  modification t o  the  OBJECT. This process is 
iden t i ca l  t o  t h a t  used to  get  l e f t  modification of the  ACTOR. 
3. Find CONTEXT modification of the  ACT. Preposit ional  phrases 
which modify the  main verb a r e  located. A s  prepositional objects  
a r e  found, they have t h e i r  l e f t  modification at tached by a process 
iden t i ca l  t o  t h a t  used i n  s t eps  1 and 2 f o r  t h e  ACTOR and the  
OBJECT. Note t h a t  no r i g h t  modification is  attempted f o r  ob jec t s  
of prepositions. 
4. Find r i g h t  modification (preposit ional  phrase) f o r  the  ACTOR. 
5. Find r i g h t  modification f o r  the  OBJECT. 
6. Locate s ing le  word (adverb) modification of t h e  ACT, This 
process works from r i g h t  t o  l e f t  through a l l  remaining unattached 
words of t h e  input. 
As each modification is iden t i f i ed ,  nodes a r e  at tached t o  the  grow 
ing parse network. Simple modification nodes a r e  at tached using the  
MODIFY a t t r i b u t e ;  phrase modi f~ca t ion  i s  constructed using a CONTEXT 
substructure and at tached with a CNTXT a t t r ibu te .  
Upon completing the  parsing, t h e  score f a r  the  newly constructed 
parse network is colnpared with the score f o r  the  previous bes t  network. 
The higher scoring one is retained t o  the  next i t e ra t ion .  
The scheme developed f o r  scoring a parse  network is as follows: 
1. 4-3 i s  added f o r  both the  a c t o r  and t h e  objec t  when they a r e  
i den t i f i ed .  A network wi th  a n u l l  ob jec t  would ge t  only +3 f o r  
i ts  actor. 
2. Add $1 f o r  each s i n g l e  word modifier.  
3. Add +1 f o r  each prepos i t ional  phrase. Note tha t  this i s  j u s t  
t h e  prepos i t ion  plus i ts  objec t .  Modification of the ob jec t  scores  
add i t i ona l  points .  
4 .  After  t he  network is created,  sub t r ac t  + 1  f o r  each word of t he  
input ,  including undefined words, t h a t  was not  used i n  the  crea t ion  
of t h e  network. 
Upon termination of the  algorithm, the re  w i l l  be a "best" parse  net- 
work which represents  t h e  meaning of t he  input .  I n  case seve ra l  networks 
had t h e  same "best" score, then disambiguation of meaning is deferred,  t o  
be resolved according t o  the  memory matching process described i n  sec t lon  5. 
To de t ec t  and con t ro l  the parsing of "garbage1' Input ,  t h e r e  is a 
threshold value f o r  the  parse  network score  t h a t  must be exceeded before 
t h e  parse w i l l  be accepted. The current  threshold value is zero. A parsing 
t h a t  does not exceed t h i s  value 1s re j ec t ed  and leads t o  t he  response of 
To i l l u s t r a t e  t he  way t h e  parsing algorithm works, consider t he  
questxon DOES BRANDT OWN A RED ANIMAL BOOK? 
Afte r  the  DO-question form of t h e  input  i s  recognized, the DOES and the  
quest ion mark a r e  discarded leaving 
BRANDT OWN A RED ANIMAL BOOK 
t o  be processed. I n  MEMORY, these  w o r d s  have the  following usages: 
BRANDT - pos-noun; ISA BOY. 
OWN - posiverb; GENERIC ACTION nodes a r e  (PERSON OWN THING) and 
(PERSON OWN ANIMAL). 
A - pos=artxcle. 
RED - pos=adlective; I S A  COLOR. 
ANIMAL - posPneun. 
BOOK - pos-noun; ISA THING. 
-41- 
I n  t h i s  example, each word has only one usage so, i n  terms of t h e  
algorithm i n  Figure 3, t h e  top l e v e l  loop (1) w i l l  be i t e r a t e d  once. A t  
s t e p  (2). the re  are no pronaun r e f e r e n t s  t o  resolve.  For the  verb "OWN", 
t he re  are two t r i p l e e  t h a t  must be matched t o  the  input. 
U s i y  (PERSON OWN THING), i n  step (5) w e  compile a list containing 
BRANDT as i ts  s i n g l e  entry  t o  be used a s  an ACTOR candidate (BRANDT ( I S A )  
PERSON). Similarly, t h e  s e t  of OBJECT candidates contains BOOK s i n c e  
BOOK (MA) THING. Now, i n  s t e p  (6),  t h e  only poss ib le  combination of 
ACTOR and OBJECT, (BRANDT OWN BOOK) i s  formed and passed t o  s t e p  (7) 
where t h e  skeleton of t h i s  network is formed. 
In s t e p  (8) t h e  procedure for adding modification i s  applxed. 
There is  no l e f t  modification poss ible  f o r  BRAEIOT. llowever, f o r  BOOK, 
the re  a r e  th ree  words, ANIMAL, RED and A, t o  ~ t s  l e f t  t h a t  have not been 
used i n  t h e  pa rse  so far. A l l  a r e  found t o  nlodify BOOK. 
The score  f o r  t h l s  parse  network i s  9, 3 each f o r  a semantfcally 
acceptable ACTOR and OBJECT p l u s  one each f o r  A, RED, and ANIMAL. There 
a r e  no unused o r  undefined words m t h e  input  so  nothing i s  subtracted.  
Now, consider what happens when a second meaning of OWN, (PERSON 
OWN ANIMAL), is  used i n  s t e p s  (5) through (8). Again, the  s e t  of 
s u i t a b l e  ACTORS w ~ l l  be t h e  s ingle ton,  BRAPIDT. 
The set of s u ~ t a b l e  o b j e c t s  contains only ANIMAL. Therefore, t h e  
t r l p l e ,  (PERSON OWN ANIMAL), matches (BRANDT OWN ANIMAL) and the node 
f o r  ANIMAL allows A and RED a s  m o d ~ f i e r s .  However, t h e r e  1s no way t o  
a t t ach  BOOK t o  t h e  p a r s e  network. T h ~ s  second parsing g e t s  a score  of 
7 (ACMR = +3, OBJECT = +3, A= + 1, RED = +1, and BOOK = -1) . 
As a second example, consider  the  s t r i n g :  
DID BRANJ)T WILL THE PROPERTY TO YOU? 
with  
BRAM)T - posPnoun; ISA PERSON. 
WILL - postnoun; ISA PERSON. 
WILL - posaverb; GA node is (PERSON WILL THING).  
(TO PERSON) i s  opt iona l  context  modif icat ion.  
TElE - pos=a r t i c l e .  
PROPERTY - pas=noun; ISA THING. 
TO - pos=preposi t  ion ,  
JTMMY3 - pos=noun; ISA PERSON. 
One poss ib le  s e t  of d e f i n i t i o n s  (usages) of the  words wi th  WILL a s  a 
noun w i l l  not  inc lude  any verbs. Therefore, t h a t  combination i s  r e j ec t ed  
immediately i n  s t e p  (3) of t he  pars ing  algorithm. The o the r  pos s ib l e  s e t  
conta ins  WILL a s  a verb. 
Once t h a t  s e t  of usages is  declded on, t h e  parslng 1s straight-forward 
i n  t he  manner s l m i l a r  t o  t ha t  used I n  t h e  p revmus  example. The malor 
d i f f e r ence  I n  t h l s  input  is the  ex is tence  of Lhe phrase "TO JIMMY3 (as 
transformed from "TO YOU" i n  s t e p  (2)) .  Context modif icat ion f o r  the  ACT 
is searched f o r  aefore  r i g h t  modif icat ion of t he  OBJECT s o  "TO JIMMY3" i s  
properly a t tached  to  WILL I n  t h e  parse ,  However, the node f o r  PROPERTY 
d id  not  conta ln  any pa t t e rns  f o r  r i g h t  m o d i f l c a t l ~ n  s o  t h a t  phrase could 
not  have been a t tached  t o  t h e  OBJECT anyway. The complete parse f o r  t h ~ s  
i npu t  has a s co re  of 8. 
5. MEMORY SEARCHING 
5.1 OVERVIEW 
Figure 6. General fonh of a memory 
match s t ruoture .  
The s t ra tegy used i n  MEMORY searching is s imi lar  i n  one respect t o  
the  parsing procedure. Namely, during the  search process a s t ruc tu re  is 
constructed and a score is calculated t o  measure the degree of s i m i l a r i t y  
between the  question and the  candidate answer. Kowever, unlike the  
exhaustive process used i n  obtaining a parse, the memory searching procedurt 
is ra ther  Gelective and does not examine a l l ,  o r  even a large p a r t ,  of 
MEMORY. 
-44- 
5.2 THE MEMORY MATCII STRUCTURE 
5.2.1 GENERAL STRUCTURE 
For each match attempted between a parse  network and an ACTION INSTANCE 
node i n  MEMORY, a complete, new memory match s t r u c t u r e  is generated. This 
s t r u c t u r e  has a s t a t i c  component of 8 r e g i s t e r s  plus anywhere from zero t o  
four var iable  length,  linked lists attached t o  it  a t  various places -- 
(see Figure 6). These l inked lists a r e  used f o r  col lec t ing information 
about word modification. 
5.2.2 ACTOR, ACT AND OBJECT COllPARISON RESULTS 
Three of the r e g i s t e r s ,  corresponding t o  the ACTOK, ACT and OBJECT 
comparison r e s u l t s ,  form the hea r t  of the  memory match s t ructure .  In  each 
r e g i s t e r  is recorded t h e  exact kind of match between input and MEMORY. 
Three values a r e  contained f o r  the  ACT comparison resu l t s :  a pointer  
t o  the SIMPLE GENERIC node i n  )EMORY f o r  the  ACT aiven i n  the  parse network, 
a pointer  t o  the  list which has the comparison between the modifiers of 
input and of the  METIORY s t ruc tu re ,  and an ind ica to r  pf the type of match. 
Five possible types of matches can occur f o r  ACTs. 
1. (+O) No match. 
2. (+5) Exact match. 
3. (+4) The input and MEMORY ACTs a r e  synonyms. 
4. (+4)  The input  and MEMORY ACTs a r e  antonyms. 
5.. (+0) The ACT was missing from e i t h e r  the  input o r  the  MEMORY 
node. 
The r e g i s t e r s  f o r  ACTOR and OBJECT a r e  iden t i ca l .  Like the ACT, they 
contain three  items of information: a pointer  t o  the SIMPLE GENERIC node 
f o r  the  ACTOR i n  the parse network, a pointer  t o  the  modifier list and t h e  
match type indicator .  The match type ind ica to r  f o r  the  ACTOR is divided 
i n t o  th ree  subunits. F i r s t ,  the  number, s ingular  o r  p l u r a l ,  of the  ACTOR 
is given. Second, the  type of reEerence t0  the  input ACTOR is  xecorded 
as e i t h e r  a apec i f i c  ins tance (i.e.,  i t  re fe r red  t o  a p a r t i c u l a r  ins tance 
of the  ACTOR) o r  a s  an i n d e f i n i t e  reference ( referred t o  a c l a s s  of 
ACTORS r a t h e r  than a s p e c i f i c  one). Final ly ,  the re  is  the  r e s u l t  of t h e  
match between input and MEMORY. 
1. (+O) No match. 
2. (+5) Exact match- 
3. (+4) The input ACTOR is a member of the  s e t  named by the  
MEMORY ACTOR, i. e., input  ACTOR (ISA) MEMORY ACTOR. 
4. (+4) The input ACTOR is the  name of a set f 6 r  which the  
MEMORY ACTOR is a member, i. e. , MEMORY ACTOR (ISA) input ACTOR. 
5. (+4) Input ACTOR matched a synonym of the  MEMORY ACTOR- 
6 .  (4-4) Input ACTOR matched an ACTOR of another node i n  MEMORY of 
the  form A ACTOR^ BE ACTORZ" where ACTOR1 matched, t h e  input ACTOR 
and ACTOR2 matched the MEMORY ACTOR, o r  v ice  versa,  i.e., input 
ACTOR BE x and x BE MEMORY ACTOR. 
7. (+2) Did not f i n d  an ins tance of the  input ACTOR i n  W O R Y  
but did f ind an ins tance of a member of the  set the  input ACTOR 
would belong t o  3.f i t  had been i n  memory, i.e., t h e  two r e l a t i o n s  
of input ACTOR (ISA) x and MEMORY ACTOR (ISA) x both hold f o r  some 
s u i t a b l e  cakegory x. 
8. (-I-0) ACTOR missing from e i t h e r  input  o r  MEMORY. 
5.2.3 WORD MODIFICATION RESULTS 
Registers of the  match s t r u c t u r e  corresponding t o  t h e  ACTION, ACTOR, 
ACT and OBJECT nodes can a l l  have modifier lists attached.  
The word modification l ist  is s i n g l y  l inked; each ifem on the  list 
is given by two r e g i s t e r s  and represents  a s i n g l e  modification, e i t h e r  
s ing le  word o r  phrase. A l l  words o r  phrases t h a t  modify, say, the  ACTOR, 
w i l l  be on the  satne list. However, i f  any of those words is modified by a 
-46- 
word o r  phrase,  then i t  w i l l  have a l ist  a t tached containing a l l  its modi- 
f i e r s .  Thus, f o r  any component of the input,  word modification is r e a l l y  a 
t r e e  of s u b l t s t s  whose s t r u c t u r e  is  determined by the  input word re la t ions .  
Six r e s u l t s  of  the  match of modifiers a r e  possible.  
1. (+O) Was not compared because previous l e v e l  modlficatlon did 
not  match. 
2. (+2) Exact match. 
3. (+2) Exaqt match i f  i n f l e c t i o n s  are ignored, e . g . , s ingu la r  
matching p lu ra l .  
4. (+I) One of t h e  two modification words is a member of the  s e t  
named by the o ther ,  i.e., t h e r e  is an ISA chain leading from one to 
the  other. 
5. (+I) The two words a r e  both from a s e t  of mutually exclus ive  
elements, e.g., matching RED d t h  BLUE. 
6. (+a) No match because t h e  modifier appeared i n  only one of 
the two (input and MEMORY) pla tes .  
5.2.4 EXAMPLE STRUCTURES 
Consider t h e  question: 
(1) DOES A PERSON HAVE A RED BOOK? 
This w i l l  be parsed a s  (PERSON MVE BOOK) with RED modifying BOOK. The 
two i n d e f i n i t e  a r t i c l e s  w i l l  a l s o  be p a r t  of t h e  parse network but a r e  not 
matched s ince  a r t i c l e s  a r e  not  included i l i  any memory s t ructures .  The 
mashing of t h i s  parse network with a MEMORY s t r u c t u r e  f o r  
(2) BRANDT OWNS A RED BOOK 
would y i e l d  a memory match s t r u c t u r e  wi th  the following propert iee.  
~ a t c h  score  = 15; Maximum poss ib le  score  = 1.7 
MEMORY ACTOR = BRANDT (PERSON). 
Input mode = singular, indefinite. 
MQlORY mode * shgular. 
Match type = XEMORY (ISA) input. 
MEMORY ACT = OWNS (HAVE). 
Match type = synonym 
MEMORY OBJECT - BOOK' (BOOK) 
Input mode = singular, indefinite. 
MEMORY mode = singular. 
Match type = exact match. 
Modifiers: 
RED - Location = input and M E M W  
Match type = exact match. 
(i-4 points) 
(t4 points) 
(+5 points) 
(i-2 points) 
If (1) above is matched against the NEMORY structure representing 
(3) BRANDT HAS A BLUE ANIMAL BOOK 
The following memory match structure will result. 
Match score = 15; Maximum possible score a 17 
MEMORY ACTOR = BRANDT (PERSON) 
Input mode = singular, indefinite. 
MEHORY mode = singular. 
Match rype = MEMORY (ISA) input. 
MEMORY ACT = HAVE (HAS) 
Match rype = exact match (in? lections are 
ignored) 
(+4 points) 
(t5 points) 
QEMORY OBJECT = BOOK (BOOK) 
Input mode - singular, indefinite. 
MEMORY node = singular. 
Match type = exact match. (+5 points) 
Modifiers : 
RED - Location = inpm and MEMORY. (+1 point) 
(Note: RED matches BLUE as mutually exclusive elements 
from the same set.) 
ANIMAL - Location = MEMORY only. (+0 points) 
This second MEMORY node matches the input as well (+I5 score) as 
the first because of the slightly better ACT match even though the OWECT 
is closer in the first case. It serves to indicate some of the problems 
ghat are encountered by the procedure during matching. 
(1) DO f o r  a l l  pa r se  networks: 
(2) . Compute maximum poss ib l e  match score  f o r  t h i s  network. 
(3) . Don't search  f o r  t h i s  network i f  maximum i s  not  good enough. 
(4) . Get  list o f  synonyms and antonyms f o r  p a r s e  network ACT. 
(5) . DO u n t i l  a reasonable match has  been obtained o r  u n t i l  no more 
. . ACTORs can b e  found: 
. . 
(6) . . Se lec t  an ACTOR t o  match on. 
. . 
(7) . . DO f o r  a l l  INSTANCES of t h a t  ACTOR: 
.-? 
(8) . . . DO f o r  a l l  ACTION INSTANCES which have t h a t  ACTOR: 
Create  t he  ske le ton  f o r  a memory match s t r u c t u r e .  
Compare modif iers  of the  ACTION INSTANCE nodes. 
Compare ACTORs and t h e i r  modi f ie rs  . 
Compare ACTS and t h e i r  modif iers  . 
Compare OBJECTS and t h e i r  modif iers .  
I f  no good OBJZCTS, then t r y  a l t e r n a t e s .  
Accumulate t he  t o t a l  match sco re  f o r  the  s t r u c t u r e .  
Add s t r u c t u r e  t o  t he  l ist  f o r  t h a t  ACT6R INSTANCE. 
. * .  . 
. . . END-DO 
. . .  
. . END-00 
(17) . . Save b e s t  match s t r u c t u r e s  f o r  t h a t  ACTOR. 
. . 
. END-DO 
(18) . Fur ther  prune t h e  set of b e s t  match s t r u c t u r e s  
END-DO 
Figure 7: The memory search algorithm. 
5.3 MATCH1 NG #EMORY 
5.3.1 I3AS IC ALGORITHM 
The algorithm used t o  search memory (see Fieure 7) examines a l imi ted  
subset of a l l  s t ruc tu res  i n  MEMORY while t ry ing  t o  match the input. The 
search i e  r e s t r i c t e d  t o  ACTION INSTANCE nodes i n  E-RY t h a t  have e i t h e r  
the  same o r  a c losely  re la ted  ACTOR. The object  of the  algorithm is t o  
obtain a small s e t  of the  bes t  matches of the parse network with an 
ACTION INSTANCE node i n  MEMORY. 
To handle ambiguous input,  i.e., mul t ip le  parse networks, the matching 
procedure must be repeated f o r  each network passed on by the  parser  (see 
seep (1)). Unpromising networks a r e  eliminated i n  s t e p s  (2) and (3). 
Synonym and antonym lists a r e  compiled i n  s t ep  (4). 
The termination c r i t e r i a  f o r  MEMORY searching is the  discovery of a 
su i t ab le  match o r  the exhaustion of t h e  set of s u i t a b l e  ACTORS used t o  
d i rec t  the  search. The adequacy of the  match between input and MEMORY is 
the  memory match score--the accumulation of many component scores  which 
measure the  s i lp i l a r i ty  of corresponding p a r t s  of two s t ruc tu res .  The 
term$nation c r i t e r i o n  f o r  a s u i t a b l e  match is based on the value of t h i s  
score r e l a t i v e  t o  the  network's maximum poss ible  score  determined i n  s t e p  
(2). The threshold foL a "suitable1' match is current ly  s e t  a t  70 percent 
of the maximum )oss ible  score. The number 70 is  not pe r fec t  i n  any sense 
but was se lec ted i n  a trial-and-error fashion. 
5.3.2 SELECTING SUITABLE ACTORS 
The se lec t ion of a c t o r s  t o  Control the  range of the  search procedure 
i s  designed t o  provide a reasonable set of nodes c losely  re la ted  t o  the  
input ACTOR. This se lec t ion  procedure is used pnly when no ACTION INSTANCE 
nodes with the  input ACTOR produce s u f f i c i e n t l y  good matches. 
-50- 
There a re  f i v e  a l t e rna te  bethods, described below, f o r  gett ing new 
ACTOR candidates. Not a l l  of these f ive  are  always used, however. The 
ones t o  use and the order i n  which they a r e  t o  be applied is  determined by 
the  mode and number of the input ACTOR. 
1. Search memory of ACTION INSTANCE nodes of the form 
"ACTOR1 BE ACTOR2" where e i the r  ACTOR1 or  ACTOR2 matches the 
input ACTOR exactly. Collect the unmatched members of a l l  
these nodes f o r  use a s  new ACTOR candidates. For example, 
suppose the ACTOR, BRANDT, was not successful a t  generating a 
good match. Search fo r  INSTANCES of (X BE BRANDT) and 
(BRANDT BE X) where X is i n  the same general hierarchy as  
BRANDT , e .g . , (BRANDT IS SECOND-GWER) . Now SECOND-GRADER 
can be used as a source fo r  more ACTION IHSTANCE nodes to  search. 
2. Use a l l  nodes above the ACTOR i n  its hierarchy. 
3. Use a l l  nodes below the ACTOR i n  i t s  hierarchy. 
4. Use a l l  synonyms of the input ACTOR. 
5. Find a l l  INSTANCE nodes tha t  a r e  i n  t h e  same s e t  a s  the 
input ACTOR, i,e.,  search the other nodes i n  the ISA chain 
rooted i n  the node immediately above the input ACTOR i n  i t s  
hierarchy. For example, f o r  an input ACTOR, BRANDT, we have 
BRANDT (ISA) BOY. Search the cliain which gives a l l  subdets 
of BOY, but excludes BRANDT. 
For spec i f i c  mode input ACTORs, t he  above procedures a r e  executed i n  
the order: 1, 2, 4, 5. Procedure 3 i s  not used f o r  speci f ic  input 
ACTORs s ince  i t s  purpoce is  t o  f ind speci f ic  instances fo r  general references. 
For indef in i te  input ACTORs, the procedures a r e  executed i n  the order: 1, 2, 
3, 4. Probedure 5 is not used since i t  would lead t o  too diversif ied a s e t  
of potent ia l  candidates. For instance, consider PERSON a s  the  input ACTOR. 
I f  we had PERSON (ISA) THING, and ANIMAL (ISA) THING, ACTORs could be 
selected tha t  a r e  only vaguely re la ted  t o  the input. 
This s e t  of procedures can be executed twice. The f i r s t  time, the  input 
ACTOR is used as i t  appeared i n  the  input. The second time, i t s  number is 
changed, i.e., f o r  4 s ingu la r  ACTOR, the second tinre through, a11 comparisons 
and searches would be performed f o r  its p l u r a l .  
I n  order  f o r  t h i s  procedure t o  work, t h e  mode of t h e  input  ACTOR must 
be known. This  is  deterained as follows: 
1. An ACTOR is i n d e f i n i t e  i f  
a )  i t  is an i n d e f i n i t e  proqoun, 
b) i t  has modif iers  but  none is a d e f i n i t e  determiner 
( the ,  t h i s ,  these ,  t h a t ,  tlxose) o r  
c) i t  is p l u r a l  and has no modif iers .  
2. An ACTOR is s p e c i f i c  i f  
a )  i t  is modified by a def iriite determiner o r  
b) it is s ingu la r  and has no modifiers .  
6. PRODUCTION OF OUTPUT 
6.1 OVERVIEW 
Problems t o  be solved before "natural"  sounding output  can be produced) 
a r e  
1. what information should b e  used i n  t h e  tesponse to  a given 
input ,  
2. how should the  response be s t ruc tu red  so  c e r t a i n  p a r t s  a r e  p toper ly  
emphasized and 
3. how can the response be made t o  appear n a t u r a l  i n  t h e  sense of 
being l i k e  a similar remark a person might make? 
Clearly,  t he re  a r e  many f a c t o r s  which play a p a r t  i n  how a person 
decides what t o  say a t  any tine during the  course of a conversation. Such 
f a c t o r s  include t h e  p lace  of occurrence, t he  reason f o r  t h e  conversat ion,  
the  r o l e s  assumed by t h e  pa r t i c ipan t s ,  t he  type of information passed and 
the motives of the  p a r t i d p a n t s .  These f a c t o r s  have nbt been s tudied ,  
indeed, have not  even been exhaust ively i d e n t i f i e d ,  during t h i s  research.  
What  has been done, however, is the  development of  a few simple procedures 
t h a t  w i l l  generate reasonable sounding answers t o  DO-questions based on the  
r e s u l t s  .of memory search.  
6.2 THE OUTPUT PRODUCTION LIST 
The only temporary s t r u c t u r e  c rea ted  duriilg tile ou tput  phase is a 
simple doubly l i nked  List of elements represent ing  t h e  words and 
punctua t ion  of t h e  answer t o  be p r i n t e d  (see Figure 8). 
> : 
: [ func t ion]  : : [funct ion]  1->. - \ I [ func t ion  ] 1 
Figure 8: S t ruc tu re  of t h e  output  product ion list 
As t h e  output  product ion r o u t i n e s  analyze t h e  r e l evan t  memory match 
s t r u c t u r e s ,  they produce t h e  output; product ion list an element a t  a  time. 
Each element of  t h e  list has  t h e  ful lowin& two r e g i s t e r  format: 
l l eg i s t e r  1 - conta ins  t he  forward and backward l i n k s  t o  t h e  o t h e r  
elements i n  tlhe list.  
Regis te r  2 - conta ins  a  word func t ion  i n d i c a t o r  (represented as 
[ func t ion ]  i n  Figure 6 )  and a MEMORY p o i n t e r  t o  t h e  GENERIC 
node f o r  t h e  word (given a s  GEN [ 1). The p o i n t e r  can be used 
by t h e  f i n a l  p r i n t  rou t ines  t o  t r a c e  back through t h e  word's 
GENERIC node t o  its TYPE node t o  r e t r i e v e  i t s  t e x t  r ep re sen ta t i on .  
The word func t ion  is  an i n d i c a t o r  t o  he lp  out  t h e  rou t ine  t h a t  makes 
the output  grammatical. Curren t ly  10  d i f f e r e n t  func t ion  codes which 
correspond very loose ly  t o  sentence p a r t s  a r e  used. 
Subjec t  . 
Verb. 
Object.  
P repos i t i ona l  ob j ec t .  
Prepos i t ion .  
Modifier of subjec t .  
Modif ier  of verb. 
l l od i f i e r  of ob j ec t .  
l l b d i f i e r  of p r e p o s i t i o n a l  ob j ec t .  
Other ( inc ludes  punctuat ion) .  
6.3 PRODUCTION IBTHOD 
6.3.1 RESPONDING TO INPUT NOT UNDERSTOOD 
When the  input is not understood o r  no answer has been found i n  memory, 
enough information should be returned s o  t h a t  the  person knows why he d id  
not  receive the expected response. The procedure t h a t  is used to  de tec t  
tha t  s i t u a t i o n  and produce a response current ly  worm as follows: 
1. I f  input was not parsed, respond with "1 DON'T UNDERSTAND 
THAT." Then p r i n t  a l l  undefined words i n  the form: I DON'T 
KNOW IdliAT word1 (OR trord2 OR w r d 3  . . . ) IIISANS . " For example, 
the input "DOES BRANDT RIDE A BICYCLE?" would generate the  
response "I DON'T UUDERSTAND THAT. I DON'T KNOW WHAT RIDE OR 
BICYCLE I W J S  ." 
2. I f  the  input i s  parsed, but there  was a poor, o r  no, memory 
match, then check the  'GENERIC node f o r  the  ACT to  s e e  i f  we 
respond with "YES", "EIO", "I DON'T KNOW" o r  some "canned" 
response. What is searched f o r  is an a t t r i b u t e  which is some- 
times present i n  ACT nodes and i s  t h e  answer t o  be given when 
no match i s  found with kCMORY. An example is the. ACT "KNOW". 
I f  the  program i s  asked "DO YOU KNOW x?" and x is not  i n  
MEtlORY, then the response w i l l  be "NO." r a the r  than "I DON'T 
KNOW. 
I f  the input  is parsed and the re  is a good memory match then the  
following s t e p s  w i l t  be executed. 
3. Check the  OBJECT. I f  the re  i s n o n e  and one i s ' r e q u i r e d  
by this  ACT then, i f  the re  a r e  undefined words, p r i n t  
"I DON'T UNDERSTAND THAT." plus t h e  undefined words. I f  
the re  a re  no undefined words, p r i n t  "I DON'T UNDERSTAND YOUR 
USE OF act." Subs t i tu te  the  current  ACT i n  the  output f o r  
"act" . 
4. Check the  ACTUK. If i t  is not  d i r e c t l y  r e la ted  t o  the  in- 
put ACTOR but is a member of the  same s e t  Ci.e., ~ r :  was 
se lec ted  during the  memory search for  producing a c t o r s  a s  
given i n  sect ion 5.2.3), p r i n t  "I DON'T KNOW." Iiowever, i n  
t h i s  case, do not terminate the output production here. Pass 
the  good memory match s t r u c t u r e  involving t h i s  a c t o r  on t o  
the  next procedure t o  be used t o  produce more output,  As an 
example, the  inpu t  "DOES BILL IIAVE A BOOR?" would generate 
the  response "I DON'T KNOW. BRAlJllT U S  A RED BOOK." 
5. Check the  ACT. I f  i t  does not  match, p r i n t  "I DON'T KNOW." 
p l u s  the undefined words, f f any. 
6.3.2 DETERMINING MODE OF RESPONSE 
The answer t o  a DO-question h a s  an o p t i o n a l  i n t e r j e c t i o n  o f  YES o r  
NO which precedes t h e  answer and is determined a s  follows: 
1. S e t  t h e  mode t o  YES un less  t h e  MEMORY ACTOR i s  a subse t  
of t h e  inpu t  ACTOR. I n  t h a t  case ,  t h e  response slrould not  
have e i t h e r  YES o r  'NO. This w i l l  occur when s p e c i f i c  d a t a  
a r e  being used t o  answer a ques t ion  of a genera l  nature .  
For example, the  inpu t  "DO PEOPLE OWN TlIIIJGS?" would be 
answered by "RRANDT OlnqS A RED BOOK." The YES is omitted 
s i n c e  tilc ques t ion  I ~ a s  not  been answered i n  genera l .  Do 
not  cont inue examination of  the  match f o r  c o n t r a d i c t i o n  
when t h i s  occurs.  
2. Search t h e  memory match s t r u c t u r e  looking f o r  contra-  
d i c t o r y  d a t a  t h a t  would change t h e  mode from YES t o  NO. I f  
t h c  inpu t  ACT matched an antonym, s e t  t h e  mode t o  NO. 
3. Check t h e  f i r s t  l e v e l  modi f i e r s  o f  t h e  ACT. Look f o r  
c o n t r a d i c t i o n ,  i .e.,  modi f i e r s  from t h e  same s e t  t h a t  a r e  
mutually exclus ive .  I f  any a r e  found, r e v e r s e  t h e  s e t t i n g  
of t h e  mode. For example, t h e  inpu t  "DID LIRANDT PLAY OUTSIDE?" 
which would match t h e  MEMORY node, {'BRANDT PLAYED INSIDE." 
would have a nega t ive  mode since INSIDE and OUTSIDE a r e  
mutually exclus ive .  
4. Check t h e  OBJECT f o r  c o n f l i c t i n g  modif iers .  Also change 
mode i f  t h e  OBJECT is  no t  d i r e c t l y  r e l a t e d  t o  t h e  inpu t  
OBJECT bu t  is an element from t h e  same s e t  as t h e  input  
OBJECT. Th i s  w i l l  be t h e  c a s e  when matching o b j e c t s  such as 
GFEEN I300K wi th  YELLOW BOOK o r  OLD BOOK wi th  NEW BOOK. 
5. F i n a l l y  check a l l  s i n g l e  word modi f i e r s  of t h e  ACT t h a t  
were p resen t  only i n  the  inpu t  or IMblORY looking f o r  nega t ive  
modif iers .  These would be  words l i k e  NOT, NEVER, e t c .  
6.3.3 PRODUCING A NORMAL NiSWER 
A t  t h e  top l e v e l  i n  t h i s  procedure is  t h e  d e c i s i o n  of how much 
of t h e  answer t o  p r i n t .  The r u l e s  used are :  a)  i f  t h e  answer is an 
e x a c t  match of t h e  i n p u t  wi th  r e s p e c t  t o  t h e  three .  major components, 
don' t  p r i n t  anything except f o r  t h e  YES o r  NO, b) i f  t h e  answer is  an- 
exact match except  f o r  t h e  OBJECT, then p r i n t  only  the ~ B J E C T  o r  C) i.f 
t h e  ACTOR o r  ACT does n o t  match, then p r i n t  the- whole MEMORY node given 
by t h e  memory match s t r u c t u r e .  
The list elements r e p r e s e n t i n g  t h e  output  t o  be p r i n t e d  f o r  t h e  
ACTOR o r  OBJECT a r e  generated by t h e  fol lowing procedure. 
1. Add t h e m n e r i c  node f o r  t h e  ACTOR (OBJECT) t o  t h e  end 
of t h e  ou tpu t  production list. S e t  i ts  func t ion  type t o  
SUBJECT (OBJECT). 
2. Put  t h e  modif iers  i n t o  tlie l i s t  inunediately i n  f r o n t  o f  
tile ADTOR (OBJECT). Use only  modi f i e r s  t h a t  were presen t  i n  
the'blEMORY node o r  t h a t  rnatched between MEMORY and inpu t .  
These modif iers  a r e  conta ined i n  t h e  l i n k e d  l i s t  a t t ached  t o  
the  memory match structure f o r  t h e  ACTOR (OBJECT). 
3. Put an a r t i c l e  b e f o r e  t h e  modi f i e r s ,  i f  required.  
4 .  Add p r e p o s i t i o n a l  modif icat ion o f  t h e  ACTOR (OBJECT) a t  
t h e  end of t h e  list. Thi$ i s  accomplished by adding t h e  
p repos i t ion  followed by t h e  p r e p o s i t i o n a l  ob jec t .  Then t h e  
modif icat ion on t h e  p r e p p s i t i o n a l  o b j e c t  i s  added be,tween 
t h e  two. 
5. I f  t h e  ACTOR (OBJECT) is t h e  same f o r  both  i n p u t  and 
lfE3fORY but  some Input-only modif icat ion e x i s t s ,  then add "BUT 
NOT" p lus  t h e  inpu t  node and t h e  input-only modif iers .  An 
example of t h i s  is  where "A BLUE BOOK BUT NOT NU ANIMAL BOOK" 
i s  given i n  response t o  "DO I HAVE A BLUE ANIMAL BOOR?" 
6.3.4 EWCIBG OUTPUT GRhEalATICAL 
The procedure used t o  f i x  up t h e  ou tpu t  examines t h e  elements of 
t h e  output  l ist  and, using t h e  word f u n c t i o n s  a s  s p e c i f i e d  i n  those  
elements and t h e  p r o p e r t i e s  given i n  t h e  words' GENERIC nodes, a t t empts  
t o  apply the. four  r u l e s  beluw: 
1. JIbIMY3 and t h e  person 's  name g e t  t r a n s l a t e d  t o  the 
pronouns "I" ind "YOU". A t  t h i s  s t a g e ,  t h e  form of t h e  
pronoun may b e  wrong. 
2. G e t  person ahd number of t h e  pronouns t o  agree  wi th  t h e  
ACT (main verb).  Change a pronoun t o  its possess ive  form i f  
i t  is used as a modi f i e r  o f  another  sentence element,  
3. S e t  t h e  proper ve rb  tense.  
4. Convert the o b j e c t s  i n  t h e  sen tence  (main OBJECT and 
p r e p o s i t i o n a l  o b j e c t s )  t o  ob j e c r i v e  case.  
-56- 
A s t e p  i n  making the  output grammatical t h a t  was g i w n  before was 
the  generation of an a r t i c l e  a s  a modifier. An i n d e f i n i t e  a r t i c le  
(A o r  AN) is always used; tlie one t o  be se lec ted  is given by the follow'fng 
r u l e s  : 
1. uon't use an a r t i c l e  i f  t he  word modified is a pronoun or  
a proper name. 
2.  Otherwise, s e l e c t  A o r  AN according t o  t h e  f i r s t  l e t t e r  
of the  word i t  w i l l  precgde. 
Following the  completion of t h i s  operation the  output is pr in ted .  
7. DISCUSSION 
7.1 1USITLTS 
7.1.1 OUJECTIVES MET 
The ob jec t ives  of t h i s  research,  a s  r e s ta ted  from sect ion 1, a r e  
the  development of th ree  components a b l e  to  carry  on a "natural" dialogue 
with humans: an extefidible memory model, procedures f o r  determinine t h e  
meaning of the input and procedures t o  allow t h e  model t o  converse 
"naturally" with a human. 
The current  design of memory, although a s t a r t  i n  the r igl i t  d i rec t ion,  
is f a r  from complete. Thexe i s  mucll t h a t  cannot now be represented with 
the  s t r u c t u r e s  avai lable .  However, t h e  design of memory is f l ex ib le .  
Extensions can be added t o  represent more complex surface  s t r u c t u r e s  v i a  
INSTANCE nodes and a l s o  t o  capture more of the  underlying s t r u c t u r e  of the  
word meanings themselves i n  terms of GENERIC nodes. 
Other f ea tu res  t h a t  an extended memory model v i l l  need are the  
c a p a b i l i t i e s  f o r  supporting more e laborate  question answering, t h e  
probessing of imperatives and t h e  in tegra t ion  of new information i n t o  the 
-5 7- 
ex i s t ing  s t ruc tu res .  We be l i eve  tha t  the  current  design w i l l  allow such 
procedures t o  he developed. 
The parser  now implemented does a reasonably good job -on the  
r e s t r i c t e d  input glven i t .  The bas ic  parsing philosophy, i .e . ,  the 
se lec t ion  of one of severa l  poss ible  networks based on semantic densi ty ,  
is believed to  be a sound way t o  approach the problem of  determining 
meaning. 
The development of output generation procedures has j u s t  s t a r t e d .  
The current ly  implemented procedures a r e  not general o r  powerful enough 
t b  handle more than a few. specia l ized s i tua t ions .  Work is  required i n  
t h i s  area  t o  f i r s t  develop a l a r g e  number of s p e c i f i c  r u l e s  from which 
more general  r u l e s  can be deduced. 
We have i d e n t i f i e d  t en  fea tu res  which charac te r i ze  "natural" language 
understanding processes. Speci f ica l ly ,  they are the  a b i l i t y .  to: 
(1) Work with p a r t i a l  i n ?  ormation ( t o  make p laus ib le  inferences  
about missing information, e.g., defau l t  values from frames), 
(2)  Work with overlapping and conf l i c t ing  information (not t o  
r e j e c t  i t  out of hand, o r  seek only consis tent  information, o r  
t o  ass ign a l l  t r u t h  value F, but t o  s i f t  through i t  t o  r e j e c t  
t h a t  which - based on experience o r  knowledge - is implausible 
o r  to  temporarily suspend judgement), 
(3) Retain ambiguity u n t i l  disambituation is  absolute ly  ca l l ed  f o r ,  
(4) Perform "short" chains of deduct ions ,  
(5) Engage i n  common sense reasoning (i .e . ,  t he re  is knowledge of 
the  proper t ies  of commonplace ob jec t s ,  events,  e t c  .), 
(6) Pose questions i n  order  t o  confirm expecta t ions  o r  t o  e l i c i t  
more information about some subjact  mat ter  of personal  i n t e r e s t ,  
(71 Construct, modify and e x t r a c t  information from a model of the 
in ten t ions ,  i n t e r e s t s ,  s k i l l s *  motiyations,  e t c . ,  of t h e  o the r  p a r t y ,  
-58- 
(8) Interrogate  and update a s imi lar  model of the  a t t i t u d e s ,  be l i e f s ,  
a b i l i t i e s ,  goals, etc.,  of i t s e l f ,  
(9) Be sens i t ive  t o  the p l a u s i b i l i t y  of information received (see 
Norman's "Charles' Sickens" problem, discussed l a t e r ) ,  
(10) Be aware of the  context i n  which the conversation is occurring 
(~lonnan's "Empire S ta te  Building" problem, discussed below, i d e n t i f i e s  
one kind of con-text; the r o l e s  of the  p a r t i e s  involved, e.g., parent- 
chi ld ,  teacher-student, superior-subordinate; bureaucratic o f f i c i a l -  
c l i e a ,  is a second context; other context types a re  no doubt   resent), 
With JIEIMY3 we have only begun t o  address t h i s  list of a t t r ibu tes .  In  pa r t i -  
cular ,  JIElMY3 i l l u s t r a t e s  one approach to  coping with items 12 and #3. 
Other prograpes pose avenues of a t t a d  f o r  o ther  items of the list (e.g., #I, 
84, #5), but the  extendabil i ty of tlmse programs t o  other fea tures  l i s t e d  i s  
questionable - a s  i t  is fo r  JIMMY3. However, by grappling with what we 
consider t o  be the fundamental problem of designing the  system from the 
s t a r t  t o  be extendable, we believe JIlIMY3 can be grown t o  cope wi th  other 
f ea tu res  on t h a t  list, and thus approach being a "natural" language 
understanding system. 
7.1.2 DOES THE PROGRAM REALLY UNDERSTAND? 
The answer t o  t h i s  questidn is  plrbbably no. In the more res t r i c t ed  
sense of ,  can the program r e l a t e  the input t o  something i t  already knows, 
the answer is yes. A l l  input surface s t ruc tu res  are matched during the 
question answering phase to  slrnilar s t ruc tu res  i n  memory. However, given 
a s ing le  i i p u t  i n  i so la t ion ,  the program does not understand what i t  means. 
It has no de f in i t ions  f o r  individual  words. I t  i s  t rue  athat there  are 
re la t ionships  between words v ia  t h e  chains and hierarchies  but t h i s  is  not 
enough. Knowing tha t  BRANDT (158) PERSON does not help a t  a l l  i n  under- 
standing, i f  the  model does not what a PERSON is,  
-59- 
7.2 LIMITATIONS 
A majori ty of the  model's general  l i m i t a t i o n s  a re  the  r e s u l t  of 
def ic ienciee  i n  the  memory s t ructure .  For instance,  c e r t a i n  types af 
input cannbt be parsed because of inadequate s t r u c t u r a l  building u n i t  
of membry . 
7.2.1 MEMORY MODEL 
I f  the  current  philosophy of what i s  t o  beas to red  i n  memory i s  
mainrainmd, i .e . ,  only surface  s t r u c t u r e s  a r e  t o  be represented,  then the  
most ser ious  l imi ta t ion  is  the  i n a b i l i t y  t o  represent  more kinds of s t r i n g s  
of English. Nothing more complex than simple (ACTOR ACT OBJECT) f a c t s  r;au 
now be recorded. I n  genefal ,  thexe is no way of r e l a t i n g  d i f fexen t  f a c t s  
t o  show causation, r e s u l t ,  presupposition, e t c .  
I f  w e  look a t  memory as more than a place i n  which surface  s t r i n g s  a r e  
stored, then there  a r e  many shortcomings. Nowhere i n  the current  s t r u c t u r e  
a r e  words given def tn i t ions .  A s  Por ACTs, the  o r i g i n a l  design ca l l ed  f o r  
the building of h ie ra rch ies  of ACTS i n  a manner s i m i l a r  t o  h ie ra rch ies  of 
"thin&$". Instead ACTS could be broken down to  some b a s i c  pr imi t ives  of 
action.  Dif ferent  ACT5 can then be compared, not  by looking a t  t h e i r  h le r -  
archies  , but  by comparing t h e i r  common pr imi t ive  ac t ions .  
7.2.2 PARSER 
The s ing le  major l i m i t a t i o n  of the  current  parser  i s  i t s  i n a b i l i t y  t o  
parse anything other  than simple DO-questions and statements. This is not  
a theore t i ca l  shortcoming; i t  simply a question of implementation e f f o r t .  
Hodeuef, there  a r e  problems ,with f m p a r s e r  Zrrespective of t h i s  major 
l imi ta t ion.  These problems a r e  qu i t e  o f t e n  re la ted  t o  de f i c ienc ies  i n  the  
memory s t ructure .  
The p a r s e r  w i l l  c u r r e n t l y  f a i l  i f  i t  does n o t  f i n d  a semant ical ly  
accep tab le  parse. I n  cases  such as t h i s ,  i t  should be allowed t o  f ind ,  
ins tead~ ,  a s y n t a c t i c a l l y  acceptable  pa rse  t h a t  would be marked as a 
semantic anomaly. 
Undefined words a r e  now ignored. The a b i l i t y  t o  make reasonable 
p r e d i c t i o n s  about t h e  funct ion and meaning of u n d e f i ~ e d  words would be 
des i rab le .  
7.2.3 OUTPUT PRODUCTION 
The most s e r i o u s  l i m i t a t i o n  i n  t h e  production of output  has  been 
t h e  l ack  of time t o  develop more extensive  procedures. Examples of new 
r u l e s  t h a t  a r e  n o t  y e t  implemented t h a t  could be used t o  produce answers 
a r e  t h e  following: 
1. Use more than one memory match s t r u c t u r e  t o  produce t h e  
answer. Currept ly  only  t h e  b e s t  one is  used. For example, 
t h e  quest ion "DOES ANYONE HAVE A RED TOY?" could be answered 
by combining t h e  two memory matches, "BRANDT IUVE RED BOOK" 
and "JENA HAVE RED BALL," t o  g e t  "YES. JENA HAS A RED BALL AND 
BRANDT HAS A RED  BOOK,^^ 
2. Several  equal ly  good memory matches could be combined t o  
produce a s i n g l e  answer t~ l l i ch  conta ins  a compound a c t o r  o r  
ob jec t .  The quest ion "DOES BIUNDT HAVE A BOOK?" matches two 
memory s t r u c t u r e s  equal ly  wel l .  These two matches, "BRANDT 
IlAVE, RED BOOK" and "BTLANDT HAVE BLUE BOOK," could be combined 
t o  form t h e  response "YES. HE 1US A RED ONE AND A BLUE ONE." 
Note t h a t  work a l s o  needs t o  be done wi th  respec t  t o  pronoun 
s u b s t i t u t i  ~n ( i .  e., ONE f o r  BOOK) befose  t h e  above response 
could be produced. 
3. I f  t h e  memory search f a i l s  and t h e r e  was no a c t o r  i n  memory, 
t h i s  should be  repor ted e x p l i c i t l y  r a t h e r  than saying "I IXN'T 
KMW." For example, consider  t h e  quest ion "DOES RILL MAXWELL 
HAVE A SON?" I f  BILL@MN(WI?LL is no t  p resen t  i n  MEMORY, r e p o r t  
"I DON'T KNOtJ BILL MAXJELL .I1 However, i f  t h e  memory match 
procedures were success fu l  i n  f ind ing  gener ic  information such 
as "bEN HAVE SONS" then t h e  phrase  "BUT IIE COULD HAVE A SON." 
could a l s o  be generated. 
-61- 
Other l imi ta t ions  a r e  t h e  i n a b i l i t y  t o  handle d e f i n i t e  a r t i c l e s  o r  
t o  a t tach modifying c lauses  o r  phrases t o  help  dis t inguis l l  p a r t s  of 
output. The production procedures allow only s u p e r f i c i a l  treatment of 
the  ACT. 
Typical of simple questions t h a t  current ly  cannot be answered is 
"DOES BRANDT 11AV13 TWO BOOKS?''. There is no mechanism f o r  counting o r  com- 
paring occurrences of re levant  f a c t s  i n  memory during t h e  search process. 
nor of using them i n  t h e  output production phase. 
7.3 IGNORED PROBLEEIS 
7.3.1 INTENTIONS AND MOTIVATIONS 
The most ser ious  problem is t h e  lack of any modelling of in ten t ions  
and motivations of t h e  person ta lking wi th  JIIIMY3 (or of J I M 3  i t s e l f ) .  
In order f o r  a dialog t o  be sustained f o r  any length of time with a sense  
of continuity,  models of t h e  person and of JIIMY3 a r e  requtred. Two 
psychological models should be maintained by t h e  program t o  ind ica te  
1) what the  program thinks about i t s e l f  and the  person and 2) what t h e  
program thinks the  person thinks about himself and the  program. A t  a l l  
times during the  conversation, information i n  the  models w i l l  speci fy  f o r  
both the  person and the  pragram what each a) van t s  t o  know, b) wants t o  
t e l l ,  c) already knows, d)  f e e l s ,  e )  believes, etc., with respect  t o  t h e  
context of t h e  conversatian up t o  t h a t  point .  Also required w i l l  be 
information about t h e  motives and physical  and mental a t t r i b u t e s  and 
c w b i l i t i e s  of each par t i c ipan t  from each po in t  of view. In tegra t ion  of 
these models i n t o  the  workings of the  parsing,  memory searching and output 
-62- 
production phases of the program will be a necessary step towards more 
complete natural language understanding, 
For example, to continue that hypothetical dialogue of section 1, 
one would like to be able to see sohe exchanges such as: 
H: Was the window expensive to repair? 
NCP: Why do you ask? 
H: Why do you want to know? 
NCP: I fear I may be responsible for the debt. 
H: I thought that might be the casq. 
NCP: You mean I do owe you money? 
H: No, it's just that I regret making you feel uncomfortable. 
NCP: Then why did you ask if the window was expensive to repair? 
H: To test your power of deduction. 
NCP: You really don't seem to understand me. 
Computer analysis of such complex interchanges is dependent upon the existence 
of psychological models of both parties. Successful realization is probably 
many years off. 
7.3.2 NORMlU?"S PROBLEMS 
With respect to answering capabilities in language understanding 
understanding models, Norman [I)] has presented several basic problems that 
should be considered. These are illustrated as: 1) the telephone number 
problem, 2) the three drugstores p'roblem and 3) the Empire State Building 
problem. 
The first problem concerns the appropriate response to the question: 
"What is Charles Dickens' phone number?" The normal human response of 
"That's a stupid question." or "Phones weren't invented then. requires 
the action of a plausibility check on the question before an attempt is 
made to find an answer. 
-63- 
The t h r e e  drugotor* problem considers  ww t h e  co r rec t  r eac t ion  
should be t o  t h e  statement: "I went t o  t h r e e  drugstores." This is  
r e a l l y  a problem of integraking current  wi th  pas t  knowledge and of 
determining presuppositions t o  t h e  new da ta .  FOK ins tance ,  t he  program 
should note  (or  ask) Why, didn' t  t he  o the r  two drugstores have what you 
were looking fo r?  
The~Empire S t a t e  Building probfem r e f e r s  t o  the  context  and scope 
of any p a r t i c u l a r  question. The quest ion,  "Where is the  Empire S t a t e  
Building?" requi res  d i f f e r e n t  answers depending on the  context  of t he  
conversation. To paraphrase Norman's response: I f  asked t h e  quest ion i n  
Russia, t he  answer most l i k e l y  would be "In the  United States."; i f  asked 
by an a d u l t  i n  Europe, "In New York City."; i f  asked by someone i n  
New York City,  then "On 34th Street." would be appropriate .  
From these  examples, i t  is obvious t h a t  much more is  involved i n  
developing a language understanding system t h a t  answers quest ions "natural ly" 
than j u s t  t he  a b i l i t y  t o  pa r se  input  co r rec t ly ,  look up an answer and 
report  it. We hope w e  have more c lea r ly  i d e n t i f i e d  some of t h e  des i r ab le  
cha rac t e r i s t i c s ,  and we hope we have i l l u s t r a t e d  some progress toward t h e  
u l t imate  goal. If  JIMMY3 allows o the r  problems from our list t o  be addressed 
a lso ,  we s h a l l  f e e l  for tunate ,  indeed. 
REFERENCES 
[l] Anderson, John R. and Gordon I[. U o ~ ~ e r ,  Iturnan Associat ive Nenlory, 
Wiley and Sons, 1974. 
[2] Bobrow, Daniel G. , and Allan Col l ins  (eds . ) , Represen t a t  ion  - and 
Understanding, k a d e m i c  Press ,  1975. 
[3] Brotm, John Seely nnd Burton, Richard R., "Multiple Representations 
of Knowledge f o r  T u t o r i a l  Reasoning," i n  123, 
C41 Carbonell,  Jaime K., "A1 i n  C A I :  An A r t i f i c i a l  Intelligence Approach 
. - 
t o ~ ~ o m ~ u r e r - ~ i d e d  1ns t rue t ion ,"  Transact ions l lan-~aehine Sys tarns, 
Vol. MIIS-11, 1970, pp. 190-202. 
[5] Colby , Kenneth H., A r t i f i c i a l  Paranoia: Computer Simulation of 
Paranoid Processes, Pergamon Press ,  1975. 
[6] Lindsay, Kobeat ,K., "In Defense of Ad lIoc Systems," i n  1151. 
[7] Newell, Allen, "11euristic Programing:  I l l -S t ruc tured  Problems," 
i n  J.S. Aronofsky (ed.), Progress & Operations Research, Wiley, 1969. 
[a] Newell, Allen, e t  a l . ,  Speech Understandin& Systems, American 
Elsevier ,  1973. 
[9] Norman, Donald A., "Mernory , Knowledge, and the  Answering of Questions,' '  
i n  R.L. Solso (ed.),  Contemporary -- I ssues  i n  Cognitive ~ s ~ c h o l o ~ y :  -The
Loyola Symposium, Winston, 1973. 
[ lo ]  Norman, Donald A . ,  navid E .  Rumelhart and t h e  LNR Research Group, 
Explorat ions Cognition, Freeman and Co., 1975. 
[ll] Qui l l ian ,  #. Ross, "Semantic ~ e m o r y  ," i n  Marvin Ifinsky (ed. ) , 
Semantic Information Processin&, MIT Press,  1968. 
[12] Rustin, Randall, Natural Language Processing, Algorithmics Press,  
1973. 
[13] Schank, Roger, "Conceptual Dependency: A Theory of Natural Langauge 
Understanding," Cognitive P S Y C ~ O ~ O R Y ,  vol. 3, 1972, pp. 552-631. 
[14] Schank, Roger, " ~ d e n t i f i c a t i o n  of Conceptualizations U n d e r l y i n ~  
Natural  Language, " i n  [15]. 
[153 Schank, Roger and Kenneth Colby (eds. ) , Computer Nodels of Thought 
and Language, Freeman, 1973. 
-
[16] Weizenbaum; Jooeph, "ELIZA: A Computer Program f o r  t h e  Study of 
Natural  Language Conununicat3on between Men and Zfachine," Conununf cqt ions  
of the  ACM, Vol. 9, 1967, pp. 36-54. 
--- 
[l7] Milks, Yorick, "An A r t i f i c i a l  I n t e l l i g e n c e  Approach to Machine 
~ r a n s l a t i u n , "  i n  [15]. 
[18] Wilks, Yorick, "The Stanford Machine Trans la t ion  ~ r o j e c t , "  i n  [12]. 
[19] Winograd, Terry, "Understanding Natural  Language ,I1 Cognitive 
Psychology, Vol. 3, 1972, pp. 1-191. 
[20] Woods, Will ian~, "An Experimental Pars ing System f o r  Trans i t ion  
Network Grammars," i n  [12]. 
American Journal of Computational Lingllirtics Microfiche 61 : 71 
FIFTEENTH ANNUAL MEETING 
A S S O C I A T I O N  F O R  C O f l P U T A T I O N A L  
L I N G U I S T I C S  
The decision to hold the 1977 meeting of the Association 
in March was taken only in October 1976. The six-month 
interval has not allowed advance distribution of abstracts 
through the Journal. The abstracts accepted for presenta- 
tion are published here for the record, and for the use 
of members who could not attend the meeting. 
Some or all of these papers may be published in future 
issues. 
The program, on frames 72 and 73, gives frame numbers for 
all abstracts. 
ASSOCIATI(M FOR C [ ] M F u r A T I ~  LINGUISTICS 
Fifteenth m u a l  Ueeting 
CEtorgetbwn mivers i  ty,  Washington D.C. 16-17 March 1977 
Palrns m e ,  Walsh Building; 36th Street between N and Prospect 
wednesdav. 16 March 
Session i IANcXla MDDEKS AND 'IWHNIQUES 
St  OC A Bi-Directional Parser for ATN Gramnars 74 
G. B r o w  and W. A. Woods, 801t Beranek and NevaMn 
9: 30 The Efficient Integration of Syntactic Processing with 
Casedriented Semailtic Interpretation 75 
R. J. Bobrow and M. Bates, Bolt Beranek and Fhman 
10:00 Same Properties of Arbitrary Fhrase Structure Ianguages 
and Translation 76 
W. Buttehann , Ohio Sta te  lsliversity 
10:30 The Augmented Finite State Machine Approach to  Synthesis 
by Rule 7 7 
T. K a c m e k ,  University of Pennsylvania 
11:OO A lexicon for a Cdmplter Question-Anmering System 78 
M. Evens and R. Smith, I l l i n o i s  Ins t i tu te  of- Technology 
11:30 Pragress &port on REL 79 
B,  8. Ttmnpon and F. B, Thompson, California Ins t i tu te  
of Wchrmlogy 
Session I1 Panel Discussion: SPEECH Ihn;rERSI'ANUfNG AND CCMPWI'ATI~I; 
EIWXTISTICS - A CRITICAL EWINAW(Xi OF 'ME AREW PKUEC 
2:00 Chairman: J. Allen, M.I.T. 
Wrticipants: R. m y ,  Carnegie-Mellon Uliversity 
D. E. Walker, Stanford Research Institute 
W. A. Waods, Bolt Beranek atwl NePaMn 
5:30 Business Meeting, Paul Chapin, ACL Resident ,  presiding 
6:  30 Adjourn for dinner 
8200 Dinner and Presidential Rddress by Paul Chapin 
The Foundry, 1050 30th Street, Washiwton 
$14.00; mke reservations ear ly  Wednesday morning 
40 person l imi t ,  because of restaurant c a p c i t y  
Fifteenth Annual Meeting 
Georgetown [lniversity, Washington D.C. 16-17 March 1977 
Palms bunget Walsh Building; 36th Street betwen N and Prospect 
Buraday, 17 March 
Session I11 RMBfMS OF DISCOURSE 
9:00 Parsing Free Narrative 
N. Sager and C. Insolio, New York Cltliversity 
9: 30 Disoourse Connectives 
B. Phillips, Uliversity of I l l i n o i s  
10:00 A Frmewrk for Processing Dialogue 
G. E. B r m ,  M.I.T. 
10:30 Natural Language Prqremning: Kitchen Recipes 
L. A, Miller, 3 3 3  
11:00 A-ting the Spatial Metaphor 
J. Ri Hobbs, C i t y  University of New York 
11: 30 Inferring an Antecedent 
B. Nash-Webber, B o l t  Beranek and N e m  
m i s t r a t i o n  fees: $10.00 Umber 
$ 5.00 Student 
$15.00 Other 
No advanced registration procedures; individual dues for ACL mdership  
of $15 ?or the year 1977 m y  b paid at the m&ting. (Institutional 
dues are $30 for 1977.) 
ACL Meeting 1977 
G. Brown and W .  A. Woods 
Bi -d i rec t iona l  Parsing With  ATN Grammars 
The S y n t a c t i c  component o f  HWIM, t h e  BBN Speech 
Understanfing ___ _ -- w, is a  middle-out , b i -d i r  e c t i o n a l  pa r se r  f o r  
Augmented Trarls i t ion NetworK grammars. Severa l  o f  t h e  HWIM 
c o n t r o l  s t r a t e g i e s  r e q u i r e  a  pa r se r  t h a t  can e v a l u a t e  an i s o l a t e d  
sequence of  words ( c a l l e d  an " i s l a n d n )  somewhere i n  t h e  middle o f  
an u t t e r a n c e  t o  determine whether it is  a  p o s s i b l e  fragment o f  a  
complete sentence .  I f  s o ,  t h e  pa r se r  is required  t o  make 
p r e d i c t i o n s  f b r  a l l  o f  t h e  p o s s i b l e  words t h a t  could be used t o  
extend the  fragment i n  each d i r e c t i o n .  In t h i s  t a l k  we w i l l  
d e s c r i b e  a  pa r s ing  a lgor i thm which e f f i c i e n t l y  achieves  t h e s e  
c a p a b i l i t i e s .  
The HWIM parse r  can be viewed as a b i - d i r e c t i o n a l  
g e n e r a l i z a t i o n  of  Ear ley  s algor i thm extended t o  handle 
c o n t e x t - s e n s i t i v e  , ATN grammars. The a lgor i thm s t o r e s  the  
computations i n  "segment" and " i s l a n d "  c o n f i g u r a t i o n s  indexed by 
end s t a t e s  and boundaries.  ( A  segment is a  p a r t i a l  pa r se  t h a t  is 
conta ined comple t r ry  wi th in  one l e v e l  o f  t h e  t r a n s i t i o n  network 
grammar .) Organizing t h e  computation i n t o  segment and i s l and  
c o n f i g u r a t i o n s  el iminate 's  t h e  need f o r  a  s t a c k ,  t h u s  so lv ing a  
d i f f i c u l t  problem in middle-out pars ing .  
In the  usual  ATN formalism, the  grammar i s  w r i t t e n  a s  
i f  t o  be processed from l e f t  t o  r i g h t ,  and i n  genera l  some of t h e  
a r c  a c t i o n s  w i l l  be dependent on o the r  a c t i o n s  t o  t h e i r  l e f t  i n  
t h e  grammar. To insure  t h e  c o r r e c t  handling of such 
context-dependent a r c  a c t i o n s  by t h e  b i - d i r e c t i o n a l  p a r s e r ,  t h e  
grammar wr i t e r  mu$t s p e c i f y  t h e  "scope" o f  any such a c t i o n .  
Except for  the  e x p l i c i t  d e c l a r a t i o n  o f  scopes f o r  
context-dependent a c t i o n s ,  a  b i - d i r e c t i o n a l  ATN grammar i s  
e x a c t l y  l i k e  an o rd ina ry  ATN, and l e f t - t o - r i g h t  ATN grammars can 
be converted t o  b i - d i r e c t i o m l  opera t ion  simply by adding scope 
s t a t ements .  
Although developed i n  the  c o n t e x t  of  a speech 
under s tanding a p p l i c a t i o n  we f e e l  t h a t  t h e  b i - d i r e c t i o n a l  , 
middle-out pars ing  a lgor i thm a l s o  has a p p l i c a t i o n s  i n  t e x t  
pa r s ing  f o r  problems such a s  e r r o r  c o r r e c t i o n ,  p a r t i a l  
i n t e r p r e t a t i o n  o f  sentence  fragments,  and management of 
combinator i c s  i n  long sentences .  
ACL Meeting 1977 
R. J. Bobrow and M. Bates 
THE EFFICIENT INTEGRATION OF SYNTACTIC PROCESSING 
WITH CASE-ORIENTED SEMANTIC INTERPRETATION 
It has lon been the goal  of those wr i t ing  na tu ra l  langua e  
processing sysfems t o  express syn tac t i c  c o n s t r a i n t s  i n  broa%. 
general  way while usinrz t i ~ h t  scmantlc c o n s t r a i n t s  t o  ulde the  
parsing and t o  i n t e r p r e t  the  r e s u l t i n g  s t r u c t u r e s .  ?he system 
described here uses an a ~ ~ m e n t e d  t r a n s i t i o n  network grammar 
t o  e the r  w i t h  a case-oriented dic t ionary  t o  achieve a  c lose  and 
e f f i c i e n t  in teqra t ion  o r  the  syn tac t i c  processin with the  case 
s t r u c t u r e s  (which include semantic and p roper t i e s  of 
ob jec t s )  . 
The ATN def ines ,  using normal s n t a c t i c  c a t e  o r i e s ,  a very 
general  surface  s t ruc tu re  of aboue the  capab l l f ty  of the  LUNAR 
and GSP systems. If case s t r u c t u r e s  and semantic inf'ornation (including in te rp re ta t lon  r u l e s )  a r e  omitted from the d i c t i o n a r  
r e la ted  "deep s t ructures"  f o r  syn tac t i c  paraphrases. 
I the  grammar functions a s  a  standard pa r se r ,  producin3 c lose  y 
The system provides mechanisms fo r  users  t o  def ine  semantic 
in te rp re ta t ion  r u l e s  and case  frame checks which a r e  t o  be 
applied a t  various points  i n  t h e  pa r s ins  process. Thus 
const i tuents  may be In terpre ted a s  soon a s  they a r e  parsed,  and 
the s t r u c t u r e  of the semantic i n t e r p r e t a t i o n s  thus produced may 
be checked when, f i l l i n g  the  case  frames f o r  higher s t r u c t u r e s .  
Since the "most l l k e l y  l o c a l  i n t e r p r e t a t i o n  may not f i t  the  case 
requirements of contaming s t r u c t u r e s ,  the  system provides a 
general  coercion mechanism t o  r e i n t e r p r e t  a cons t i tuen t  i n  l i g h t  
of its context  when necessary. To f a c i l i t a t e  r e i n t e r p r e t a t i o n ,  
as  well a s  c e r t a i n  anaphoric references ,  the  o r i g l n a l  s y n t a c t i c  
s t r u c t u r e  is maintained throughout the parsing together with any 
semantic interpretations. 
The present system i s  being used a s  the na tu ra l  language 
front-end f o r  a sophis t ica ted  message processing and f i l i n g  
system. Ultimately, we hope t o  have a  general  system which can 
be adapted t o  o the r  na tu ra l  language input s stems by 
new dic t ionary  e n t r i e s  and aamantic in te rp reea t ion  lunct!K:ifdinq 
ACL Meeting 1977 
W Buttelman 
SOME PROPERTIES OF ARBITRARY PHRASE STRUCTURE LANGUAGES AND TRANSLATION 
DERIVED USING A FORELAL MODEL OF PIRASE STRUCTURE SYNTAX AND SEM.ilNTICS 
Abstract 
The notion of a phrase structure linguistic description is introduced -- 
a pair, D = ((1,s) where G is an arbitrary phrase structure grammar and S is a 
formal semantics (defined in the paper). S may be either context free or con- 
text sensitive. S models the following notion of meaning: the meaningful units 
of language are phrases; the meaning of a phrase is a function of its syntactic 
structure and of the meanings of its constituents and of its semantic context. 
This concept is a generalization of semantic notions due to Tarski, later suggested 
by Thompson and by Katz and Fodor, and recently popularized for programming lan- 
guages in Knuth's synthesized attributes and for natural languages by Montague. 
The (phrase structure) language of D, L(D), is the set of ordered pairs (w,m) 
where w i s  a sentence of G and m is a meaning assigned to w by S. 
We prove the following results: The set of phrase structure languages is 
just the set of products of r.e. sets. Every phrase structure language has a 
description using a regular grammar and a context free semantics. For every 
description D with an qnrestricted grammar and context sensitive semantics there 
is a description D' using a context free grammar and context free semantics such 
that L(D) = L(D'). Furthermore, D and Q are "strongly equivalent" in the sense 
that the phrase trees assigned by Dl to each sentence are just the skeleton trees 
of the phrase structures assigned by D to the sentence. The notions of "weak" 
and "strong equivalence" are extended to semantics (if two descriptions are 
strongly equivalent in a semantic sense, then the structure of their semantic 
functions is identical -- in a programming sense, the same programs can be used 
to compute the meanings of the same ~"entencesj. In this sense, D and D' are not 
strongly equivalent. However, if D has a context: free semantics, then D and D' 
are semantically strongly equivalent. Also, we prove that there is a description 
Dm' for L(D) using a context sensitive semantics which is strongly equivalent to 
D in both the syntactic and semantic senses, 
Next we define translation on phrase structure languages and consider a par- 
ticularly appealing strategy for translation, which we call "syntax-controlled" 
translation. (I have avoided the term "syntax-directed" because it has had 
differing uses in t11e li~erature.) We prove the following results: Eveiy com- 
putable translation is definable as a syntax-controlled translation. For two 
arbitrary descriptions D and D', it is undecidable whether any syntax-controlled 
translation from L(D) to L(D1) exists. We give an algorithm which, given two 
arbitrary descriptions D and D', will halt and produce the definition (program) 
of a syntax-controlled translation from L(D) to L(D1)  if and only if such a 
translation definable by D and D' exists. 
Syntax-controlled translation requires no semantic computation at translate 
time (for which one pays a dear price in the time required to generate syntax- 
controlled translators). For a syntax-controlled translation which produces a 
ACL Meeting 1977 
single target  sentence having a meaning i n  common with the source sentence, the 
time complexity is 0 (ptw) where p is parsing time and w is the weight of the 
source phrase structure. To produce the smallest s e t  of ta rge t  sentences such 
that  each target sentence has a t  l ea s t  one meaning i n  common with the source .and 
such that  a l l  t ranslatable meanings of the source are  represented requires 
0 ((aCn) (cn! ) f (pi-cn) 3 
tihe, where c and d are  constants, n is the source sentence length, f is  the time 
to  check for  semantic va l id i ty  of a parse, and p is the time to prcduce a l l  parses. 
Finally, we consider phrase s tructure language descriptions having both 
inherited and synthetic meaning. No new languages can be defined, but the use 
of inherited meaning lcads i n  a natural  way t o  a notion of semantic-controlled 
translation. 
T. Kaczmarek 
The AuRnented F i n i t e  S t a t e  E-lachine - 
A !:ore E f f i c i en t  Approach t o  Synthesis  by Rule 
The au~mented t r a n s i t i o n  network ( A T N ) ,  which has 
proven usefu l  f o r  n a t u r a l  l a n ~ u a p e  u n d e r s t a n d i n ~ ,  has been 
reformulated and r e s t r i c t e d  s l i q h t l y  t o  ye i ld  a mechanism 
termed the aucrrnented f i n i t e  s t a t e  machine (AFSbl). The AFSll 
i s  b e i n ~  used to  do speech syn thes i s  by r u l e ,  a process bv 
which phonetic t r a n s c r i p t i o n s  of speech a r e  converted i n t o  
synthesizer  parane tersb  The r u l e s  f o r  syn thes i s  i n  t h i s  
approach take the form of procedures which a r e  cond i t i ona l ly  
executed dependin8 on con tex tb  )lost previous syn thes i s  by 
rule systems began w i t h  a t ransformational  component and 
added procedural s t a t e n e n t s .  I n  t h i s  system procedural 
statements have been added t o  a much simpler mechanism, the 
f i n i t e  s t a t e  nachine, The advantame of t h e  AFSIf approach is 
t h a t  the phonetic s t r i n g  may be processed in  a  s i n g l e  l i n e a r  
pass 
ACL Meeting 1977 
M. Evens and R. Smith 
A LEXICON FOR A COMPUTER QUESTION-ANSWERING SYSTEM 
Computer question-answering systems and o the r  models of na tu ra l  
language processing need lexicons t h a t  a r e  much l a r g e r  than those 
ava i l ab l e  tioday (c f .  Simmons. 1970 and Becker, 1975). But the  models 
cu r r en t ly  i n  operat ion (e.g. Winograd, 1971) a l ready  consume a l l  ava i l -  
ab l e  high-speed memory i n  l a rge  computer systems. Lexica l  r e l a t i o n s  
as developed by Raphael (1968), Apresyan e t  a l .  (1971), and Simmons 
(1973) provide a method of s t o r i n g  l e x i c a l  information i n  more compact 
form. While Schank (1973) and Wilks (1975) both claim t h a t  t he re  i's 
a f ixed un ive r sa l  s e t  of semantic primes, we argue i n  opposi t ion,  
follbwing the  Russians and Mi l le r  and Johnson-Laird (1976), t h a t  t he  
set of l e x i c a l  r e l a t i o n s  is open-ended; our  system i s  designed t o  
add new r e l a t i o n s  whenever a l e x i c a l  r e g u l a r i t y  is noticed.  
Our lexicon i s  being developed a s  an  i n t e g r a l  p a r t  o f  a computer 
question-answering system which answers mult iple-choice quest ions 
About simple ch i ld ren ' s  s t o r i e s .  It serves  a s  a global  data-base f o r  
t h i s  system - a combination lexicon-encyclopedia - and must make in-  
formation r ead i ly  ava i l ab l e  f o r  t he  parsing process, f o r  bui lding an 
i n t e r n a l  model of t he  s t o r y  being read,  and f o r  making inferences.  
One of our  test paragraphs, which comes from a t e s t  desigped f o r  f i r s t  
and second graders ,  says "Ted has a puppy. His name is Happy. Ted and 
Happy l i k e  t o  play." I n  order  t o  answer the  f i r s t  quest ion,  "The pe t  
is a: dog-boy-toy?", we need t o  know what pet means. The l e x i c a l  
en t ry  f o r  pet contains a simple d e f i n i t i o n ,  t h a t  a pe t  is an animal 
t h a t  is owned by a human, i n  a f i r s t - o r d e r  pred ica te  ca lcu lus  form: 
NCOM(PET,Z1) = ( Z2)NCOM(ANIMAL,Z1) .NCOM(HllMAN,Z2) .R(OlJN,Z2,Z1). I n  
order  t o  answer t h l s  ques t ion  we a l s o  need t o  know t h a t  a puppy i s  a 
young dog. This information : NCOMf PUPPY ,Zl) "- NCOM(DOG,Z1). PROP 
(AGE,Z1,YOUNG) could be p a r t  of the  l e x i c a l  e n t r y  f o r  puppy. We would, 
of course, need axioms of t he  same form a s  wel l  f o r  t he  e n t r i e s  f o r  
k i t t e n ,  lamb, e t c .  Instead w e  express t h i s  informarion by using a 
l e x i c a l  r e l a t i o n ,  CHILD. The l e x i c a l  e n t r y  f o r  puppy therefore  con- 
t a i n s  CHILD dog; the l e x i c a l  en t ry  f o r  k i t t e n  contains CHILD &; and 
t h e  l e x i c a l  e n t r y  fo r  CHILD contains t h e  aximn scheme from which t h e  
r e l evan t  axioms a r e  formed when needed. 
For verbs ,  corresponding t o  each case  r e l a t i o n  t h e r e  is a l e x i c a l  
r e l a t i o n  which points  t o  t yp i ca l  f i l l e r s  of t h a t  case s l o t .  The l e x i -  
cal e n t r y  f o r  includes TAGENT baker and TLOC kitchen4 It a l s o  includes 
T where T is b e  well-known taxonomy r e l a t i o n ,  s o  t h a t  i f  the  
s t o r y  says t h a t  Mother baked a cake w e  can i n f e r  t h a t  she  one and 
CAUSE bakP. s o  t ha t  we can deduce t h a t  the cake has baked. The se lec-  
eional  r e s t r i c t i o n s  t h a t  h e l p  us t e l l  ins tances  of Wl and !x& 
apa r t  can a l s o  be expressed compactly us ing  the  T r e l a n o n .  We a f s o  
need t o  make deductions from main verbs  i n  predica te  complement: con- 
s t r u c t i o n s ;  deduction6 such a s  , the speaker 's  view of t h e  t r u t h  of t h e  
ACL Meeting 1977 
proposition stated i n  the complement as derived from the fac t iv i ty  
of the verb ( in  these s#  :ies the reader must infer that  everything 
that  mother says i s  true!). Lexical entr ies  for main verbs tha t  
take predicate complements contain pointers to the implication class. 
These relations can then be expanded t o  give the proper axiom schemes. 
The use of lexical relations allows us to,express both syntactic 
and semantic information i n  a form that  i s  compact, easy to retr ieve,  
and that provides effective .input to both parsing and deductive 
processes. 
B. H. Thompson and F. B. Thompson 
A Progress Report on REL 
The REL (Rapidly Extensible Language) System. is now i n  operational 
prototype form. An experimental version of the system was 
demonstrated i n  1973 and s ince has undergone very thorough revision 
and clean up. The REL English grammar, which includes an extensive 
arithmetical compclnent, has been improved and extended. The 
system can be demonstrated and made available for  user tes t ing  on 
IBM 360/370 computers using most operating systems, e.g. TSO, CMS. 
A user's Reference Manual is now i n  preparation and will be 
available a t  the time of the conference. 
The basic system philosophy has remained the  same,, namely t o  
provide the user with a t oo l  for  natural  man-machine communication 
tha t  can eas i ly  be sui ted t o  h is  individual needs. Thus the system 
provides the user with the capabili ty t o  modify m d  extend h i s  data 
b w e  and language package. Such modification can be carr ied out 
by statements about the data base items; for  example: 
John was not a student a f t e r  June 1, 1976. 
will remove John from the student c lass  a s  of t ha t  date. Extensions 
can be carried out by adding new primitive individuals,  c lasses ,  and 
relat ions,  as well as  through def in i t iona l  capabi l i t i es  which 
allow for  defining new concepts i n  terms of exis t ing ones. A s  a 
par t  of t h i s  capablli ty , verbs can be introduced by paraphrases, 
for example : 
def:ships "carry" coa1:the cargo of ships is  coal 
and then used i n  a question such as: 
What s t r a t eg i c  materials were carr ied by USSR ships  i n  19631 
ACL Meeting 1971 
N. Sager and C .  Insolio 
Par sing Free Narrative 
The r e su l t s  of an experiment i n  parsing narrat ive t ex t s  
a re  presented. The t e x t s  were discharge s m a r i e s  obtained 
from a hospital  ' s computerized f i l e s  of pat ient  records. 
Each document s ta tes  the  background of the case, the  r e su l t s  
of the  physical examination and laboratory t e s t s ,  t he  time 
course. of the i l l nes s  i n  the hospital ,  diagnosis, s t a tu s  on 
discharge, eto. These t ex t s  a r e  par t icu lar ly  in te res t ing  
because they are unedited--cryptic phrases are mixed with 
f u l l  sentences, punctuation i s  not consistent, and spel l ing 
errors  and abbr,eviations abound. In short, t he  material  is 
f ree  narrat ive as  one would find it i n  a technical enviroment 
whsre repbrts  are dictated and where there would be motivation 
for  processing the data  i n  t h e i r  natural  language form. The paper 
w l l l  a s c r i b e  how the aboye d i f f i c u l t i e s  were t rea ted  and w i l l  
present s t a t i s t i c a l  r e su l t s  of t he  experiment, such as  t he  number 
of sentences correctly parsed vs. the t o t a l  number of sentences 
and t he  average parsing times fo r  different  types of sentences. 
I n  addition, the special problems due t o  commas, conjunctions, 
quantifiers,  and run on sentences w i l l  be discussed. 
ACL Meeting 1977 
B. P h i l l i p 8  DISCOURSE CONNECTIVES 
I n  essence current  systems of discourse ana lys i s  w p  sur face  s t ruc tu re s  
i n to  underlying causal chains of propositions. As the  sur face  form is e l l i p t i c ,  
i t  is necessary t o  include a knowledge base by means of which omitted l ink ing  
prbpositions of the discourse may be inferred,  rendeting them e x p l i c i t  i n  t he  
underlying representation. 
Cause is not t he  only l i n k  between propositions, however; a l s o  used a r e  
s y l l o g i s t i c  and analogic mechanisms, statements of r e l a t i v e  b e l i e f ,  and processes 
of decomposition and abs t rac t ion ,  the  l a s t  being t he  expl ica t ion  of abs t r ac t  
concepts. 
M d i t i v e  discourse connectives - 'because', ' so ' ,  e t c . ,  a r e  reaLizat ions of 
the  l i nks  between propositions i n  these modes of discourse construct ion.  There 
a r e  a l s o  adversative connectives, such a s  'however ' , ' bu t ' ,  e t c .  , tha t  cannot 
be so  explained. They must be in te rpre ted  a s  s i gna l s  t o  tu rn  off  inference 
mechanisms. 
To understand the need f o r  adversat ive connectives, we first need t o  
recognize two kinds of p ropos i t io t~s ,  episodic  and systemic. The former encode 
spec i f i c  a c t s  and s t a t e s ,  e.g., 'Thomason won the  e lec t ion  f o r  t he  governorship 
of I l l i n o i s ' ,  whereas the l a t t e r  a r e  generalized ca tegor ica l  statements,  e.g., 
'b i rds  have wings'. The content of discourse is usual ly episodic. The 
knowledge base contains both kinds of propositions, there  a r e  episodic and 
systemic memories. 
There is a predict ive component i n  t he  process of understanding discourse.  
A s t a u d  s i t ua t i on  s e t s  up expectancies which may e i t h e r  become the  unstated 
l ink ing  propositions, or  may be e x p l i c i t l y  s t a t ed ,  and hence confirmed, a t  
a l a t e r  point.  The predict ions a r e  s e t  up by systemic memory'. An episodic 
proposition has a counterpart i n  systemic memory, e -g . ,  
(1) John a t e  cheese. (Episodic) 
(2) Person eat  food. (Systemic) 
The predict ions a r e  associated with systemic memory, e.g., 
(3) Person e a t  food CAUSE person not hungry. 
Thus given (11, a l a t e r  expectancy of (4) would be s e t  up by (3) 
(4) John was not hungry. 
But systemic knowledge contains general izat ions,  not  inv io lab le  t r u th s ,  and t he  
inference may not be va l id .  This can be marked by the  use  of an adversat ive 
connective: 
( 5 )  John a t e  t he  cheese, but he was still  hungry. 
ACL Meeting 1977 
G .  P. Brown 
A FRAMEWORK FOR PROCESSING DIALOGUE 
Thts report describes a framework for handing dxed-initiative English 
dialogue in a conale session environment, with emphasis on recognition. W ithin this 
framework, both Iinguistk and non-linguistic actlvlties are madelled by structures called 
malods, which are a declarative form of procedural knowledge. Our  design focusses on 
unlts of Ilnguistlc.actlvity larger than the speech act, so that the pragmatic and semantic 
context of an utterance can be used to guide Its interpretation. Also important is the  
treatment of Indirect illocutions, e.g., the different ways to ask a question, give a command, 
etc 
Our basic approach has been to combine careful structural distinctions 
with a mixed recognition strategy. The central distinction is in the way that utterances can 
be related to the methods in the dialogue model. First, an utterance (called an initiator) may 
Introduce a method that corresponds to one of the standard activities in an environment (for 
example, asking a question at an information desk or requesting help from a consultant). 
Second, an  utterance may correspond to a step in a standard path In a method already 
underway; here, a standard path is a normally expected succession of activity steps. Third, 
m utterance may be part of recovery discussion, which Is generated when when some 
violation of standard expectations occurs, necessitating clarification, correction, etc. Flnally, 
an utterance may belong to mdadiscussion, a relatively constrained class whose function is to 
lay out the context for other utterances so that these may be identified w~th the appropriate 
method step. 
Given the static model of dialogue embodied in the methods, the problem is 
to find the correct method step that relates to a particular input. We handle this problem by 
deflning a set of special structures to aid in matchag, by using the methods to generate 
expectations dynamtcally, and by differentiating overall matching strategies according to the 
four utterance classes described. 
The  ideas presented here have been implemented In a prototype system called 
Susie Software, whtch is embedded in OWL-I The OWL system is currently under 
development in the Knowledge-Bases Systems Group at the M.I.T. Laboratory for Computer 
Science. This research was supported by the Advanced Research P r o F u  Agency of the 
Department of Defense and was monitored by the Office of Naval Research under Contract 
Number N00014-75CMj61. 
ACL Meeting 1977 83 
Abstract submitted for presentation at The Association for Computational Linguistic8 
March 16-17, 1977, Georgetown University, Washington, D, C 
L. A. Miller 
Natural Language Programming : 
Kitchen Recipes 
Laboratory studies of computer programming by naive programmers 
indicated that, for formal programming languages, most behavioral e r r o r s  
are associated with specification of the transfer-of-control characteristics 
Subsequent studies revealed that it is this feature which most discriminates 
between formal computer programming and "natural language" programming: 
the former embeds the data-manipulation actions within a complex control 
structure whereas the latter emphasizes f irst  the action, followed by suhsequent 
qualifications. This ACTION-QUALIFICA T I 0  style i s  su strikipgly different 
from the CONTROL-ACTION style of programming computers that a study of 
natural language programming by professionals was initiated. The objective 
of the investigation is  to determine the mechanisms whereby process inforniatian 
is communicated and to assess the oft-asserted (but empirically untesied) 
I 1  imprecision" and "ambiguity" of natural language usage in procedural donlains 
Potentially, such an investigation could result in an alternative to formal 
programming languages for the linguistic man-machine interface --  e,  g., Natural 
Language Procedure Specification. 
We report on our progress tp date in the analysis of a corpus of recipes from 
'I'he JOY of Cooking. Our present understanding of the communication process in 
recipes is  that the imperative verb is  a call to some procedure which returns a 
case-frame into which are mapped the remaining object- group and verb-qualifier 
elements of the surface text. We present statistics concerning case frequencies, 
syntactic structures, and word usage, and we detail our approach for the automatic 
comprehension and symbolic modelling of the activities invblved in recipe 
execution (we are using Heidornts NLP LISP system). 
ACL MeePlng 1977 84 
J . p, Hobbe A~XO~MODATXNG TliE SIJATI ,\L bU:T1\1'1 l(1H 
~ i n ~ u i s t s  and psycho log i s t s  have f requcnt lp  noted t h a t  i n  
~ , , g l i s l ~  and o t h e r  languages one o f t e n  appeals t o  s p a t i a l  metaphors 
when speaking of a b s t r a c t  ideas  (9,1,3). For example, we speak of 
**lli & p  h 110 es1I, pr ices t l ,  "deep thought", "being p o l i t i c s u ,  
"a book pn sociology", l l s e t t l n g  t h e  idea", e t c .  Heretofore,  t h i s  
has been only an obscrvation.  Evcn Schank's work, w i t h  i t s  
dccompositions i n t o  YTTUWS, ATRANS, and MTKANS, is onlv suegcs t ive  
of an underlying un i ty ,  and Jackendoff I s  c l a s s i f i c a t i o n  of word 
senses i n t o  p o s i t i o n a l ,  possess iona l ,  idenf i f i c a t i a n a l ,  and 
c i rcumstan t i a l  modes remains only a c l a s s i  f i ca t inn .  This pal-er 
desc r ibes  an approach which a t i l i ~ e s  tire s p t i a l  metaphor i n  
cons t ruc t ing  econornical d e f i n i t i o n s  of H a l l - p r ~ r p o s e l  words t h d t  have 
previously de f i ed  p r e c i s e  specif  j ca t ion ,  and a method f o r  
i n t e r p r e t i n g  these  words i n  context  which t r e a t s  metdphor not  as  
an anomoly but a s  t h e  na tu ra l  s t a t e  of a f f a i r s .  
The basic idea is  t o  clef rne words i n  t c r n s  of very genera l  s p a t i a l  
p red ica tes  and then,  i n  t h e  a n a l y s i s  of a give2 t e x t ,  t o  ~ c e k  a more 
s p e c i f i c ,  context-dependent i n  t e r p r e t a t i n n ,  nr kinding,  j u s t  as 
a  compiler o r  i n t c a p r e t e r  seeks bindings f o r  t h e  v a r i a b l e s  and 
procedrire names mentioned i n  a  program. 
I n t e r p r e t a t i o n  a s  Binding: I n  yrograrn~r~infr l a n g u a ~ c s ,  there is  
normally a f ixed  means of ctetermihing bindinqs.  e i t h e r  by fa l lowing 
a chain of access modulcn (2: or by consu l t ing  an a - l i s t  o r  
PUNARG-frozen environment. 
Van E~nden & Kowalski (8) have presented unothcr outlook.  I n  a 
mechanical theorem proving sys te~u ,  they show h o w  Horn c l d r ~ s e s  
nay be viewed as procedure d e c l a r a t i o n s  i n  which t h e  p o s i t i v e  
l i t e r a l  i s  a procedure name, the  negat ive  l i t e r a l s  the  procedure 
ACL Meeting 1977 85 
body, and each n e g a t i v e  l i  t e r a l  a  c a l l  t o  another  procedure.  t i  set  
ef Horn c l a u s e s  is a n o n - d e t e r s i n i s t i c  program, non-de te rmin i s t i c  
because s e v e r a l  tforn c l a u s e s  may have t h e  same p o s i t i v e  l i t e r a l .  
That is, the  procedure name in  a  procedure c a l l  may be bound t o  one 
of s e v e r a l  d i f f e r m t  p r o c e d u r ~  bodies. Resolut ion is  an a t t empt  t o  
bind a procedure name i n  f i  way t h a t  l e a d s  t o  t h e  d e s i r e d  r e f u t a t i o n .  
Put i n  another  way, we may view the  i n f e r c n c e  ";\>Bt1 a s  s p e c i f y i n g  
A a s  a p o s s i b l e  b inding f o r  H. 
Montague (6,4) developed a  v a r i e t y  of i n t e n s i o n a l  l o g i c  
as a  represen ta t ion  f o r  n a t u r a l  language. I n  h i s  formalism, 
i n d i v i d u a l  words can be d e f i n e d  a s  f u n c r i ~ n s  expressed i n  terms of 
i n t e n s i o n s ,  i.e. v a r i a b l e s  and procedure names. S y n t s c t i c  r e l d t i o n s  
i n  English a r e  t r a n q l a t e d  i n t o  f u n c t i o n  a p p l i c a t i o n s  i n  
i n t e n s i o n a l  logic. These func t ion  a p p l i c a t i o n s  bind t h e  i n t e n s i o n s  
t o  s p e c i f i c  i n t e r p r e t a t i o n s .  In this way the  meanings ,of indivPdua1 
words a r e  composed i n t o  the  meaning of t h e  sentence. However, t h e  
binding mechanism is q u i t e  f i x t d ,  making the  f o r n ~ a l i s m  insuf  f  i c i e p  t l y  
f l e x i b l e  f o r  t h e  wliole range of n a t u r a l  language. 
Our approach combines Montagpets wi th  t h a t  of Van Emden & 
Kowalski. A s  i n  Mon taguc 's  approach, i n d i v i d u a l  words a r e  d e f i n e d  
i n  terms of genecal pre r l i ca tes  t h a t  may be  viewed as unbound 
p r e d i c a t e  namea, and. t h e i r  b ind ings  i n  a gjvcn t ex t  are determined 
From s y n t a c t i c a l l y  r e l a t e &  words. Ilowever, t h e  b ind ing  mechanism 
is  not f i x e d ,  but as  with Van Bmdcn & Kowalski, i t  is a s e a r c h  f o r  a 
chain  of in fe rence  which culnl inates  in  an express ion  invo lv ing  t h e  
gehera l  p red ica te .  An example is given belod. I n  a d d i t i o n ,  a 
dynamic o r d e r i n g  determined by con tex t  is imposed on the  axioms i n  
t h e  d a t a  base of l e x i c a l  and world k ~ i i i w l c d g e ,  d e f i n i n g  an o r d e r i n g  
on c h a i n s  of inference.  The, bind ing  is chosen which is given by t h e  
dWL Meeting 1977 86 
chain of inference Irighest in  t h i s  ordering. 
The Spa t i a l  Metaphor: A t  tile base of the ~ c x i c o n ,  o r  s e t  of 
axioms, are a m d l l  number of pr imit ive not ions w i t h  a highly 
s p a t i a l  o r  v i sua l  flavor.  Among these a re  * * ~ c a l e ~ ~  o r  a  p a r t i a l  
ordering defined by possible changes of s t a t e ,  the r e l a t i on  "onw 
which places poin ts  on the sca l e ,  and* %itt1 which among o ther  th ings  
r e l a t e s  an e n t i t y  t o  a  point  on a  scale .  Moreover, "at1' is re la ted  
t o  predic-ation: f o r  an e n t i t y  t o  be at a predicat ion is f o r  the 
e n t i t y  t o  be one of its arguments, a s  i l l u s t r a t e d  by the equivalence 
John is  hard a t  work 3 John is working hard. 
Concepts a t  higher l eve l s  of the Lexicon are  defined i l l  terms of 
these basic s p a t i a l  conccpts. For example, "to think of"' o r  "to 
have i n  mindt8 i s  defined as a va r i e ty  of "att1 Time i s  a sca le ,  and 
an event may be 5 a point  on t h a t  scale .  A s e t  may a l so  be though t  
of as a scale and i t s  elements a s  being points on the scale .  Kote 
tha t  t h i s  takes ser ious ly  tlre visual image one h a s  of a  s e t  as the 
elements spread out  before one. 
Final ly ,  " a l l - p ~ r p o s e ' ~  words such a s  t h e  common adverbs and 
preposi t ions arc defined in  terms of the bas ic  concepts l i k e  "scalett ,  
"ann, and glatw. In t h e  ana lys i s  of a t ex t ,  we f i nd  in t e rp re t a t ions  
f o r  these basic concepts by f inding chain8 of inference from 
proper t ies  of the arguments of the "411-purpose" war-ds to g,ropos i t i o n s  
involving the basic  concepts. 
Simplified Exanlple: Consider llJolin i s  i n  pol i t ics" .  S~r>pose 
atin" means to  be at a poikt on a ?talc. We nrust f ind  bindings fo r  t h e  
underlined words. P o l i t i c s  i s  a s e t  of a c t i v i t i e s  d i rec ted  toward 
the goal  of obtaining dnd using power i n  an ort;anization. r\ seta  is 
q q  a scale .  The typ ica l  a c t i v i t y  is on the Scale.  For John to  be 
a t  such an a c t i v i t y  is f o r  him t o  Ire one of the p a r t i t i p a n t s  i n  it. -
MX Meeting 1977 87 
I'I~US, f o r  John t o  be i n  p o l i t i c s  i s  f o r  h i m  t o  engage i n  the  
a c t i v i t i e s  t h a t  c l ~ a r a c t e r i z e  p o l i t i c s .  
Otliez examples i l l u s t r a t i r ~ g  the d iskinct ic-n  be.tween "in" and 
I1onft and tlie meaning of that elusive adverb "even" will be presented.  
Signif icance:  T h i s  work represen t s .  an arlvance i n  our  
understanding of how meanings of worcls am, composed i n t o  the  meanings 
of l a r g e r  s t r e t c h e s  of t e x t ,  and of the e f f e c t  of con tex t  on 
i n t e r p r e t a t i o n .  Moreover, i t  is the  r e s u l t  of a happy blend of 
computational o r  l o g i c a l  tcchnioue with l i n g u i s t i c  and psychological  
ins igh t s .  
Bibliography 
1. Aschi S.X. 'The Metaphor: A Psychological  Inqu i ryn  i n  dl Rcnley, 
cd. Docun~ents of Ciestal t Psychology, 1961. 
2. Bobrow, D. & B. We@reit. "A Model and Stack Implementation of 
M i l l  t iple  .Environmentsf1 CACM, Octohe r 1973, p. 591. 
3. Clark, Herbert  11. l1Space, Time, Semantics, and t h e  Child'' i n  
Cogni t ive  Develnpment and the Acqr~ i s i  t i o n  of Langilage. 2973. 
4. Hobbs, J. B S. Rosenschein. Making Computational Sense of 
Montague 's I n  tenssonal  Logic. Report No. NSO-11, Courant 
I n s t i t u t e  of Mathematical Sciences. December 1976. 
5. Jackendoff, Ray. "Toward an Expl  ana to ry  Semantic Representa t ionv 
L i n g u i s t i c  Inquiry ,  Winter 1976. p. 89. 
6. Mon tague,  Richard. "The Proper Treatment of Quan t i f i ca t ion  i n  
Ordinary-English11 i n  Approaches to Natural  Canguage, 1973. 
7. Schank, R., N. Goldhun, d. Rieger, & C. Riesbeck. IfInference 
and Paraphrase by Computer1' JACM, July 1975. P. 309. 
8. Van Emden, M.H. & R.A. Kowalski, 'The Semantics of P r e d i c a t e  
Logic a s  a Programming Language" JAW, October 1976. p. 733. 
9. Whorf, Ben jamin. 'The Rela t ion of Habi tual  Thought and Heliav i o r  
t o  Languagef1 i n  Language, Thought, k Reqlity, 2956. 
ACL Meetipg 1977 
I n f e r t i n q  a n  A n t e c e d a n t  
C o m v u t a t i o n a l  r e s e a r c h  o n  p r o n o m i n a l  a n a p h c r a  h a s  c e n t e r e d  
a r o u n d  t h e  p r o b l e m  o f  ' ' r e f e r e n c e  c e s o l u t i o n " ,  i.e. the p r o b l e m  of 
c h o o s i n q  t h e  c o r r e c t  a n t e c e d a n t  f o r  a n  a n a p h o r  jc e x p r e s s i o n  f rom 
a se t  of  s e v e r a l  p o s s i b l e  c a n d i d a  tes. B u t  r e f e r e n c e  r e s o l u t i o n ,  
t h o u q h  a complex process r a q n i r  i h q  t h e  i n  t a r a c t  i o n  of many 
s o u r c e s  of k n u w l e d q e ,  is really o n l y  h a l f  t h e  p r o b l e m .  The o t h e r  
h a l f  i n v o l v e s  a c t u a l 1  y  f i n d i n g  t h e  c a n d i d a t e s .  Tn c u r r e n t  n a t u r a l  
lanquaqe s y s t e m s ,  t h i s  h a l f  o f  t h e  ~ r o b l e m  h a s  teen handled i n  a 
r a t h e r  24 f a s h i o n  o r  h a s  been i q n 0 r e . l  e n t i r e l y .  I n  t h e s e  
s y s t e m s ,  the set off p o s s i b l e  a n t e c e d a n t s  f o r  a Fronoun is u s u a l l y  
culled o f f  a h i s t o r y  l ist o f  o b j e c t s  i n t r o d u c e d  e a r l i e r  i n  t h e  
d i s c o u r s e .  V a r i o u s  h e u r i s t i c s  inc1 u d i n g  r e c e n c y ,  s t r u c t u r a l  
c o n s t r a i n t s ,  s e m a n t i c  se lec t  i o n a l  r e s t r i c t i o n s ,  # n o v n  
h i g h e r -  l e v e l  t a s k  o r  d i s c o u r s e  o r q a n i z a t i o n ,  a n d  c a s e  a n d  n u m b e r  
a q r e e m e n t  a r e  t h e n  a p p l i e d ,  i n  o r d e r  t o  c h o o s e  t h e  b e s t - f i t t  ing 
c a n d i d a t e .  
On t h e  one h s n d ,  it h a s  l o n g  been r e c o a n i z e d  t h a t  i n f e r e n c e  
may be n e e d e d  t o  f i n d  p o s s i b l e  a n t e c e d a n t s  f o r  a d e f i n i t e  n o u n  
phrase, F o r  e x a m p l e ,  i n  
A l e e r i n q  face a p p e a r e d  at. Mary's window. 
Slie c a l l e r 1  t h e  p o l i c e  t o  =rest t h ~  Ean. 
at  l e a s t  o n e  i n f e r e n c e  r u l e  r e l a t i n q  man a n d  face i s  needed  t o  
f i q u r e  o u t  a possible a n t e c e d a n t  f o r  "the man". 
The p o i n t  T s h a l l  be a a k i n q  i n  t h i s  F a p e r  is t h a t  i n f e r e n c e  
may be r e q u i r e d  , t o  c o n j u r e  u p  p o s s i b l e  a n t e c e  E a n t s  f o r  p r o n o u n s  
as J e l l .  S x a m c l e s  l i k e  t h e  f o l l o w i n q  w i l l  b e  u s e d  t o  i l l u s t r a t e  
tbis p o i n t .  
I s a w  a m a r r i e d  c o u p l e  w a l k i n g  i n  t h e  p a r k .  
Be had on a w f u l  p l a i d  s h o r t s ,  a n d  s h e  h a d  on a d a s h i k i .  
r he = t h e  h u s b a n d ,  s h e  = t h e  w i f e 1  
J o h n  b l r n d e r l  some f l o u r  a n d  wa'ter a n d  used 4. t o  seal  
t h e  I i a  onf o t 1 1 9  p o t .  
r i t  = the f l o u r - u a t e r  m i x t u r e ' ]  
Mary q a v e  each q i r l  a T - s h i r t .  
T h e y  t -hanked h e r  f o r  them. 
r t h e m  = t h e  set of T - s h i r t s ,  each of r h i c h  Nary 
qave t o  some g i r l  1 
T s h a l l  show t h a t  a n y  p r o n o u n  r ~ s o l u t i o n  p r c c e d u r e ,  e v e n  o n e  
t h a t  uses h i q h l v  s o p h i s t i c a t e d  s y n t a c t i c ,  soman t i c  a n d  p r a q m a t i c  
chccks, c a n n a t  cpstrict. i t se l f  t o  c o n s i i c r i n q  o n l y  o h  j ec ts  a n d  
events  q i v e n  ~ x p l i c i t l y  i n  the t 9 x t .  I n  a d d i t i o r  I s h a l l  s h o v  how 
t h e  needed a n t s c e d a n t g  c a n  be  i n f e r r e d ,  c s i n g  a f o r m a l  
E e p r e s a n t s t i  or. l a n s u a s e  f o r  E n q l i s h  a e s c r i b c - d  e lsewharc. 
