Efficient Construction of Underspecified Semantics under 
Massive Ambiguity 
J ochen  D5rre* 
Institut fiir maschinelle Sprachverarbeitung 
University of Stuttgart 
Abst ract  
We investigate the problem of determin- 
ing a compact underspecified semantical 
representation for sentences that may be 
highly ambiguous. Due to combinatorial 
explosion, the naive method of building se- 
mantics for the different syntactic readings 
independently is prohibitive. We present 
a method that takes as input a syntac- 
tic parse forest with associated constraint- 
based semantic onstruction rules and di- 
rectly builds a packed semantic structure. 
The algorithm is fully implemented and 
runs in O(n4log(n)) in sentence length, if 
the grammar meets some reasonable 'nor- 
mality' restrictions. 
1 Background 
One of the most central problems that any NL sys- 
tem must face is the ubiquitous phenomenon of am- 
biguity. In the last few years a whole new branch de- 
veloped in semantics that investigates underspecified 
semantic representations i  order to cope with this 
phenomenon. Such representations do not stand for 
the real or intended meaning of sentences, but rather 
for the possible options of interpretation. Quanti- 
fier scope ambiguities are a semantic variety of am- 
biguity that is handled especially well by this ap- 
proach. Pioneering work in that direction has been 
(Alshawi 92) and (Reyle 93). 
More recently there has been growing interest in 
developing the underspecification approach to also 
cover syntactic ambiguities (cf. (Pinkal 95; EggLe- 
beth 95; Schiehlen 96)). Schiehlen's approach is out- 
standing in that he fully takes into account syntactic 
*This research as been carried out While the au- 
thor visited the Programming Systems Lab of Prof. 
Gert Smolka t the University of Saarland, Saarbriicken. 
Thanks to John Maxwell, Martin Miiller, Joachim 
Niehren, Michael Schiehlen, and an anonymous reviewer 
for valuable feedback and to all at PS Lab for their help- 
ful support with the OZ system. 
constraints. In (Schiehlen 96) he presents an algo- 
rithm which directly constructs a single underspec- 
ified semantic structure from the ideal "underspeci- 
fled" syntactic structure, a parse forest. 
On the other hand, a method for producing 
"packed semantic structures", in that case "packed 
quasi-logical forms", has already been used in the 
Core Language Engine, informally described in (A1- 
shawi 92, Chap. 7). However, this method only pro- 
duces a structure that is virtually isomorphic to 
the parse forest, since it simply replaces parse for- 
est nodes by their corresponding semantic oper- 
ators. No attempt is made to actually apply se- 
mantic operators in the phase where those "packed 
QLFs" are constructed. Moreover, the packing of 
the QLFs seems to serve no purpose in the process- 
ing phases following semantic analysis. Already the 
immediately succeeding phase "sortal filtering" re- 
quires QLFs to be unpacked, i.e. enumerated. 
Contrary to the CLE method, Schiehlen's method 
actively packs semantic structures, even when they 
result from distinct syntactic structures, extracting 
common parts. His method, however, may take time 
exponential w.r.t, sentence length. Already the se- 
mantic representations it produces can be exponen- 
tially large, because they grow linear with the num- 
ber of (syntactic) readings and that can be exponen- 
tial, e.g., for sentences that exhibit the well-known 
attachment ambiguity of prepositional phrases. It is 
therefore an interesting question to ask, whether we 
can compute compact semantic representations from 
parse forests without falling prey to exponential ex- 
plosion. 
The purpose of the present paper is to show that 
construction of compact semantic representations 
like in Schiehlen's approach from parse forests is not 
only possible, but also cheap, i.e., can be done in 
polynomial time. 
To illustrate our method we use a simple DCG 
grammar for PP-attachment .ambiguities, adapted 
from (Schiehlen 96), that yields semantic represen- 
tations (called UDI~Ss) according to the Underspec- 
ified Discourse Representation Theory (Reyle 93; 
KampReyle 93). The grammar is shown in Fig. 1. 
386 
start(DRS) --> s(\[ .... itop\], \[\],DRS). 
s(\[Evenc,VerbL,DomL\],DRS i,PRS_o) --> 
np(\[X,VerbL,DomL\],DRS_i,DRSI), 
vp(\[Event,X,VerbL,DomL\],DRS1,DRS_o). 
s(\[Event,VerbL,DomL\],DRS_i,DRS_o) --> 
s(\[Event,VerbL, DomL\],DRSi,DRSl) ,  
pp(\[Event,VerbL,DomL\],DRSi,DRSo).  
vp(\[Ev,X,VerbL,DomL\],DRS_i ,DRSo) --> 
vt(\[Ev,X,Y,VerbL,DomL\],DRS_i,DRSI),  
np(\[Y,VerbL,DomL\],DRSI,DRS_O). 
np(\[X, VbL,DomL\],DRS i,DRS_o) --> 
det(\[X,Nou~,L,VbL,DomL\],DRS_i,DRSI), 
n(\[X,NounL,DomL\],DRSI,DRSo).  
n(\[X,NounL,DomL\],DRS i,DRS_o) --> 
n(\[X,NounL,DomL\],DRS i,DRSI), 
pp(\[X,NounL,DomL\],DRSI,DRS_o). 
pp(\[X,L,DomL\],DRS_i,DRS o) --> 
prep(Cond,X,Y), 
np(\[Y,L,DomL\], \[L:CondlDRS i\],DRS o). 
vt(\[Ev, X,Y,L,_DomL\],DRS_i,DRS) --> \[saw\], 
\[DRS=\[L:see(Ev,X,Y) IDRS_i\]}. 
det(\[X,Lab,VerbL,_\] ,DRS i,DRS) --> 
\[a\], 
\[DRS=\[It(Lab, ltop),It(VerbL,Lab), 
Lab:XIDRS_i\], 
gensym(l,Lab),gensym(x,X)}. 
det(\[X,ResL;VbL,DomL\],DRSi,DRS) --> 
\[ every  \ ] ,  
(DRS=\[lt(L,DomL), lt(VbL,ScpL),ResL:X, 
L:every(ResL,ScpL) IDRS_i\], 
gensym(l,L),gensym(l,ResL), 
gensym(l,ScpL),gensym(x,X)}. 
np(\[X .... \],DRS_i,DRS) --> \[i\], 
\[DRS=\[itop:X,anchor(X, speaker) IDRS_i\], 
gensyrn(x,X)}. 
n(\[X,L,_\],DRS, \[L:man(X) IDRS\]) --> \[man\]. 
n(\[X,L,_\],DRS, \[L:hilI(X) IDRS\]) --> \[hill\]. 
prep(on(X,Y),X,Y) --> \[on\]. 
prep(with(X,Y),X,Y) --> \[with\]. 
Figure h Example DCG 
The UDRSs constructed by the grammar are flat 
lists of the UDRS-constraints I <__ l' (subordination 
(partial) ordering between labels; Prolog represen- 
tation: i t  ( l , l ' ) ) ,  l : Cond (condition introduction 
in subUDRS labeled l), I : X (referent introduc- 
tion in l), l : GenQuant( l ' , l ")  (generalised quan- 
tifier) and an anchoring function. The meaning of a 
UDKS as a set of denoted DRSs can be explained 
as follows. 1 All conditions with the same label form 
a subUDRS and labels occurring in subUDRSs de- 
note locations (holes) where other subUDRSs can 
be plugged into. The whole UDRS denotes the set 
of well-formed DRSs that can be formed by some 
plugging of the subUDRSs that does not violate the 
ordering <. Scope of quantifiers can be underspec- 
ified in UDRSs, because subordination can be left 
partial. 
In our example grammar every nonterminal has 
three arguments. The 2nd and the 3rd argument rep- 
resent a UDRS list as a difference list, i.e., the UDRS 
is "threaded through". The first argument is a list of 
objects occurring in the UDRS that play a specific 
role in syntactic ombinations of the current node. 2 
An example of a UDRS, however a packed UDRS, is 
shown later on in ?5. 
To avoid the dependence on a particular grammar 
formalism we present our method for a constraint- 
based grammar abstractly from the actual constraint 
1Readers unfamiliar with DRT should think of these 
structures as some Prolog terms, representing semantics, 
built by unifications according to the semantic rules. It is 
only important o notice how we extract common parts 
of those structures, irrespective of the structures' mean- 
ings. 
~E.g., for an NP its referent, as well as the upper and 
lower label for the current clause and the top label. 
system employed. We only require that semantic 
rules relate the semantic 'objects' or structures that 
are associated with the nodes of a local tree by em- 
ploying constraints. E.g., we can view the DCG rule 
s ~ np vp as a relation between three 'seman- 
tic construction terms' or variables SereS, SemNP, 
SemVP equivalent to the constraints 
Seres = \[ \[Event, VerbL,DomL, TopL\] , DRS_i, DRS_o\] 
SemNP = \[\[X,VerbL,DomL,TopL\] ,DRS_i,DRSI\] 
SemVP = \[ \[Event, X, VerbL, DomL, TopL\] , DRS 1, DRS_o\] 
Here is an overview of the paper. ?2 gives the pre- 
liminaries and assumptions needed to precisely state 
the problem we want to solve. ?3 presents the ab- 
stract algorithm. Complexity considerations follow 
in ?4. Finally, we consider implementation issues, 
present results of an experiment in ?5, and close with 
a discussion. 
2 The  Prob lem 
As mentioned already, we aim at calculating from 
given parse forests the same compact semantic struc- 
tures that have been proposed by (Schiehlen 96), 
i.e. structures that make explicit the common parts 
of different syntactic readings, so that subsequent 
semantic processes can use this generalised infor- 
mation. As he does, we assume a constraint-based 
grammar, e.g. a DCG (PereiraWarren 80) or HPSG 
(PollardSag 94) , in which syntactic onstraints and 
constraints that determine a resulting semantic rep- 
resentation can be seperated and parsing can be per- 
formed using the syntactic onstraints only. 
Second, we assume that the set of syntax trees 
can be compactly represented as a parse forest 
(cf. (Earley 70; BillotLang 89; Tomita 86)). Parse 
forests are rooted labeled directed acyclic graphs 
with AND-nodes (standing for context-free branch- 
387 
s s s 
np 
5 
n n 
p / \ / / 
np np 
12 / ,3 
PP 
18 
16 
PP 
19 
np v d n p d n p 
23 24 25 26 27 28 29 30 
np 
22 
d n 
31 32 
I saw a man on the hill with the tele 
Figure 2: Example of a parse forest 
ing) and OR-nodes (standing for alternative sub- 
trees), that call be characterised asfollows (cf. Fig. 2 
for an example).3 
1. The terminal yield as well as the label of two 
AND-nodes are identical, if and only if they 
both are children of one OR-node. 
2. Every tree reading is .a valid parse tree. 
Tree readings of such graphs are obtained by replac- 
ing any OR-node by one of its children. Parse forests 
can represent an exponential number of phrase 
structure alternatives in o(n 3) space, where n is the 
length of the sentence. The example uses the 3 OR- 
nodes (A, B, C) and the AND-nodes 1 through 32 
to represent 5 complete parse trees, that would use 
5 x 19 nodes. 
Third, we assume the rule-to-rule hypothesis, i.e., 
3The graphical representation f an OR-node is a box 
surroux~ding its children, i.e. the AND-OR-graph struc- 
ture of ~ is o~.  
ND 
that the grammar associates with each local tree a 
'semantic rule' that specifies how to construct he 
mother node's semantics from those of its children. 
Hence, input to the algorithm is 
? a parse forest 
? an associated semantic rule for every local tree 
(AND-node together with its children) therein 
? and a semantic representation for each leaf 
(coming from a semantic lexicon). 
To be more precise, we assume a constraint lan- 
guage C over a denumerable set of variables X, 
that is a sublanguage of Predicate Logic with equal- 
ity and is closed under conjunction, disjunction, 
and variable renaming. Small greek letters ?, ? will 
henceforth denote constraints (open formulae) and 
letters X, Y, Z (possibly with indeces) will denote 
variables. Writing ?(X1, . . . ,  Xk) shall indicate that 
X1 . . . .  , Xk are the free variables in the constraint ~. 
Frequently used examples for constraint languages 
are the language of equations over first-order terms 
388 
for DCGs, 4 PATR-style feature-path equations, or 
typed feature structure description languages (like 
the constraint languages of ALE (Carpenter 92) or 
CUF (D6rreDorna 93)) for HPSG-style grammars. 
Together with the constraint language we require 
a constraint solver, that checks constraints for satis- 
fiability, usually by transforming them into a normal 
form (also called 'solved form'). Constraint solving 
in the DCG case is simply unification of terms. 
The semantic representations mentioned before 
are actually not given directly, but rather as a con- 
straint on some variable, thus allowing for partiality 
in the structural description. To that end we assume 
that every node in the parse forest u has associated 
with it a variable Xv that is used for constraining the 
(partial) semantic structure of u. The semantics of 
a leaf node # is hence given as a constraint ? , (X , ) ,  
called a leaf constraint. 
A final assumption that we adopt concerns the na- 
ture of the 'semantic rules'. The process of semantics 
construction shall be a completely monotonous pro- 
cess of gathering constraints that never leads to fail- 
ure. We assume that any associated (instantiated) 
semantic rule r(u) of a local tree (AND-branching) 
u(ul, . . . ,u~) determines u's semantics Z(u) as fol- 
lows from those of its children: 
Z(,,) = 3X,, , . . .  3X~, (?~(,,)(X,,, X, , , , . . . ,  X,,,) A 
Z(I."I) A . . .  A E(Uk) ). 
The constraint Cr(v)(Xv, Xv l , . . . ,  X~)  is called the 
rule constraint for ~,. It is required to only depend 
on the variables X~, X~I, . . . ,  X~,. Note that if the 
same rule is to be applied at another node, we have 
a different rule constraint. 
Note that any F,(~,) depends only on Xv and can 
be thought of as a unary predicate. Now, let us con- 
sider semantics construction for a single parse tree 
for the moment. The leaf constraints together with 
the rules define a semantics constraint Z(~,) for ev- 
ery node u, and the semantics of the full sentence 
is described by the T-constraint of the root node, 
~,(root). In the T-constraints, we actually can sup- 
press the existential quantifiers by adopting the con- 
vention that any variable other than the one of the 
current node is implicitly existentially bound on the 
formula toplevel. Name conflicts, that would force 
variable renaming, cannot occur. Therefore ~(root) 
is (equivalent to) just a big conjunction of all rule 
constraints for the inner nodes and all leaf con- 
straints. 
Moving to parse forests, the semantics of an OR- 
node u(~,l,..., uk) is to be defined as 
z(,.,) = 3x~, . . .  3x,~(z( , ,~)  ^  x~=x~,  v . . .  
v z(~k) ^  x~=x~), 
4DCG shall refer in this paper to a logically pure ver- 
sion, Definite Clause Grammars based on pure PROLOC, 
involving no nonlogical devices like Cut, var/1, etc. 
specifying that the set of possible (partial) semantic 
representations foru is the union of those of u's chil- 
dren. However, we can simplify this formula once and 
for all by assuming that for every OR-node there is 
only one variable Xu that is associated with it and all 
of its children. Using the same variable for ul ... uk 
is unproblematic, because no two of these nodes can 
ever occur in a tree reading. Hence, the definition we 
get is 
~"\](IJ) : Z(I\]I) V . . .  V Z(lYk). 
Now, in the same way as in the single-tree case, we 
can directly "read off" the T-constraint for the whole 
parse forest representing the semantics of all read- 
ings. Although this constraint is only half the way 
to the packed semantic representation we are aim- 
ing at, it is nevertheless worthwhile to consider its 
structure a little more closely. Fig. 3 shows the struc- 
ture of the F,-constraint for the OR-node B in the 
example parse forest. 
In a way the structure of this constraint directly 
mirrors the structure of the parse forest. However, 
by writing out the constraint, we loose the sharings 
present in the forest. A subformula coming from a 
shared subtree (as Z(18) in Fig. 3) has to be stated 
as many times as the subtree appears in an unfolding 
of the forest graph. In our PP-attachment example 
the blowup caused by this is in fact exponential. 
On the other hand, looking at a T-constraint as a 
piece of syntax, we can represent this piece of syntax 
in the same manner in which trees are represented in 
the parse forest, i.e. we can have a representation f 
Z(root) with a structure isomorphic to the forest's 
graph structure, s In practice this difference becomes 
a question of whether we have full control over the 
representations the constraint solver employs (or any 
other process that receives this constraint as input). 
If not, we cannot contend ourselves with the possi- 
bility of compact representation f constraints, but 
rather need a means to enforce this compactness on 
the constraint level. This means that we have to in- 
troduce some form of functional abstraction i to the 
constraint language (or anything equivalent that al- 
lows giving names to complex constraints and refer- 
encing to them via their names). Therefore we en- 
hance the constraint language as follows. We allow 
to our disposition a second set of variables, called 
names, and two special forms of constraints 
1. def(<name>, <constraint>) 
name definition 
2. <name> name use 
with the requirements, that a name may only be 
used, if it is defined and that its definition is unique. 
Thus, the constraint Z(B) above can be written as 
(?r(6) A. . .  A ?26 A N 
V ?~(7) A . . .  A ?26 A N ) 
A def(N, ?r(18) A?27 A ?r(21) A?2S A?29) 
5The packed QLFs in the Core Language Engine (A1- 
shawl 92) are an example of such a representation. 
389 
6r(6) A 623 A d)r(10) A 024 A 6r(12i A (~25 A Cr(lS) A 626 A ~r(IS} A ~27 A (~r(21) A628 A ~29 
. z( s) 
~(6) 
v 
6r(7) A ~r(14) A 623 A Or(17) A 624 A 6r(20) A 625 A 626/~ ~r(18) A ~27 A 6r(21) A 628 A 629 
E(181 
Figure 3: Constraint E(B) of example parse forest 
The packed semantic representation as con- 
structed by the method described so far still calls 
for an obvious improvement. Very often the dif- 
ferent branches of disjunctions contain constraints 
that have large parts in common. However, although 
these overlaps are efficiently handled on the rep- 
resentational level, they are invisible at the logical 
level. Hence, what we need is an algorithm that fac- 
tores out common parts of the constraints on the 
logical level, pushing disjunctions down. 6 There are 
two routes that we can take to do this efficiently. 
In the first we consider only the structure of the 
parse forest, however ignore the content of (rule or 
leaf) constraints. I.e. we explore the fact that the 
parts of the E-constraints in a disjunction that stem 
from nodes shared by all disjuncts must be identical, 
and hence can be factored out /  More precisely, we 
can compute for every node v the set must-occur(v) 
of nodes (transitively) dominated by v that must oc- 
cur in a tree of the forest, whenever u occurs. We can 
then use this information, when building the disjunc- 
tion E(u) to factor out the constraints introduced 
by nodes in must-occur(v),  i.e., we build the fac- 
tor ? = Av'emust-occur(v) Z(u') and a 'remainder' 
constraint E(ui)\~ for each disjunct. 
The other route goes one step further and takes 
into account he content of rule and leaf constraints. 
For it we need an operation genera l i se  that can be 
characterised informally as follows. 
For two satisfiable constraints ? and ~, 
genera l i se (? ,  !b) yields the triple ~, ?', ~3', 
such that ~ contains the 'common part' of 
? and 19 and ?' represents the 'remainder' 
6\~ and likewise 19' represents 19\~. 
6Actually, in the E(B) example such a factoring 
makes the use of the name N superfluous. In general, 
however, use of names is actually necessary to avoid ex- 
ponentially large constraints. Subtrees may be shared 
by quite different parts of the structure, not only by dis- 
juncts of the same disjunction. In the PP-attachment ex- 
ample, a compression of the E-constraint to polynomial 
size cannot be achieved with factoring alone. 
7(Maxwell IIIKaplan 93) exploit the same idea for 
efficiently solving the functional constraints hat an LFG 
grammar associates with a parse forest. 
The exact definition of what the 'common part' or 
the 'remainder' shall be, naturally depends on the 
actual constraint system chosen. For our purpose it 
is sufficient o require the following properties: 
If genera l i se (~.  19) ~-~ (~, ~', ~b'), then ~ I- 
andOf -~ando=~A?'and~b-=~A~b' .  
We shall call such a generalisation operation sim- 
plifying if the normal form of ~ is not larger than 
any of the input constraints' normal form. 
Example :  An example for such a generalisa- 
tion operation for PROLOG'S constraint system 
(equations over first-order terms) is the so-called 
anti-unify operation, the dual of unification, that 
some PROLOG implementations provide as a library 
predicate, s Two terms T1 and T2 'anti-unify' to T, 
iff T is the (unique) most specific term that sub- 
sumes both T1 and T2. The 'remainder constraints' 
in this case are the residual substitutions al and a2 
that transform T into T1 or T2, respectively. 
Let us now state the method informally. We use 
genera l i se  to factor out the common parts of dis- 
junctions. This is, however, not as trivial as it might 
appear at first sight. Genera l i se  should operate 
on solved forms, but when we try to eliminate the 
names introduced for subtree constraints in order 
to solve the corresponding constraints, we end up 
with constraints that are exponential in size. In the 
following section we describe an algorithm that cir- 
cumvents this problem. 
3 The  A lgor i thm 
We call an order < on the nodes of a directed 
acyclic graph G = (N, E) with nodes N and edges E 
bottom-up, iff whenever (i, j) E E ("i is a predecessor 
to j") ,  then j < i. 
For the sake of simplicity let us assume that 
any nonterminal node in the parse forest is binary 
branching. Furthermore, we leave implicit, when 
conjunctions of constraints are normalised by the 
constraint solver. Recall that for the generalisation 
operation it is usually meaningful to operate on 
Santi_unify in Quintus Prolog , term_subsumer in 
Sicstus Prolog. 
390 
Input :  ? parse'forest, leaf and rule constraints as described above 
? array of variables X~ indexed by node s.t. if v is a child of OR-node v', then Xv = Xv, 
Data  st ructures:  ? an array SEM of constraints and an array D of names, both indexed by node 
? a stack ENV of def constraints 
Output :  a constraint representing a packed semantic representation 
Method:  ENV := nil 
process nodes in a bottom-up order 
doing with node u: 
if u is a leaf then 
SEM\[v\] := ?, 
D\[v\] : :  t rue  
eiseif v is AND(v1, v2) then 
SEIVlIv\] := Cr(,) A SEM\[vl\] A SEM\[v2\] 
if D\[vl\] = t rue  then D\[v\] := D\[u2\] 
elseif Dive\] = t rue  then D\[v\] := D\[vl\] 
else D\[v\] := newname 
push def(D\[v\], D\[vl\] A D\[v2\]) onto ENV 
end 
elseif v is OR(v1, v2) then 
let GEN, REM1, REM2 such that 
generalise(SEM\[vl\], SEM\[v2\]) ~-+ (GEN, REM1, REM2) 
SEM\[v\] := GEN 
D\[v\] := newname 
push def(D\[v\], REM1 A D\[vl \]  V REM2 A D\[v2\]) onto ENV 
end return SEM\[root\]  A D\[root\] A ENV 
Figure 4: Packed Semantics Construction Algorithm 
solved forms. However, at least the simplifications 
t rue  A ? -- ? and ? A t rue =-- ? should be assumed. 
The Packed Semantics Construction Algorithm is 
given in Fig. 4. It enforces the following invariants, 
which can easily be shown by induction. 
1. Every name used has a unique definition. 
2. For any node v we have the equivalence ~(v) - 
SEM\[u\] A \[D\[v\]\], where \[D\[u\]\] shall denote the 
constraint obtained from D\[v\] when recursively 
replacing names by the constraints they are 
bound to in ENV. 
3. For any node u the constraint SEM\[v\] is never 
larger than the ~-constraint of any single tree 
in the forest originating in u. 
Hence. the returned constraint correctly represents 
the semantic representation for all readings. 
4 Complex i ty  
The complexity of this abstract algorithm depends 
primarily on the actual constraint system and gen- 
eralisation operation employed. But note also that 
the influence of the actual semantic operations pre- 
scribed by the grammar can be vast, even for the 
simplest constraint systems. E.g., we can write a 
DCGs that produce abnormal large "semantic struc- 
tures" of sizes growing exponentially with sentence 
length (for a single reading). For meaningful gram- 
mars we expect his size function to be linear. There- 
fore, let us abstract away this size by employing 
a function fa(n) that bounds the size of semantic 
structures (respectively the size of its describing con- 
straint system in normal form) that grammar G as- 
signs to sentences of length n. 
Finally, we want to assume that generalisation is 
simplifying and can be performed within a bound of 
g(m) steps, where m is the total size of the input 
constraint systems. 
With these assumptions in place, the time com- 
plexity for the algorithm can be estimated to be (n 
= sentence length, N = number of forest nodes) 
O(g( fc (n)  ) " N) <_ O(g( fa(n)  ) . n3), 
since every program step other than the generali- 
sation operation can be done in constant ime per 
node. Observe that because of Invariant 3. the input 
constraints to generalise are bounded by fc as any 
constraint in SEM. 
In the case of a DCG the generalisation oper- 
ation is ant i_uni fy ,  which can be performed in 
o(n. log(n)) time and space (for acyclic s t ruc -  
tures). Hence, together with the assumption that 
the semantic structures the DCG computes can 
be bounded linearly in sentence length (and are 
acyclic), we obtain a O(n. log(n). N) < O(n41og(n)) 
total time complexity. 
391 
SEM\[top\]: 
\[itop : xl, 
anchor(xl,'Speaker') 
ii : see(el ,x l ,x2) ,  
i t (12,1top),  
i t( l l ,12),  
12 : x2, 
12 : man(x2), 
A : on(B,x3), 
i t (13,1top),  
It(A,15), 
14 : x3, 
13 : every(14,15),  
14 : hi l l (x3),  
C : with(D,x4),  
i t (16,1top),  
it(C,16), 
16 : x4, 
16 : tele(x4)\] 
D\[top\] (a Prolog goal): 
dEnv(509,1,\[B,A,D,C\]) 
ENV (as Prolog predicates): 
deny(506, i, A) 
( A=\[e{, l l \ ]  
i A= \[x2, 12 \] 
dEnv(339, i, A) :- 
( A= \[C,B,C,B\] 
; A= Ix3,14 ... .  \] 
) 
dEnv(509, 2, A) :- 
( A= \[el, l l ,x3,14\] 
; A= Ix2,12,C,B\], 
dEny(339, I, \ [C,B,x2,12\])  
) 
dEnv(509, i, A) :- 
( A=\[G,F ,e I , I I \ ] ,  
deny(506, i, \[G,F\]) 
A= \[E,D,C,B\] , 
dEny(509, 2, \[E,D,C,B\]) 
Figure 5: Packed UDRS: conjunctive part (left column) and disjunctive binding environment 
5 Imp lementat ion  and  Exper imenta l  
Resu l ts  
The algorithm has been implemented for the PRO- 
LOG (or DCG) constraint system, i.e., constraints 
are equations over first-order terms. Two implemen- 
tations have been done. One in the concurrent con- 
straint language OZ (SmolkaTreinen 96) and one in 
Sicstus Prolog. 9 The following results relate to the 
Prolog implementation, l? 
Fig. 5 shows the resulting packed UDRS for the 
example forest in Fig. 2. Fig. 6 displays the SEM 
part as a graph. The disjunctive binding environ- 
ment only encodes what the variable referents B and 
D (in conjunction with the corresponding labels A 
and C) may be bound to to: one of el, x2, or x3 (and 
likewise the corresponding label). Executing the goal 
deny (509,1, \[B, A, D, C\] ) yields the five solutions: 
A = i i, B = e l ,  C = i i ,  D = e l  ? ; 
A = 12,  B = x2,  C = i i ,  D = e l  ? ; 
A = 11,  B = el ,  C = 14,  D = x3  ? ; 
A = 12,  B = x2.  C = 12,  D = x2  ? ; 
A = 12,  B = x2,  C = 14,  D = x3  ? ; 
no 
I ?- 
Table 1 gives execution times used for semantics 
construction of sentences of the form I saw a man 
(on a hill) n for different n. The machine used for 
?The OZ implementation has the advantage that fea- 
ture structure constraint solving is built-in. Our imple- 
mentation actually represents he DCG terms as a fea- 
ture structures. Unfortunately it is an order of magni- 
tude slower than the Prolog version. The reason for this 
presumably ies in the fact that meta-logical operations 
the algorithm needs, like generalise and copy_term 
have been modeled in OZ and not on the logical level 
were they properly belong, namely the constraint solver. 
1?This implementation is available from 
http://www.ims.uni-stuttgart.de/'jochen/CBSem. 
12 ~ 
x2 14 15 I 
man(x2) I x3  
' \[ hill?x3) x~ I 
11 ' A / 
I see(el'x l'x2) l ?n(B'x3) J 
ltop 
anchor(x 1 ,' Speaker') 
I. 
C 
I with(D,x4) 
Figure 6: Conjunctive part of UDRS, graphically 
n 
2 
4 
6 
8 
10 
12 
14 
16 
Readings 
5 35 
42 91 
429 183 
4862 319 
58786 507 
742900 755 
9694845 1071 
129Mio. 1463 
AND- + OR-nodes Time 
4 msec 
16 msec 
48 msec 
l l4  msec 
220 msec 
430 msec 
730 msec 
i140 msec 
Table 1: Execution times 
the experiment was a Sun Ultra-2 (168MHz), run- 
ning Sicstus 3.0~3. In a further experiment an n-ary 
ant i_uni fy  operation was implemented, which im- 
proved execution times for the larger sentences, e.g., 
the 16 PP sentence took 750 msec. These results ap- 
proximately fit the expectations from the theoretical 
complexity bound. 
392 
6 D iscuss ion  
Our algorithm and its implementation show that it 
is not only possible in theory, but also feasible in 
practice to construct packed semantical representa- 
tions directly from parse forests for sentence that ex- 
hibit massive syntactic ambiguity. The algorithm is 
both in asymptotic omplexity and in real numbers 
dramatically faster than an earlier approach, that 
also tries to provide an underspecified semantics for 
syntactic ambiguities. The algorithm has been pre- 
sented abstractly from the actual constraint system 
and can be 2dapted to any constraint-based gram- 
mar formalism. 
A critical assumption for the method has been 
that semantic rules never fail, i.e., no search is in- 
volved in semantics construction. This is required 
to guarantee that the resulting constraint is a kind 
of 'solved form' actually representing so-to-speak the 
free combination of choices it contains. Nevertheless, 
our method (modulo small changes to handle failure) 
may still prove useful, when this restriction is not 
fulfilled, since it focuses on computing the common 
information of disjunctive branches. The conjunctive 
part of the output constraint of the algorithm can 
then be seen as an approximation of the actual re- 
sult, if the output constraint is satisfiable. Moreover, 
the disjunctive parts are reduced, so that a subse- 
quent full-fledged search will have considerably less 
work than when directly trying to solve the original 
constraint system. 
References  
H. Alshawi (Ed.). The Core Language Engine. 
ACL-MIT Press Series in Natural Languages Pro- 
cessing. MIT Press, Cambridge, Mass., 1992. 
S. Billot and B. Lang. The Structure of Shared 
Forests in Ambiguous Parsing. In Proceedings of 
the 27th Annual Meeting of the A CL, University of 
British Columbia, pp. 143-151, Vancouver, B.C., 
Canada, 1989. 
B. Carpenter. ALE: The Attribute Logic Engine 
User's Guide. Laboratory for Computational Lin- 
guistics, Philosophy Department, Carnegie Mellon 
University, Pittsburgh PA 15213, December 1992. 
J. DSrre and M. Dorna. CUF - -  A Formalism for 
Linguistic Knowledge Representation. I  J. DSrre 
(Ed.), Computational Aspects of Constraint-Based 
Linguistic Description I, DYANA-2 deliverable 
R1.2.A. ESPRIT, Basic Research Project 6852, 
July 1993. 
J. Earley. An Efficient Context-Free Parsing Algo- 
rithm. Communications of the ACM, 13(2):94- 
102, 1970. 
M. Egg and K. Lebeth. Semantic Underspeci- 
fication and Modifier Attachment Ambiguities. 
In J. Kilbury and R. Wiese (Eds.), Integra- 
tive Ansiitze in der Computerlinguistik. Beitriige 
zur 5. Fachtagung der Sektion Computerlinguis- 
tik der Deutschen Gesellschaft fiir Spraehwis- 
senschaft (DGfS), pp. 19-24. Dfisseldorf, Ger- 
many, 1995. 
H. Kamp and U. Reyle. From Discourse to Logic. In- 
troduction to Modeltheoretic Semantics of Natural 
Language, Formal Logic and Discourse Represen- 
tation Theory. Studies in Linguistics and Philoso- 
phy 42. Kluwer Academic Publishers, Dordrecht, 
The Netherlands, 1993. 
J. T. Maxwell III and R. M. Kaplan. The Inter- 
face between Phrasal and Functional Constraints. 
Computational Linguistics, 19(4):571-590, 1993. 
F. C. Pereira and D. H. Warren. Definite Clause 
Grammars for Language Analysis--A Survey of 
the Formalism and a Comparison with Aug- 
mented Transition Networks. Artificial Intelli- 
gence, 13:231-278, 1980. 
M. Pinkai. Radical Underspecification. In Pro- 
ceedings of the lOth Amsterdam Colloquium, pp. 
587-606, Amsterdam, Holland, December 1995. 
ILLC/Department of Philosophy, University of 
Amsterdam. 
C. Pollard and I. A. Sag. Head Driven Phrase 
Structure Grammar. University of Chicago Press, 
Chicago, 1994. 
U. Reyle. Dealing with Ambiguities by Underspecifi- 
cation: Construction, Representation, and Deduc- 
tion. Journal of Semantics, 10(2):123-179, 1993. 
M. Schiehlen. Semantic Construction from Parse 
Forests. In Proceedings of the 16th International 
Conference on Computational Linguistics, Copen- 
hagen, Denmark, 1996. 
G. Smolka and R. Treinen (Eds.). DFKI Oz Doc- 
umentation Series. German Research Center 
for Artificial Intelligence (DFKI), Stuhlsatzen- 
hausweg 3, D-66123 Saarbriicken, Germany, 1996. 
http://www.ps.uni-sb.de/oz. 
M. Tomita. Efficient Parsing for Natural Languages. 
Kluwer Academic Publishers, Boston, 1986. 
393 
