Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 487?495,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Collocation Extraction Using Monolingual Word Alignment Method 
 
Zhanyi Liu1,2, Haifeng Wang2, Hua Wu2, Sheng Li1 
1Harbin Institute of Technology, Harbin, China 
2Toshiba (China) Research and Development Center, Beijing, China 
{liuzhanyi,wanghaifeng,wuhua}@rdc.toshiba.com.cn 
lisheng@hit.edu.cn 
 
  
 
Abstract 
Statistical bilingual word alignment has been 
well studied in the context of machine trans-
lation. This paper adapts the bilingual word 
alignment algorithm to monolingual scenario 
to extract collocations from monolingual cor-
pus. The monolingual corpus is first repli-
cated to generate a parallel corpus, where 
each sentence pair consists of two identical 
sentences in the same language. Then the 
monolingual word alignment algorithm is 
employed to align the potentially collocated 
words in the monolingual sentences. Finally 
the aligned word pairs are ranked according 
to refined alignment probabilities and those 
with higher scores are extracted as colloca-
tions. We conducted experiments using Chi-
nese and English corpora individually. Com-
pared with previous approaches, which use 
association measures to extract collocations 
from the co-occurring word pairs within a 
given window, our method achieves higher 
precision and recall. According to human 
evaluation in terms of precision, our method 
achieves absolute improvements of 27.9% on 
the Chinese corpus and 23.6% on the English 
corpus, respectively. Especially, we can ex-
tract collocations with longer spans, achiev-
ing a high precision of 69% on the long-span 
(>6) Chinese collocations. 
1 Introduction 
Collocation is generally defined as a group of 
words that occur together more often than by 
chance (McKeown and Radev, 2000). In this pa-
per, a collocation is composed of two words oc-
curring as either a consecutive word sequence or 
an interrupted word sequence in sentences, such 
as "by accident" or "take ? advice". The collo-
cations in this paper include phrasal verbs (e.g. 
"put on"), proper nouns (e.g. "New York"), idi-
oms (e.g. "dry run"), compound nouns (e.g. "ice 
cream"), correlative conjunctions (e.g. "either ? 
or"), and the other commonly used combinations 
in following types: verb+noun, adjective+noun, 
adverb+verb, adverb+adjective and adjec-
tive+preposition (e.g. "break rules", "strong tea", 
"softly whisper", "fully aware", and "fond of"). 
Many studies on collocation extraction are 
carried out based on co-occurring frequencies of 
the word pairs in texts (Choueka et al, 1983; 
Church and Hanks, 1990; Smadja, 1993; Dun-
ning, 1993; Pearce, 2002; Evert, 2004). These 
approaches use association measures to discover 
collocations from the word pairs in a given win-
dow. To avoid explosion, these approaches gen-
erally limit the window size to a small number. 
As a result, long-span collocations can not be 
extracted1. In addition, since the word pairs in 
the given window are regarded as potential col-
locations, lots of false collocations exist. Al-
though these approaches used different associa-
tion measures to filter those false collocations, 
the precision of the extracted collocations is not 
high. The above problems could be partially 
solved by introducing more resources into collo-
cation extraction, such as chunker (Wermter and 
Hahn, 2004), parser (Lin, 1998; Seretan and We-
hrli, 2006) and WordNet (Pearce, 2001). 
This paper proposes a novel monolingual 
word alignment (MWA) method to extract collo-
cation of higher quality and with longer spans 
only from monolingual corpus, without using 
any additional resources. The difference between 
MWA and bilingual word alignment (Brown et 
al., 1993) is that the MWA method works on 
monolingual parallel corpus instead of bilingual 
corpus used by bilingual word alignment. The 
                                                 
1  Here, "span of collocation" means the distance of two 
words in a collocation. For example, if the span of the col-
location (w1, w2) is 6, it means there are 5 words interrupt-
ing between w1 and w2 in a sentence. 
487
monolingual corpus is replicated to generate a 
parallel corpus, where each sentence pair con-
sists of two identical sentences in the same lan-
guage, instead of a sentence in one language and 
its translation in another language. We adapt the 
bilingual word alignment algorithm to the mono-
lingual scenario to align the potentially collo-
cated word pairs in the monolingual sentences, 
with the constraint that a word is not allowed to 
be aligned with itself in a sentence. In addition, 
we propose a ranking method to finally extract 
the collocations from the aligned word pairs. 
This method assigns scores to the aligned word 
pairs by using alignment probabilities multiplied 
by a factor derived from the exponential function 
on the frequencies of the aligned word pairs. The 
pairs with higher scores are selected as colloca-
tions. 
The main contribution of this paper is that the 
well studied bilingual statistical word alignment 
method is successfully adapted to monolingual 
scenario for collocation extraction. Compared 
with the previous approaches, which use associa-
tion measures to extract collocations, our method 
achieves much higher precision and slightly 
higher recall. The MWA method has the follow-
ing three advantages. First, it explicitly models 
the co-occurring frequencies and position infor-
mation of word pairs, which are integrated into a 
model to search for the potentially collocated 
word pairs in a sentence. Second, a new feature, 
fertility, is employed to model the number of 
words that a word can collocate with in a sen-
tence. Finally, our method can obtain the long-
span collocations. Human evaluations on the ex-
tracted Chinese collocations show that 69% of 
the long-span (>6) collocations are correct. Al-
though the previous methods could also extract 
long-span collocations by setting the larger win-
dow size, the precision is very low. 
In the remainder of this paper, Section 2 de-
scribes the MWA model for collocation extrac-
tion. Section 3 describes the initial experimental 
results. In Section 4, we propose a method to 
improve the MWA models. Further experiments 
are shown in Sections 5 and 6, followed by a dis-
cussion in Section 7. Finally, the conclusions are 
presented in Section 8. 
2 Collocation Extraction With Mono-
lingual Word Alignment Method 
2.1 Monolingual Word Alignment 
Given a bilingual sentence pair, a source lan-
guage word can be aligned with its correspond- 
 
Figure 1. Bilingual word alignment 
ing target language word. Figure 1 shows an ex-
ample of Chinese-to-English word alignment. 
In Figure 1, a word in one language is aligned 
with its counterpart in the other language. For 
examples, the Chinese word "??/tuan-dui" is 
aligned with its English translation "team", while 
the Chinese word "???/fu-ze-ren" is aligned 
with its English translation "leader". 
In the Chinese sentence in Figure 1, there are 
some Chinese collocations, such as (??/tuan-
dui, ???/fu-ze-ren). There are also some Eng-
lish collocations in the English sentence, such as 
(team, leader). We separately illustrate the collo-
cations in the Chinese sentence and the English 
sentence in Figure 2, where the collocated words 
are aligned with each other. 
 
(a) Collocations in the Chinese sentence 
 
(b) Collocations in the English sentence 
Figure 2. Word alignments of collocations in 
sentence 
Comparing the alignments in Figures 1 and 2, 
we can see that the task of monolingual colloca-
tions construction is similar to that of bilingual 
word alignment. In a bilingual sentence pair, a 
source word is aligned with its corresponding 
target word, while in a monolingual sentence, a 
word is aligned with its collocates. Therefore, it 
is reasonable to regard collocation construction 
as a task of aligning the collocated words in 
monolingual sentences. 
?? ??? ? ??  ??  ?  ? ??  ??   ? 
tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong . 
The team leader plays a key role in the project undertaking . 
The team leader plays a key role in the project undertaking.
The team leader plays a key role in the project undertaking. 
?? ??? ? ??  ??  ?  ? ??  ??   ? 
tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong .
??  ??? ? ??  ??  ?  ? ??  ??   ?
tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong . 
488
Statistical bilingual word alignment method, 
which has been well studied in the context of 
machine translation, can extract the aligned bi-
lingual word pairs from a bilingual corpus. This 
paper adapts the bilingual word alignment algo-
rithm to monolingual scenario to align the collo-
cated words in a monolingual corpus. 
Given a sentence with l words },...,{ 1 lwwS = , 
the word alignments ]},1[|),{( liaiA i ?=  can be 
obtained by maximizing the word alignment 
probability of the sentence, according to Eq. (1). 
)|(maxarg SApA
A
?=
??
                    (1) 
Where Aai i ?),(  means that the word iw  is 
aligned with the word 
ia
w . 
In a monolingual sentence, a word never col-
locates with itself. Thus the alignment set is de-
noted as }&],1[|),{( ialiaiA ii ??= . 
We adapt the bilingual word alignment model, 
IBM Model 3 (Brown et al, 1993), to monolin-
gual word alignment. The probability of the 
alignment sequence is calculated using Eq. (2). 
???
==
l
j
jaj
l
i
ii lajdwwtwnSAp j
11
),|()|()|()|( ?   (2) 
Where i?  denotes the number of words that are 
aligned with iw . Three kinds of probabilities are 
involved: 
- Word collocation probability )|(
jaj
wwt , 
which describes the possibility of wj collo-
cating with 
ja
w ;  
- Position collocation probability d(j, aj, l), 
which describes the probability of a word 
in position aj collocating with another 
word in position j; 
- Fertility probability )|( ii wn ? , which de-
scribes the probability of the number of 
words that a word wi can collocate with 
(refer to subsection 7.1 for further discus-
sion). 
Figure 3 shows an example of word alignment 
on the English sentence in Figure 2 (b) with the 
MWA method. In the sentence, the 7th word 
"role" collocates with both the 4th word "play" 
and the 6th word "key". Thus, )|( 74 wwt  and 
)|( 76 wwt  describe the probabilities that the 
word "role" collocates with "play" and "key",  
 
Figure 3. Results of MWA method 
respectively. )12,7|4(d  and )12,7|6(d  describe 
the probabilities that the word in position 7 col-
locates with the words in position 4 and 6 in a 
sentence with 12 words. For the word "role", 7?  
is 2, which indicates that the word "role" collo-
cates with two words in the sentence. 
To train the MWA model, we implement a 
MWA tool for collocation extraction, which uses 
similar training methods for bilingual word 
alignment, except that a word can not be aligned 
to itself. 
2.2 Collocation Extraction 
Given a monolingual corpus, we use the trained 
MWA model to align the collocated words in 
each sentence. As a result, we can generate a set 
of aligned word pairs on the corpus. According 
to the alignment results, we calculate the fre-
quency for two words aligned in the corpus, de-
noted as ),( ji wwfreq . In our method, we filtered 
those aligned word pairs whose frequencies are 
lower than 5. Based on the alignment frequency, 
we estimate the alignment probabilities for each 
aligned word pair as shown in Eq. (3) and (4). 
? ?=
?w j
ji
ji wwfreq
wwfreq
wwp
),(
),(
)|(  (3) 
? ?=
?w i
ji
ij wwfreq
wwfreq
wwp
),(
),(
)|(  (4) 
With alignment probabilities, we assign scores 
to the aligned word pairs and those with higher 
scores are selected as collocations, which are 
estimated as shown in Eq. (5). 
2
)|()|(
),( ijjiji
wwpwwp
wwp
+=      (5) 
3 Initial Experiments 
In this experiment, we used the method as de-
scribed in Section 2 for collocation extraction. 
Since our method does not use any linguistic in-
formation, we compared our method with the  
The team leader plays a key role in the project undertaking . 
(1)        (2)           (3)           (4)      (5)   (6)      (7)      (8)    (9)        (10)               (11)               (12) 
The team leader plays a key role in the project undertaking .
(1)        (2)           (3)           (4)      (5)   (6)      (7)      (8)    (9)        (10)               (11)              (12) 
489
02
4
6
8
10
12
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
Pr
ec
is
io
n 
(%
)
Our method (Probability)
Log-likelihood ratio
 
Figure 4. Precision of collocations 
baseline methods without using linguistic knowl-
edge. These baseline methods take all co-
occurring word pairs within a given window as 
collocation candidates, and then use association 
measures to rank the candidates. Those candi-
dates with higher association scores are extracted 
as collocations. In this paper, the window size is 
set to [-6, +6]. 
3.1 Data 
The experiments were carried out on a Chinese 
corpus, which consists of one year (2004) of the 
Xinhua news corpus from LDC 2 , containing 
about 28 millions of Chinese words. Since punc-
tuations are rarely used to construct collocations, 
they were removed from the corpora. To auto-
matically estimate the precision of extracted col-
locations on the Chinese corpus, we built a gold 
set by collecting Chinese collocations from 
handcrafted collocation dictionaries, containing 
56,888 collocations. 
3.2 Results 
The precision is automatically calculated against 
the gold set according to Eq. (6). 
)(#
)(#
Top
goldTop
N
N
C
CC
precision
?
?= I            (6) 
Where CTop-N and Cgold denote the top colloca-
tions in the N-best list and the collocations in the 
gold set, respectively. 
We compared our method with several base-
line methods using different association meas-
ures3: co-occurring frequency, log-likelihood 
                                                 
2 Available at: http://www.ldc.upenn.edu/Catalog/Catalog 
Entry.jsp?catalogId=LDC2007T03 
3 The definitions of these measures can be found in Man-
ning and Sch?tze (1999). 
0
20
40
60
80
100
0.0 2.5 3.7 4.8 5.8 6.8 7.8 8.9
log(frequency)
(%
)
Precision
Alignment Probability
 
Figure 5. Frequency vs. precision/alignment 
probability 
ratio, chi-square test, mutual information, and t-
test. Among them, the log-likelihood ratio meas-
ure achieves the best performance. Thus, in this 
paper, we only show the performance of the log-
likelihood ratio measure. 
Figure 4 shows the precisions of the top N col-
locations as N steadily increases with an incre-
ment of 1K, which are extracted by our method 
and the baseline method using log-likelihood 
ratio as the association measure. 
The absolute precision of collocations is not 
high in the figure. For example, among the top 
200K collocations, about 4% of the collocations 
are correct. This is because our gold set contains 
only about 57K collocations. Even if all colloca-
tions in the gold set are included in the 200K-
best list, the precision is only 28%. Thus, it is 
more useful to compare precision curves for col-
locations in the N-best lists extracted by different 
methods. In addition, since this gold set only in-
cludes a small number of collocations, the preci-
sion curves of our method and the baseline 
method are getting closer, as N increases. For 
example, when N is set to 200K, our method and 
the baseline method achieved precisions of 
4.09% and 3.12%, respectively. And when N is 
set to 400K, they achieved 2.78% and 2.26%, 
respectively. For convenience of comparison, we 
set N up to 200K in the experiments. 
From the results, it can also be seen that, 
among the N-best lists with N less than 20K, the 
precision of the collocations extracted by our 
method is lower than that of the collocations ex-
tracted by the baseline, and became higher when 
N is larger than 20K. 
In order to analyze the possible reasons, we 
investigated the relationships among the fre-
quencies of the aligned word pairs, the alignment 
490
xy
b =4
b =2
 
Figure 6.  xbey /?=  
probabilities, and precisions of collocations, 
which are shown in Figure 5. From the figure, 
we can see (1) that the lower the frequencies of 
the aligned word pairs are, the higher the align-
ment probabilities are; and (2) that the precisions 
of the aligned word pairs with lower frequencies 
is lower. According to the above observations, 
we conclude that it is the word pairs with lower 
frequencies but higher probabilities that caused 
the lower precision of the top 20K collocations 
extracted by our method. 
4 Improved MWA Method 
According to the analysis in subsection 3.2, we 
need to penalize the aligned word pairs with 
lower frequencies. In order to achieve the above 
goal, we need to refine the alignment probabili-
ties by using a penalization factor derived from a 
function on the frequencies of the aligned word 
pairs. This function )(xfy =  should satisfy the 
following two conditions, where x  represents 
the log function of frequencies. 
(1) The function is monotonic. When x  is set to 
a smaller number, y  is also small. This re-
sults in the penalization on the aligned word 
pairs with lower frequencies. 
(2) When ??x , y  is set to 1. This means that 
we don?t penalize the aligned word pairs 
with higher frequencies. 
According to the above descriptions, we pro-
pose to use the exponential function in Eq. (7).  
    xbey /?=  (7)
Figure 6 describes this function. The constant 
b in the function is used to adjust the shape of the 
line. The line is sharp with b set to a small num-
ber, while the line is flat with b set to a larger 
number. In our case, if b is set to a larger number,  
0
5
10
15
20
25
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
Pr
ec
is
io
n 
(%
)
Refined probability
Probability
Baseline (Log-likelihood ratio)
 
Figure 7. Precision of collocations extracted by 
the improved method 
we assign a larger penalization weight to those 
aligned word pairs with lower frequencies. 
According to the above discussion, we can use 
the following measure to assign scores to the 
aligned words pairs generated by the MWA 
method. 
)),(log(
 
2
)|()|(
),(
ji wwfreq
b
ijji
jir
e
wwpwwp
wwp
?
?+=
  (8) 
Where wi and wj are two aligned words. p(wi|wj) 
and p(wj|wi) are alignment probabilities as shown 
in Eq. (3) and (4). )),(log( ji wwfreq  is the log 
function of the frequencies of the aligned word 
pairs (wi, wj). 
5 Evaluation on Chinese corpus 
We used the same Chinese corpus described in 
Section 3 to evaluate the improved method as 
shown in Section 4. In the experiments, b  was 
tuned by using a development set and set to 25. 
5.1 Precision 
In this section, we evaluated the extracted collo-
cations in terms of precision using both auto-
matic evaluation and human evaluation. 
Automatic Evaluation 
Figure 7 shows the precisions of the colloca-
tions in the N-best lists extracted by our method 
and the baseline method against the gold set in 
Section 3. For our methods, we used two differ-
ent measures to rank the aligned word pairs: 
alignment probabilities in Eq. (5) and refined 
491
 Our method Baseline 
True 569 290 
A 25 16 
B 5 4 
C 240 251 
False 
D 161 439 
Table 1. Manual evaluation of the top 1K Chi-
nese collocations. The precisions of our method 
and the baseline method are 56.9% and 29.0%, 
respectively. 
alignment probabilities in Eq. (8). From the re-
sults, it can be seen that with the refined align-
ment probabilities, our method achieved the 
highest precision on the N-best lists, which 
greatly outperforms the best baseline method. 
For example, in the top 1K list, our method 
achieves a precision of 20.6%, which is much 
higher than the precision of the baseline method 
(11.7%). This indicates that the exponential func-
tion used to penalize the alignment probabilities 
plays a key role in demoting most of the aligned 
word pairs with low frequencies. 
Human Evaluation 
In automatic evaluation, the gold set only con-
tains collocations in the existing dictionaries. 
Some collocations related to specific corpora are 
not included in the set. Therefore, we selected 
the top 1K collocations extracted by our im-
proved method to manually estimate the preci-
sion. During human evaluation, the true colloca-
tions are denoted as "True" in our experiments. 
The false collocations were further classified into 
the following classes. 
A: The candidate consists of two words that 
are semantically related, such as (?? doctor,  
?? nurse). 
B: The candidate is a part of the multi-word 
(? 3) collocation. For example, (?? self, ??  
mechanism) is a part of the three-word colloca-
tion (?? self, ?? regulating, ?? mecha-
nism). 
C: The candidates consist of the adjacent 
words that frequently occur together, such as (? 
he, ? say) and (? very, ? good). 
D: Two words in the candidates have no rela-
tionship with each other, but occur together fre-
quently, such as (?? Beijing, ? month) and 
(? and, ? for). 
Table 1 shows the evaluation results. Our 
method extracted 569 true collocations, which  
0
2
4
6
8
10
12
0 1 2 3 4 5 6 7 8 9 10 11 12
Training corpus (Months)
Pr
ec
is
io
n 
(%
)
Our method
Baseline
 
Figure 8. Corpus size vs. precision 
are much more than those extracted by the base-
line method. Further analysis shows that, in addi-
tion to extracting short-span collocations, our 
method extracted collocations with longer spans 
as compared with the baseline method. For ex-
ample, (?? in, ?? state) and (?? because, 
?? so) are two long-span collocations. Among 
the 1K collocations, there are 48 collocation can-
didates whose spans are larger than 6, which are 
not covered by the baseline method since the 
window size is set to 6.  And 33 of them are true 
collocations, with a higher precision of 69%. 
Classes C and D account for the most part of 
the false collocations. Although the words in 
these two classes co-occur frequently, they can 
not be regarded as collocations. And we also 
found out that the errors in class D produced by 
the baseline method are much more than that of 
those produced by our method. This indicates 
that our MWA method can remove much more 
noise from the frequently occurring word pairs. 
In Class A, the two words are semantically re-
lated and occur together in the corpus. These 
kinds of collocations can not be distinguished 
from the true collocations by our method without 
additional resources. 
Since only bigram collocations were extracted 
by our method, the multi-word (? 3) collocations 
were split into bigram collocations, which caused 
the error collocations in Class B4. 
Corpus size vs. precision 
Here, we investigated the effect of the corpus 
size on the precision of the extracted collocations. 
We evaluated the precision against the gold set 
as shown in the automatic evaluation. First, the 
whole corpus (one year of newspaper) was split 
into 12 parts according to the published months. 
Then we calculated the precisions as the training 
                                                 
4 Since only a very small faction of collocations contain 
more than two words, a few error collocations belong to 
Class B. 
492
020
40
60
80
100
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
R
ec
al
l (
%
)
Our method
Baseline
 
Figure 9. Recall on the Chinese corpus 
corpus increases part by part. The top 20K collo-
cations were selected for evaluation. 
Figure 8 shows the experimental results. The 
precision of collocations extracted by our method 
is obviously higher than that of collocations ex-
tracted by the baseline method. When the size of 
the training corpus became larger, the difference 
between our method and the baseline method 
also became bigger. When the training corpus 
contains more than 9 months of corpora, the pre-
cision of collocations extracted by the baseline 
method did not increase anymore. However, the 
precision of collocations extracted by our method 
kept on increasing. This indicates the MWA 
method can extract more true collocations of 
higher quality when it is trained with larger size 
of training data. 
5.2 Recall 
Recall was evaluated on a manually labeled sub-
set of the training corpus. The subset contains 
100 sentences that were randomly selected from 
the whole corpus. The sentence average length is 
24. All true collocations (660) were labeled 
manually. The recall was calculated according to 
Eq. (9). 
)(#
)(#
subset
subsetTop
C
CC
recall N
I?=               (9) 
Here, CTop-N denotes the top collocations in the 
N-best list and Csubset denotes the true colloca-
tions in the subset. 
Figure 9 shows the recalls of collocations ex-
tracted by our method and the baseline method 
on the labeled subset. The results show that our 
method can extract more true collocations than 
the baseline method. 
0
20
40
60
80
100
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
R
ec
al
l (
%
)
Our method
Baseline
 
Figure 10. Recall on the English corpus 
 Our method Baseline 
True 591 355 
A 11 4 
B 19 20 
C 200 136 
False
D 179 485 
Table 2. Manual evaluation of the top 1K Eng-
lish collocations. The precisions of our method 
and the baseline method are 59.1% and 35.5%, 
respectively. 
In our experiments, the baseline method ex-
tracts about 20 millions of collocation candidates, 
while our method only extracts about 3 millions 
of collocation candidates5. Although the colloca-
tions of our method are much less than that of the 
baseline, the experiments show that the recall of 
our method is higher. This again proved that our 
method has the stronger ability to distinguish 
true collocations from false collocations. 
6 Evaluation on English corpus 
We also manually evaluated the proposed 
method on an English corpus, which is a subset 
randomly extracted from the British National 
Corpus6. The English corpus contains about 20 
millions of words. 
6.1 Precision 
We estimated the precision of the top 1K collo-
cations. Table 2 shows the results. The classifica-
tion of the false collocations is the same as that 
in Table 1. The results show that our methods 
outperformed the baseline method using log- 
                                                 
5 We set the threshold to 7.88 with a confidence level  of 
005.0=?  (cf. page 174 of Chapter 5 in (McKeown and 
Radev, 2000) for more details). 
6 Available at: http://www.hcu.ox.ac.uk/BNC/ 
493
05
10
15
20
0 20 40 60 80 100 120 140 160 180
Top-N collocation (K)
Pr
ec
is
io
n 
(%
)
 
Figure 11. Fertility vs. precision 
likelihood ratio. And the distribution of the false 
collocations is similar to that on the Chinese cor-
pus. 
6.2 Recall 
We used the method described in subsection 5.2 
to calculate the recall. 100 English sentences 
were labeled manually, obtaining 205 true collo-
cations. Figure 10 shows the recall of the collo-
cations in the N-best lists. From the figure, it can 
be seen that the trend on the English corpus is 
similar to that on the Chinese corpus, which in-
dicates that our method is language-independent. 
7 Discussion 
7.1 The Effect of Fertility 
In the MWA model as described in subsection 
2.1, i?  denotes the number of words that can 
align with iw . Since a word only collocates with 
a few other words in a sentence, we should set a 
maximum number for ? , denote as max? . 
In order to set max? , we examined the true col-
locations in the manually labeled set described in 
subsection 5.2. We found that 78% of words col-
locate with only one word, and 17% of words 
collocate with two words. In sum, 95% of words 
in the corpus can only collocate with at most two 
words. According to the above observation, we 
set max?  to 2. 
In order to further examine the effect of max?  
on collocation extraction, we used several differ-
ent max?  in our experiments. The comparison 
0
1
2
3
4
5
6
7
8
0 20 40 60 80 100
Span of collocation
lo
g(
#(
al
ig
ne
d 
w
or
d 
pa
irs
))
 
Figure 12. Distribution of spans 
results are shown in Figure 11. The highest pre-
cision is achieved when max?  is set to 2. This 
result verifies our observation on the corpus. 
7.2 Span of Collocation 
One of the advantages of our method is that 
long-span collocations can be reliably extracted. 
In this subsection, we investigate the distribution 
of the span of the aligned word pairs. For the 
aligned word pairs occurring more than once, we 
calculated the average span as shown in Eq. (10). 
),(
);,(
),(
ji
corpuss
ji
ji wwfreq
swwSpan
wwAveSpan
?
= ?  (10) 
Where, );,( swwSpan ji  is the span of the words 
wi and wj in the sentence s; ),( ji wwAveSpan  is 
the average span. 
The distribution is shown in Figure 12. It can 
be seen that the number of the aligned word pairs 
decreased exponentially as the average span in-
creased. About 17% of the aligned word pairs 
have spans longer than 6. According to the hu-
man evaluation result for precision in subsection 
5.1, the precision of the long-span collocations is 
even higher than that of the short-span colloca-
tions. This indicates that our method can extract 
reliable collocations with long spans. 
8 Conclusion 
We have presented a monolingual word align-
ment method to extract collocations from mono-
lingual corpus. We first replicated the monolin-
gual corpus to generate a parallel corpus, in 
which each sentence pair consists of the two 
identical sentences in the same language. Then 
we adapted the bilingual word alignment algo-
rithm to the monolingual scenario to align the 
10
3
2
1
max
max
max
max
=
=
=
=
?
?
?
?
494
potentially collocated word pairs in the monolin-
gual sentences. In addition, a ranking method 
was proposed to finally extract the collocations 
from the aligned word pairs. It scores collocation 
candidates by using alignment probabilities mul-
tiplied by a factor derived from the exponential 
function on the frequencies. Those with higher 
scores are selected as collocations. Both Chinese 
and English collocation extraction experiments 
indicate that our method outperforms previous 
approaches in terms of both precision and recall. 
For example, according to the human evaluations 
on the Chinese corpus, our method achieved a 
precision of 56.9%, which is much higher than 
that of the baseline method (29.0%). Moreover, 
we can extract collocations with longer span. 
Human evaluation on the extracted Chinese col-
locations shows that 69% of the long-span (>6) 
collocations are correct. 
References 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2): 263-311. 
Yaacov Choueka, S.T. Klein, and E. Neuwitz. 1983. 
Automatic Retrieval of Frequent Idiomatic and 
Collocational Expressions in a Large Corpus. 
Journal for Literary and Linguistic computing, 
4(1):34-38. 
Kenneth Church and Patrick Hanks. 1990. Word As-
sociation Norms, Mutual Information, and Lexi-
cography. Computational Linguistics, 16(1):22-29. 
Ted Dunning. 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. Computational 
Linguistics, 19(1): 61-74. 
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis, 
University of Stuttgart. 
Dekang Lin. 1998. Extracting Collocations from Text 
Corpora. In Proceedings of the 1st Workshop on 
Computational Terminology, pp. 57-63. 
Christopher D. Manning and Hinrich Sch?tze. 1999. 
Foundations of Statistical Natural Language Proc-
essing, Cambridge, MA; London, U.K.: Bradford 
Book & MIT Press. 
Kathleen R. McKeown and Dragomir R. Radev. 2000. 
Collocations. In Robert Dale, Hermann Moisl, and 
Harold Somers (Ed.), A Handbook of Natural Lan-
guage Processing, pp. 507-523. 
Darren Pearce. 2001. Synonymy in Collocation Ex-
traction. In Proceedings of NAACL-2001 Workshop 
on Wordnet and Other Lexical Resources: Applica-
tions, Extensions and Customizations, pp. 41-46. 
Darren Pearce. 2002. A Comparative Evaluation of 
Collocation Extraction Techniques. In Proceedings 
of the 3rd International Conference on Language 
Resources and Evaluation, pp. 651-658. 
Violeta Seretan and Eric Wehrli. 2006. Accurate Col-
location Extraction Using a Multilingual Parser. In 
Proceedings of the 21st International Conference 
on Computational Linguistics and 44th Annual 
Meeting of the Association for Computational Lin-
guistics (COLING/ACL-2006), pp. 953-960 
Frank Smadja. 1993. Retrieving Collocations from 
Text: Xtract. Computational Linguistics, 19(1): 
143-177. 
Joachim Wermter and Udo Hahn. 2004. Collocation 
Extraction Based on Modifiability Statistics. In 
Proceedings of the 20th International Conference 
on Computational Linguistics (COLING-2004), pp. 
980-986. 
495
