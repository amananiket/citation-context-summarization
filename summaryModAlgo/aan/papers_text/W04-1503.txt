A Simple String-Rewriting Formalism for Dependency Grammar
Alexis NASR
Lattice, UFRL
Universite? Paris 7
F-75005 Paris
France
alexis.nasr@linguist.jussieu.fr
Owen RAMBOW
Columbia University
Department of Computer Science
1214 Amsterdam Avenue
New York, NY 10027-7003, USA
rambow@cs.columbia.edu
Abstract
Recently, dependency grammar has gained renewed
attention as empirical methods in parsing have
emphasized the importance of relations between
words, which is what dependency grammars model
explicitly, but context-free phrase-structure gram-
mars do not. While there has been much work on
formalizing dependency grammar and on parsing
algorithms for dependency grammars in the past,
there is not a complete generative formalization of
dependency grammar based on string-rewriting in
which the derivation structure is the desired depen-
dency structure. Such a system allows for the defi-
nition of a compact parse forest in a straightforward
manner. In this paper, we present a simple gen-
erative formalism for dependency grammars based
on Extended Context-Free Grammar, along with
a parser; the formalism captures the intuitions of
previous formalizations while deviating minimally
from the much-used Context-Free Grammar.
1 Introduction
Dependency grammar has a long tradition in syn-
tactic theory, dating back to at least Tesnie`re?s work
from the thirties. Recently, it has gained renewed
attention as empirical methods in parsing have em-
phasized the importance of relations between words
(see, e.g., (Collins, 1997)), which is what depen-
dency grammars model explicitly, but context-free
phrase-structure grammars do not. In this paper, we
address an important issue in using grammar for-
malisms: the compact representation of the parse
forest. Why is this an important issue? It is well
known that for non-toy grammars and non-toy ex-
amples, a sentence can have a staggeringly large
number of analyses (for example, using a context-
free grammar (CFG) extracted from the Penn Tree-
bank, a sentence of 25 words may easily have
1,000,000 or more analyses). By way of an exam-
ple of an ambiguous sentence (though with only two
readings), the two dependency representations for
the ambiguous sentence (1) are given in Figure 1.
(1) Pilar saw a man with a telescope
It is clear that if we want to evaluate each possi-
ble analysis (be it using a probabilistic model or a
different method, for example a semantic checker),
we cannot efficiently do so if we enumerate all
cases.1 We have two options: we can either use a
greedy heuristic method for checking which does
not examine all possible solutions, which entails
we may miss the optimal solution, or we perform
our checking operation on a representation which
encodes all options in a compact representation.
This is possible because the exponential number of
possible analyses (exponential in the length of the
input sentence) share subanalyses, thus making a
polynomial-size representation possible. This rep-
resentation is called the shared parse forest and it
has been extensively studied for CFGs (see, for ex-
ample, (Lang, 1991)). To our knowledge, there has
been no description of the notion of shared parse
forest for dependency trees to date. In this paper, we
propose a formalization which is very closely based
on the shared parse forest for CFG. We achieve this
by defining a generative string-rewriting formal-
ism whose derivation trees are dependency trees.
The formalism, and the corresponding shared parse
forests, are used in a probabilistic chart parser for
dependency grammar, which is described in (Nasr
and Rambow, 2004b).
While there has been much work on formalizing
dependency grammar and on parsing algorithms for
dependency grammars in the past, we observe that
there is not, to our knowledge, a complete gener-
ative formalization2 of dependency grammar based
on string-rewriting in which the derivation structure
is exactly the desired dependency structure. The
most salient reason for the lack of such a gener-
ative dependency grammar is the absence of non-
1We would like to thank two anonymous reviewers for use-
ful comments.
2While the notions are related historically and conceptually,
we refer to a type of mathematical formalization, not to the
school of linguistics known as ?Generative Grammar?.
terminal symbols in a dependency tree, which pre-
vents us from interpreting it as a derivation struc-
ture in a system that distinguishes between termi-
nal and nonterminal symbols. The standard solu-
tion to this problem, proposed by Gaifman (1965),
is to introduce nonterminal symbols denoting lex-
ical categories, as depicted in figure 2 (called the
?labelled phrase-structure trees induced by a depen-
dency tree? by Gaifman (1965)). Clearly, the ?pure?
dependency tree can be derived in a straightforward
manner. The string rewriting system described in
(Gaifman, 1965) generates as derivation structures
this kind of trees.
There is however a deeper problem when con-
sidering dependency trees as derivation structures,
following from the fact that in a dependency tree,
modifiers3 are direct dependents of the head they
modify, and (in certain syntactic contexts) the num-
ber of modifiers is unbounded. Thus, if we wish to
obtain a tree as shown in Figure 2, we need to have
productions whose right-hand side is of unbounded
size, which is not possible in a context-free gram-
mar. Indeed, the formalization of dependency gram-
mar proposed by Gaifman (1965) is unsatisfactory
in that it does not allow for an unbounded number
of modifiers!
In this paper, we follow a suggestion made by
Abney (1996) and worked out in some detail in
(Lombardo, 1996)4 to extend Gaifman?s notation
with regular expressions, similar to the approach
used in extended context-free grammars. The re-
sult is a simple generative formalism which has
the property that the derivation structures are de-
pendency trees, except for the introduction of pre-
terminal nodes as shown in Figure 2. We do not
mean to imply that our formalism is substantially
different from previous formalizations of depen-
dency grammar; the goal of this paper is to present
a clean and easy-to-use generative formalism with
a straightforward notion of parse forest. In partic-
ular, our formalism, Generative Dependency Gram-
mar, allows for an unbounded number of daughter
nodes in the derivation tree through the use of reg-
ular expressions in its rules. The parser uses the
3We use the term modifier in its linguistic sense as a type of
syntactic dependency (another type being argument). We use
head (or mother) and dependent (or daughter) to refer to nodes
in a tree. Sometimes, in the formal and parsing literature, mod-
ifier is used to designate any dependent node, but we consider
that usage confusing because of the related but different mean-
ing of the term modifier that is well-established in the linguistic
literature.
4In fact, much of our formalism is very similar to (Lom-
bardo, 1996), who however does not discuss parsing (only
recognition), nor the representation of the parse forest.
corresponding finite-state machines which straight-
forwardly allows for a binary-branching representa-
tion of the derivation structure for the purpose of
parsing, and thus for a compact (polynomial and
not exponential) representation of the parse forest.
This formalism is based on previous work presented
in (Kahane et al, 1998), which has been substan-
tially reformulated in order to simplify it.5 In par-
ticular, we do not address non-projectivity here, but
acknowledge that for certain languages it is a cru-
cial issue. We will extend our basic approach in the
spirit of (Kahane et al, 1998) in future work.
The paper is structured as follows. We start
out by surveying previous formalizations of depen-
dency grammar in Section 2. In Section 3, we intro-
duce several formalisms, including Generative De-
pendency Grammar. We present a parsing algorithm
in Section 4, and mention empirical results in Sec-
tion 5. We then conclude.
2 Previous Formalizations of Dependency
Grammar
We start out by observing that ?dependency gram-
mar? should be contrasted with ?phrase structure
grammar?, not ?CFG?, which is a particular formal-
ization of phrase structure grammar. Thus, just as it
only makes sense to study the formal properties of
a particular formalization of phrase structure gram-
mar, the question about the formal properties of de-
pendency grammar in general is not well defined,
nor the question of a comparison of a dependency
formalism with dependency grammar.
There have been (at least) four types of formal-
izations of dependency grammars in the past.6 None
of these approaches, to our knowledge, discuss the
notion of shared parse forest. The first approach
(for example, (Lombardo and Lesmo, 2000)) fol-
lows Gaifman (1965) in proposing traditional string
rewriting rules, which however do not allow for an
unbounded number of adjuncts.
In the second approach, the dependency structure
is constructed in reference to a parallel (?deeper?)
structure (Sgall et al, 1986; Mel?c?uk, 1988). Be-
cause the rules make reference to other struc-
5Kahane et al (1998) present three different types of rules,
for subcategorization, modification, and linear precedence. In
the formalism presented in this paper, they have been collapsed
into one.
6We leave aside here work on tree rewriting systems such
as Tree Adjoining Grammar, which, when lexicalized, have
derivation structures which are very similar to dependency
trees. See (Rambow and Joshi, 1997) for a discussion related
to TAG, and see (Rambow et al, 2001) for the definition of a
tree-rewriting system which can be used to develop grammars
whose derivations faithfully mirror syntactic dependency.
saw
 HH
Pilar man
 HH
a with
telescope
a
saw


HH
HH
Pilar man
a
with
telescope
a
Figure 1: Two dependency trees
Pilar
N
a
D man
a
VV
saw
D man
a
sawN
P
with
Pilar
N
N
a
D telesope
N P
with N
telesopeD
Figure 2: Two dependency trees with lexical categories
tures, these approaches cannot be formalized in
a straightforward manner as context-free rewriting
formalisms.
In the third approach, which includes formaliza-
tions of dependency structure such as Dependency
Tree Grammar of Modina (see (Dikovsky and Mod-
ina, 2000) for an overview), Link Grammar (Sleator
and Temperley, 1993) or the tree-composition ap-
proach of Nasr (1996), rules construct the depen-
dency tree incrementally; in these approaches, the
grammar licenses dependency relations which, in a
derivation, are added to the tree one by one, or in
groups. In contrast, we are interested in a string-
rewriting system; in such a system, we cannot add
dependency relations incrementally: all daughters
of a node must be added at once to represent a sin-
gle rewrite step.
In the fourth approach, the dependency grammar
is converted into a headed context-free grammar
(Abney, 1996; Holan et al, 1998), also the Basic
Dependency Grammar of Beletskij (1967) as cited
in (Dikovsky and Modina, 2000). This approach al-
lows for the recovery of the dependency structure
both from the derivation tree and from a parse for-
est represented in polynomial space. (In fact, our
parsing algorithm draws on this work.) However,
the approach of course requires the introduction of
additional nonterminal nodes. Finally, we observe
that Recursive Transition Networks (Woods, 1970)
can be used to encode a grammar whose deriva-
tion trees are dependency trees. However, they are
more a general framework for encoding grammars
than a specific type of grammar (for example, we
can also use them to encode CFGs). In a some-
what related manner, Alshawi et al (2000) use cas-
caded head automata to derive dependency trees, but
leave the nature of the cascading under-formalized.
Eisner (2000) provides a formalization of a system
that uses two different automata to generate left and
right children of a head. His formalism is very close
to the one we present, but it is not a string-rewriting
formalism (and not really generative at all). We
are looking for a precise formulation of a genera-
tive dependency grammar, and the question has re-
mained open whether there is an alternate formal-
ism which allows for an unbounded number of ad-
juncts, introduces all daughter nodes at once in a
string-rewriting step, and avoids the introduction of
additional nonterminal nodes.
3 Formalism
In this section we first review the definition of Ex-
tended Context-Free Grammar and then show how
we use it to model dependency derivations. An Ex-
tended Context-Free Grammar (or ECFG for short)
is like a context-free grammar (CFG), except that
the right-hand side is a regular expression over the
terminal and nonterminal symbols of the grammar.
At each step in a derivation, we first choose a rewrite
rule (as we do in CFG), and then we choose a string
which is in the language denoted by the regular ex-
pression associated with the rule. This string is then
treated like the right-hand side of a CFG rewrite
rule.
In the following, if G is a grammar and R a regu-
lar expression, then L(G) denotes the language gen-
erated by the grammar and L(R) the language de-
noted by the regular expression. If F is a class of
grammars (such as CFG), then L(F) denote the class
of languages generated by the grammars in F. We
now give a formal definition, which closely follows
that given by Albert et al (1999).7
A Extended Context-Free Grammar is a 4-
tuple (VN, VT, P, S), where:
? VN is a finite set of nonterminal symbols,
? VT is a finite set of terminal symbols (disjoint
from VN),
? P is a finite set of rules, which are ordered
pairs consisting of an element of VN and a reg-
ular expression over VN ? VT,
? S, a subset of VN, contains the possible start
symbols.
We will use the traditional arrow notation (??)
to write rules.
For A ? VN and u, v ? (VN ? VT)? we say that
uAv yields uwv (written uAv =? uwv) if A ??
R is in P and w is in L(R). The transitive closure
of the yield relation (denoted ?=? ) is defined in the
usual manner.
The language generated by a Extended Context-
Free Grammar is the set {w ? V ?T | A
?=? w,A ?
S}.
We now define a restricted version of ECFG
which we will use for defining dependency gram-
mars. The only new formal requirement is that
the rules be lexicalized in the sense of (Joshi and
Schabes, 1991). For our formalism, this means
that the regular expression in a production is such
that each string in its denoted language contains at
least one terminal symbol. Linguistically speaking,
this means that each rule is associated with exactly
7ECFG has been around informally since the sixties (e.g.,
the Backus-Naur form); for a slightly different formalization,
see (Madsen and Kristensen, 1976), whose definition allows
for an infinite rule set.
one lexical item (which may be multi-word). We
will call this particular type of Extended Context-
Free Grammar a lexicalized Extended Context-
Free Grammar or, for obvious reasons, a Genera-
tive Dependency Grammar (GDG for short). When
we use a GDG for linguistic description, its left-
hand side nonterminal will be interpreted as the lex-
ical category of the lexical item and will represent
its maximal projection.8
A Generative Dependency Grammar is a lexi-
calized ECFG.
It is sometimes useful to have dependency repre-
sentations with labeled arcs (typically labeled with
syntactic functions such as SUBJ for subject or ADJ
for adjunct). There are different ways of achieving
this goal; here, we discuss the use of feature struc-
tures in conjunction with the nonterminal symbols,
for example N[gf:subj] instead of just N. Fea-
ture structures are of course useful for other reasons
as well, such as expressing morphological features.
In terms of our formalism, the use of bounded fea-
ture structures can be seen as a shorthand notation
for an increased set of nonterminal symbols. The
use of feature structures (rather than simple nonter-
minal symbols) allows for a more perspicuous rep-
resentation of linguistic relations through the use of
underspecification. Note that the use of underspec-
ified feature structures in rules can potentially lead
to an exponential increase (exponential in the size
of the set of feature values) of the size of the set of
rules if rules contain underspecified feature struc-
tures on the right-hand side. However, we note that
the feature representing, grammatical function will
presumably always be fully specified on the right-
hand side of a rule (the head determines the func-
tion of its dependents). Underspecification in the
left-hand side of a rule only leads to linear compact-
ification of the rule set.
We now give a toy linguistic example. We let
GLing be (VN, VT, P, S) as follows:
? VN = {V,N,D,A,Adv}
? VT = { Pilar, saw, man, a, telescope, with, tall,
very }
? P consists of the following rules:
p1 : V ?? N saw N P ?
p2 : N ??
(Pilar | D (A) (man | telescope) P ?)
8For practical purposes, we can separate the lexicon (which
assigns lexical categories to lexemes) from the syntactic rules
(which hold for all lexemes of a class), as does Gaifman (1965),
resulting in a straightforward notational extension to our for-
malism.
V
p1=? N saw N P
p2p2p3=? Pilar saw D man with N
p6p2=? Pilar saw a man with D telescope
p6=? Pilar saw a man with a telescope
Figure 3: A sample GDG derivation
p3 : P ?? with N
p4 : A ?? Adv? tall
p5 : Adv ?? very
p6 : D ?? a
? S = {V }
A derivation is shown in Figure 3; the corre-
sponding derivation tree is shown in the right part
of Figure 2. As can be seen, the derivation structure
is a dependency tree, except for the use of pretermi-
nals, as we desired.
The first part of the following theorem follows
from the existence of a Greibach Normal Form for
ECFG (Albert et al, 1999). The second part follows
immediately from the closure of CFG under regular
substitution.
L(GDG) = L(ECFG) = L(CFG).
Of course, ECFG, GDG and CFG are not strongly
equivalent in the standard sense for string rewrit-
ing systems of having the same sets of derivation
trees. Clearly, ECFG can generate all sets of deriva-
tion trees that GDG can, while CFG cannot (because
of the unbounded branching factor of ECFG and of
GDG); ECFG can also generate all sets of deriva-
tion trees that CFG can, while GDG cannot (because
of the lexicalization requirement). ECFG thus has
a greater strong generative capacity than CFG and
GDG, while those of GDG and CFG are incompa-
rable.
It is interesting to notice the difference between
the rewriting operation of a nonterminal symbol as
defined for a ECFG or a GDG and the equivalent
rewriting steps with a weakly equivalent CFG. A
GDG rewriting operation of a symbol X using a
rule r is decomposed in two stages, the first stage
consists in choosing a string w which belongs to the
set denoted by the right-hand side of r. During the
second stage, X is replaced by w. These two stages
are of a different nature, the first concerns the gener-
ation of CFG rules (and hence a CFG) using a GDG
while the second concerns the generation of a string
using the generated CFG. The equivalent rewriting
operation (X ? w) with a CFG does not distin-
guish the same two stages, both the selection of w
and the rewriting of X as w are done at the same
time.
man
(m2,4)
(m2,3)
(m3,3)
(m3,2) (m2,4)
(m2,3) telescopesaw
(m2,4)
(m2,4)
Pilar a
(D,2)
with
a
(D,2)
(m1,4) (m1,4)
(m1,4)
(m1,3)
(m1,2)
Figure 4: A packed parse forest
4 Parsing Algorithm
The parsing algorithm given here is a simple exten-
sion of the CKY algorithm. The difference is in the
use of finite state machines in the items in the chart
to represent the right-hand sides of the rules of the
ECFG.9 A rule with category C as its left-hand side
will give rise to a finite state machine which we call
a C-rule FSM; its final states mark the completed
recognition of a constituent of label C .
CKY-Style parsing algorithm for Extended
Context-Free Grammars.
Input. A ECFG G and an input string W =
w1 ? ? ?wn.
Output. The parse table T for W such that ti,j
contains (M, q) iff M is a C-rule-FSM, q is one
of the final states of M , and we have a derivation
C +=?wi ? ? ?wj . If i = j, ti,j also contains the in-
put symbol wi.
Method.
? Initialization: For each i, 1 ? i ? n, add wi
to ti,i.
? Completion: If ti,j contains either the input
symbol w or an item (M, q) such that q is
a final state of M , and M is a C-rule-FSM,
then add to ti,j all (M ?, q?) such that M ? is a
rule-FSM which transitions from a start state
to state q? on input w or C . Add a single back-
pointer from (M ?, q?) in ti,j to (M, q) or w in
ti,j .
9Recent work in the context of using ECFG for pars-
ing SGML and XML proposes an LL-type parser for ECFG
(Bru?ggemann-Klein and Wood, 2003); their approach also ex-
ploits the automaton representation of the right-hand side of
rules, as is natural for an algorithm dealing with ECFG.
1 2 3
N saw N
4 Pm1
1 3 4
Pilar
A manD
2 Ptelescope
telescope
man
m2
with N
2 31m3
Figure 5: Three rules FSM m1, m2 and m3. m1 is a V-rule-FSM corresponding to rule p1, m2 is an
N-rule-FSM which corresponds to rule p2 and m3 is a P-rule-FSM which corresponds to rule p3
P(3)
P(2) N(4)
N(3) telescopewith
a
D(2)
V(5)
V(5)
V(4)
sawV(3)
N(4)
Pilar
man
N(4)
N(3)
a
D(2)
V(5)
P(3)
P(2) N(4)
N(3) telescopewith
a
D(2)
N(4)
man
N(4)
N(3)
a
D(2)
V(4)
sawV(3)
N(4)
Pilar
Figure 6: A parse forest
? Scanning: If (M1, q1) is in ti,k, and tk+1,j
contains either the input symbol w or the item
(M2, q2) where q2 is a final state and M2 is a
C-rule-FSM, then add (M1, q) to ti,j (if not al-
ready present) if M1 transitions from q1 to q on
either w or C . Add a double backpointer from
(M1, q) in ti,j to (M1, q1) in ti,k (left back-
pointer) and to either w or (M2, q2) in tk+1,j
(right backpointer).
At the end of the parsing process, a packed parse
forest has been built. The packed forest correspond-
ing to the parse of sentence Pilar saw a man with a
telescope, using the grammar of Section 3 is repre-
sented in Figure 4. The nonterminal nodes are la-
beled with pairs (M, q) where M is an rule-FSM
and q a state of this FSM. Three rule-FSMs corre-
sponding to rules p1, p2 and p3 have been repre-
sented in Figure 5.
Obtaining the dependency trees from the packed
parse forest is performed in two stages. In a first
stage, a forest of binary syntagmatic trees is ob-
tained from the packed forest and in a second stage,
each syntagmatic tree is transformed into a depen-
dency tree. We shall not give the details of these
processes. The two trees resulting from de-packing
of Figure 4 are represented in Figure 6. The dif-
ferent nodes of the syntagmatic tree that will be
grouped in a single node of the dependency trees
have been circled.
5 Empirical Results
While the presentation of empirical results is not the
object of this paper, we give an overview of some
empirical work using ECFG for natural language
processing in this section. For full details, we refer
to (Nasr and Rambow, 2004a; Nasr and Rambow,
2004b; Nasr, 2004).
The parser presented in Section 4 above has been
implemented. We have investigated the use the
parser in a two-step probabilistic framework. In a
first step, we determine which rules of the ECFG
should be used for each word in the input sentence.
(Recall that a grammar rule encodes the active and
passive valency, as well as how any arguments are
realized, for example, fronted or in canonical posi-
tion.) This step is called supertagging and has been
suggested and studied in the context of Tree Adjoin-
ing Grammar by Bangalore and Joshi (1999). In
a second step, we use a probabilistic ECFG where
the probabilities are non-lexical and are based en-
tirely on the grammar rules. We extract the most
probable derivation from the compact parse forest
using dynamic programming in the usual manner.
This non-lexical probability model is used because
the supertagging step already takes the words in the
sentence into account. The probabilities can be en-
coded directly as weights on the transitions in the
rule-FSMs used by the parser.
The ECFG grammar we use has been automati-
cally extracted from the Penn Treebank (PTB). In
fact, we first extract a Tree Insertion Grammar fol-
lowing the work of (Xia et al, 2000; Chen, 2001;
Chiang, 2000), and then directly convert the trees
of the obtained TAG into automata for the parser.
It is clear that one could also derive an explicit
ECFG in the same manner. The extracted gram-
mar has about 4.800 rules. The probabilities are
estimated from the corpus during extraction. Note
that there are many different ways of extracting an
ECFG from the PTB, corresponding to different the-
ories of syntactic dependency. We have chosen to
directly model predicate-argument relations rather
than more surface-oriented syntactic relations such
as agreement, so that all function words (determin-
ers, auxiliaries, and so on) depend on the lexical
word. Strongly governed prepositions are treated as
part of a lexeme rather than as full prepositions.
We have investigated several different ways of
modeling the probability of attaching a sequence of
modifiers at a certain point in the derivation (con-
ditioning on the position of the modifier in the se-
quence or conditioning on the previous modifier
used). We found that using position or context im-
proves on using neither.
We have performed two types of experiments: us-
ing the correct ECFG rule for each word, and as-
signing ECFG rules automatically using supertag-
ging. In the case of using the correct supertag, we
obtain unlabeled dependency accuracies of about
98% (i.e., in about 2% of cases a word is assigned
a wrong governor). Automatic supertagging (us-
ing standard n-gram tagging methods) for a gram-
mar our size has an accuracy of about 80%. This
is also approximately the dependency accuracy ob-
tained when parsing the output of a supertagger. We
conclude from this performance that if we can in-
crease the performance of the supertagger, we can
also directly increase the performance of the parser.
Current work includes examining which grammati-
cal distinctions the grammar should make in order to
optimize both supertagging and parsing (Toussenel,
2004).
6 Conclusion
We have presented a generative string-rewriting
system, Extended Context-Free Grammar, whose
derivation trees are dependency trees with un-
bounded branching factor. We have shown how we
can reuse the representation of shared parse forests
well-known from CFGs for Extended Context-Free
Grammar. The question arises whether we can rep-
resent the shared parse forest in a manner more di-
rectly in the spirit of dependency. This question was
investigated by (Nasr, 2003). He shows that the fac-
toring realized in the shared forest presented here
and which is the key to the polynomial represen-
tation of a potentially exponential set, can be done
directly on the dependency trees by introducing the
notion of dependency sets.
References
Abney, Steven (1996). A grammar of projections.
Unpublished manuscript, Universita?t Tu?bingen.
Albert, Ju?rgen; Giammarresi, Dora; and Wood, Der-
ick (1999). Extended context-free grammars and
normal form algorithms. In Champarnaud, Jean-
Marc; Maurel, Denis; and Ziadi, Djelloul, ed-
itors, Automata Implementations: Third Inter-
national Workshop on Implementing Automata
(WIA?98), volume 1660 of LNCS, pages 1?12.
Springer Verlag.
Alshawi, Hiyan; Bangalore, Srinivas; and Douglas,
Shona (2000). Learning dependency translation
models as collections of finite-state head trans-
ducers. cl, 26(1):45?60.
Bangalore, Srinivas and Joshi, Aravind (1999).
Supertagging: An approach to almost parsing.
Computational Linguistics, 25(2):237?266.
Bru?ggemann-Klein, Anne and Wood, Derick
(2003). The parsing of extended context-free
grammars. Unpublished manuscript, Technische
Universita?t Mu?nchen and Hong Kong University
of Science & Technology.
Chen, John (2001). Towards Efficient Statistical
Parsing Using Lexicalized Grammatical Infor-
mation. PhD thesis, University of Delaware.
Chiang, David (2000). Statistical parsing with an
automatically-extracted tree adjoining grammar.
In 38th Meeting of the Association for Compu-
tational Linguistics (ACL?00), pages 456?463,
Hong Kong, China.
Collins, Michael (1997). Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the As-
sociation for Computational Linguistics, Madrid,
Spain.
Dikovsky, Alexander and Modina, Larissa (2000).
Dependencies on the other side of the curtain.
Traitement Automatique des Langues, 41(1):79?
111.
Eisner, Jason (2000). Bilexical grammars and their
cubic-time parsing algorithms. In Bunt, Harry C.
and Nijholt, Anton, editors, New Developments
in Natural Language Parsing. Kluwer Academic
Publishers.
Gaifman, Haim (1965). Dependency systems and
phrase-structure systems. Information and Con-
trol, 8:304?337.
Holan, Toma?s?; Kubon?, Vladislav; Oliva, Karel; and
Pla?tek, Martin (1998). Two useful measures of
word order complexity. In Kahane, Sylvain and
Polgue`re, Alain, editors, Processing of Depen-
dency Grammars: Proceeding of the Workshop,
pages 21?28, Montre?al, Canada. ACL/COLING.
Joshi, Aravind K. and Schabes, Yves (1991). Tree-
adjoining grammars and lexicalized grammars.
In Nivat, Maurice and Podelski, Andreas, editors,
Definability and Recognizability of Sets of Trees.
Elsevier.
Kahane, Sylvain; Nasr, Alexis; and Rambow, Owen
(1998). Pseudo-projectivity: A polynomially
parsable non-projective dependency grammar. In
36th Meeting of the Association for Computa-
tional Linguistics and 17th International Con-
ference on Computational Linguistics (COLING-
ACL?98), pages 646?652, Montre?al, Canada.
Lang, Bernard (1991). Towards a uniform formal
framework for parsing. In Tomita, M., editor,
Current Issues in Parsing technology, chapter 11,
pages 153?171. Kluwer Academic Publishers.
Lombardo, Vincenzo (1996). An Earley-style
parser for dependency grammars. In Proceedings
of the 16th International Conference on Compu-
tational Linguistics (COLING?96), Copenhagen.
Lombardo, Vincenzo and Lesmo, Leonardo (2000).
A formal theory of dependency syntax with
empty units. Traitement automatque des langues,
41(1):179?209.
Madsen, O.L. and Kristensen, B.B. (1976). LR-
parsing of extended context-free grammars. Acta
Informatica, 7:61?73.
Mel?c?uk, Igor A. (1988). Dependency Syntax: The-
ory and Practice. State University of New York
Press, New York.
Nasr, Alexis (1996). Un syste`me de reformula-
tion automatique de phrases fonde? sur la The?orie
Sens-Texte : application aux langues contro?le?es.
PhD thesis, Universite? Paris 7.
Nasr, Alexis (2003). Factoring sufrace syntac-
tic structures. In First International Conference
on Meaning-Text Theory, pages 249?258, Paris,
France.
Nasr, Alexis (2004). Grammaires de de?pendances
ge?ne?ratives: expe?riences sur le Corpus Paris 7.
Unpublished manuscript, Universite? Paris 7.
Nasr, Alexis and Rambow, Owen (2004a). Depen-
dency parsing based on n-best-path supertagging.
Unpublished manuscript, Universite? Paris 7 and
COlumbia University.
Nasr, Alexis and Rambow, Owen (2004b). Su-
pertagging and full parsing. In Proceedings of
the Workshop on Tree Adjoining Grammar and
Related Formalisms (TAG+7), Vancouver, BC,
Canada.
Rambow, Owen and Joshi, Aravind (1997). A for-
mal look at dependency grammars and phrase-
structure grammars, with special consideration
of word-order phenomena. In Wanner, Leo,
editor, Recent Trends in Meaning-Text Theory,
pages 167?190. John Benjamins, Amsterdam and
Philadelphia.
Rambow, Owen; Vijay-Shanker, K.; and Weir,
David (2001). D-Tree Substitution Grammars.
Computational Linguistics, 27(1).
Sgall, P.; Hajic?ova?, E.; and Panevova?, J. (1986).
The meaning of the sentence and its semantic and
pragmatic aspects. Reidel, Dordrecht.
Sleator, Daniel and Temperley, Davy (1993). Pars-
ing english with a link grammar,. In Proceedings
of the Third International Workshop on Parsing
Technologies IIWPT?93).
Toussenel, Franc?ois (2004). Why supertagging is
hard. In Proceedings of the Workshop on Tree
Adjoining Grammar and Related Formalisms
(TAG+7), Vancouver, BC, Canada.
Woods, William A. (1970). Transition network
grammars for natural language analysis. Com-
mun. ACM, 3(10):591?606.
Xia, Fei; Palmer, Martha; and Joshi, Aravind
(2000). A uniform method of grammar extrac-
tion and its applications. In Proc. of the EMNLP
2000, Hong Kong.
