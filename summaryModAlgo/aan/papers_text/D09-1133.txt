Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1280?1288,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
 
Improving Nominal SRL in Chinese Language with Verbal SRL In-
formation and Automatic Predicate Recognition 
 
Junhui Li?   Guodong Zhou??   Hai Zhao??  Qiaoming Zhu?  Peide Qian? 
? Jiangsu Provincial Key Lab for Computer Information Processing Technologies
School of Computer Science and Technology 
Soochow University, Suzhou, China 215006 
? Department of Chinese, Translation and Linguistics 
City University of HongKong, China 
Email: {lijunhui,gdzhou,hzhao,qmzhu,pdqian}@suda.edu.cn
 
                                                          
? Corresponding author 
Abstract 
This paper explores Chinese semantic role la-
beling (SRL) for nominal predicates. Besides 
those widely used features in verbal SRL, 
various nominal SRL-specific features are 
first included. Then, we improve the perform-
ance of nominal SRL by integrating useful 
features derived from a state-of-the-art verbal 
SRL system. Finally, we address the issue of 
automatic predicate recognition, which is es-
sential for a nominal SRL system. Evaluation 
on Chinese NomBank shows that our research 
in integrating various features derived from 
verbal SRL significantly improves the per-
formance. It also shows that our nominal SRL 
system much outperforms the state-of-the-art 
ones. 
1. Introduction 
Semantic parsing maps a natural language sen-
tence into a formal representation of its meaning. 
Due to the difficulty in deep semantic parsing, 
most of previous work focuses on shallow se-
mantic parsing, which assigns a simple structure 
(such as WHO did WHAT to WHOM, WHEN, 
WHERE, WHY, HOW) to each predicate in a 
sentence. In particular, the well-defined seman-
tic role labeling (SRL) task has been drawing 
more and more attention in recent years due to 
its importance in deep NLP applications, such as 
question answering (Narayanan and Harabagiu, 
2004), information extraction (Surdeanu et al, 
2003), and co-reference resolution (Ponzetto and 
Strube, 2006). Given a sentence and a predicate 
(either a verb or a noun) in it, SRL recognizes 
and maps all the constituents in the sentence into 
their corresponding semantic arguments (roles) 
of the predicate. According to the predicate 
types, SRL could be divided into SRL for verbal 
predicates (verbal SRL, in short) and SRL for 
nominal predicates (nominal SRL, in short). 
During the past few years, verbal SRL has 
dominated the research on SRL with the avail-
ability of FrameNet (Baker et al, 1998), Prop-
Bank (Palmer et al, 2005), and the consecutive 
CoNLL shared tasks (Carreras and M?rquez, 
2004 & 2005) in English language. As a com-
plement to PropBank on verbal predicates, 
NomBank (Meyers et al, 2004) annotates nomi-
nal predicates and their corresponding semantic 
roles using similar semantic framework as 
PropBank. As a representative, Jiang and Ng 
(2006) pioneered the exploration of various 
nominal SRL-specific features besides the tradi-
tional verbal SRL-related features on NomBank. 
They achieved the performance of 72.73 and 
69.14 in F1-measure on golden and automatic 
syntactic parse trees, respectively, given golden 
nominal predicates. 
For SRL in Chinese, Sun and Jurafsky (2004) 
and Pradhan et al (2004) pioneered the research 
on Chinese verbal and nominal SRLs, respec-
tively, on small private datasets. Taking the ad-
vantage of recent release of Chinese PropBank 
(Xue and Palmer, 2003) and Chinese NomBank 
(Xue, 2006a), Xue and his colleagues (Xue and 
Palmer 2005; Xue 2006b; Xue, 2008) pioneered 
the exploration of Chinese verbal and nominal 
SRLs, given golden predicates. Among them, 
Xue and Palmer (2005) studied Chinese verbal 
SRL using Chinese PropBank and achieved the 
performance of 91.3 and 61.3 in F1-measure on 
golden and automatic syntactic parse trees, re-
spectively. Xue (2006b) extended their study on 
Chinese nominal SRL and attempted to improve 
the performance of nominal SRL by simply in-
1280
 cluding the Chinese PropBank training instances 
into the training data for nominal SRL on Chi-
nese NomBank. However, such integration was 
empirically proven unsuccessful due to the dif-
ferent nature of certain features for verbal and 
nominal SRLs. Xue (2008) further improved the 
performance on both verbal and nominal SRLs 
with a better syntactic parser and new features. 
Ding and Chang (2008) focused on argument 
classification for Chinese verbal predicates with 
hierarchical feature selection strategy. They 
achieved the classification precision of 94.68% 
on golden parse trees on Chinese PropBank. 
This paper focuses on Chinese nominal SRL. 
This is done by adopting a traditional verbal 
SRL architecture to handle Chinese nominal 
predicates with additional nominal SRL-specific 
features. Moreover, we significantly enhance the 
performance of nominal SRL by properly inte-
grating various features derived from verbal 
SRL. Finally, this paper investigates the effect of 
automatic nominal predicate recognition on the 
performance of Chinese nominal SRL. Although 
previous research (e.g. CoNLL?2008) in English 
nominal SRL reveals the importance of auto-
matic predicate recognition, there has no re-
ported research on automatic predicate 
recognition in Chinese nominal SRL. 
The rest of this paper is organized as follows: 
Section 2 introduces Chinese NomBank while 
the baseline nominal SRL system is described in 
Section 3 with traditional and nominal SRL-
specific features. Then, the baseline nominal 
SRL system is improved by integrating useful 
features derived from verbal SRL (Section 4) 
and extended with automatic recognition of 
nominal predicates (Section 5). Section 6 gives 
experimental results and discussion. Finally, 
Section 7 concludes the paper.    
2. Chinese NomBank 
Chinese NomBank (Xue, 2006a) adopts similar 
semantic framework as NomBank, and focuses 
on Chinese nominal predicates with their argu-
ments in Chinese TreeBank. The semantic ar-
guments include:  
1) Core arguments: Arg0 to Arg5. Generally, 
Arg0 and Arg1 denotes the agent and the 
patient, respectively, while arguments from 
Arg2 to Arg5 are predicate-specific.  
2) Adjunct arguments, which are universal to 
all predicates, e.g. ArgM-LOC for locative, 
and ArgM-TMP for temporal. 
 
All the arguments are annotated on parse tree 
nodes with their boundaries aligning with the 
spans of tree nodes. Figure 1 gives an example 
with two nominal predicates and their respective 
arguments, while the nominal predicate ???
/investment? has two core arguments, ?NN(??
/foreign businessman)? as Arg0 and ?NN(??
/bank)? as Arg1, and the other nominal predicate 
??? /loan? also has two core arguments, 
?NP(???? /Bank of China)? as Arg1 and 
Figure 1: Two nominal predicates and their arguments in the style of NomBank. 
? 
?? ?? ??
??
???
P 
NN NN NN
VV
NN NN 
Arg0/Rel1 Rel1 Arg1/Rel1
NP 
PP 
Arg0/Rel2 
ArgM-MNR/Rel2 Rel2 
NP 
CD
QP
NP
VP
VP
??? ?? 
? 
NN NN 
PU 
NP 
Arg1/Rel2 
IP
?? ?? 
Sup/Rel2
Bank of China 
to 
Foreign  Investment  Bank 
provide
4 billion
RMB loan 
. 
Bank of China provides 4 billion RMB loan to Foreign Investment Bank. 
1281
 ?PP(??????? /to Foreign Investment 
Bank)? as Arg0,  and 1 adjunct argument, 
?NN(???/RMB)? as ArgM-MNR, denoting 
the manner of loan. It is worth noticing that 
there is a (Chinese) NomBank-specific label in 
Figure 1, Sup (support verb) (Xue, 2006a), in 
helping introduce the arguments, which occur 
outside the nominal predicate-headed noun 
phrase. This is illustrated by the nominal predi-
cate ???/loan?, whose Arg0 and Arg1 are both 
realized outside the nominal predicate-headed 
noun phrase, NP(????????/4 billion 
RMB loan). Normally, a verb is marked as a 
support verb only when it shares some argu-
ments with the nominal predicate. 
3. Baseline: Chinese Nominal SRL 
Popular SRL systems usually formulate SRL as 
a classification problem, which annotates each 
constituent in a parse tree with a semantic role 
label or with the non-argument label NULL. Be-
sides, we divide the system into three consecu-
tive phases so as to overcome the imbalance 
between the training instances of the NULL 
class and those of any other argument classes.  
Argument pruning. Here, several heuristic 
rules are adopted to filter out constituents, which 
are most likely non-arguments. According to the 
argument structures of nominal predicates, we 
categorize arguments into two types: arguments 
inside NP (called inside arguments) and argu-
ments introduced via a support verb (called out-
side arguments), and handle them separately. 
For the inside arguments, the following three 
heuristic rules are applied to find inside argu-
ment candidates: 
z All the sisters of the predicate are candi-
dates. 
z If a CP or DNP node is a candidate, its chil-
dren are candidates too. 
z For any node X, if its parent is an ancestral 
node of the predicate, and the internal 
nodes along the path between X and the 
predicate are all NPs, then X is a candidate. 
For outside arguments, we look for the sup-
port verb of the focus nominal predicate, and 
then adopt the rules as proposed in Xue and 
Palmer (2005) to find the candidates for the sup-
port verb, since outside argument candidates are 
introduced via this support verb. That to say, the 
argument candidates of the support verb are re-
garded as outside argument candidates of the 
nominal predicate. However, as support verbs 
are not annotated explicitly in the testing phase, 
we identify intervening verbs as alternatives to 
support verbs in both training and testing phases 
with the path between the nominal predicate and 
intervening verb in the form of 
?VV<VP>[NP>]+NN?, where ?[NP>]+? denotes 
one or more NPs.  Our statistics on Chinese 
NomBank shows that 51.96% of nominal predi-
cates have no intervening verb while 48.04% of 
nominal predicates have only one intervening 
verb. 
Taken the nominal predicate ???/loan? in 
Figure 1 as an example, NN(???/RMB) and 
QP(??? /4 billion) are identified as inside 
argument candidates, while PP(??????
?/to Foreign Investment Bank) and NP(???
?/Bank of China) are identified as outside ar-
gument candidates via the support verb VV(?
?/provide). 
Argument identification. A binary classifier 
is applied to determine the candidates as either 
valid arguments or non-arguments. It is worth 
pointing out that we only mark those candidates 
that are most likely to be NULL (with probabil-
ity > 0.90) as non-arguments. Our empirical 
study shows that this little trick much benefits 
nominal SRL, since argument identification for 
nominal predicates is much more difficult than 
that for verbal predicates and thus many argu-
ments would have been falsely marked as non-
arguments if the threshold is set as 0.5. 
Argument classification. A multi-class classi-
fier is employed to label identified arguments 
with specific argument labels (including the 
NULL class for non-argument). 
In the following, we first adapt some tradi-
tional features, which have been proven effec-
tive in verbal SRL, to nominal SRL, and then 
introduce several nominal SRL-specific features. 
3.1. Traditional Features 
Using the feature naming convention as adopted 
in Jiang and Ng (2006), Table 1 lists the tradi-
tional features, where ?I? and ?C? indicate the 
features for argument identification and classifi-
cation, respectively. Among them, the predicate 
class (b2) feature was first introduced in Xue 
and Palmer (2005) to overcome the imbalance of 
the predicate distribution in that some predicates 
can be only found in the training data while 
some predicates in the testing data are absent 
from the training data. In particular, the verb 
class is classified along three dimensions: the 
number of arguments, the number of framesets 
and selected syntactic alternations. For example, 
1282
 the verb class of ?C1C2a? means that it has two 
framesets, with the first frameset having one 
argument and the second having two arguments. 
The symbol ?a? in the second frameset repre-
sents a type of syntactic alternation. 
 
Feature Remarks: b1-b5(C, I), b6-b7(C) 
b1 Predicate: the nominal predicate itself. (??
/loan) 
b2 Predicate class: the verb class that the predi-
cate belongs to. (C4a) 
b3 Head word (b3H) and its POS (b3P).  (??
/bank, NN) 
b4 Phrase type: the syntactic category of the 
constituent. (NP) 
b5 Path: the path from the constituent to the 
nominal predicate. 
 (NP<IP>VP>VP>NP>NP>NN) 
b6 Position: the positional relationship of the 
constituent with the predicate. ?left? or 
?right?. (left) 
b7 First word (b7F) and last word (b7L) of the 
focus constituent. (??/China, ??/bank) 
Combined features: b11-b14(C, I), b15(C) 
b11: b1&b4;       b12: b1&b3H;       b13: b2&b4;  
b14: b2&b3H;    b15: b5&b6 
Table 1: Traditional features and their instantiations 
for argument identification and classification, with 
NP(????/Bank of China)  as the focus constitu-
ent and NN(??/loan) as the nominal predicate, re-
garding Figure 1. 
3.2. Nominal SRL-specific Features 
To capture more useful information in the predi-
cate-argument structure, we also study addi-
tional features which provide extra information. 
Statistics on Chinese NomBank show that about 
40% of pruned inside candidates are arguments. 
Since inside arguments usually locate near to the 
nominal predicate, its surroundings are expected 
to be helpful in SRL. Table 2 shows the features 
in better capturing the details between inside 
arguments and nominal predicates. Specially, 
features ai6 and ai7 are sister-related features, 
inspired by the features related with the 
neighboring arguments in Jiang and Ng (2006). 
Statistics on NomBank and Chinese Nom-
Bank show that about 20% and 22% of argu-
ments are introduced via a support verb, 
respectively. Since a support verb pivots outside 
arguments and the nominal predicate on its two 
sides, support verbs play an important role in 
labeling these arguments. Here, we also identify 
intervening verbs as alternatives to support verbs 
since support verbs are not explicitly in the test-
ing phase. Table 3 lists the intervening verb-
related features (ao1-ao4, ao11-ao14) employed 
in this paper. 
 
Feature Remarks 
ai1 Whether the focus constituent is adjacent to 
the predicate. Yes or No. (Yes) 
ai2 The headword (ai2H) and pos (ai2P) of the 
predicate?s nearest right sister. (??/bank, 
NN) 
ai3 Whether the predicate has right sisters. Yes 
or No. (Yes) 
ai4 Compressed path of b5: compressing se-
quences of identical labels into one. 
(NN<NP>NN) 
ai5 Whether the predicate has sisters. Yes or 
No. (Yes) 
ai6 For each sister of the focus constituent, 
combine b3H&b4&b5&b6. ( ? ?
/bank&NN & NN<NP>NN&right) 
ai7 Coarse version of ai6, b4&b6. (NN&right) 
Table 2: Additional features and their instantiations  
for inside argument candidates, with ?NN(??
/foreign businessman)? as the focus constituent and 
?NN(?? /investment)? as the nominal predicate, 
regarding Figure1. 
 
Feature Remarks 
ao1 Intervening verb itself. (??/provide) 
ao2 The verb class that the intervening verb 
belongs to. (C3b) 
ao3 The path from the focus constituent to the 
intervening verb. (NP<IP>VP>VP>VV) 
ao4 The compressed path of ao3: compressing 
sequences of identical labels into one. 
(NP<IP>VP>VV) 
Combined features: ao11-ao14 
ao11: ao1&ao3;      ao12: ao1&ao4;    
ao13: ao2&ao3;      ao14: ao2&ao4. 
Table 3: Additional features and their instantiations 
for outside argument candidates, with ?NP(????
/Bank of China)? as the focus constituent and ???
/loan? as the nominal predicate, regarding Figure1. 
Feature selection. Some Features proposed 
above may not be effective in tasks of identifica-
tion and classification. We adopt the greedy fea-
ture selection algorithm as described in Jiang 
and Ng (2006) to pick up positive features em-
pirically and incrementally according to their 
contributions on the development data. The al-
gorithm repeatedly selects one feature each time 
which contributes most, and stops when adding 
any of the remaining features fails to improve 
the performance. As far as the SRL task con-
cerned, the whole feature selection process could 
be done as follows: 1). Feature selection for ar-
gument identification: run the selection algo-
1283
 rithm with the basic set of features (b1-b5, b11-
b14) to pick up effective features from (ai1-ai7, 
ao1-ao4, ao11-ao14); 2). Feature selection for 
argument classification: fix the output returned 
in step1 as the feature set of argument identifica-
tion, and run the selection algorithm with the 
basic set of features (b1-b7, b11-b15) to select 
positive features from (ai1-ai7, ao1-ao4, ao11-
ao14) for argument classification. 
4. Integrating Features derived from 
Verbal SRL 
Since Chinese PropBank and NomBank are an-
notated on the same data set with the same lexi-
cal guidelines (e.g. frame files), it may be 
interesting to investigate the contribution of 
Chinese verbal SRL on the performance of Chi-
nese nominal SRL. In the frame files, argument 
labels are defined with regard to their semantic 
roles to the predicate, either a verbal or nominal 
predicate. For example, in the frame file of 
predicate ???/loan?, the borrower is always 
labeled with Arg0 and the lender labeled with 
Arg1. This can be demonstrated by the follow-
ing two sentences: ???/loan? is annotated as a 
nominal and a verbal predicate in S1 and S2, 
respectively. 
S1 [Arg1 ????/Bank of China] [Arg0 ???
????/to Foreign Investment Bank] ??
/provide [Rel??/loan] 
S2  [Arg0 ????/Bank of China] [Arg1 ???
????/from Foreign Investment Bank] [Rel 
??/loan] 
Therefore, it is straightforward to augment 
nominal training instances with verbal ones. 
However, Xue (2006b) found that simply adding 
the training instances for verbal SRL to the 
training data for nominal SRL and indiscrimi-
nately extracting the same features in both ver-
bal and nominal SRLs hurt the performance. 
This may be due to that certain features (e.g. the 
path feature) are much different for verbal and 
nominal SRLs. This can be illustrated in sen-
tences S1 and S2: the verbal instances in S2 are 
negative for semantic role labeling of the nomi-
nal predicate ???/loan? in S1, since ????
?/Bank of China? takes opposite roles in S1 
and S2. So does ????????/(from/to) 
Foreign Investment Bank?. 
Although several support verb-related features 
(ao1-ao4, ao11-ao14) have been proposed, one 
may still ask how large the role support verbs 
can play in nominal SRL. It is interesting to note 
that outside arguments and the highest NP 
phrase headed by the nominal predicate are also 
annotated as arguments of the support verb in 
Chinese PropBank. For example, Chinese Prop-
Bank marks ?????/Bank of China? as Arg0 
and ?????????/4 billion RMB loan? 
as Arg1 for verb ???/provide? in Figure1. Let 
OA be the outside argument, VV be the support 
verb, and NP be the highest NP phrase headed 
by the nominal predicate NN, then there exists a 
pattern ?OA VV NN? in the sentence, where the 
support verb VV plays a certain role in trans-
ferring roles between OA and NN. For example, 
if OA is the agent of VV, then OA is also the 
agent of phrase VP(VV NN). Like the example 
in Figure1, supposing a NP is the agent of sup-
port verb ???/provide? as well as VP phrase 
(??????????? /provide 4 billion 
RMB loan?), we can infer that the NP is the 
lender of the nominal predicate ???/loan? in-
dependently on any other information, such as 
the NP content and the path from the NP to the 
nominal predicate ???/loan?.  
Let C be the focus constituent, V be the inter-
vening verb, and NP be the highest NP headed 
by the nominal predicate. Table 4 shows the fea-
tures (ao5-ao8, p1-p7) derived from verbal SRL. 
In this paper, we develop a state-of-the-art Chi-
nese verbal SRL system, similar to the one as 
shown in Xue (2008), to achieve the goal. Based 
on golden parse trees on Chinese PropBank, our 
Chinese verbal SRL system achieves the per-
formance of 92.38 in F1-measure, comparable to 
Xue (2008) which achieved the performance of 
92.0 in F1-measure. 
 
Feature Remarks 
ao5 Whether C is an argument for V. Yes or No
ao6 The semantic role of C for V. 
ao7 Whether NP is an argument for V. Yes or No
ao8 The semantic role of NP for V. 
Combined features: p1-p7 
p1: ao1&ao5;         p2: ao1&ao6;    p3: ao1&ao5&b1; 
p4: ao1&ao6&b1;  p5: ao1&apo7;  p6: ao1&ao8;  
p7: ao5&ao7. 
Table 4: Features derived from verbal SRL. 
5. Automatic Predicate Recognition 
Unlike Chinese PropBank where almost all the 
verbs are annotated as predicates, Chinese Nom-
Bank only marks those nouns having arguments 
as predicates. Statistics on Chinese NomBank 
show that only 17.5% of nouns are marked as 
predicates. It is possible that a noun is a predi-
1284
 cate in some cases but not in others. Previous 
Chinese nominal SRL systems (Xue, 2006b; 
Xue, 2008) assume that nominal predicates have 
already been manually annotated and thus are 
available. To our best knowledge, there is no 
report on addressing automatic recognition of 
nominal predicates on Chinese nominal SRL. 
Automatic recognition of nominal predicates 
can be cast as a binary classification (e.g., Predi-
cate vs. Non-Predicate) problem. This paper 
employs the convolution tree kernel, as proposed 
in Collins and Duffy (2001), on automatic rec-
ognition of nominal predicates. 
Given the convolution tree kernel, the key 
problem is how to extract a parse tree structure 
from the parse tree for a nominal predicate can-
didate. In this paper, the parse tree structure is 
constructed as follows: 1) starting from the 
predicate candidate?s POS node, collect all of its 
sister nodes (with their headwords); 2). recur-
sively move one level up and collect all of its 
sister nodes (with their headwords) till reaching 
a non-NP node. Specially, in order to explicitly 
mark the positional relation between a node and 
the predicate candidate, all nodes on the left side 
of the candidate are augmented with tags 1 and 2 
for nodes on the right side. Figure 2 shows an 
example of the parse tree structure with regard 
to the predicate candidate ???/loan? as shown 
in Figure 1. 
In our extra experiments we found global sta-
tistic features (e.g. g1-g5) about the predicate 
candidate are helpful in a feature vector-based 
method for predicate recognition. Figure 2 
makes an attempt to utilize those features in ker-
nel-based method. We have explored other ways 
to include those global features. However, the 
way in Figure 2 works best.  
 
 
Let the predicate candidate be w0, and its left 
and right neighbor words be w-1 and w1, respec-
tively. The five global features are defined as 
follows. 
g1 Whether w0 is ever tagged as a verb in the 
training data? Yes or No. 
g2 Whether w0 is ever annotated as a nominal 
predicate in the training data? Yes or No. 
g3 The most likely label for w0 when it occurs 
together with w-1 and w1. 
g4 The most likely label for w0 when it occurs 
together with w-1. 
g5 The most likely label for w0 when it occurs 
together with w1. 
6. Experiment Results and Discussion 
We have evaluated our Chinese nominal SRL 
system on Chinese NomBank with Chinese 
PropBank 2.0 as its counterpart. 
6.1. Experimental Settings 
This version of Chinese NomBank consists of 
standoff annotations on the files (chtb_001 to 
1151.fid) of Chinese Penn TreeBank 5.1. Fol-
lowing the experimental setting in Xue (2008), 
648 files (chtb_081 to 899.fid) are selected as 
the training data, 72 files (chtb_001 to 040.fid 
and chtb_900 to 931.fid) are held out as the test 
data, and 40 files (chtb_041 to 080.fid) as the 
development data, with 8642, 1124, and 731 
propositions, respectively. 
As Chinese words are not naturally segmented 
in raw sentences, two Chinese automatic parsers 
are constructed: word-based parser (assuming 
golden word segmentation) and character-based 
parser (with automatic word segmentation). 
Here, Berkeley parser (Petrov and Klein, 2007)1 
is chosen as the Chinese automatic parser. With 
regard to character-based parsing, we employ a 
Chinese word segmenter, similar to Ng and Low 
(2004), to obtain the best automatic segmenta-
tion result for a given sentence, which is then 
fed into Berkeley parser for further syntactic 
parsing. Both the word segmenter and Berkeley 
parser are developed with the same training and 
development datasets as our SRL experiments. 
The word segmenter achieves the performance 
of 96.1 in F1-measure while the Berkeley parser 
gives a performance of 82.5 and 85.5 in F1-
measure on golden and automatic word segmen-
tation, respectively2.  
??? 1 In addition, SVMLight with the tree kernel 
function (Moschitti, 2004) 3  is selected as our 
classifier. In order to handle multi-classification 
                                                          
1 Berkeley Parser. http://code.google.com/p/berkeleyparser/ 
2 POSs are not counted in evaluating the performance of 
word-based syntactic parser, but they are counted in evalu-
ating the performance of character-based parser. Therefore 
the F1-measure for the later is higher than that for the for-
mer. 
3 SVM-LIGHT-TK. http://dit.unitn.it/~moschitt/ 
Figure 2: Semantic sub-tree for nominal predicate
RMB 
?? 
loan 
?? 1 
provide 
??? 1 
4 billion 
VV1 
NN1 NN 
NPQP1 
NP
VP 
g1 ?. g5
1285
 problem in argument classification, we apply the 
one vs. others strategy, which builds K classifi-
ers so as to separate one class from all others. 
For argument identification and classification, 
we adopt the linear kernel and the training pa-
rameter C is fine-tuned to 0.220. For automatic 
recognition of nominal predicates, the training 
parameter C and the decay factor ?  in the con-
volution tree kernel are fine-tuned to 2.0 and 0.2, 
respectively. 
6.2. Results with Golden Parse Trees and 
Golden Nominal Predicates 
Effect of nominal SRL-specific features 
 
 Rec.(%) Pre.(%) F1 
traditional features 62.83 73.58 67.78 
+nominal SRL-specific  
features 
69.90 75.11 72.55 
Table 5: The performance of nominal SRL on the 
development data with golden parse trees and golden 
nominal predicates 
After performing the greedy feature selection 
algorithm on the development data, features 
{ao1, ai6, ai2P, ai5, ao2, ao12, ao14}, as pro-
posed in Section 3.2, are selected consecutively 
for argument identification, while features {ai7, 
ao1, ai1, ao2, ai5, ao4} are selected for argument 
classification. Table 5 presents the SRL results 
on the development data. It shows that nominal 
SRL-specific features significantly improve the 
performance from 67.78 to 72.55 ( ) 
in F1-measure. 
05.0;2 <p?
Effect of features derived from verbal SRL 
 
Features Rec.(%) Pre.(%) F1 
baseline 67.86 73.63 70.63  
+ao5 68.15 73.60 70.77 (+0.14)
+ao6 67.66 72.80 70.14 (-0.49)
+ao7 68.20 75.41 71.62 (+0.99)
+ao8 68.30 75.39 71.67 (+1.04)
+p1 67.91 74.40 71.00 (+0.37)
+p2 67.76 74.20 70.83 (+0.20)
+p3 67.96 74.69 71.16 (+0.53)
+p4 68.01 74.18 70.96 (+0.33)
+p5 68.01 75.01 71.39 (+0.76)
+p6 68.20 75.12 71.49 (+0.86)
+p7 68.40 75.70 71.87 (+1.24)
Table 6: Effect of features derived from verbal SRL 
on the performance of nominal SRL on the test data 
with golden parse trees and golden nominal predi-
cates. The first row presents the performance using 
traditional and nominal SRL-specific features. 
 
 
 Rec.(%) Pre.(%) F1 
baseline  67.86 73.63 70.63 
+features derived 
from verbal SRL
68.40 77.51 72.67 
Xue (2008) 66.1 73.4 69.6 
Table 7: The performance of nominal SRL on the test 
data with golden parse trees and golden nominal 
predicates 
 
Table 6 shows the effect of features derived 
from verbal SRL in an incremental way. It 
shows that only the feature ao6 has negative ef-
fect due to its strong relevance with intervening 
verbs and thus not included thereafter. Table 7 
shows the performance on the test data with or 
without using the features derived from the ver-
bal SRL system. It shows these features signifi-
cantly improve the performance ( ) 
on nominal SRL. Table 7 also shows our system 
outperforms Xue (2008) by 3.1 in F1-measure. 
05.0;2 <p?
6.3. Results with Automatic Parse Trees 
and Golden Nominal Predicates 
In previous section we have assumed the avail-
ability of golden parse trees during the testing 
process. Here we conduct experiments on auto-
matic parse trees, using the Berkeley parser. 
Since arguments come from constituents in 
parse trees, those arguments, which do not align 
with any syntactic constituents, are simply dis-
carded. Moreover, for any nominal predicate 
segmented incorrectly by the word segmenter, 
all its arguments are unable to be labeled neither. 
Table 8 presents the SRL performance on the 
test data by using automatic parse trees. It shows 
that the performance drops from 72.67 to 60.87 
in F1-measure when replacing golden parse trees 
with word-based automatic ones, partly due to 
the absence of 6.9% arguments in automatic 
trees, and wrong POS tagging of nominal predi-
cates. Table 8 also compares our system with 
Xue (2008). It shows that our system also out-
performs Xue (2008) on Chinese NomBank. 
 Rec. (%) Pre. (%) F1 
This paper 56.95(53.55) 66.74(66.69) 60.87(59.40)
Xue (2008) 53.1 (52.9) 62.9 (62.3) 57.6 (57.3) 
Table 8: The performance of nominal SRL on the test 
data with automatic parse trees and golden predicates. 
Here, the numbers outside the parentheses indicate 
the performance using a word-based parser, while the 
numbers inside indicate the performance using a 
character-based parser4. 
                                                          
4 About 1.6% nominal predicates are mistakenly segmented 
by the character-based parser, thus their arguments are 
missed directly. 
1286
 6.4. Results with Automatic Nominal Predi-
cates 
So far nominal predicates are assumed to be 
manually annotated and available. Here we turn 
to a more realistic scenario in which both the 
parse tree and nominal predicates are automati-
cally obtained. In the following, we first report 
the results of automatic nominal predicate rec-
ognition and then the results of nominal SRL on 
automatic recognition of nominal predicates. 
Results of nominal predicate recognition 
Parses g1-g5 Rec.(%) Pre.(%) F1 
no 91.46 88.93 90.18 golden 
yes 92.62 89.36 90.96 
word-based yes 86.39 81.80 84.03 
character-based yes 84.79 81.94 83.34 
Table 9: The performance of automatic nominal 
predicate recognition on the test data 
 
Table 9 lists the predicate recognition results, 
using the parse tree structure, as shown in Sec-
tion 5, and the convolution tree kernel, as pro-
posed in Collins and Duffy (2001). The second 
column (g1-g5) indicates whether the global fea-
tures (g1-g5) are included in the parse tree struc-
ture. We have also defined a simple rule that 
treats a noun which is ever a verb or a nominal 
predicate in the training data as a nominal predi-
cate. Based on golden parse trees, the rule re-
ceives the performance of 81.40 in F1-measure. 
This suggests that our method significantly out-
performs the simple rule-based one. Table 9 also 
shows that: 
z As a complement to local structural informa-
tion, global features improve the performance 
of automatic nominal predicate recognition 
by 0.78 in F1-measure. 
z The word-based syntactic parser decreases 
the F1-measure from 90.96 to 84.03, mostly 
due to the POSTagging errors between NN 
and VV, while the character-based syntactic 
parser further drops the F1-measure by 0.69, 
due to automatic word segmentation. 
Results with automatic predicates 
 
Parses Predicates Rec.(%) Pre.(%) F1 
golden 68.40 77.51 72.67 golden 
automatic 65.07 74.65 69.53 
golden 55.95 66.74 60.87 word-
based automatic 52.67 59.56 55.90 
golden 53.55 66.69 59.40 character-
based automatic 50.66 59.60 54.77 
Table 10: The performance of nominal SRL on the 
test data with the choices of golden/automatic parse 
trees and golden/automatic predicates 
In order to have a clear performance comparison 
among nominal SRL on golden/automatic parse 
trees and golden/automatic predicates, Table 10 
lists all the results in those scenarios. 
6.5. Comparison 
Chinese nominal SRL vs. Chinese verbal SRL 
Comparison with Xue (2008) shows that the per-
formance of Chinese nominal SRL is about 20 
lower (e.g. 72.67 vs. 92.38 in F1-measure) than 
that of Chinese verbal SRL, partly due to the 
smaller amount of annotated data (about 1/5) in 
Chinese NomBank than that in Chinese Prop-
Bank. Moreover, according to Chinese Nom-
Bank annotation criteria (Xue 2006a), even 
when a noun is a true deverbal noun, not all of 
its modifiers are legitimate arguments or ad-
juncts of this predicate. Only arguments that can 
co-occur with both the nominal and verbal forms 
of the predicate are considered in the NomBank 
annotation. This means that the judgment of ar-
guments is semantic rather than syntactic. These 
facts may also partly explain the lower nominal 
SRL performance, especially the performance of 
argument identification. This can be illustrated 
by the statistics on the development data that 
96% (40%) of verbal (nominal) predicates? sis-
ters are annotated as arguments. Finally, the 
predicate-argument structure of nominal predi-
cates is more flexible and complicated than that 
of verbal predicates as illustrated in Xue (2006a). 
Chinese nominal SRL vs. English nominal 
SRL 
Liu and Ng (2007) reported the performance of 
77.04 and 72.83 in F1-measure on English Nom-
Bank when golden and automatic parse trees are 
used, respectively. Taking into account that Chi-
nese verbal SRL achieves comparable perform-
ance with English verbal SRL on golden parse 
trees, the performance gap between Chinese and 
English nominal SRL (e.g. 72.67 vs. 77.04 in 
F1-measure) presents great challenge for Chi-
nese nominal SRL. Moreover, while automatic 
parse trees only decrease the performance of 
English nominal SRL by about 4.2 in F1-
measure, automatic parse trees significantly de-
crease the performance of Chinese nominal SRL 
by more than 12 in F1-measure due to the much 
lower performance of Chinese syntactic parsing. 
7. Conclusion 
In this paper we investigate nominal SRL in 
Chinese language. In particular, some nominal 
SRL-specific features are included to improve 
1287
 the performance. Moreover, various features 
derived from verbal SRL are properly integrated 
into nominal SRL. Finally, a convolution tree 
kernel is adopted to address the issue of auto-
matic nominal predicates recognition, which is 
essential in a nominal SRL system.  
To our best knowledge, this is the first re-
search on 
1) Exploring Chinese nominal SRL on auto-
matic parse trees with automatic predicate 
recognition; 
2) Successfully integrating features derived 
from Chinese verbal SRL into Chinese nomi-
nal SRL with much performance improve-
ment. 
Acknowledgement  
This research was supported by Project 
60673041 and 60873150 under the National 
Natural Science Foundation of  China, Project 
2006AA01Z147 under the  ?863? National 
High-Tech Research and Development of China, 
and Project BK2008160 under the Natural Sci-
ence Foundation of the Jiangsu province of 
China. We also want to thank Dr. Nianwen Xue 
for share of the verb class file. We also want to 
thank the reviewers for insightful comments. 
References  
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet Project. In 
Proceedings of COLING-ACL 1998. 
Xavier Carreras and Lluis M?rquez. 2004. Introduc-
tion to the CoNLL-2004 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2004.  
Xavier Carreras and Lluis M?rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2005.  
Michael Collins and Nigel Duffy. 2001. Convolution 
Kernels for Natural Language. In Proceedings of 
NIPS 2001.  
Weiwei Ding and Baobao Chang. 2008. Improving 
Chinese Semantic Role Classification with Hierar-
chical Feature Selection Strategy. In Proceedings 
of EMNLP 2008. 
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic 
role Labeling of NomBank: a Maximum Entropy 
Approach. In Proceedings of EMNLP 2006.  
Chang Liu and Hwee Tou Ng. 2007. Learning Predic-
tive Structures for Semantic Role Labeling of 
NomBank. In Proceedings of ACL 2007. 
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. 
Zielinska, B. Yong, and R. Grishman. 2004. Anno-
tating Noun Argument Structure for NomBank. In 
Proceedings of LREC 2004.  
Alessandro Moschitti. 2004. A Study on Convolution 
Kernels for Shallow Semantic Parsing. In Pro-
ceedings of ACL 2004. 
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion Answering based on Semantic Structures. In 
Proceedings of COLING 2004.  
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-
of-speech Tagging: One-at-a-time or All-at-once? 
Word-based or Character-based? In Proceedings 
of EMNLP 2004. 
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics. 
Slav Petrov. and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In Proceesings of 
NAACL 2007.  
Simone Paolo Ponzetto and Michael Strube. 2006. 
Semantic Role Labeling for Coreference Resolu-
tion. In Proceedings of EACL 2006. 
Sameer Pradhan, Honglin Sun, Wayne Ward, James 
H. Martin, and Dan Jurafsky. 2004. Parsing Ar-
guments of Nominalizations in English and Chi-
nese. In Proceedings of NAACL-HLT 2004.  
Honglin Sun and Daniel Jurafsky. 2004. Shallow 
Semantic Parsing of Chinese. In Proceedings of 
NAACL 2004.  
Mihai Surdeanu, Sanda Harabagiu, John Williams 
and Paul Aarseth. 2003. Using Predicate-argument 
Structures for Information Extraction. In Proceed-
ings of ACL 2003. 
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Lluis M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of 
Syntactic and Semantic Dependencies. In Pro-
ceedings of CoNLL 2008. 
Nianwen Xue and Martha Palmer. 2003. Annotating 
the Propositions in the Penn Chinese TreeBank. In 
Proceedings of 2nd SIGHAN Workshop on Chinese 
Language Processing.  
Nianwen Xue and Martha Palmer. 2005. Automatic 
Semantic Role Labeling for Chinese verbs. In 
Proceedings of IJCAI 2005.  
Nianwen Xue. 2006a. Annotating the Predicate-
Argument Structure of Chinese Nominalizations. 
In Proceedings of the LREC 2006. 
Nianwen Xue. 2006b. Semantic Role Labeling of 
Nominalized Predicates in Chinese. In Proceed-
ings of HLT-NAACL 2006. 
Nianwen Xue. 2008. Labeling Chinese Predicates 
with Semantic Roles. Computational Linguistics, 
34(2):225-255. 
1288
