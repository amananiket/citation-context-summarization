Parsing Strategies with 'Lexicalized' Grammars: 
Appl icat ion to Tree Adjoining Grammars * 
Yves SCHABES,  Anne  ABE ILLE**and  Arav ind  K. JOSHI  
Department of Computer and Information Science 
University of Pennsylvania 
Philadelphia PA 19104-6389 USA 
schabes~linc.cis.upenn.edu abeille~cis.upenn.edu joshi~eis.upenn.edu 
ABSTRACT 
In this paper we present a general parsing strategy that 
arose from the development of an Earley-type parsing al- 
gorithm for TAGs (Schabes and Joshi 1988) and from re- 
cent linguistic work in TAGs (Abeille 1988). 
In our approach elementary structures are associated 
with their lexical heads. These structures specify extended 
domains of locality (as compared to a context-free gram- 
mar) over which constraints can be stated. These con- 
straints either hold within the elementary structure itself 
or specify what other structures can be composed with a 
given elementary structure. 
We state the conditions under which context-free based 
grammars can be 'lexicalized' without changing the lin- 
guistic structures originally produced. We argue that even 
if one extends the domain of locality of CFGs to trees, us- 
ing only substitution does not givo the freedom to choose 
the head of each structure. We show how adjunction al- 
lows us to 'lexicalize' a CFG freely. 
We then show how a 'lexicalized' grammar naturally 
follows from the extended omain of locality of TAGs and 
present some of the linguistic advantages ofour approach. 
A novel general parsing strategy for 'lexicalized' gram- 
mars is discussed. In a first stage, the parser builds a set 
structures corresponding to the input sentence and in a 
second stage, the sentence is parsed with respect o this 
set. The strategy is independent of the linguistic theory 
adopted and of the underlying rammar formalism. How- 
ever, we focus our attention on TAGs. Since the set of 
trees needed to parse an input sentence is supposed to be 
finite, the parser can use in principle any search strategy. 
Thus, in particular, a top-down strategy can be used since 
problems due to recursive structures are eliminated. The 
parser is also able to use non-local information to guide 
the search. 
We then explain how the Earley-type parser for TAGs 
can be modified to take advantage of this approach. 
*This work is partially supported by ARO grant DAA29-84-9- 
007, DARPA grant N0014-85-K0018, NSF grants MCS-82-191169 
and DGR-84-10413. The second author is also partially supported by 
J.W. Zellldja grant. The authors would llke to thank Mitch Marcus 
for his helpful conunents about this work. Thanks are also due to 
Ellen Hays. 
**Visiting from University of Paris VII. 
1 'Lexicalization' of grammar for- 
malisms 
Most of the current linguistics theories tend to give lexical 
accounts of several phenomena that used to be consid- 
ered purely syntactic. The information put in the lexi- 
con is therefore increased and complexified (e.g. lexical 
rules in LFG, used also by HPSG, or Gross 1984's lexicon- 
grammar). But the question of what it means to 'lexical- 
ize' a grammar is seldom addressed. The possible conse- 
quences of this question for parsing are not fully investi- 
gated. We present how to 'lexicalize' grammars uch as 
CFGs in a radical way, while possibly keeping the rules in 
their full generality. If one assumes that the input sentence 
is finite and that it cannot be syntactically infinitely am- 
biguous, the 'lexicalization' simplifies the task of a parser. 
We say that a grammar formalism is 'lexicalized' if it 
consists of: 
? a finite set of structures to be associated with lexical 
items, which usually will be heads of these structures, 
? an operation or operations for composing the 
structures. 1 The finite set of structures define the 
domain of locality over which constraints are speci- 
fied and these are local with respect o their lexical 
heads. 
Not every grammar formalism in a given form is in a 
'lexicalized' form. For example, a CFG, in general, will 
not be in a 'lexicalized' form. However, by extending its 
domain of locality, it can be 'lexicalized'. We require that 
the 'lexicalized' grammar produces not only the same lan- 
guage as the original grammar, but also the same struc- 
tures (or tree set)? 
We propose to study the conditions under which such 
a 'lexicalization' is possible for CFGs and TAGs. The 
domain of locality of a CFG can be extended by using 
a tree rewriting system that only uses substitution. We 
state the conditions under which CFGs can be 'lexlcalized' 
without changing the structures originally produced. We 
argue that even if one extends the domain of locality of 
CFGs to trees, using only substitution does not give the 
freedom to  choose the head of each structure. We then 
1 By 'lexicalization' we mean tlmt in each structure there is a lex- 
ical item that is realized. We do not mean just adding features (such 
as head) and unification equations to the rules of the formalism. 
Categorlal grammars are 'lexicaUzed' according to our definition, 
However, they do not correspond in a simple way to a rtde-based 
system that could be used for top-down recognition. 
57,~ 
show how adjunction enables one to freely 'lexicalize' a 
CFG. 
2 'Lexical izat ion'  of  CFGs  
The domain of locality of CFGs can be easily extended 
by using a tree rewriting grammar. This tree rewriting 
grammar consists of a set of trees that are not restricted to 
be of depth one (as in CFGs). It uses only substitution as 
a combining operation. Substitution can take place only 
on non-terminal nodes of the frontier of each tree. The 
language is defined to be the set of strings on the frontiers 
of trees whose roots are labeled by a distinguished symbol 
S. It is easy to see that the set of languages generated 
by this tree rewriting grammar is exactly the same set as 
context-free languages. 
If no recarsive chain rules exist, it is formally possible 
to 'lexicalize' a CFG with this tree rewriting grammar, a 
Recursive chain rules are disallowed since they introduce 
unbounded structures with no lexical items attached to 
them. 
Although a CFG can be 'lexicalized' by using trees, it is 
not possible to choose freely the lexical item that plays the 
role of the head for each structure. Consider the following 
example: 
S ~ NP  VP  
VP  "-~ adv VP  
VP  --+ v 
NP  --+ n 
The grammar can be qexicalized' as follows: 
S 
NP VP 
/ \  
adv VP 
VP VP NP 
A I I 
adv VP v n 
However, in this 'lexiealization' one is forced to choose 
adv as the head of the structure given in the first tree. It is 
not possible to choose the verb v as the head of this struc- 
ture. If one tried to do so, recursion on the substitution 
of the VP node would be inhibited. 
2"his example shows that although it is possible to 'lexi- 
calize' CFGs, substitution alone does not allow us to freely 
choose the lexical heads. Substitution alone forces us to 
make choices that might not be syntactically and seman- 
tically justified. 
Tree adjoining grammars (TAGs) are also a tree-based 
system, ltowever, the major composition operation in 
TAGs is adjoining or adjunct ion.  I t  builds a new tree 
from an auxiliary tree # and a tree c~ (a is any tree, initial, 
auxiliary or derived by adjunction). The resulting tree is 
called a derived tree. Let t~ be atree containing a node 
n labeled by X and let # be an auxiliary tree whose root 
node is also labeled by X. Then the adjunction of fl to a 
at node n results a tree 7 as shown in Figure 1. Adjunetion 
enables to factor recursion from local dependencies. 
aNote that  a CFG in Greibach normal  form can be 'lexicallzed' 
trivially. But since G~eihach normal  form of a given CFG might not 
generate the same tree set as  the  original grammar,  it cannot be 
used as a 8.eneral method for 'lexicaUzation'. 
(a) (g) 
A 
Figure 1: The mechanism of adjunction 
The previous CFG can be 'lexicalized' by using adjunc- 
tion as follows: 4
s 
NP VP NP VP 
I I A 
v n adv VP 
The auxiliary tree rooted by VP can be inserted in the 
S tree on tile VP node by adjunction. Using adjunction 
one is thus able to choose the appropriate l xical item as 
head. This example illustrates the fact that a CFG with 
no recursive chain rules can be 'lexicalized' in TAGs, and 
that if that is done the head can be freely chosen. 
3 TAGs  and ' lexical izat ion'  
TAGs are 'naturally' lexicalized because they used an ex- 
tended domain of locality. TAGs were first introduced 
by Joshi, Levy and Takabashi (1975) and Joshi (1985). 
For more details on the original definition of TAGs, we 
refer the reader to Joshi (1985), Kroch and Joshi (1985) 
or Vijay-Shanker (1987). It is known that Tree Adjoin- 
ing Languages (TALs) are mildly context-sensitive. TALs 
properly contain context-free languages. It is also possi- 
ble to encode a context-free grammar with auxiliary trees 
using adjunction only. However, although the languages 
correspond, the possible ncoding does not directly reflect 
the original context-free grammar since this encoding uses 
adjunction. 
Although adjunction is more powerful than substitution 
and could be used to simulate it, in recent linguistic work 
in TAG (Abeill~ 1988) substitution has been used in ad- 
dition to adjunction in order to obtain appropriate struc- 
tural descriptions in certain cases, such as verbs taking 
two sentential arguments (e.g. "John equates olving this 
problem with doing the impossible"). Adding substitution 
does not change the mathematical properties of TAGs. 
We describe very briefly the Tree Adjoining Grammar 
formalism with adjunction and substitution. 
A Tree Adjoining Grammar  is a tree-based system 
that consists of.three finite sets of trees: I, A and L. The 
trees in I O A tJ L are called e lementary trees. 
The trees in I are called initial trees. Initial trees rep- 
resent basic sententiai structures. They are usually con- 
sidered as projections of the verb and they take nominal 
4We chose v as lexical head of the S tree but  we could have chosen 
n instead (although it is not motivated). 
579 
complements. Initial trees (see the left tree in Figure 2) 
are rooted in S and their frontier consists of terminal sym- 
bols (including the empty string) and non-terminal nodes 
to be substituted for. 
The trees in A are called auxi l iary trees. They can 
represent constituents which ar e adjuncts to basic struc- 
tures (adverbial). They can also represent basic senten- 
tial structures corresponding to verbs or predicates taking 
sentential complements. Auxiliary trees (see right tree in 
Figure 2) are characterized as follows: 
? internal nodes are labeled by non-terminals; 
? leaf nodes are labeled by terminals or by non-terminal 
nodes to be substituted except for exactly one node 
(called the foot node) labeled by a non-terminal on 
which only adjunction can apply; furthermore the la- 
bel of the foot node is the same as the label of the 
root node. 
Initial ~:  Auxiliary ~c~: 
$ 
; ; 
substitution nodes 
Figure 2: Schematic initial and auxiliaxy trees 
The trees in L are called lexical trees. They repre- 
sent basic categories or constituents which serve as argu- 
ments, to initial or auxiliary trees. They are reduced to a 
pre-terminal node in the case of simple categories or are 
expanded into tree structures in the case 0f compounds. 
Structurally they are characterized the same way as initial 
trees except hat they are not necessary rooted by S. 
As noted in Section 2, the major composition operation 
in TAGs is adjunct ion.  
We define subst i tut ion in TAGs to take place on spec- 
ified nodes on the frontiers of elementary trees. When a 
node is marked to be substituted, no adjunction can take 
place on that node. Furthermore , substitution is always 
mandatory. In case of substitution on a node labeled by 
S (sentential complement), only trees derived from initial 
trees (therefore rooted by S) can be substituted. In all 
other cases, any tree derived from a lexlcal tree rooted 
by the same label as the given node can be substituted. 
The resulting tree is obtained by replacing the node by 
the derived tree. Substitution is illustrated in Figure 3. 
We conventionally mark substitution odes by a down 
ar row (~). 
We define the tree set  of a TAG G, T(G) to be the set 
of all derived trees starting from initial trees in I. Further- 
more, the str ing language generated by a TAG, ?:(G), 
is defined to be the set of all terminal strings of the trees 
in T(G). 
Grammar ules defined by the linguistic theory are not 
the same as the rules used by the parser--let us refer to 
them as parser  rules. A parser rule is defined to be 
a structure encoding a rule of the grammar (or a set of 
rules) instantiated bythe parser when it comes to alex- 
580 
/\ 
Figure 3: Mechanism of substitution 
ical item (considered to 'yield' the rule(s)). It is thus a 
unique object. It is individualized by the lexical item, 
which is itself individualized by its position in the input 
string. The lexical item is directly inserted into the struc- 
ture corresponding to the parser ule, and such a rule can 
only occur once. Lexleal i tems are differentiated by their 
realization in the input sentence and also their position in 
the sentence. Therefore a given rule corresponds to ex- 
actly one lexical item in the input sentence. 
The structures are produced by lexical items which serve 
as heads. If a structure has only one terminal, the terminal 
is the head of the structure; if there are several terminals, 
the choice of the head is linguistically motivated, e.g. by  
the principles of X theory. S also has to be considered as 
the projection of a lexical head, usually V. Each lexical 
item corresponds to as many entries as there are possible 
category or argument structures. 
The category structure is a lexical tree that is not 
necessarily reduced to a single category. It corresponds to 
the maximal projection of a category in the case of simple 
phrases, to the entire compound, in the case of compound 
categories. 
Category structures can be of two different kinds: 
? lexical trees reduced to a single category: ~ 
DET 
NP PP 
DET N of N$ DET 
, I I t hell) JR bunch(i )
? lexical trees that consist of a phrase: 
NP NP 
A I 
D~ N N 
I I 
boy (I) Ma r y (I) 
The argument  st ructure is not reduced to a list of 
arguments as the usual subcategorization frames. It is the 
syntactic structure constructed with the lexlcal value of 
the predicate and with all the nodes for its arguments. The 
argument structure for a predicate is its maximal struc- 
ture. An argument is present in the argumefit structure 
even if it is optional and its optionality is stated in the 
structure. 
SThe index in "parentheses on a lexical item that produces the 
structure ncodes the position of the lexical item in the string. 
A simple case of a argument structure is a verb with 
its subcategorized arguments. For example, the verb saw 
(at position i) generates the following structures (among 
others): 6 
S 8 
NPo$ VP 
V NPI$ V $ 
I I 
saw(l) eaw(i) 
The left structure corresponds to: 
0 Jolt. 1 aaw 2 llary a ( i=  2) 
and the other to: 
0 John  1 saw 2 that  3 Mary  4 le f t  5.  ( i - - - -2 )  
An argument structure can correspond to either one or 
a set of syntactic surface structures. The lexical head 
will then produce a set of possible trees, one for NP0 saw 
NP1 and another for whol did  NP 0 see e i ?, for exam- 
ple. If one defines principles for building such sets of 
trees, these principles will correspond to syntactic rules 
in a derivation-based theory of grammar. 
Category and argument structures thus instantiated as 
the parser scans the input string are combined together in 
a sentence t~tructure by adjoining or substituting. 
As Gross (1984), we consider verbs, nouns, and adjec- 
tives as predicates yielding sentences. They can take nomi- 
nal or sentential arguments. If the predicate takes nominal 
arguments it produces an initial tree. If it takes a senten- 
tial argument then it produces an auxiliary tree. Putting 
arguments into predicates i done by substituting nomi- 
nal arguments or by adjoining a predicate structure to its 
sentential argument. 
Adjuncts are represented as auxiliary trees rooted by 
the category of the node they are adjoined to. They are 
also produced by a head. They can be reduced to a basic 
category or take nominal or sentential arguments intro- 
duced by substitution. 
Example,~ of Adjuncts: 
S vp 
A A 
S VP PP S S 
A A A 
S ADV P NP$ SC S~ 
l I I probablYll) during(i) while(i) 
4 Parsing 'lexicalized' grammars 
If we have a 'lexicalized' grammar, the grammar of the 
parser can be reduced to a set of structures whose nature 
depends on the input string and whose size is proportional 
to the length of the sentence (if we suppose that the num- 
ber of structures associated with a lexical item is finite). 
Since each structure' ( rule') corresponds to a token in the 
?We put indices on categories to express syntactic roles (0 for 
subject, 1 for object). 
sentence, it can be used only once. Rules are now differen- 
tiated by their realization in the sentence. The number of 
rules that can be used for a given sentence is bounded and 
is proportional to the length of the sentence. Since each 
rule can be used once, recursion does not lead to the usual 
non-termination problem. Once a structure has been cho~ 
sen for a given token, the other possible structures for the 
same token do not participate in the parse. Of course, if 
the sentence is ambiguous, there may be more than one 
choice. 
If one adopts an off-line parsing algorithm, the parsing 
problem is reduced to the following two steps: 
t First produce the set of structures corresponding to
each word in the sentence. This step performs the role 
of an expanded morphological nalysis (or tagging). 
? Then put the argument structures into the predicate 
structures. This step performs a modified syntactic 
analysis. 
In principle any parsing strategy can be applied to execute 
the second step, since the number of structures produced 
js finite and since each of them corresponds to a token in 
the input string, the search space is finite and termination 
is guaranteed. In principle, one can proceed inside out, left 
to right or in any other way. Of course, standard parsing 
algorithm can be used too. In particular, we can use the 
top-down parsing strategy without encountering the usual 
problems due to recursion. Problems in the prediction step 
of the Earley parser used for unification-based formalisms 
no longer exist. The use of restrictors as proposed by 
Shieber (1985) is no longer necessary and the difficulties 
caused by treating subcategorization as a feature is no 
longer a problem. 
By assuming that the number of structures associated 
with a lexical item is finite, since each structure has a lexi- 
cal item attached to it, we implicitly make the assumption 
that an input string of finite length cannot be syntactically 
infinitely ambiguous. 
Since the trees are produced by the input string, the 
parser can use information that might be non-local to 
guide the search. For example, consider the language gen- 
erated by the following CFG (example due to Mitch Mar- 
cus): 
S ~ A IB  
A ~ aAlax 
B -* aBlay 
This grammar generates the language:{a*x} U {a'y}. 
In a standard CFG parsing algorithm, A's and B's will be 
built until the last token in the input (x or y) is recog- 
nized. It would require unbounded look-ahead to decide 
which rule (3 -+ A or S ~ B) to choose. One can encode 
the grammar in TAG as follows: 
S S A B 
A A 
A A 
a x a y 
Suppose that the heads of the initial trees are respec- 
tively x and y and that a is the head of both auxiliary 
581 
trees. Then, if the elementary trees are built according to 
the input string, and if a top-down strategy is used, only 
A or B trees will be built. 
An application concerns the parsing of discontinuous 
constituents. They are recognized even if there are un- 
bounded insertions between their components and even if 
their 'head' is the last element of the string. 
In the two-step strategy described here, before the first 
step is taken, there is no grammar. After the first step, we 
have a grammar whose size is proportional to the length 
of the input string. The size of the grammar to be taken 
into consideration i  the analysis of the parsing complexity 
of grammar formalisms has been reduced to an amount 
proportional to the length of the input. Although we have 
not yet investigated the implication of this approach on 
some complexity results, we feel that some of them might 
be improved. 
It is possible to express the parsing problem in a de- 
cidable deduction system on trees (similar to Lambek's 
deduction system on categories (1958 and 1961)). The 
grammar can be thought as a five-tuple (VN, ~, O, S, Lex) 
where: 
? VN is a finite set of non-terminal symbols, 
? ~ is a finite set of alphabet symbols, 
? O is the set of trees constructed with P,* and VN (the 
elements of Z* having ranked 0). 
? Lex is the lexicon , i.e. a function from lexical items 
to finite subsets of O: P?' --+ 2?(finite). 
A sequent is defined to be of the form: 
Vl,.. -, rn ~ A, where ri E O and A E VN 
Two inference rules combine two trees of the left hand 
side to form a new one. One inference rule corresponds 
to adjunction of two trees, and the other to substitution of 
a node in one tree by the other tree. Once two trees are 
combined, they are replaced by the resulting tree in the 
left hand side of the seouent. This facts takes into account 
that each tree corresponds to a single lexical item in the 
input string. Therefore each tree can be used only once. 
Axioms of the system are of the form: 
v ---+ A 
where r is a completed tree rooted by A. 
The sequent 
T1," ? " ,Tn "----+ A 
is said to be provable if the sequent can be reduced (by 
the inference rules) to an axiom; we write: 
~- r l , . . . , r ,  --+ A. 
Since there are finitely many ways to combine a finite num- 
ber of trees with each other, the system is decidable. 
The language generated by such system is defined to be: 
= {a i , ' . .  ,anl3rl e Lex(al) s. t. ~- r l , ' " , rn  ----+ S} 
Also, one can state a necessary condition on the correct- 
ness of a sentence similar to the category count theorem 
of van Benthem (1985 and 1986). 
5 Extending the Earley-type 
parser for TAGs 
An Earley-type parser for TAGs has been proposed by 
Schabes and Joshi (1988a). It takes as input a TAG and 
a sentence to be parsed. It places no restrictions on the 
grammar. The algorithm is a bottom-up arser that uses 
top-down filtering. It is able to parse constraints on ad- 
junction, substitution and feature structures for TAGs as 
defined by Vijay-Shanker (1987) and Vijay-Shanker and 
Joshi (1988). It is able to parse directly CFGs and TAGs. 
Thus it embeds the essential aspects of PATR-II as defined 
by Shieber (1984 and 1986). Its correctness was proven in 
Sehabes and Joshi (1988b). The concepts of dotted rule 
and states have been extended to TAG trees. The algo- 
rithm as described by Schabes and Joshi (1988a) manip- 
ulates states of the form: 
s = \[a, dot, side, pos, l, fl, fi, star, t\[, b\[, snbst?\] 
where a is a tree, dot is the address of the dot in the tree, 
side is the side of the symbol the dot is on (left or right), 
pos is the position of the dot (above or below), star is an 
address in a and l, f~, fr, star, t~, b~ are indices of positions 
in the input string. The variable subst? is a boolean that 
indicates whether the tree has been predicted for substi- 
tution. 
The algorithm uses nine processes: 
? The Scanner  allows lexical items to be recognized. 
? Move dot  down and Move dot  up perform a tree 
traversal that allow the parser to scan the input from 
left to right. 
? The Left P red ic tor  predicts an adjunetion if it is 
possible. 
? Suppose that the auxiliary tree that we left-predicted 
has been recognized as far as its foot, then the Left 
Completor  tries to recognize what was pushed under 
the foot. 
? Once the subtree pushed under the foot has been rec- 
ognized, the R ight  P red ic tor  tries to recognize the 
other half of the auxiliary tree. 
? If the auxiliary tree has been totally recognized, the 
Right  Completor  tries to recognize the rest of the 
tree in which the auxiliary tree has been adjoined. 
? The Subst i tu t ion  Pred ic tor  performs the same op- 
erations as Earley's original predictor. It predicts for 
substitution (when appropriate) all lexical trees or ini- 
tial trees that could be substituted. 
? If the tree that we predicted for substitution has 
been totally recognized, the Subst i tu t ion  Comple-  
tor  tries to recognize the rest of the tree in which we 
predicted a substitution. 
The Earley-type parser can be extended to take advan- 
tage of  the lexicon-based strategy proposed earlier. Once 
the input string has been scanned and the corresponding 
elementary trees have been built, the parser will proceed 
bottom-up using the top-down filtering from the initial 
trees that have been built. In order to take into account 
that each tree is unique and therefore can be used only 
once, a new component r is added to the states. A state 
is now defined to be: 
s = \[a, dot, side, pos, l, fl, fr, star, t~, b~, subst?, r\] 
r encodes the trees corresponding to the input string that 
have not yet been used: 
r ~--- {{"/11, """ , '~'lk},"" : ,  { '~ml,"""  , '~mk}} 
where {7i l , ' " ,7~j} is the set of trees generated by the 
lexical item a~. 
582 
The left predictor must be modified so that it predicts 
only trees that are in the set F of the given state. As soon 
as one tree (say 7in) is used, the entire set of trees cor- 
responding to the same token ({711," ' ,7i j})  cannot be 
used later on. Of course, all competitive paths are taken 
in parallel as in the usual Earley parser. The way that 
F is modified by the Left Predictor is illustrated in the 
following figure: 
A addedtoSi 
r=((~ ,...a } ..... {~ ..... ~ ), ..., (v ..... v 1} r=({~ ,...,~ } ........ (~ ..... r }1 
n lr il is ml mt 11 lr ml rat 
Figure 4: Update of F in the Left Predictor 
The tree 71u is predicted and therefore the trees corre- 
sponding to the token ai ({ 'Y / l , - ' " ,  "/is}) are removed from 
1 a" 
The scanner must also be slightly modified since the 
head of the structure is differentiated not only by its lexical 
value but al,'~o by its position in the string. 
6 Conclusion 
In this paper we presented a general parsing strategy based 
on 'lexicalized' grammar. We defined the notion of lexi- 
calization of a grammar. We showed how a CFG can be 
'lexicalized' by using only substitution. But the use of 
adjunction permits 'lexicalization' with linguistically mo- 
tivated structures. TAGs have been shown to be naturally 
'lexicalized'. Then we gave an overview of the specific lex- 
icon of TAGs. The %xicalization ~ of grammar lead us to 
introduce a two step parsing strategy. The first step picks 
up the set of structures corresponding to each word in the 
sentence. The second step puts the argument structures 
into predicate structures. Therefore, the relationship be- 
tween the morphological nd syntactic analyses has been 
modified. In the first step, structures instead of categories 
are associated with lexical items. The strategy has been 
shown to be able to use non-local information in the in- 
put string. Also problems due to recursion are eliminated. 
The grammar of the parser has been reduced to a set of 
structures whose size is proportional to the length Of the 
input sentence. Furthermore, the parsing strategy applies 
to any parsing algorithm; in particular top-down. It can 
be formalized into a decidable deduction system that has 
finite search space for a sentence of finite length. The 
Earley-type parser for TAGs has been easily extended to 
take advantage of this strategy. 
R e f e r e n c e s  
Abeilld, Anne, 1988. Parsing French with Tree Adjoining 
Grammar: some Linguistic Accounts. In Proceeding of the 12 ?' 
International Conference on Computational Linguistics. 
van Benthem, Johns, 1985. Lambek Calculus. Manuscript, 
Filosofisch Instituut, Rijks Universiteit, Groningen. 
van Benthem, Johan, 1986. Essays on Logical Semantics, 
Chapter 7, pages 123-150. D. Reidel Publishing Company. 
Gross, Manriee, 1984. Lexicon-Grammar nd the Syntactic 
Analysis of French. In Proceeding of the 10 th International 
Conference on Computational Linguistics. 
Joshi, Aravind K., 1985. How Much Context-Sensitivlty is Nec- 
essary for Characterizing Structural Descriptions--Tree Ad- 
joining Grammars. In Dowry, D.; Karttunen, L.; and Zwicky, 
A. (editors), Natural Language Processing--Theoretical, Com- 
putational and Psychological Perspectives. Cambridge Univer- 
sity Press, New York. Originally presented in 1983. 
Joshi, A. K.; Levy, L. S.; and Ta~ahashi, M, 1975. Tree Ad- 
junct Grammars. J Comput. Syst. Sci. 10(1). 
Kroch, A. and Joshi, A. K., 1985. Linguistic Relevance of 
Tree Adjoining Grammars. Technical Report MS-CIS-85-18, 
Department ofComputer and Information Science, University 
of Pennsylvania. 
Lambek, Joachim, 1958. The Mathematics of Sentence Struc- 
ture. American Mathematical Monthly 65:154-170. 
Lambek, Joachim, 1961. On the Calculus of Syntactic Types. 
In Proceedings of the Symposium on Applied Mathematics, 
pages 166-178. 
Schabes, Yves and Joshi, Aravind K., 1988 (a). An Earley- 
Type Parsing Algorithm for Tree Adjoining Grammars. In 26 th 
Meeting of the Association for Computational Linguistics. 
Schabes, Yves and Joshi, Aravind K., \]988 (b). An Earley- 
type Parser for Tree Adjoining Grammars. Technical Report, 
Department of Computer and Information Science, University 
of Pennsylvania. 
Shieber, Stuart M., 1984. The Design of a Computer Language 
for Linguistic Information. In 22 ~d Meeting of the Association 
for Computational Linguistics, pages 362-366. 
Shieber, Stuart M., 1985. Using Restriction to Extend Pars- 
ing Algorithms for Complex-feature-based Formalisms. In 
23 rd Meeting of the Association for Computational Linguistics, 
pages 82-93. 
Shieber, Stuart M., 1986. An Introduction to Unification-Based 
Approaches to Grammar. Center for the Study of Language 
and Information, Stanford, CA. 
Vijay-Shanker, K., 1987. A Study of Tree Adjoining Gram- 
mars. PhD thesis, Department of Computer and Information 
Science, University of Pennsylvania. 
Vijay-Shanker, K. and Joshi, A.K., 1988. Feature Structure 
Baaed Tree Adjoining Grammars. In Proceedings of the 12 th 
International Conference on Computational Linguistics. 
583 
