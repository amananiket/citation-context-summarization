Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 200?207,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Grammar-driven Convolution Tree Kernel for Se-
mantic Role Classification 
 
Min ZHANG1     Wanxiang CHE2     Ai Ti AW1     Chew Lim TAN3     
Guodong ZHOU1,4     Ting LIU2     Sheng LI2     
 
1Institute for Infocomm Research   
{mzhang, aaiti}@i2r.a-star.edu.sg 
2Harbin Institute of Technology 
{car, tliu}@ir.hit.edu.cn   
lisheng@hit.edu.cn 
3National University of Singapore 
tancl@comp.nus.edu.sg 
4 Soochow Univ., China 215006 
gdzhou@suda.edu.cn 
 
 
 
 
 
Abstract 
Convolution tree kernel has shown promis-
ing results in semantic role classification. 
However, it only carries out hard matching, 
which may lead to over-fitting and less ac-
curate similarity measure. To remove the 
constraint, this paper proposes a grammar-
driven convolution tree kernel for semantic 
role classification by introducing more lin-
guistic knowledge into the standard tree 
kernel. The proposed grammar-driven tree 
kernel displays two advantages over the pre-
vious one: 1) grammar-driven approximate 
substructure matching and 2) grammar-
driven approximate tree node matching. The 
two improvements enable the grammar-
driven tree kernel explore more linguistically 
motivated structure features than the previ-
ous one. Experiments on the CoNLL-2005 
SRL shared task show that the grammar-
driven tree kernel significantly outperforms 
the previous non-grammar-driven one in 
SRL. Moreover, we present a composite 
kernel to integrate feature-based and tree 
kernel-based methods. Experimental results 
show that the composite kernel outperforms 
the previously best-reported methods. 
1 Introduction 
Given a sentence, the task of Semantic Role Label-
ing (SRL) consists of analyzing the logical forms 
expressed by some target verbs or nouns and some 
constituents of the sentence. In particular, for each 
predicate (target verb or noun) all the constituents in 
the sentence which fill semantic arguments (roles) 
of the predicate have to be recognized. Typical se-
mantic roles include Agent, Patient, Instrument, etc. 
and also adjuncts such as Locative, Temporal, 
Manner, and Cause, etc. Generally, semantic role 
identification and classification are regarded as two 
key steps in semantic role labeling. Semantic role 
identification involves classifying each syntactic 
element in a sentence into either a semantic argu-
ment or a non-argument while semantic role classi-
fication involves classifying each semantic argument 
identified into a specific semantic role. This paper 
focuses on semantic role classification task with the 
assumption that the semantic arguments have been 
identified correctly. 
Both feature-based and kernel-based learning 
methods have been studied for semantic role classi-
fication (Carreras and M?rquez, 2004; Carreras and 
M?rquez, 2005). In feature-based methods, a flat 
feature vector is used to represent a predicate-
argument structure while, in kernel-based methods, 
a kernel function is used to measure directly the 
similarity between two predicate-argument struc-
tures. As we know, kernel methods are more effec-
tive in capturing structured features. Moschitti 
(2004) and Che et al (2006) used a convolution 
tree kernel (Collins and Duffy, 2001) for semantic 
role classification. The convolution tree kernel 
takes sub-tree as its feature and counts the number 
of common sub-trees as the similarity between two 
predicate-arguments. This kernel has shown very 
200
promising results in SRL. However, as a general 
learning algorithm, the tree kernel only carries out 
hard matching between any two sub-trees without 
considering any linguistic knowledge in kernel de-
sign. This makes the kernel fail to handle similar 
phrase structures (e.g., ?buy a car? vs. ?buy a red 
car?) and near-synonymic grammar tags (e.g., the 
POS variations between ?high/JJ degree/NN? 1 and 
?higher/JJR degree/NN?) 2. To some degree, it may 
lead to over-fitting and compromise performance. 
This paper reports our preliminary study in ad-
dressing the above issue by introducing more lin-
guistic knowledge into the convolution tree kernel. 
To our knowledge, this is the first attempt in this 
research direction. In detail, we propose a gram-
mar-driven convolution tree kernel for semantic 
role classification that can carry out more linguisti-
cally motivated substructure matching. Experimental 
results show that the proposed method significantly 
outperforms the standard convolution tree kernel on 
the data set of the CoNLL-2005 SRL shared task. 
The remainder of the paper is organized as fol-
lows: Section 2 reviews the previous work and Sec-
tion 3 discusses our grammar-driven convolution 
tree kernel. Section 4 shows the experimental re-
sults. We conclude our work in Section 5. 
2 Previous Work 
Feature-based Methods for SRL: most features 
used in prior SRL research are generally extended 
from Gildea and Jurafsky (2002), who used a linear 
interpolation method and extracted basic flat fea-
tures from a parse tree to identify and classify the 
constituents in the FrameNet (Baker et al, 1998). 
Here, the basic features include Phrase Type, Parse 
Tree Path, and Position. Most of the following work 
focused on feature engineering (Xue and Palmer, 
2004; Jiang et al, 2005) and machine learning 
models (Nielsen and Pradhan, 2004; Pradhan et al, 
2005a). Some other work paid much attention to the 
robust SRL (Pradhan et al, 2005b) and post infer-
ence (Punyakanok et al, 2004). These feature-
based methods are considered as the state of the art 
methods for SRL. However, as we know, the stan-
dard flat features are less effective in modeling the 
                                                          
1 Please refer to http://www.cis.upenn.edu/~treebank/ for the 
detailed definitions of the grammar tags used in the paper. 
2 Some rewrite rules in English grammar are generalizations of 
others: for example, ?NP? DET JJ NN? is a specialized ver-
sion of ?NP? DET NN?. The same applies to POS. The stan-
dard convolution tree kernel is unable to capture the two cases. 
syntactic structured information. For example, in 
SRL, the Parse Tree Path feature is sensitive to 
small changes of the syntactic structures. Thus, a 
predicate argument pair will have two different 
Path features even if their paths differ only for one 
node. This may result in data sparseness and model 
generalization problems. 
Kernel-based Methods for SRL: as an alternative, 
kernel methods are more effective in modeling 
structured objects. This is because a kernel can 
measure the similarity between two structured ob-
jects using the original representation of the objects 
instead of explicitly enumerating their features. 
Many kernels have been proposed and applied to 
the NLP study. In particular, Haussler (1999) pro-
posed the well-known convolution kernels for a 
discrete structure. In the context of it, more and 
more kernels for restricted syntaxes or specific do-
mains (Collins and Duffy, 2001; Lodhi et al, 2002; 
Zelenko et al, 2003; Zhang et al, 2006) are pro-
posed and explored in the NLP domain. 
Of special interest here, Moschitti (2004) proposed 
Predicate Argument Feature (PAF) kernel for SRL 
under the framework of convolution tree kernel. He 
selected portions of syntactic parse trees as predicate-
argument feature spaces, which include salient sub-
structures of predicate-arguments, to define convo-
lution kernels for the task of semantic role classifi-
cation. Under the same framework, Che et al (2006) 
proposed a hybrid convolution tree kernel, which 
consists of two individual convolution kernels: a Path 
kernel and a Constituent Structure kernel. Che et al 
(2006) showed that their method outperformed PAF 
on the CoNLL-2005 SRL dataset.  
The above two kernels are special instances of 
convolution tree kernel for SRL. As discussed in 
Section 1, convolution tree kernel only carries out 
hard matching, so it fails to handle similar phrase 
structures and near-synonymic grammar tags. This 
paper presents a grammar-driven convolution tree 
kernel to solve the two problems 
3 Grammar-driven Convolution Tree 
Kernel 
3.1 Convolution Tree Kernel 
In convolution tree kernel (Collins and Duffy, 
2001), a parse tree T  is represented by a vector of 
integer counts of each sub-tree type (regardless of 
its ancestors): ( )T? = ( ?, # subtreei(T), ?), where 
201
# subtreei(T) is the occurrence number of the ith 
sub-tree type (subtreei) in T. Since the number of 
different sub-trees is exponential with the parse tree 
size, it is computationally infeasible to directly use 
the feature vector ( )T? . To solve this computa-
tional issue, Collins and Duffy (2001) proposed the 
following parse tree kernel to calculate the dot 
product between the above high dimensional vec-
tors implicitly. 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
( , ) ( ), ( )
 ( ) ( )
 ( , )
(( ) ( ))
i isubtree subtreei n N n N
n N n N
K T T T T
I n I n
n n
? ?
? ?
? ?
=< >
=
= ?
?? ? ?
? ?
 
where N1 and N2 are the sets of nodes in trees T1 and 
T2, respectively, and ( )
isubtree
I n  is a function that is 
1 iff the subtreei occurs with root at node n and zero 
otherwise, and 1 2( , )n n?  is the number of the com-
mon subtrees rooted at n1 and n2, i.e., 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? = ??  
1 2( , )n n? can be further computed efficiently by the 
following recursive rules: 
Rule 1: if the productions (CFG rules) at 1n  and 
2n  are different, 1 2( , ) 0n n? = ; 
Rule 2: else if both 1n  and 2n  are pre-terminals 
(POS tags), 1 2( , ) 1n n ?? = ? ; 
Rule 3: else,  
1( )
1 2 1 21
( , ) (1 ( ( , ), ( , )))nc n
j
n n ch n j ch n j?
=
? = + ?? ,  
where 1( )nc n is the child number of 1n , ch(n,j) is 
the jth child of node n  and ? (0< ? <1) is the decay 
factor in order to make the kernel value less vari-
able with respect to the subtree sizes. In addition, 
the recursive Rule 3 holds because given two 
nodes with the same children, one can construct 
common sub-trees using these children and com-
mon sub-trees of further offspring. The time com-
plexity for computing this kernel is 1 2(| | | |)O N N? . 
3.2 Grammar-driven Convolution Tree 
Kernel 
This Subsection introduces the two improvements 
and defines our grammar-driven tree kernel. 
 
Improvement 1: Grammar-driven approximate 
matching between substructures. The conven-
tional tree kernel requires exact matching between 
two contiguous phrase structures. This constraint 
may be too strict. For example, the two phrase 
structures ?NP?DT JJ NN? (NP?a red car) and 
?NP?DT NN? (NP->a car) are not identical, thus 
they contribute nothing to the conventional kernel 
although they should share the same semantic role 
given a predicate. In this paper, we propose a 
grammar-driven approximate matching mechanism 
to capture the similarity between such kinds of 
quasi-structures for SRL. 
First, we construct reduced rule set by defining 
optional nodes, for example, ?NP->DT [JJ] NP? or 
?VP-> VB [ADVP]  PP?, where [*] denotes op-
tional nodes. For convenience, we call ?NP-> DT 
JJ NP? the original rule and ?NP->DT [JJ] NP? the 
reduced rule. Here, we define two grammar-driven 
criteria to select optional nodes: 
1) The reduced rules must be grammatical. It 
means that the reduced rule should be a valid rule 
in the original rule set. For example, ?NP->DT [JJ] 
NP? is valid only when ?NP->DT NP? is a valid 
rule in the original rule set while ?NP->DT [JJ 
NP]? may not be valid since ?NP->DT? is not a 
valid rule in the original rule set. 
2) A valid reduced rule must keep the head 
child of its corresponding original rule and has at 
least two children. This can make the reduced rules 
retain the underlying semantic meaning of their 
corresponding original rules. 
Given the reduced rule set, we can then formu-
late the approximate substructure matching mecha-
nism as follows: 
11 2 1 2,
( , ) ( ( , ) )
a bi ji j
T r ri j
M r r I T T ?
+
= ??              (1)  
where 1r is a production rule, representing a sub-tree 
of depth one3, and 1
i
rT is the i
th variation of the sub-
tree 1r by removing one ore more optional nodes
4, 
and likewise for 2r and 2
j
rT . ( , )TI ? ? is a function 
that is 1 iff the two sub-trees are identical and zero 
otherwise. 1? (0? 1? ?1) is a small penalty to penal-
                                                          
3 Eq.(1) is defined over sub-structure of depth one. The ap-
proximate matching between structures of depth more than one 
can be achieved easily through the matching of sub-structures 
of depth one in the recursively-defined convolution kernel. We 
will discuss this issue when defining our kernel. 
4 To make sure that the new kernel is a proper kernel, we have 
to consider all the possible variations of the original sub-trees. 
Training program converges only when using a proper kernel. 
202
ize optional nodes and the two parameters ia  and 
jb stand for the numbers of occurrence of removed 
optional nodes in subtrees 1
i
rT and 2
j
rT , respectively. 
1 2( , )M r r returns the similarity (ie., the kernel 
value) between the two sub-trees 1r and 2r  by sum-
ming up the similarities between all possible varia-
tions of the sub-trees 1r and 2r . 
Under the new approximate matching mecha-
nism, two structures are matchable (but with a small 
penalty 1? ) if the two structures are identical after 
removing one or more optional nodes. In this case, 
the above example phrase structures ?NP->a red 
car? and ?NP->a car? are matchable with a pen-
alty 1?  in our new kernel. It means that one co-
occurrence of the two structures contributes 1?  to 
our proposed kernel while it contributes zero to the 
traditional one. Therefore, by this improvement, our 
method would be able to explore more linguistically 
appropriate features than the previous one (which is 
formulated as 1 2( , )TI r r ). 
Improvement 2: Grammar-driven tree nodes ap-
proximate matching. The conventional tree kernel 
needs an exact matching between two (termi-
nal/non-terminal) nodes. But, some similar POSs 
may represent similar roles, such as NN (dog) and 
NNS (dogs). In order to capture this phenomenon, 
we allow approximate matching between node fea-
tures. The following illustrates some equivalent 
node feature sets:  
? JJ, JJR, JJS 
? VB, VBD, VBG, VBN, VBP, VBZ 
? ?? 
where POSs in the same line can match each other 
with a small penalty 0? 2? ?1. We call this case 
node feature mutation. This improvement further 
generalizes the conventional tree kernel to get bet-
ter coverage. The approximate node matching can 
be formulated as: 
21 2 1 2,
( , ) ( ( , ) )
a bi ji j
fi j
M f f I f f ?
+
= ??           (2) 
where 1f is a node feature, 1
if is the ith mutation 
of 1f and ia is 0 iff 1
if and 1f are identical and 1 oth-
erwise, and likewise for 2f . ( , )fI ? ? is a function 
that is 1 iff the two features are identical and zero 
otherwise. Eq. (2) sums over all combinations of 
feature mutations as the node feature similarity. 
The same as Eq. (1), the reason for taking all the 
possibilities into account in Eq. (2) is to make sure 
that the new kernel is a proper kernel.  
The above two improvements are grammar-
driven, i.e., the two improvements retain the under-
lying linguistic grammar constraints and keep se-
mantic meanings of original rules. 
 
The Grammar-driven Kernel Definition: Given 
the two improvements discussed above, we can de-
fine the new kernel by beginning with the feature 
vector representation of a parse tree T as follows: 
( )T? =? (# subtree1(T), ?, # subtreen(T))       
where # subtreei(T) is the occurrence number of the 
ith sub-tree type (subtreei) in T. Please note that, 
different from the previous tree kernel, here we 
loosen the condition for the occurrence of a subtree 
by allowing both original and reduced rules (Im-
provement 1) and node feature mutations (Im-
provement 2). In other words, we modify the crite-
ria by which a subtree is said to occur. For example, 
one occurrence of the rule ?NP->DT JJ NP? shall 
contribute 1 times to the feature ?NP->DT JJ NP? 
and 1?  times to the feature ?NP->DT NP? in the 
new kernel while it only contributes 1 times to the 
feature ?NP->DT JJ NP? in the previous one. Now 
we can define the new grammar-driven kernel 
1 2( , )GK T T as follows: 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
( , ) ( ), ( )
( ) ( )
 ( , )
(( ) ( ))
i i
G
subtree subtreei n N n N
n N n N
K T T T T
I n I n
n n
? ?
? ?
? ?
? ?=< >
? ?=
?= ?
?? ? ?
? ?
 (3) 
where N1 and N2 are the sets of nodes in trees T1 and 
T2, respectively. ( )
isubtree
I n?  is a function that is 
1 2
a b? ?? iff the subtreei occurs with root at node n 
and zero otherwise, where a and b are the numbers 
of removed optional nodes and mutated node fea-
tures, respectively. 1 2( , )n n??  is the number of the 
common subtrees rooted at n1 and n2, i.e. , 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? ? ?? = ??         (4) 
Please note that the value of 1 2( , )n n?? is no longer 
an integer as that in the conventional one since op-
tional nodes and node feature mutations are consid-
ered in the new kernel. 1 2( , )n n??  can be further 
computed by the following recursive rules:  
 
203
============================================================================ 
Rule A: if 1n and 2n are pre-terminals, then: 
1 2 1 2( , ) ( , )n n M f f??? = ?                          (5) 
where 1f and 2f are features of nodes 1n and 2n re-
spectively, and 1 2( , )M f f  is defined at Eq. (2).  
Rule B: else if both 1n and 2n are the same non-
terminals, then generate all variations of the subtrees 
of depth one rooted by 1n and 2n (denoted by 1nT  
and 2nT  respectively) by removing different optional 
nodes, then: 
 
1
1
1 2 1 2,
( , )
1 21
( , ) ( ( , )
   (1 ( ( , , ), ( , , )))
a bi ji j
T n ni j
nc n i
k
n n I T T
ch n i k ch n j k
? ?
+
=
?? = ? ?
?? + ?
?
?
(6) 
 
where  
? 1inT and 2jnT stand for the ith and jth variations in 
sub-tree set 1nT and 2nT , respectively. 
? ( , )TI ? ? is a function that is 1 iff the two sub-
trees are identical and zero otherwise.  
? ia and jb stand for the number of removed op-
tional nodes in subtrees 1
i
nT and 2
j
nT , respectively. 
? 1( , )nc n i returns the child number of 1n in its ith 
subtree variation 1
i
nT . 
? 1( , , )ch n i k  is the kth child of node 1n  in its ith 
variation subtree 1
i
nT , and likewise for 2( , , )ch n j k . 
? Finally, the same as the previous tree kernel, 
? (0< ? <1) is the decay factor (see the discussion 
in Subsection 3.1). 
 
Rule C: else 1 2( , ) 0n n?? =  
  
============================================================================ 
 
Rule A accounts for Improvement 2 while Rule 
B accounts for Improvement 1. In Rule B, Eq. (6) 
is able to carry out multi-layer sub-tree approxi-
mate matching due to the introduction of the recur-
sive part while Eq. (1) is only effective for sub-
trees of depth one. Moreover, we note that Eq. (4) 
is a convolution kernel according to the definition 
and the proof given in Haussler (1999), and Eqs (5) 
and (6) reformulate Eq. (4) so that it can be com-
puted efficiently, in this way, our kernel defined by 
Eq (3) is also a valid convolution kernel. Finally, 
let us study the computational issue of the new 
convolution tree kernel. Clearly, computing Eq. (6) 
requires exponential time in its worst case. How-
ever, in practice, it may only need  1 2(| | | |)O N N? . 
This is because there are only 9.9% rules (647 out 
of the total 6,534 rules in the parse trees) have op-
tional nodes and most of them have only one op-
tional node. In fact, the actual running time is even 
much less and is close to linear in the size of the 
trees since 1 2( , ) 0n n?? =  holds for many node 
pairs (Collins and Duffy, 2001). In theory, we can 
also design an efficient algorithm to compute Eq. 
(6) using a dynamic programming algorithm (Mo-
schitti, 2006). We just leave it for our future work. 
3.3 Comparison with previous work 
In above discussion, we show that the conventional 
convolution tree kernel is a special case of the 
grammar-driven tree kernel. From kernel function 
viewpoint, our kernel can carry out not only exact 
matching (as previous one described by Rules 2 
and 3 in Subsection 3.1) but also approximate 
matching (Eqs. (5) and (6) in Subsection 3.2). From 
feature exploration viewpoint, although they ex-
plore the same sub-structure feature space (defined 
recursively by the phrase parse rules), their feature 
values are different since our kernel captures the 
structure features in a more linguistically appropri-
ate way by considering more linguistic knowledge 
in our kernel design. 
Moschitti (2006) proposes a partial tree (PT) 
kernel which can carry out partial matching be-
tween sub-trees. The PT kernel generates a much 
larger feature space than both the conventional and 
the grammar-driven kernels. In this point, one can 
say that the grammar-driven tree kernel is a spe-
cialization of the PT kernel. However, the impor-
tant difference between them is that the PT kernel 
is not grammar-driven, thus many non-
linguistically motivated structures are matched in 
the PT kernel. This may potentially compromise 
the performance since some of the over-generated 
features may possibly be noisy due to the lack of 
linguistic interpretation and constraint. 
Kashima and Koyanagi (2003) proposed a con-
volution kernel over labeled order trees by general-
izing the standard convolution tree kernel. The la-
beled order tree kernel is much more flexible than 
the PT kernel and can explore much larger sub-tree 
features than the PT kernel. However, the same as 
the PT kernel, the labeled order tree kernel is not 
grammar-driven. Thus, it may face the same issues 
204
(such as over-generated features) as the PT kernel 
when used in NLP applications. 
 Shen el al. (2003) proposed a lexicalized tree 
kernel to utilize LTAG-based features in parse 
reranking. Their methods need to obtain a LTAG 
derivation tree for each parse tree before kernel 
calculation. In contrast, we use the notion of op-
tional arguments to define our grammar-driven tree 
kernel and use the empirical set of CFG rules to de-
termine which arguments are optional. 
4 Experiments 
4.1 Experimental Setting 
Data: We use the CoNLL-2005 SRL shared task 
data (Carreras and M?rquez, 2005) as our experi-
mental corpus. The data consists of sections of the 
Wall Street Journal part of the Penn TreeBank 
(Marcus et al, 1993), with information on predi-
cate-argument structures extracted from the Prop-
Bank corpus (Palmer et al, 2005). As defined by 
the shared task, we use sections 02-21 for training, 
section 24 for development and section 23 for test. 
There are 35 roles in the data including 7 Core 
(A0?A5, AA), 14 Adjunct (AM-) and 14 Reference 
(R-) arguments. Table 1 lists counts of sentences 
and arguments in the three data sets. 
  
 Training Development Test
Sentences 39,832 1,346 2,416
Arguments 239,858 8,346 14,077
Table 1: Counts on the data set 
 
We assume that the semantic role identification 
has been done correctly. In this way, we can focus 
on the classification task and evaluate it more accu-
rately. We evaluate the performance with Accu-
racy. SVM (Vapnik, 1998) is selected as our classi-
fier and the one vs. others strategy is adopted and 
the one with the largest margin is selected as the 
final answer. In our implementation, we use the bi-
nary SVMLight (Joachims, 1998) and modify the 
Tree Kernel Tools (Moschitti, 2004) to a grammar-
driven one. 
 
Kernel Setup: We use the Constituent, Predicate, 
and Predicate-Constituent related features, which 
are reported to get the best-reported performance 
(Pradhan et al, 2005a), as the baseline features. We 
use Che et al (2006)?s hybrid convolution tree ker-
nel (the best-reported method for kernel-based 
SRL) as our baseline kernel. It is defined as 
(1 )  (0 1)hybrid path csK K K? ? ?= + ? ? ? (for the de-
tailed definitions of pathK and csK , please refer to 
Che et al (2006)). Here, we use our grammar-
driven tree kernel to compute pathK and csK , and we 
call it grammar-driven hybrid tree kernel while Che 
et al (2006)?s is non-grammar-driven hybrid convo-
lution tree kernel.  
We use a greedy strategy to fine-tune parameters. 
Evaluation on the development set shows that our 
kernel yields the best performance when ? (decay 
factor of tree kernel), 1? and 2? (two penalty factors 
for the grammar-driven kernel), ? (hybrid kernel 
parameter) and c (a SVM training parameter to 
balance training error and margin) are set to 0.4, 
0.6, 0.3, 0.6 and 2.4, respectively. For other parame-
ters, we use default setting. In the CoNLL 2005 
benchmark data, we get 647 rules with optional 
nodes out of the total 6,534 grammar rules and de-
fine three equivalent node feature sets as below: 
? JJ, JJR, JJS 
? RB, RBR, RBS 
? NN, NNS, NNP, NNPS, NAC, NX 
 
Here, the verb feature set ?VB, VBD, VBG, VBN, 
VBP, VBZ? is removed since the voice information 
is very indicative to the arguments of ARG0 
(Agent, operator) and ARG1 (Thing operated). 
 
Methods Accuracy (%) 
 Baseline: Non-grammar-driven 85.21 
 +Approximate Node Matching 86.27 
 +Approximate Substructure 
Matching 
87.12 
 Ours: Grammar-driven Substruc-
ture and Node Matching 
87.96 
Feature-based method with poly-
nomial kernel (d = 2) 
89.92 
 
Table 2: Performance comparison 
4.2 Experimental Results 
Table 2 compares the performances of different 
methods on the test set. First, we can see that the 
new grammar-driven hybrid convolution tree kernel 
significantly outperforms ( 2? test with p=0.05) the 
205
non-grammar one with an absolute improvement of 
2.75 (87.96-85.21) percentage, representing a rela-
tive error rate reduction of 18.6% (2.75/(100-85.21)) 
. It suggests that 1) the linguistically motivated 
structure features are very useful for semantic role 
classification and 2) the grammar-driven kernel is 
much more effective in capturing such kinds of fea-
tures due to the consideration of linguistic knowl-
edge. Moreover, Table 2 shows that 1) both the 
grammar-driven approximate node matching and the 
grammar-driven approximate substructure matching 
are very useful in modeling syntactic tree structures 
for SRL since they contribute relative error rate re-
duction of 7.2% ((86.27-85.21)/(100-85.21)) and 
12.9% ((87.12-85.21)/(100-85.21)), respectively; 2) 
the grammar-driven approximate substructure 
matching is more effective than the grammar-driven 
approximate node matching. However, we find that 
the performance of the grammar-driven kernel is 
still a bit lower than the feature-based method. This 
is not surprising since tree kernel methods only fo-
cus on modeling tree structure information. In this 
paper, it captures the syntactic parse tree structure 
features only while the features used in the feature-
based methods cover more knowledge sources.  
In order to make full use of the syntactic structure 
information and the other useful diverse flat fea-
tures, we present a composite kernel to combine the 
grammar-driven hybrid kernel and feature-based 
method with polynomial kernel: 
(1 )      (0 1)comp hybrid polyK K K? ? ?= + ? ? ?  
Evaluation on the development set shows that the 
composite kernel yields the best performance when 
? is set to 0.3. Using the same setting, the system 
achieves the performance of 91.02% in Accuracy 
in the same test set. It shows statistically significant 
improvement (?2 test with p= 0.10) over using the 
standard features with the polynomial kernel (? = 0, 
Accuracy = 89.92%) and using the grammar-driven 
hybrid convolution tree kernel (? = 1, Accuracy = 
87.96%). The main reason is that the tree kernel 
can capture effectively more structure features 
while the standard flat features can cover some 
other useful features, such as Voice, SubCat, which 
are hard to be covered by the tree kernel. The ex-
perimental results suggest that these two kinds of 
methods are complementary to each other. 
In order to further compare with other methods, 
we also do experiments on the dataset of English 
PropBank I (LDC2004T14). The training, develop-
ment and test sets follow the conventional split of 
Sections 02-21, 00 and 23. Table 3 compares our 
method with other previously best-reported methods 
with the same setting as discussed previously. It 
shows that our method outperforms the previous 
best-reported one with a relative error rate reduction 
of 10.8% (0.97/(100-91)). This further verifies the 
effectiveness of the grammar-driven kernel method 
for semantic role classification. 
  
Method Accuracy (%)
Ours (Composite Kernel)      91.97 
Moschitti (2006): PAF kernel only    87.7 
Jiang et al (2005): feature based    90.50 
Pradhan et al (2005a): feature based    91.0 
 
Table 3: Performance comparison between our 
method and previous work 
 
Training Time Method 
  4 Sections  19 Sections
Ours: grammar-
driven tree kernel 
~8.1 hours ~7.9 days 
Moschitti (2006): 
non-grammar-driven 
tree kernel 
~7.9 hours ~7.1 days 
 
Table 4: Training time comparison 
 
Table 4 reports the training times of the two ker-
nels. We can see that 1) the two kinds of convolu-
tion tree kernels have similar computing time. Al-
though computing the grammar-driven one requires 
exponential time in its worst case, however, in 
practice, it may only need 1 2(| | | |)O N N?  or lin-
ear and 2) it is very time-consuming to train a SVM 
classifier in a large dataset.  
5 Conclusion and Future Work 
In this paper, we propose a novel grammar-driven 
convolution tree kernel for semantic role classifica-
tion. More linguistic knowledge is considered in 
the new kernel design. The experimental results 
verify that the grammar-driven kernel is more ef-
fective in capturing syntactic structure features than 
the previous convolution tree kernel because it al-
lows grammar-driven approximate matching of 
substructures and node features. We also discuss 
the criteria to determine the optional nodes in a 
206
CFG rule in defining our grammar-driven convolu-
tion tree kernel. 
The extension of our work is to improve the per-
formance of the entire semantic role labeling system 
using the grammar-driven tree kernel, including all 
four stages: pruning, semantic role identification, 
classification and post inference. In addition, a 
more interesting research topic is to study how to 
integrate linguistic knowledge and tree kernel 
methods to do feature selection for tree kernel-
based NLP applications (Suzuki et al, 2004). In 
detail, a linguistics and statistics-based theory that 
can suggest the effectiveness of different substruc-
ture features and whether they should be generated 
or not by the tree kernels would be worked out. 
References  
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The 
Berkeley FrameNet Project. COLING-ACL-1998  
Xavier Carreras and Llu?s M?rquez. 2004. Introduction to 
the CoNLL-2004 shared task: Semantic role labeling. 
CoNLL-2004  
Xavier Carreras and Llu?s M?rquez. 2005. Introduction to 
the CoNLL-2005 shared task: Semantic role labeling. 
CoNLL-2005  
Eugene Charniak. 2000. A maximum-entropy-inspired 
parser. In Proceedings ofNAACL-2000 
Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 
2006. A hybrid convolution tree kernel for semantic 
role labeling. COLING-ACL-2006(poster) 
Michael Collins and Nigel Duffy. 2001. Convolution 
kernels for natural language. NIPS-2001 
 Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics, 
28(3):245?288 
David Haussler. 1999. Convolution kernels on discrete 
structures. Technical Report UCSC-CRL-99-10 
Zheng Ping Jiang, Jia Li and Hwee Tou Ng. 2005. Se-
mantic argument classification exploiting argument 
interdependence. IJCAI-2005 
T. Joachims. 1998. Text Categorization with Support 
Vecor Machine: learning with many relevant fea-
tures. ECML-1998 
Kashima H. and Koyanagi T. 2003. Kernels for Semi-
Structured Data. ICML-2003 
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello 
Cristianini and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learn-
ing Research, 2:419?444 
Mitchell P. Marcus, Mary Ann Marcinkiewicz  and Bea-
trice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational 
Linguistics, 19(2):313?330 
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. ACL-2004 
Alessandro Moschitti. 2006. Syntactic kernels for natu-
ral language learning: the semantic role labeling 
case. HLT-NAACL-2006 (short paper)  
Rodney D. Nielsen and Sameer Pradhan. 2004. Mixing 
weak learners in semantic parsing. EMNLP-2004 
Martha Palmer, Dan Gildea and Paul Kingsbury. 2005. 
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1) 
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne 
Ward, James H. Martin and Daniel Jurafsky. 2005a. 
Support vector learning for semantic argument classi-
fication. Journal of Machine Learning 
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James 
Martin and Daniel Jurafsky. 2005b. Semantic role la-
beling using different syntactic views. ACL-2005 
Vasin Punyakanok, Dan Roth, Wen-tau Yih and Dav Zi-
mak. 2004. Semantic role labeling via integer linear 
programming inference. COLING-2004 
Vasin Punyakanok, Dan Roth and Wen Tau Yih. 2005. 
The necessity of syntactic parsing for semantic role 
labeling. IJCAI-2005 
Libin Shen, Anoop Sarkar and A. K. Joshi. 2003. Using 
LTAG based features in parse reranking. EMNLP-03 
Jun Suzuki, Hideki Isozaki and Eisaku Maede. 2004. 
Convolution kernels with feature selection for Natu-
ral Language processing tasks. ACL-2004 
Vladimir N. Vapnik. 1998. Statistical Learning Theory. 
Wiley 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. EMNLP-2004 
Dmitry Zelenko, Chinatsu Aone, and Anthony Rich-
ardella. 2003. Kernel methods for relation extraction. 
Machine Learning Research, 3:1083?1106 
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. COLING-ACL-2006 
207
