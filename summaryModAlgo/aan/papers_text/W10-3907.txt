Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 40?49,
Beijing, August 2010
A Look inside the Distributionally Similar Terms
Kow Kuroda
kuroda@nict.go.jp
Jun?ichi Kazama
kazama@nict.go.jp
National Institute of Information and Communications Technology (NICT)
Kentaro Torisawa
torisawa@nict.go.jp
Abstract
We analyzed the details of aWeb-derived
distributional data of Japanese nominal
terms with two aims. One aim is to
examine if distributionally similar terms
can be in fact equated with ?semanti-
cally similar? terms, and if so to what
extent. The other is to investigate into
what kind of semantic relations con-
stitute (strongly) distributionally similar
terms. Our results show that over 85%
of the pairs of the terms derived from
the highly similar terms turned out to
be semantically similar in some way.
The ratio of ?classmate,? synonymous,
hypernym-hyponym, and meronymic re-
lations are about 62%, 17%, 8% and 1%
of the classified data, respectively.
1 Introduction
The explosion of online text allows us to enjoy
a broad variety of large-scale lexical resources
constructed from the texts in the Web in an un-
supervised fashion. This line of approach was
pioneered by researchers such as Hindle (1990),
Grefenstette (1993), Lee (1997) and Lin (1998).
At the heart of the approach is a crucial working
assumption called ?distributional hypothesis,? as
with Harris (1954). We now see an impressive
number of applications in natural language pro-
cessing (NLP) that benefit from lexical resources
directly or indirectly derived from this assump-
tion. It seems that most researchers are reason-
ably satisfied with the results obtained thus far.
Does this mean, however, that the distribu-
tional hypothesis was proved to be valid? Not
necessarily: while we have a great deal of con-
firmative results reported in a variety of research
areas, but we would rather say that the hypothe-
sis has never been fully ?validated? for two rea-
sons. First, it has yet to be tested under the pre-
cise definition of ?semantic similarity.? Second,
it has yet to be tested against results obtained at
a truly large scale.
One of serious problems is that we have seen
no agreement on what ?similar terms? mean and
should mean. This paper intends to cast light
on this unsolved problem through an investiga-
tion into the precise nature of lexical resources
constructed under the distributional hypothesis.
The crucial question to be asked is, Can distri-
butionally similar terms really be equated with
semantically similar terms or not? In our investi-
gation, we sought to recognize what types of se-
mantic relations can be found for pairs of terms
with high distributional similarity, and see where
the equation of distributional similarity with se-
mantic similarity fails. With this concern, this
paper tried to factor out as many components of
semantic similarity as possible. The effort of fac-
torization resulted in the 18 classes of semantic
(un)relatedness to be explained in ?2.3.1. Such
factorization is a necessary step for a full valida-
tion of the hypothesis. To meet the criterion of
testing the hypothesis at a very large scale, we
analyzed 300,000 pairs of distributionally simi-
lar terms. Details of the data we used are given
in ?2.2.
This paper is organized as follows. In ?2, we
present our method and data we used. In ?3, we
present the results and subsequent analysis. In
?4, we address a few remaining problems. In ?5,
we state tentative conclusions.
40
2 Method and Data
2.1 Method
The question we need to address is how many
subtypes of semantic relation we can identify in
the highly similar terms. We examined the ques-
tion in the following procedure:
(1) a. Select a set of ?base? terms B.
b. Use a similarity measure M to con-
struct a list of n terms T = [ti,1, ti,2,. . . ,
ti, j, . . . , ti,n] where ti, j denotes the j-
th most similarity term in T against
bi ? B. P(k) are pairs of bi and ti,k, i.e.,
the k-th most similar term to bi.
c. Human raters classify a portion Q of
the pairs in P(k) with reference to
a classification guideline prepared for
the task.
Note that the selection of base set B can be
independent of the selection of T . Note also that
T is indexed by terms in B. To encode this, we
write: T [bi] = [ti,1, ti,2,. . . , ti, j, . . . , ti,n].
2.2 Data
For T , we used Kazama?s nominal term cluster-
ing (Kazama and Torisawa, 2008; Kazama et al,
2009). In this data, base set B for T is one mil-
lion terms defined by the type counts of depen-
dency relations, which is roughly equated with
the ?frequencies? of the terms. Each base term
in B is associated with up to 500 of the most dis-
tributionally similar terms. This defines T .
For M, we used the Jensen-Shannon diver-
gence (JS-divergence) base on the probability
distributions derived by an EM-based soft clus-
tering (Kazama and Torisawa, 2008). For con-
venience, some relevant details of the data con-
struction are described in Appendix A, but in a
nutshell, we used dependency relations as dis-
tributional information. This makes our method
comparable to that used in Hindle (1990). The
statistics of the distributional data used were as
follows: roughly 920 million types of depen-
dency relations1) were automatically acquired
1)The 920 million types come in two kinds of context
triples: 590 million types of (t, p,v) and 320 million types
from a large-scale Japanese Web-corpus called
the Tsubaki corpus (Shinzato et al, 2008) which
consists of roughly 100 million Japanese pages
with six billion sentences. After excluding hapax
nouns, we had about 33 million types of nouns
(in terms of string) and 27 million types of verbs.
These nouns were ranked by type count of the
two context triples, i.e., (t, p,v) and (n?, p?, t). B
was determined by selecting the top one million
terms with the most variations of context triples.
2.2.1 Sample of T [b]
For illustration, we present examples of the
Web-derived distributional similar terms. (2)
shows the 10 most distributionally similar terms
(i.e., [t1070,1, t1070,2, . . . , t1070,10] in T (b1070))
where b1070 = ????? (piano) is the 1070-th
term in B. Likewise, (3) shows the 10 most dis-
tributionally similar terms [t38555,1, t38555,2, . . . ,
t38555,10] in T (b38555)) where b38555 = ??????
???? (Tchaikovsky) is the 38555-th term in B.
(2) 10 most similar to ?????
1. ?????? (Electone; electronic or-
gan) [-0.322]
2. ????? (violin) [-0.357]
3. ?????? (violin) [-0.358]
4. ??? (cello) [-0.358]
5. ?????? (trumpet) [-0.377]
6. ??? (shamisen) [-0.383]
7. ???? (saxophone) [-0.39]
8. ???? (organ) [-0.392]
9. ?????? (clarinet) [-0.394]
10. ?? (erh hu) (-0.396)
(3) 10 most similar to ??????????
1. ????? (Brahms) [-0.152]
2. ????? (Schumann) [-0.163]
3. ???????? (Mendelssohn) [-
0.166]
4. ????????? (Shostakovich) [-
0.178]
5. ????? (Sibelius) [-0.18]
of (t, p?,n?), where t denotes the target nominal term, p a
postposition, v a verb, and n? a nominal term that follows t
and p?, i.e., ?t-no? analogue to the English ?of t.?
41
6. ???? (Haydn) [-0.181]
7. ???? (Ha?ndel) [-0.181]
8. ???? (Ravel) [-0.182]
9. ?????? (Schubert) [-0.187]
10. ??????? (Beethoven) [-0.19]
For construction of P(k), we had the follow-
ing settings: i) k = 1,2; and ii) for each k, we
selected the 150,000 most frequent terms (out of
one million terms) with some filtering specified
below. Thus, Q was 300,000 pairs whose base
terms are roughly the most frequent 150,000
terms in B with filtering and targets are terms
k = 1 or k = 2.
2.2.2 Filtering of terms in B
For filtering, we excluded the terms of B with
one of the following properties: a) they are in an
invalid form that could have resulted from parse
errors; b) they have regular ending (e.g., -??
, -? [event], -? [time or when], -?? [thing or
person], -? [thing], -? [person]). The reason
for the second is two-fold. First, it was desir-
able to reduce the ratio of the class of ?class-
mates with common morpheme,? which is ex-
plained in ?2.3.2, whose dominance turned out to
be evident in the preliminary analysis. Second,
the semantic property of the terms in this class
is relatively predictable from their morphology.
That notwithstanding, this filtering might have
had an undesirable impact on our results, at least
in terms of representativeness. Despite of this,
we decided to place priority on collecting more
varieties of classes.
The crucial question is, again, whether dis-
tributionally similar terms can really be equated
with semantically similar terms. Put differently,
what kinds of terms can we find in the sets con-
structed using distributionally similarity? We
can confirm the hypothesis if the most of the
term pairs are proved to be semantically simi-
lar for most sets of terms constructed based on
the distributional hypothesis. To do this, how-
ever, we need to clarify what constitutes seman-
tic similarity. We will deal with this prerequisite.
2.3 Classification
2.3.1 Factoring out ?semantic similarity?
Building on lexicographic works like Fell-
baum (1998) and Murphy (2003), we assume
that the following are the four major classes
of semantic relation that contribute to semantic
similarity between two terms:
(4) a. ?synonymic? relation (one can substi-
tute for another on an identity basis).
Examples are (Microsoft, MS).
b. ?hypernym-hyponym? relation be-
tween two terms (one can substitute
for another on un underspecifica-
tion/abstraction basis). Examples are
(guys, players)
c. ?meronymic? (part-whole) relation be-
tween two terms (one term can be a
substitute for another on metonymic
basis). Examples are (bodies, players)
[cf. All the players have strong bodies]
d. ?classmate? relation between two
terms, t1 and t2, if and only if (i) they
are not synonymous and (ii) there is a
concrete enough class such that both t1
and t2 are instances (or subclasses).2)
For example, (China, South Korea)
[cf. (Both) China and South Korea
are countries in East Asia], (Ford, Toy-
ota) [cf. (Both) Ford and Toyota are
top-selling automotive companies] and
(tuna, cod) [cf. (Both) tuna and cod
are types of fish that are eaten in the
Europe] are classmates.
For the substitution, the classmate class behaves
somewhat differently. In this case, one term can-
not substitute for another for a pair of terms. It
is hard to find out the context in which pairs like
(China, South Korea), (Ford, Toyota) and (tuna,
cod) can substitute one another. On the other
hand, substitution is more or less possible in the
other three types. For example, a synonymic pair
of (MS, Microsoft) can substitute for one another
in contexts like Many people regularly complain
2)The proper definition of classmates is extremely hard
to form. The authors are aware of the incompleteness of
their definition, but decided not to be overly meticulous.
42
pair of forms
pair of 
meaningful 
terms
x: pair with a 
meaningless 
form
u: pair of terms 
in no conceivable 
semantic relation
r: pair of terms in 
a conceivable 
semantic relation
s:* synonymous 
pair in the 
broadest sense
a: acronymic 
pair
v: allographic 
pair
n: alias pair
e: erroneous 
pair
f: quasi-
erroneous pair
v*: notational 
variation of the 
same term
m: misuse pair
o: pair in other, 
unindentified 
relation
h: hypernym-
hyponym pair
k**: classmate 
in the broadest 
sense
k*: classmate 
without obvious 
contrastiveness
c*: contrastive 
pairs d: antonymic 
pair
c: contrastive 
pair without 
antonymity
p: meronymic 
pair
t: pair of terms 
with inherent 
temporal order
y: undecidable
k: classmate 
without shared 
morpheme
w: classmate 
with shared 
morpheme
s: synonymous 
pair of different 
terms
Figure 1: Classification tree for semantic relations used
about products { i. MS; ii. Microsoft }. A
hypernym-hyponym pair of (guys, players) can
substitute in contexts like We have dozens of ex-
cellent { i. guys; ii. players } on our team. A
meronymic pair of (bodies, players) can substi-
tute for each other in contexts like They had a few
of excellent { i. bodies; ii. players} last year.
2.3.2 Classification guidelines
The classification guidelines were specified
based on a preliminary analysis of 5,000 ran-
domly selected examples. We asked four annota-
tors to perform the task. The guidelines were fi-
nalized after several revisions. This revision pro-
cess resulted in a hierarchy of binary semantic
relations as illustrated in Figure 1, which sub-
sumes 18 types as specified in (5). The essen-
tial division is made at the fourth level where
we have s* (pairs of synonyms in the broadest
sense) with two subtypes, p (pairs of terms in
the ?part-whole? relation), h (pairs of terms in
the ?hypernym-hyponym? relation), k** (pairs
of terms in the ?classmate? relation), and o (pairs
of terms in any other relation). Note that this
system includes the four major types described
in (4). The following are some example pairs of
Japanese terms with or without English transla-
tions:
(5) s: synonymic pairs (subtype of s*) in
which the pair designates the same en-
tity, property, or relation. Examples
are: (??, ??) [both mean root], (?
?????,????) [(supporting mem-
ber, cooperating member)], (????
?, ?????) [(invoker of the pro-
cess, parent process)], (????????
?, ?????) [(venture business, ven-
ture)], (????, ???????) [(op-
posing hurler, opposing pitcher)], (?
?, ???) [(medical history, anamne-
ses)],
n: alias pairs (subtype of s*) in which
one term of the pair is the ?alias? of
the other term. Examples are (Steve
Jobs, founder of Apple, Inc.), (Barak
Obama, US President), (???,????
???), (???, ????)
43
a: acronymic pair (subtype of s*) in
which one term of the pair is the
acronym of of the other term. Ex-
amples are: (DEC, Digital Equip-
ment), (IBM, International Business
Machine) (Microsoft ?, MS ?), (??
?, ????), (????, ??),
v: allographic pairs (subtype of s*) in
which the pair is the pair of two forms
of the same term. Examples are:
(convention centre, convention cen-
ter), (colour terms, color terms), (??
???, ????), (????, ????),
(??????????, ????????
???), (??, ??), (????, ????
), (???, ??), (????, ?????),
(??, ??), (????, ????)
h: hypernym-hyponym pair in which one
term of the pair designates the ?class?
of the other term. Examples (or-
der is irrelevant) are: (thesaurus, Ro-
get?s), (?????, ?????) [(search
tool, search software)], (????, ??
??) [(unemployment measures, em-
ployment measures)], (??, ????
) [(business conditions, employment
conditions)], (???????, ???)
[(festival, music festival)], (???, ?
????) [(test agent, pregnancy test)],
(??????, ???) [(cymbidium, or-
chid)], (????,?????) [(company
logo, logo)], (????,????) [(mys-
tical experiences, near-death experi-
ences)]
p: meronymic pair in which one term of
the pair designates the ?part? of the
other term. Examples (order is ir-
relevant) are: (????, ??) [(earth,
sea)], (??, ??) [(affirmation, ad-
mission)], (??, ????) [(findings,
research progress)], (????????
?, ?????) [(solar circuit system,
exterior thermal insulation method)],
(?????, ??) [(Provence, South
France)],
k: classmates not obviously contrastive
without common morpheme (subtype
of k*). Examples are: (????, ???
?) [(self-culture, training)], (????
, ??) [(sub-organs, services)], (???
??,??????) [(Dongba alphabets,
hieroglyphs)], (Tom, Jerry)
w: classmates not obviously contrastive
with common morpheme (subtype of
k*). Examples are: (????, ????)
[(gas facilities, electric facilities)], (?
???,???) [(products of other com-
pany, aforementioned products)], (??
?, ???) [(affiliate station, local sta-
tion)], (???,????) [(Niigata City,
Wakayama City)], (?????, ???
??) [(Sinai Peninsula, Malay Penin-
sula)],
c: contrastive pairs without antonymity
(subtype of c*). Examples are: (???
??, ????) [(romanticism, natural-
ism)], (????????, ???????
????) [(mobile user, internet user)],
(???, PS2?), [(bootleg edition, PS2
edition)]
d: antonymic pairs = contrastive pairs
with antonymity (subtype of c*). Ex-
amples are: (??, ??) [(bond-
ing, disintegration)], (???, ???)
[(gravel road, pavement)], (??, ??
) [(west walls, east walls)], (???,
????) [(daughter and son-in-law,
son and daughter-in-law)], (??, ??
) [(tax-exclusive prices, tax-inclusive
prices)], (??????, ????????
) [(front brake, rear brake)], (????
??, ???????) [(tag-team match,
solo match)], (???, ???) [(wip-
ing with dry materials, wiping with
wet materials)], (??????, ??)
[(sleeveless, long-sleeved)]
t: pairs with inherent temporal order
(subtype of c*). Examples are: (??
?, ???) [(harvesting of rice, plant-
ing of rice)], (????, ????) [(day
of departure, day of arrival)], (???
?, ????) [(career decision, career
selection)], (???, ????) [(catnap,
stay up)], (??, ??) [(poaching, con-
44
traband trade)], (??, ??) [(surren-
der, dispatch of troops)], (???, ?
??) [(2nd-year student, 3rd-year stu-
dent)]
e: erroneous pairs are pairs in which
one term of the pair seems to suffer
from character-level input errors, i.e.
?mistypes.? Examples are: (???, ?
??), (???????, ???????),
(???, ???)
f: quasi-erroneous pair is a pair of terms
with status somewhat between v and e.
Examples (order is irrelevant) are: (?
???, ????) [(supoito, supoido)],
(??????, ??????) [(goru-
fubaggu, gorufugakku)], (?????,
?????) [(biggu ban, bikku ban)],
m: misuse pairs in which one term of the
pair seems to suffer from ?mistake? or
?bad memory? of a word (e is caused
by mistypes but m is not). Examples
(order is irrelevant) are: (???, ???
), (?????, ?????), (??, ??),
(???, ???), (??, ??)
o: pairs in other unidentified relation in
which the pair is in some semantic re-
lation other than s*, k**, p, h, and
u. Examples are: (??, ???) [(ul-
terior motives, possessive feeling)], (?
????,?????) [(theoretical back-
ground, basic concepts)], (?????
???, ????) [(Alexandria, Sira-
cusa)],
u: unrelated pairs in which the pair is in
no promptly conceivable semantic re-
lation. Examples are: (???, ????
) [(noncontact, high resolution)], (??
, ????) [(imitation, overinterpreta-
tion)],
x: nonsensical pairs in which either of the
pair is not a proper term of Japanese.
(but it can be a proper name with very
low familiarity). Examples are: (???
?, ???), (????, ??), (??, ??
?), (???, ??), (ma, ?????)
y: unclassifiable under the allowed time
limit.3) Examples are: (???, ???
???), (fj, ???), (??, ??),
Note that some relation types are symmetric
and others are asymmetric: a, n, h, p, and t (and
e, f, and m, too) are asymmetric types. This
means that the order of the pair is relevant, but it
was not taken into account during classification.
Annotators were asked to ignore the direction of
pairs in the classification task. In the finaliza-
tion, we need to reclassify these to get them in
the right order.
2.3.3 Notes on implicational relations
The overall implicational relation in the hier-
archy in Figure 1 is the following:
(6) a. s, k**, p, h, and o are supposed to be
mutually exclusive, but the distinction
is sometimes obscure.4)
b. k** has two subtypes: k* and c*.
c. k and w are two subtypes k*.
d. c, d and t three subtypes of c*.
To resolve the issue of ambiguity, priority was
set among the labels so that e, f < v < a < n <
p < h < s < t < d < c < w < k < m < o < u <
x < y, where the left label is more preferred over
the right. This guarantees preservation of the im-
plicational relationship among labels.
2.3.4 Notes on quality of classification
We would like to add a remark on the quality.
After a quick overview, we reclassified o and w,
because the first run of the final task ultimately
produced a resource of unsatisfactory quality.
Another note on inter-annotator agreement:
originally, the classification task was designed
and run as a part of a large-scale language re-
source development. Due to its overwhelming
size, we tried to make our development as effi-
cient as possible. In the final phase, we asked
3)We did not ask annotators to check for unknown terms.
4)To see this, consider pairs like (large bowel, bowel),
(small bowel, bowel). Are they instances of p or h? The
difficulty in the distinction between h and p becomes harder
in Japanese due to the lack of plurality marking: cases
like (Mars, heavenly body) (a case of h) and (Mars, heav-
enly bodies) (a p case) cannot be explicitly distinguished.
In fact, the Japanese term ?? can mean both ?heavenly
body? (singular) and ?heavenly bodies? (plural).
45
Table 1: Distribution of relation types
rank count ratio (%) cum. (%) class label
1 108,149 36.04 36.04 classmates without common morpheme k
2 67,089 22.35 58.39 classmates with common morpheme w
3 26,113 8.70 67.09 synonymic pairs s
4 24,599 8.20 75.29 hypernym-hyponym pairs h
5 20,766 6.92 82.21 allographic pairs v
6 18,950 6.31 88.52 pairs in other ?unidentified? relation o
7 12,383 4.13 92.65 unrelated pairs u
8 8,092 2.70 95.34 contrastive pairs without antonymity c
9 3,793 1.26 96.61 pairs with inherent temporal order t
10 3,038 1.01 97.62 antonymic pairs d
11 2,995 1.00 98.62 meronymic pairs p
12 1,855 0.62 99.23 acronymic pairs a
13 725 0.24 99.48 alias pairs n
14 715 0.24 99.71 erroneous pairs e
15 397 0.13 99.85 misuse pairs m
16 250 0.08 99.93 nonsensical pairs x
17 180 0.06 99.99 quasi-erroneous pairs f
18 33 0.01 100.00 unclassified y
17 annotators to classify the data with no over-
lap. Ultimately we obtained results that deserve
a detailed report. This history, however, brought
us to an undesirable situation: no inter-annotator
agreement is calculable because there was no
overlap in the task. This is why no inter-rater
agreement data is now available.
3 Results
Table 1 summarizes the distribution of relation
types with their respective ranks and proportions.
The statistics suggests that classes of e, f, m, x,
and y can be ignored without risk.
3.1 Observations
We noticed the following. Firstly, the largest
class is the class of classmates, narrowly defined
or broadly defined. The narrow definition of the
classmates is the conjunction of k and w, which
makes 58.39%. The broader definition of class-
mates, k**, is the union of k, w, c, d and t, which
makes 62.10%. This confirms the distributional
hypothesis.
The second largest class is the narrowly de-
fined synonymous pairs s. This is 8.7% of the
total, but the general class of synonymic pairs,
s* as the union of s, a, n, v, e, f, and m, makes
16.91%. This comes next to h and w. Notice
also that the union of k** and s* makes 79.01%.
The third largest is the class of terms in
hypernym-hyponym relations. This is 8.20% of
the total. We are not sure if this is large or small.
These results look reasonable and can be
seen as validation of the distributional hypothe-
sis. But there is something uncomfortable about
the the fourth and fifth largest classes, pairs in
?other? relation and ?unrelated? pairs, which
make 6.31% and 4.13% of the total, respectively.
Admittedly, 6.31% are 4.13% are not very large
numbers, but it does not guarantee that we can
ignore them safely. We need a closer examina-
tion of these classes and return to this in ?4.
3.2 Note on allography in Japanese
There are some additional notes: the rate of al-
lographic pairs [v] (6.92%) is rather high.5) We
suspect that this ratio is considerably higher than
the similar results that are to be expected in other
5)Admittedly, 6.92% is not a large number in an absolute
value, but it is quite large for the rate of allographic pairs.
46
languages. In fact, the range of notational varia-
tions in Japanese texts is notoriously large. Many
researchers in Japanese NLP became to be aware
of this, by experience, and claim that this is one
of the causes of Japanese NLP being less effi-
cient than NLP in other (typically ?segmented?)
languages. Our result revealed only the allogra-
phy ratio in nominal terms. It is not clear to what
extent this result is applied to the notional varia-
tions on predicates, but it is unlikely that predi-
cates have a lesser degree of notational variation
than nominals. At the least, informal analysis
suggests that the ratio of allography is more fre-
quent and has more severe impacts in predicates
than in nominals. So, it is very unlikely that we
had a unreasonably high rate of allography in our
data.
3.3 Summary of the results
Overall, we can say that the distributional hy-
pothesis was to a great extent positively con-
firmed to a large extent. Classes of classmates
and synonymous pairs are dominant. If the side
effects of filtering described in ?2.2.2 are ig-
nored, nearly 88% (all but o, u, m, x, and y)
of the pairs in the data turned out to be ?se-
mantically similar? in the sense they are clas-
sified into one of the regular semantic relations
defined in (5). While the status of the inclusion
of hypernym-hyponym pairs in classes of seman-
tically similar terms could be controversial, this
result cannot be seen as negative.
One aspect somewhat unclear in the results we
obtained, however, is that highly similar terms
in our data contain such a number of pairs in
unidentifiable relation. We will discuss this in
more detail in the following section.
4 Discussion
4.1 Limits induced by parameters
Our results have certain limits. We specify those
here.
First, our results are based on the case of
k = 1, 2 for P(k). This may be too small and
it is rather likely that we did not acquire results
with enough representativeness. For more com-
plete results, we need to compare the present re-
sults under larger k, say k = 4, 8, 16, . . .. We did
not do this, but we have a comparable result in
one of the preliminary studies. In the prepara-
tion stage, we classified samples of pairs whose
base term is at frequency ranks 13?172, 798?
1,422 and 12,673?15,172 where k = 1, 2, 3, . . . ,
9, 10.6) Table 2 shows the ratios of relation types
for this sample (k = 1, 2, 4, 8, 10).
Table 2: Similarity rank = 1, 2, 4, 8, 10
rank 1 2 4 8 10
v 18.13 10.48 3.92 2.51 1.04
o 17.08 21.24 26.93 28.24 29.56
w 13.65 13.33 14.30 12.19 12.75
s 11.74 9.14 7.05 4.64 4.06
u 11.07 16.48 17.63 20.79 20.87
h 10.50 10.29 11.17 12.96 10.20
k 7.82 8.38 7.84 7.74 8.22
d 2.58 2.00 1.57 1.16 0.85
p 2.00 1.14 1.08 1.35 1.79
c 1.43 1.05 1.27 1.35 1.89
a 1.05 1.33 0.88 0.39 0.57
x 1.05 1.14 1.27 1.64 2.08
t 0.29 0.19 0.20 0.39 0.47
f 0.10 0.10 0.00 0.10 0.09
m 0.00 0.10 0.20 0.00 0.19
#item 1,048 1,050 1,021 1,034 1,059
From Table 2, we notice that: as similarity
rank decreases, (i) the ratios of v, s, a, and d
decrease monotonically, and the ratios of v and s
decrease drastically; (ii) the ratios of o, u, and x
increases monotonically, and the ratio of o and u
increases considerably; and while (iii) the ratios
of h, k, p, w, m, and f seem to be constant. But
it is likely that the ratios of h, k, p, w, m, and f
change at larger k, say 128, 256.
Overall, however, this suggests that the differ-
ence in similarity rank has the greatest impact
on s* (recall that s and v are subtypes of s*),
o, and u, but not so much on others. Two ten-
dencies can be stated: first, terms at lower sim-
ilarity ranks become less synonymous. Second,
6)The frequency/rank in B was measured in terms of the
count of types of dependency relation.
47
the relationships among terms at lower similar-
ity ranks become more obscure. Both are quite
understandable.
There are, however, two caveats concerning
the data in Table 2, however. First, the 15 la-
bels used in this preliminary task are a subset of
the 18 labels used in the final task. Second, the
definitions of some labels are not completely the
same even if the same labels are used (this is why
we have this great of a ratio of o in Table 2. We
must admit, therefore, that no direct comparison
is possible between the data in Tables 1 and 2.
Second, it is not clear if we made the best
choices for clustering algorithm and distribu-
tional data. For the issue of algorithm, there
are too many clustering algorithms and it is hard
to reasonably select candidates for comparison.
We do, however, plan to extend our evaluation
method to other clustering algorithms. Cur-
rently, one of such options is Bayesian cluster-
ing. We are planning to perform some compar-
isons.
For the issue of what kind of distributional in-
formation to use, many kinds of distributional
data other than dependency relation are avail-
able. For example, simple co-occurrences within
a ?window? are a viable option. With a lack
of comparison, however, we cannot tell at the
present what will come about if another kind of
distributional data was used in the same cluster-
ing algorithm.
4.2 Possible overestimation of hypernyms
A closer look suggests that the ratio of
hypernym-hyponym pairs was somewhat overes-
timated. This is due to the algorithm used in our
data construction. It was often the case that head
nouns were extracted as bare nouns from com-
plex, much longer noun phrases, sometimes due
to the extraction algorithms or parse errors. This
resulted in accidental removal of modifiers be-
ing attached to head nouns in their original uses.
We have not yet checked how often this was the
case. We are aware that this could have resulted
in the overestimation of the ratio of hypernymic
relations in our data.
4.3 Remaining issues
As stated, the fourth largest class, roughly 6.31%
of the total, is that of the pairs in the ?other?
unidentified relation [o]. In our setting, ?other?
means that it is in none among the synonymous,
classmate, part-whole or hypernym-hyponym re-
lation. A closer look into some examples of
o suggest that they are pairs of terms with ex-
tremely vague association or contrast.
Admittedly, 6.31% is not a large number, but
its ratio is comparable with that of the allo-
graphic pairs [v], 6.92%. We have no explana-
tion why we have this much of an unindenfiable
kind of semantic relation distinguished from un-
related pairs [u]. All we can say now is that we
need further investigation into it.
u is not as large as o, but it has a status similar
to o. We need to know why this much amount of
this kind of pairs. A possible answer would be
that they are caused by parse errors, directly or
indirectly.
5 Conclusion
We analyzed the details of the Japanese nominal
terms automatically constructed under the ?dis-
tributional hypothesis,? as in Harris (1954). We
had two aims. One aim was to examine to see
if what we acquire under the hypothesis is ex-
actly what we expect, i.e., if distributional sim-
ilarity can be equated with semantic similarity.
The other aim was to see what kind of seman-
tic relations comprise a class of distributionally
similar terms.
For the first aim, we obtained a positive result:
nearly 88% of the pairs in the data turned out to
be semantically similar under the 18 criteria de-
fined in (5), which include hypernym-hyponym,
meronymic, contrastive, and synonymic rela-
tions. Though some term pairs we evaluated
were among none of these relations, the ratio of
o and u in sum is about 14% and within the ac-
ceptable range.
For the second aim, our result revealed that
the ratio of the classmates, synonymous, rela-
tion, hypernym-hyponym, and meronymic rela-
tions are respectively about 62%, 17%, 8% and
1% of the classified data.
48
Overall, these results suggest that automatic
acquisition of terms under the distributional hy-
pothesis give us reasonable results.
A Clustering of one million nominals
This appendix provides some details on how the
clustering of one million nominal terms was per-
formed.
To determine the similarity metric of a pair of
nominal terms (t1, t2), Kazama et al (2009) used
the Jensen-Shannon divergence (JS-divergence)
DJS(p||q) = 12D(p||M) + 12D(q||M), where pand q are probability distributions, and D =
?i p(i)log p(i)q(i) (Kullback-Leibler divergence, or
KL-divergence) of p and q, and M = 12(p+ q).We obtained p and q in the following way.
Instead of using raw distribution, Kazama et
al. (2009) applied smoothing using EM algo-
rithm (Rooth et al, 1999; Torisawa, 2001). In
Torisawa?s model (2001), the probability of the
occurrence of the dependency relation ?v,r,n? is
defined as:
P(?v,r, t?) =def ?
a?A
P(?v,r?|a)P(t|a)P(a),
where a denotes a hidden class of ?v,r? and term
t. In this equation, the probabilities P(?v,r?|a),
P(t|a), and P(a) cannot be calculated directly
because class a is not observed in a given depen-
dency data. The EM-based clustering method
estimates these probabilities using a given cor-
pus. In the E-step, the probability P(a|?v,r?)
is calculated. In the M-step, the probabilities
P(?v,r?|a), P(t|a), and P(a) are updated until
the likelihood is improved using the results of
the E-step. From the results of this EM-based
clustering method, we can obtain the probabili-
ties P(?v,r?|a), P(t|a), and P(a) for each ?v,r?, t,
and a. Then, P(a|t) is calculated by the follow-
ing equation:
P(a|t) = P(t|a)P(a)?a?AP(t|a)P(a) .
The distributional similarity between t1 and t2
was calculated by the JS divergence between
P(a|t1) and P(a|t2).
References
Fellbaum, C., ed. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Grefenstette, G. 1993. Automatic thesaurus gener-
ation from raw text using knowledge-poor tech-
niques. In In Making Sense of Words: The 9th
Annual Conference of the UW Centre for the New
OED and Text Research.
Harris, Z. S. 1954. Distributional structure. Word,
10(2-3):146?162. Reprinted in Fodor, J. A and
Katz, J. J. (eds.), Readings in the Philosophy
of Language, pp. 33?49. Englewood Cliffs, NJ:
Prentice-Hall.
Hindle, D. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-90,
pp. 268?275, Pittsburgh, PA.
Kazama, J. and K. Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Pro-
ceedings of ACL-2008: HLT, pp. 407?415.
Kazama, J., S. De Saeger, K. Torisawa, and M. Mu-
rata. 2009. Generating a large-scale analogy list
using a probabilistic clustering based on noun-
verb dependency profiles. In Proceedings of the
15th Annual Meeting of the Association for Natu-
ral Language Processing. [in Japanese].
Lee, L. 1997. Similarity-Based Approaches to Natu-
ral Language Processing. Unpublished Ph.D. the-
sis, Harvard University.
Lin, D. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING/ACL-
98, Montreal, Canda, pages 768?774.
Murphy, M. L. 2003. Semantic Relations and the
Lexicon. Cambridge University Press, Cambridge,
UK.
Rooth, M., S. Riezler, D. Presher, G. Carroll, and
F. Beil. 1999. Inducing a semantically annotated
lexicon via em-based clustering. In Proceedings
of the 37th Annual Meeting of the Association for
Computational Linguistics, pp. 104?111.
Shinzato, K., T. Shibata, D. Kawahara, C. Hashimoto,
and S. Kurohashi. 2008. TSUBAKI: An open
search engine infrastructure for developing new
information access. In Proceedings of IJCNLP
2008.
Torisawa, K. 2001. An unsupervised method for
canonicalization of Japanese postpositions. In
Proceedings of the 6th Natural Language Process-
ing Pacific Rim Symposium (NLPRS), pp. 211?
218.
49
