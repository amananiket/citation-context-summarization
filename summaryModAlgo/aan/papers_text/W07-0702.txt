Proceedings of the Second Workshop on Statistical Machine Translation, pages 9?16,
Prague, June 2007. c?2007 Association for Computational Linguistics
CCG Supertags in Factored Statistical Machine Translation
Alexandra Birch Miles Osborne Philipp Koehn
a.c.birch-mayne@sms.ed.ac.uk miles@inf.ed.ac.uk pkoehn@inf.ed.ac.uk
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW, UK
Abstract
Combinatorial Categorial Grammar (CCG)
supertags present phrase-based machine
translation with an opportunity to access
rich syntactic information at a word level.
The challenge is incorporating this informa-
tion into the translation process. Factored
translation models allow the inclusion of su-
pertags as a factor in the source or target lan-
guage. We show that this results in an im-
provement in the quality of translation and
that the value of syntactic supertags in flat
structured phrase-based models is largely
due to better local reorderings.
1 Introduction
In large-scale machine translation evaluations,
phrase-based models generally outperform syntax-
based models1. Phrase-based models are effective
because they capture the lexical dependencies be-
tween languages. However, these models, which
are equivalent to finite-state machines (Kumar and
Byrne, 2003), are unable to model long range word
order differences. Phrase-based models also lack the
ability to incorporate the generalisations implicit in
syntactic knowledge and they do not respect linguis-
tic phrase boundaries. This makes it difficult to im-
prove reordering in phrase-based models.
Syntax-based models can overcome some of the
problems associated with phrase-based models be-
cause they are able to capture the long range struc-
tural mappings that occur in translation. Recently
1www.nist.gov/speech/tests/mt/mt06eval official results.html
there have been a few syntax-based models that
show performance comparable to the phrase-based
models (Chiang, 2005; Marcu et al, 2006). How-
ever, reliably learning powerful rules from parallel
data is very difficult and prone to problems with
sparsity and noise in the data. These models also
suffer from a large search space when decoding with
an integrated language model, which can lead to
search errors (Chiang, 2005).
In this paper we investigate the idea of incorporat-
ing syntax into phrase-based models, thereby lever-
aging the strengths of both the phrase-based models
and syntactic structures. This is done using CCG
supertags, which provide a rich source of syntactic
information. CCG contains most of the structure of
the grammar in the lexicon, which makes it possi-
ble to introduce CCG supertags as a factor in a fac-
tored translation model (Koehn et al, 2006). Fac-
tored models allow words to be vectors of features:
one factor could be the surface form and other fac-
tors could contain linguistic information.
Factored models allow for the easy inclusion of
supertags in different ways. The first approach is to
generate CCG supertags as a factor in the target and
then apply an n-gram model over them, increasing
the probability of more frequently seen sequences
of supertags. This is a simple way of including syn-
tactic information in a phrase-based model, and has
also been suggested by Hassan et al (2007). For
both Arabic-English (Hassan et al, 2007) and our
experiments in Dutch-English, n-gram models over
CCG supertags improve the quality of translation.
By preferring more likely sequences of supertags,
it is conceivable that the output of the decoder is
9
more grammatical. However, its not clear exactly
how syntactic information can benefit a flat struc-
tured model: the constraints contained within su-
pertags are not enforced and relationships between
supertags are not linear. We perform experiments to
explore the nature and limits of the contribution of
supertags, using different orders of n-gram models,
reordering models and focussed manual evaluation.
It seems that the benefit of using n-gram supertag
sequence models is largely from improving reorder-
ing, as much of the gain is eroded by using a lexi-
calised reordering model. This is supported by the
manual evaluation which shows a 44% improvement
in reordering Dutch-English verb final sentences.
The second and novel way we use supertags is
to direct the translation process. Supertags on the
source sentence allows the decoder to make deci-
sions based on the structure of the input. The sub-
categorisation of a verb, for instance, might help se-
lect the correct translation. Using multiple depen-
dencies on factors in the source, we need a strat-
egy for dealing with sparse data. We propose using
a logarithmic opinion pool (Smith et al, 2005) to
combine the more specific models (which depend on
both words and supertags) with more general mod-
els (which only depends on words). This paper is the
first to suggest this approach for combining multiple
information sources in machine translation.
Although the addition of supertags to phrase-
based translation does show some improvement,
their overall impact is limited. Sequence models
over supertags clearly result in some improvements
in local reordering but syntactic information con-
tains long distance dependencies which are simply
not utilised in phrase-based models.
2 Factored Models
Inspired by work on factored language models,
Koehn et al (2006) extend phrase-based models to
incorporate multiple levels of linguistic knowledge
as factors. Phrase-based models are limited to se-
quences of words as their units with no access to
additional linguistic knowledge. Factors allow for
richer translation models, for example, the gender or
tense of a word can be expressed. Factors also allow
the model to generalise, for example, the lemma of a
word could be used to generalise to unseen inflected
forms.
The factored translation model combines features
in a log-linear fashion (Och, 2003). The most likely
target sentence t? is calculated using the decision rule
in Equation 1:
t? = argmax
t
{
M?
m=1
?mhm(s
Fs
1 , t
Ft
1 )
}
(1)
t? ?
M?
m=1
?mhm(s
Fs
1 , t
Ft
1 ) (2)
where M is the number of features, hm(s
Fs
1 , t
Ft
1 )
are the feature functions over the factors, and ? are
the weights which combine the features which are
optimised using minimum error rate training (Venu-
gopal and Vogel, 2005). Each function depends on a
vector sFs1 of source factors and a vector t
Ft
1 of tar-
get factors. An example of a factored model used in
upcoming experiments is:
t? ?
M?
m=1
?mhm(sw, twc) (3)
where sw means the model depends on (s)ource
(w)ords, and twc means the model generates (t)arget
(w)ords and (c)cg supertags. The model is shown
graphically in Figure 1.
WordWord
CCG
SOURCE TARGET
Figure 1. Factored translation with source words deter-
mining target words and CCG supertags
For our experiments we used the following fea-
tures: the translation probabilities Pr(sFs1 |t
Ft
1 ) and
Pr(tFt1 |s
Fs
1 ), the lexical weights (Koehn et al, 2003)
lex(sFs1 |t
Ft
1 ) and lex(t
Ft
1 |s
Fs
1 ), and a phrase penalty
e, which allows the model to learn a preference for
longer or shorter phrases. Added to these features
10
is the word penalty e?1 which allows the model to
learn a preference for longer or shorter sentences,
the distortion model d that prefers monotone word
order, and the language model probability Pr(t).
All these features are logged when combined in the
log-linear model in order to retain the impact of very
unlikely translations or sequences.
One of the strengths of the factored model is it
allows for n-gram distributions over factors on the
target. We call these distributions sequence models.
By analogy with language models, for example, we
can construct a bigram sequence model as follows:
p(f1, f2, . . . fn) = p(f1)
n?
i=2
p(fi|f(i?1))
where f is a factor (eg. CCG supertags) and n is
the length of the string. Sequence models over POS
tags or supertags are smaller than language models
because they have restricted lexicons. Higher or-
der, more powerful sequence models can therefore
be used.
Applying multiple factors in the source can lead to
sparse data problems. One solution is to break down
the translation into smaller steps and translate each
factor separately like in the following model where
source words are translated separately to the source
supertags:
t? ?
M?
m=1
?mhm(sw, tw) +
N?
n=1
?nhn(sc, tw)
However, in many cases multiple dependencies
are desirable. For instance translating CCG su-
pertags independently of words could introduce er-
rors. Multiple dependencies require some form of
backing off to simpler models in order to cover the
cases where, for instance, the word has been seen in
training, but not with that particular supertag. Dif-
ferent backoff paths are possible, and it would be
interesting but prohibitively slow to apply a strat-
egy similar to generalised parallel backoff (Bilmes
and Kirchhoff, 2003) which is used in factored lan-
guage models. Backoff in factored language mod-
els is made more difficult because there is no ob-
vious backoff path. This is compounded for fac-
tored phrase-based translation models where one has
to consider backoff in terms of factors and n-gram
lengths in both source and target languages. Fur-
thermore, the surface form of a word is probably the
most valuable factor and so its contribution must al-
ways be taken into account. We therefore did not use
backoff and chose to use a log-linear combination of
features and models instead.
Our solution is to extract two translation models:
t? ?
M?
m=1
?mhm(swc, tw) +
N?
n=1
?nhn(sw, tw) (4)
One model consists of more specific features m
and would return log probabilities, for example
log2Pr(tw|swc), if the particular word and supertag
had been seen before in training. Otherwise it re-
turns ?C, a negative constant emulating log2(0).
The other model consist of more general features
n and always returns log probabilities, for example
log2Pr(tw|sw).
3 CCG and Supertags
CCGs have syntactically rich lexicons and a small
set of combinatory operators which assemble the
parse-trees. Each word in the sentence is assigned a
category from the lexicon. A category may either be
atomic (S, NP etc.) or complex (S\S, (S\NP)/NP
etc.). Complex categories have the general form
?/? or ?\? where ? and ? are themselves cate-
gories. An example of a CCG parse is given:
Peter eats apples
NP (S\NP)/NP NP
>
S\NP
<
S
where the derivation proceeds as follows: ?eats?
is combined with ?apples? under the operation of
forward application. ?eats? can be thought of as a
function that takes a NP to the right and returns a
S\NP. Similarly the phrase ?eats apples? can be
thought of as a function which takes a noun phrase
NP to the left and returns a sentence S. This opera-
tion is called backward application.
A sentence together with its CCG categories al-
ready contains most of the information present in a
full parse. Because these categories are lexicalised,
11
they can easily be included into factored phrase-
based translation. CCG supertags are categories that
have been provided by a supertagger. Supertags
were introduced by Bangalore (1999) as a way of in-
creasing parsing efficiency by reducing the number
of structures assigned to each word. Clark (2002)
developed a suppertagger for CCG which uses a
conditional maximum entropy model to estimate the
probability of words being assigned particular cat-
egories. Here is an example of a sentence that has
been supertagged in the training corpus:
We all agree on that .
NP NP\NP (S[dcl]\NP)/PP PP/NP NP .
The verb ?agree? has been assigned a complex su-
pertag (S[dcl]\NP)/PP which determines the type
and direction of its arguments. This information can
be used to improve the quality of translation.
4 Experiments
The first set of experiments explores the effect of
CCG supertags on the target, translating from Dutch
into English. The last experiment shows the effect
of CCG supertags on the source, translating from
German into English. These language pairs present
a considerable reordering challenge. For example,
Dutch and German have SOVword order in subordi-
nate clauses. This means that the verb often appears
at the end of the clause, far from the position of the
English verb.
4.1 Experimental Setup
The experiments were run using Moses2, an open
source factored statistical machine translation sys-
tem. The SRILM language modelling toolkit (Stol-
cke, 2002) was used with modified Kneser-Ney dis-
counting and interpolation. The CCG supertag-
ger (Clark, 2002; Clark and Curran, 2004) was pro-
vided with the C&C Language Processing Tools3.
The supertagger was trained on the CCGBank in
English (Hockenmaier and Steedman, 2005) and in
German (Hockenmaier, 2006).
The Dutch-English parallel training data comes
from the Europarl corpus (Koehn, 2005) and ex-
cludes the proceedings from the last quarter of 2000.
2see http://www.statmt.org/moses/
3see http://svn.ask.it.usyd.edu.au/trac/candc/wiki
This consists of 855,677 sentences with a maximum
of 50 words per sentence. 500 sentences of tuning
data and the 2000 sentences of test data are taken
from the ACLWorkshop on Building and Using Par-
allel Texts4.
The German-English experiments use data from
the NAACL 2006 Workshop on Statistical Machine
Translation5. The data consists of 751,088 sentences
of training data, 500 sentences of tuning data and
3064 sentences of test data. The English and Ger-
man training sets were POS tagged and supertagged
before lowercasing. The language models and the
sequence models were trained on the Europarl train-
ing data. Where not otherwise specified, the POS
tag and supertag sequence models are 5-gram mod-
els and the language model is a 3-gram model.
4.2 Sequence Models Over Supertags
Our first Dutch-English experiment seeks to estab-
lish what effect sequence models have on machine
translation. We show that supertags improve trans-
lation quality. Together with Shen et al (2006) it is
one of the first results to confirm the potential of the
factored model.
Model BLEU
sw, tw 23.97
sw, twp 24.11
sw, twc 24.42
sw, twpc 24.43
Table 1. The effect of sequence models on Dutch-English
BLEU score. Factors are (w)ords, (p)os tags, (c)cg su-
pertags on the source s or the target t
Table 1 shows that sequence models over CCG su-
pertags in the target (model sw, twc) improves over
the baseline (model sw, tw) which has no supertags.
Supertag sequence models also outperform models
which apply POS tag sequence models (sw, twp)
and, interestingly do just as well as models which
apply both POS tag and supertag sequence mod-
els (sw, twps). Supertags are more informative than
POS tags as they contain the syntactic context of a
word.
These experiments were run with the distortion
limit set to 6. This means that at most 6 words in
4see http://www.statmt.org/wpt05/
5see http://www.statmt.org/wpt06/
12
the source sentence can be skipped. We tried setting
the distortion limit to 15 to see if allowing longer
distance reorderings with CCG supertag sequence
models could further improve performance, however
it resulted in a decrease in performance to a BLEU
score of 23.84.
4.3 Manual Analysis
The BLEU score improvement in Table 1 does not
explain how the supertag sequence models affect the
translation process. As suggested by Callison-Burch
et al(2006) we perform a focussed manual analysis
of the output to see what changes have occurred.
From the test set, we randomly selected 100
sentences which required reordering of verbs: the
Dutch sentences ended with a verb which had to be
moved forward in the English translation. We record
whether or not the verb was correctly translated and
whether it was reordered to the correct position in
the target sentence.
Model Translated Reordered
sw, tw 81 36
sw, twc 87 52
Table 2. Analysis of % correct translation and reordering
of verbs for Dutch-English translation
In Table 2 we can see that the addition of the CCG
supertag sequence model improved both the transla-
tion of the verbs and their reordering. However, the
improvement is much more pronounced for reorder-
ing. The difference in the reordering results is signif-
icant at p < 0.05 using the ?2 significance test. This
shows that the syntactic information in the CCG su-
pertags is used by the model to prefer better word
order for the target sentence.
In Figure 2 we can see two examples of Dutch-
English translations that have improved with the ap-
plication of CCG supertag sequence models. In the
first example the verb ?heeft? occurs at the end of the
source sentence. The baseline model (sw, tw) does
not manage to translate ?heeft?. The model with the
CCG supertag sequence model (sw, twc) translates it
correctly as ?has? and reorders it correctly 4 places
to the left. The second example also shows the se-
quence model correctly translating the Dutch verb at
the end of the sentence ?nodig?. One can see that it
is still not entirely grammatical.
The improvements in reordering shown here are
reorderings over a relatively short distance, two or
three positions. This is well within the 5-gram order
of the CCG supertag sequence model and we there-
fore consider this to be local reordering.
4.4 Order of the Sequence Model
The CCG supertags describe the syntactic context
of the word they are attached to. Therefore they
have an influence that is greater in scope than sur-
face words or POS tags. Increasing the order of
the CCG supertag sequence model should also in-
crease the ability to perform longer distance reorder-
ing. However, at some point the reliability of the
predictions of the sequence models is impaired due
to sparse counts.
Model None 1gram 3gram 5gram 7gram
sw, twc 24.18 23.96 24.19 24.42 24.32
sw, twpc 24.34 23.86 24.09 24.43 24.14
Table 3. BLUE scores for Dutch-English models which
apply CCG supertag sequence models of varying orders
In Table 3 we can see that the optimal order for
the CCG supertag sequence models is 5.
4.5 Language Model vs. Supertags
The language model makes a great contribution to
the correct order of the words in the target sentence.
In this experiment we investigate whether by using a
stronger language model the contribution of the se-
quence model will no longer be relevant. The rel-
ative contribution of the language mode and differ-
ent sequence models is investigated for different lan-
guage model n-gram lengths.
Model None 1gram 3gram 5gram 7gram
sw, tw - 21.22 23.97 24.05 24.13
sw, twp 21.87 21.83 24.11 24.25 24.06
sw, twc 21.75 21.70 24.42 24.67 24.60
sw, twpc 21.99 22.07 24.43 24.48 24.42
Table 4. BLEU scores for Dutch-English models which use
language models of increasing n-gram length. Column
None does not apply any language model. Model sw, tw
does not apply any sequence models, and model sw, twpc
applies both POS tag and supertag sequence models.
In Table 4 we can see that if no language model
is present(None), the system benefits slightly from
13
source:hij kan toch niet beweren dat hij daar geen exacte informatie over heeft !
reference: how can he say he does not have any precise information ?
sw, tw:he cannot say that he is not an exact information about .
sw, twc: he cannot say that he has no precise information on this !
source: wij moeten hun verwachtingen niet beschamen . meer dan ooit hebben al die landen thans onze bijstand nodig
reference: we must not disappoint them in their expectations , and now more than ever these countries need our help
sw, tw:we must not fail to their expectations , more than ever to have all these countries now our assistance necessary
sw, twc: we must not fail to their expectations , more than ever , those countries now need our assistance
Figure 2. Examples where the CCG supertag sequence model improves Dutch-English translation
having access to all the other sequence models.
However, the language model contribution is very
strong and in isolation contributes more to transla-
tion performance than any other sequence model.
Even with a high order language model, applying
the CCG supertag sequence model still seems to im-
prove performance. This means that even if we use
a more powerful language model, the structural in-
formation contained in the supertags continues to be
beneficial.
4.6 Lexicalised Reordering vs. Supertags
In this experiment we investigate using a stronger
reordering model to see how it compares to the con-
tribution that CCG supertag sequence models make.
Moses implements the lexicalised reordering model
described by Tillman (2004), which learns whether
phrases prefer monotone, inverse or disjoint orienta-
tions with regard to adjacent phrases. We apply this
reordering models to the following experiments.
Model None Lex. Reord.
sw, tw 23.97 24.72
sw, twc 24.42 24.78
Table 5. Dutch-English models with and without a lexi-
calised reordering model.
In Table 5 we can see that lexicalised reorder-
ing improves translation performance for both mod-
els. However, the improvement that was seen us-
ing CCG supertags without lexicalised reordering,
almost disappears when using a stronger reordering
model. This suggests that CCG supertags? contribu-
tion is similar to that of a reordering model. The lex-
icalised reordering model only learns the orientation
of a phrase with relation to its adjacent phrase, so its
influence is very limited in range. If it can replace
CCG supertags, it suggests that supertags? influence
is also within a local range.
4.7 CCG Supertags on Source
Sequence models over supertags improve the perfor-
mance of phrase-based machine translation. How-
ever, this is a limited way of leveraging the rich syn-
tactic information available in the CCG categories.
We explore the potential of letting supertags direct
translation by including them as a factor on the
source. This is similar to syntax-directed translation
originally proposed for compiling (Aho and Ullman,
1969), and also used in machine translation (Quirk et
al., 2005; Huang et al, 2006). Information about the
source words? syntactic function and subcategori-
sation can directly influence the hypotheses being
searched in decoding. These experiments were per-
formed on the German to English translation task,
in contrast to the Dutch to English results given in
previous experiments.
We use a model which combines more specific
dependencies on source words and source CCG su-
pertags, with a more general model which only has
dependancies on the source word, see Equation 4.
We explore two different ways of balancing the sta-
tistical evidence from these multiple sources. The
first way to combine the general and specific sources
of information is by considering features from both
models as part of one large log-linear model. How-
ever, by including more and less informative fea-
tures in one model, we may transfer too much ex-
planatory power to the more specific features. To
overcome this problem, Smith et al (2006) demon-
strated that using ensembles of separately trained
models and combining them in a logarithmic opin-
ion pool (LOP) leads to better parameter values.
This approach was used as the second way in which
14
we combined our models. An ensemble of log-linear
models was combined using a multiplicative con-
stant ? which we train manually using held out data.
t? ?
M?
m=1
?mhm(swc, tw) + ?
(
N?
n=1
?nhn(sw, tw)
)
Typically, the two models would need to be nor-
malised before being combined, but here the multi-
plicative constant fulfils this ro?le by balancing their
separate contributions. This is the first work sug-
gesting the application of LOPs to decoding in ma-
chine translation. In the future more sophisticated
translation models and ensembles of models will
need methods such as LOPs in order to balance sta-
tistical evidence from multiple sources.
Model BLEU
sw, tw 23.30
swc, tw 19.73
single 23.29
LOP 23.46
Table 6. German-English: CCG supertags are used as a
factor on the source. The simple models are combined in
two ways: either as a single log-linear model or as a LOP
of log-linear models
Table 6 shows that the simple, general model
(model sw, tw) performs considerably better than
the simple specific model, where there are multi-
ple dependencies on both words and CCG supertags
(model swc, tw). This is because there are words in
the test sentence that have been seen before but not
with the CCG supertag. Statistical evidence from
multiple sources must be combined. The first way
to combine them is to join them in one single log-
linear model, which is trained over many features.
This makes finding good weights difficult as the in-
fluence of the general model is greater, and its dif-
ficult for the more specific model to discover good
weights. The second method for combining the in-
formation is to use the weights from the separately
trained simple models and then combine them in a
LOP. Held out data is used to set the multiplicative
constant needed to balance the contribution of the
two models. We can see that this second approach is
more successful and this suggests that it is important
to carefully consider the best ways of combining dif-
ferent sources of information when using ensembles
of models. However, the results of this experiment
are not very conclusive. There is no uncertainty in
the source sentence and the value of modelling it us-
ing CCG supertags is still to be demonstrated.
5 Conclusion
The factored translation model allows for the inclu-
sion of valuable sources of information in many dif-
ferent ways. We have shown that the syntactically
rich CCG supertags do improve the translation pro-
cess and we investigate the best way of including
them in the factored model. Using CCG supertags
over the target shows the most improvement, espe-
cially when using targeted manual evaluation. How-
ever, this effect seems to be largely due to improved
local reordering. Reordering improvements can per-
haps be more reliably made using better reordering
models or larger, more powerful language models.
A further consideration is that supertags will always
be limited to the few languages for which there are
treebanks.
Syntactic information represents embedded
structures which are naturally incorporated into
grammar-based models. The ability of a flat struc-
tured model to leverage this information seems to be
limited. CCG supertags? ability to guide translation
would be enhanced if the constraints encoded in
the tags were to be enforced using combinatory
operators.
6 Acknowledgements
We thank Hieu Hoang for assistance with Moses, Ju-
lia Hockenmaier for access to CCGbank lexicons in
German and English, and Stephen Clark and James
Curran for providing the supertagger. This work was
supported in part under the GALE program of the
Defense Advanced Research Projects Agency, Con-
tract No. HR0011-06-C-0022 and in part under the
EuroMatrix project funded by the European Com-
mission (6th Framework Programme).
15
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Properties of syn-
tax directed translations. Journal of Computer and System
Sciences, 3(3):319?334.
Srinivas Bangalore and Aravind Joshi. 1999. Supertagging:
An approach to almost parsing. Computational Linguistics,
25(2):237?265.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored language
models and generalized parallel backoff. In Proceedings of
the North American Association for Computational Linguis-
tics Conference, Edmonton, Canada.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluating the role of Bleu in machine transla-
tion research. In Proceedings of the European Chapter of
the Association for Computational Linguistics, Trento, Italy.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the Asso-
ciation for Computational Linguistics, pages 263?270, Ann
Arbor, Michigan.
Stephen Clark and James R. Curran. 2004. Parsing the wsj
using ccg and log-linear models. In Proceedings of the
Association for Computational Linguistics, pages 103?110,
Barcelona, Spain.
Stephen Clark. 2002. Supertagging for combinatory categorial
grammar. In Proceedings of the International Workshop on
Tree Adjoining Grammars, pages 19?24, Venice, Italy.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation. In
Proceedings of the Association for Computational Linguis-
tics, Prague, Czech Republic. (to appear).
Julia Hockenmaier and Mark Steedman. 2005. Ccgbank man-
ual. Technical Report MS-CIS-05-09, Department of Com-
puter and Information Science, University of Pennsylvania.
Julia Hockenmaier. 2006. Creating a ccgbank and a wide-
coverage ccg lexicon for german. In Proceedings of the In-
ternational Conference on Computational Linguistics and of
the Association for Computational Linguistics, Sydney, Aus-
tralia.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. A
syntax-directed translator with extended domain of locality.
In Proceedings of the Workshop on Computationally Hard
Problems and Joint Inference in Speech and Language Pro-
cessing, pages 1?8, New York City, New York. Association
for Computational Linguistics.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of the Human
Language Technology and North American Association for
Computational Linguistics Conference, pages 127?133, Ed-
monton, Canada. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Richard Zens, Chris Dyer, Brooke
Cowan, Wade Shen, Christine Moran, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2006. Open source toolkit
for statistical machine translation. In Summer Workshop on
Language Engineering, John Hopkins University Center for
Language and Speech Processing.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical
machine translation. In MT Summit.
Shankar Kumar and William Byrne. 2003. A weighted finite
state transducer implementation of the alignment template
model for statistical machine translation. In Proceedings of
the Human Language Technology and North American As-
sociation for Computational Linguistics Conference, pages
63?70, Edmonton, Canada.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical machine translation with
syntactified target language phrases. In Proceedings of the
Conference on Empirical Methods in Natural Language Pro-
cessing, pages 44?52, Sydney, Australia.
Franz Josef Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the Associ-
ation for Computational Linguistics, pages 160?167, Sap-
poro, Japan.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed phrasal
SMT. In Proceedings of the Association for Computational
Linguistics, pages 271?279, Ann Arbor, Michigan.
Wade Shen, Richard Zens, Nicola Bertoldi, and Marcello Fed-
erico. 2006. The JHU workshop 2006 IWSLT system. In
Proceedings of the International Workshop on Spoken Lan-
guage Translation (IWSLT), pages 59?63, Kyoto, Japan.
Andrew Smith and Miles Osborne. 2006. Using gazetteers in
discriminative information extraction. In The Conference on
Natural Language Learning, New York City, USA.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005. Loga-
rithmic opinion pools for conditional random fields. In Pro-
ceedings of the Association for Computational Linguistics,
pages 18?25, Ann Arbor, Michigan.
Andreas Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings of Spoken Language Process-
ing, pages 901?904.
Christoph Tillman. 2004. A unigram orientation model for
statistical machine translation. In Proceedings of the Hu-
man Language Technology and North American Association
for Computational Linguistics Conference, pages 101?104,
Boston, USA. Association for Computational Linguistics.
Ashish Venugopal and Stephan Vogel. 2005. Considerations
in MCE and MMI training for statistical machine transla-
tion. In Proceedings of the European Association for Ma-
chine Translation, Budapest, Hungary.
16
