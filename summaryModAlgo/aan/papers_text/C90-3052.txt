Typed Unification Grammars 
Martin C. Emele, Dhni Zajac 
Project Polygloss* 
University of Stuttgart 
IMS~CL/Ifl~AIS, Keplerstrage 17, 
D - 7000 Stuttgart 1, Federal Republic of Germany 
{emele,zajac} @is.informatik.uni-st ut gart.dbp.de 
Abstract 
We introduce TFS, a computer formal- 
ism in the class of logic ibrmaiisms which 
integrates a powerful type system. Its basic 
data structures are typed feature structures. 
The type system encourages an object- 
oriented approach to linguistic description 
by providing a multiple inheritance mecha- 
nism and an inference mechanism which al- 
lows the specitication of relations between 
levels o\[ linguistic description defined as 
classes of objects. We illustrate this alc- 
proach starting from a very simple DCG, 
and show how to make use of the typing sys- 
tem to enforce general constraints and mod- 
ularize linguistic descriptions, and how fur- 
ther abstraction leads to a tlPSG-Iike gram- 
mar. 
1 In t roduct ion  
Various proposals have been made for the integra- 
tion of type intbrmation in unification-based gram- 
mar formali,nns to enforce constraints described in 
a hierarchical way where types are partially or- 
dered with a subtype relation. Authors describe 
these extensions as "inheritance grammars", "in- 
heritance networks", ':Ii;'ature sorts", "typed t~ature 
structures",...\[1, 3, 5, 13, 17, 15, 9, 11, 7, 8\]. 
These formalisms exhibit, to various degrees, one or 
several of the following properties, characteristic of
the so-called object-oriented paradigm: a high level 
of abstraction, a capacity of inference, modularity 
and distributed control. Abstraction and modular- 
ity are needed when the linguist wants to describe 
a hierarchy of concepts (like a lexical hierarchy or 
the hierarchy of phrasal categories), and to describe 
linguistic data at different levels (e.g. morphology, 
syntax, semantics). At first glance it seems rather 
natural to develop separate modules for different lin- 
guistic levels, and to describe separately their inter- 
actions; however, great difficulties are encountered 
when these modules have to be integrated. Usually, 
there are two choices. Either everything is described 
in a single place using a deeply intricate data struc- 
ture, like packing both syntactic and semantic equa- 
tions in CF rules in some LFG extensions (e.g. \[10\]); 
the price is a loss in understmtdability and general~ 
ity. Or descriptions are kept separate and the pro- 
eessing is done accordingly: first, a morphological 
phase, then a syntactic analysis, and then a semantic 
analysis, without any communication between these 
different steps \[4\]. The price is that interdependent 
constraints between these levels are lost, resulting 
in inadequate linguistic description or very complex 
control strategies at the implementation level. 
In this paper, we argue that typed unification gram- 
mars give the linguist a formal framework which has 
the desirable properties. We will give an introduc- 
tion to such a formalism, called 'IF,_ (~Iyped I"ea- 
ture Structure), which integrates disjunctions, con- 
junctions and conditional expressions of typed fea- 
ture structures. This introduction will start from a 
very simple DCG, and will show how one can write a 
DCG-like grammar in TFS, making use of the typing 
system to enforce general constraints valid for classes 
of objects and to modularize linguistic descriptions. 
We then show that further abstraction leads to a 
I-\[PSG-like grammar. It is not our goal to give here 
a formal account of the formalism (the interested 
reader should refer to \[2\] where a very clear tbrmal 
semantics on which TFS is based is given), and we 
will use an informal approach wherever possible. 
2 Typed t~ature s t ructures  and unif ication 
Tlle basic data structure of tile language is a typed 
featm'e structure: a feature structure (FS in the fol- 
lowing) with which a type can be associated. Corn- 
pared to untyped FSs (as presented in \[16\] for exam- 
ple), the TFS system offers the possibility to name 
complex FSs, and to associate constraints with these 
names, thus defining a type. 
We write feature names in small caps letters (v, ~, 
u), type symbols in upper case letters (A, B), and we 
use symbols inside a box \[~, called tags, for denoting 
shared values. For cxarnple, the typed FS, written in 
a linear form A\[F: \[\[\]B\[H: C\],a: ~\]\], is an FS of type 
A with two features f." and e, v having as a value tile 
typed FS B\[H: A\] and G having the same shared value 
aS F. 
In the system, one can specify type definitions which 
can, as a first approximation, be seen as a kind 
of template definition like in e.g. PATR-II. There 
is, however, a major difference. The system uses a 
type inference mechanism to derive new types dy- 
namically during computation whereas templates in 
PATR-II are expanded statically at compile time. 
A type that encodes agreement features can be writ- 
ten: AGR = \[num: NUM,gender: GEN\] and types NtJM 
and GEN being themselves defined as NUM = SING 
V PLUR (where the symbol "y" denotes the logical 
OR) and GEN : MASC V FEM V NEU. The types  NUM, 
SG,... do not have definitions: they are called atomic 
types.  AGR, NUM and GEN are called complex types. 
From a set of type definitions, one can extract, a par- 
tial order on type symbols. For example, from the 
*Researdl reported in this paper is partly supported by the German Ministry of Research and Technology (BMFT, Bun- 
desminister ffir Forschung und Technologie), under grant No. 08 B3116 3. The views and conclusions contained herein are those 
of the authors and should not be interpreted as representing official policies. 
293 
set of definitions above, we can derive the following 
partial order on type symbols (Fig. l) where T rep- 
resents the greatest element (no information) and 3_ 
the smallest element (inconsistent information, lead- 
ing to failure in unification). This partial order is in 
turn used to derive a lattice of type syml)ols, which is 
then exl.ended to typed FSs or(lere(1 by (typed) sub- 
stunption, forming a lattice on which the interpreter 
works (see a formal account in \[2\]). 
/ 1 " ~  GEN 
NUM : \ / 1 ~  
SIN(; PLUR MASC FEM NEU 
For exainple, the FS f l  AGR\[nnm: NUM\] subsumes the 
FS f9 AGR\[num: PLUR, gen(ler: FEM\] because f2 has 
more specifc information than f l  : no gender is spec- 
ified in fl, and the number value of f2 PLUR is more 
specific than the number value off l ,  NUM. 
Typed unification proceeds as ordinary unification 
for FSs, recnrsively corot)thing substructures at the 
same paths. When two (typed) FSs are unified, frst 
the type symbols are unified, and if this unifica- 
tion succeeds, the FSs are unified. Unification of two 
types X and Y is defined . .(. . . . as the set of) mostgeneral 
type(s) which is smaller than both x and Y: ~t is the 
greatest lower bound (GLB) of these two symbols 
in the lattice of type syml)ols. If taso types are di- 
rectly comparable, the smallest iv the result of the 
unification: HUM M PLUR = PLUR. This extension is 
consistent with the definition of the unifier of two 
FSs ~s the GLB of these structures (see, for exam- 
pie, \[16\]). 
3 Feature  types  as data  types  and  feature  
types  as re la t ions  
3.1 The L IST type as a data  type 
A list of words will be defined in a LISP-like fash- 
ion as either the END of a list or a CONS with two 
attributes first and rest: 
LIST = END V CONS\[vms'r:WORD, nEsT:LIST\]. 
WORD denotes the set of word forms, and the list. of 
words "John likes Mary" will be encoded as 
JOHN 
CONS \[Fm.s'r: CONS \[FroST: LIKES L,-,:ST: LR .<T:  ONS V"ST:  AR*l\]t,, sr: j j 
which is a well-formed list. with respect to the LIST 
definition. (We shall use in the following a more 
concise syntax for lists: END will be written as 0; 
CONS\[FIRsT:WORD, nEs'r:klST\] will be written as (WORD 
. LIST); lists will be written using the usual abbre- 
viation for lists: the list of words "John likes Mary" 
will then be written as (JOHN LIKES MARY}). 
3.2 The APPEND type as a relation 
One can also understand feature types as relations 
much like those in PROLOG.  Let us recall the clas- 
sical PROLOG definition of append: 
append( \[\] ,L,L).  
append( \[XlL1\] ,L2, \[XlL3\] ) :-  append(L1,L2,L3). 
in I>ROLOG, the arguments of a term are identi- 
fied by their positions in the term, and the presence 
2 
of all arguments i mandatory. In an FS, arguments 
(feature vahles) are not identified by their position 
but by a label, the feature, and the absence of an 
attrilmte-value pair will denote any kind of value for 
this attribute (type T). Using the TFS syntax, where 
the symbol ':-' after an FS introduces a condition, a 
definition for append can be as follows: 
APPEND = iF: 0, iS: \[i~LIST, W: \[~\]\] V 
iF: (\[XJ ? gi \]) ,  .: \[\]~L,ST, w: (~ . 1i~1)\] 
: -- APPEND\[F: ~.t-~, 13: \[~\], W: \ [~\ ] .  
Note that the tagging syntax allows to specification 
of identity between structures and a partial instance 
of the structure. This possibility (together with the 
fact. that typing iv enforced by the system) allows 
the writing of a typed version of append, in contrast 
to the untyped PROLOG version. 
3.3 Type checking as deduct ion 
Contrary to PROLOG,  there is no distinction in TFS 
between top-level types (which could be interpreted 
as predicates) and inner types (which could be inter- 
preted as arguments): they are all typed FSs, and the 
same deduction mechanism applies for the top-level 
structure ~s well as for all substructures. A (typed) 
FS is consistent with respect to a set of type def- 
initions if it unifies with the definition of its type, 
and if each of its substructures i also consistent. 
Conditions like in the definition of append above in- 
troduce additional constraints which are erased after 
having been successfidly evaluated. When a type iv 
defined as a disjunction, a structure has to be consis- 
tent with at, least, one element of the disjunction (but 
all possibilities are explored, creating as many pos- 
sible solutions as there are disjuncts). When a type 
is defined as a conjunction (using the AND operator 
noted "A"), a structure has to be consistent with 
every single element of the conjunction. The order 
used for type checking (roughly top-down) guaran- 
tees that the solution the system finds is the GLB of 
the set of definitions augmented by the initial struc- 
ture \[2\]. 
For example, the (typed) FS AGR\[num:PLUR\] is 
consistent with regard to the set of defini- 
tions above (Sect.l). The interpreter will ap- 
ply the definition of AGR at the root of the 
FS : AGR\[num:PLUR\] M num:NUM,gender:GEN\] = 
AGR\[nu nl:PLUR,geuder:GEN 
AGR\[num:MASC\] is an inconsistent (typed) FS : 
AGR\[num:MASCJ I-1 \[uum:NUM,gender:GEN\] = I be- 
cause the types MASC ail(\] NUM have only J_, the 
bottom of the lattice, ~s a common snl)type repre- 
senting inconsistent information. Note that this type 
checking process may introduce new type symbols 
also used for checking, thus defining a type inheri- 
tance mechanism. 
A full evaluation of APPEND\[w:(A B}\] produces a set 
of three FSs: 
i F: 0, ": ID(A B), w: ~\]  v F: <~A.  (>), .: ~(B) ,  w: <t~. r~>\] v e: (~A.  (m@, . :  r~0,~:  (@ ? (m .~>)\] 
4 Typed un i f i ca t ion  grammars  
4.1 DCGs  
In this section, we describe how one can (but should 
not) write grammars using this formalism. To make 
comparisons easier, we will start from the small ex- 
ample of DCG presented in \[Pereira nd Warren 80\] 
and show how this grammar (Fig.2) can be written 
in TFS. 
294 
sentenee(s(NP, VP)) --+ noun_phrase(Num,NP), verb_phrmse(Num,VV). 
noun_phrasetNum , np(Det,Noun)) -+ determiner(Nnm,Det),  noun(Nnm,Nonn). 
noun_phrase(singular,np(Name)) --~ name(Name). 
verb..phrase(Num,vp(TV,NP)) ~ trans_verb(Num,TV),noun_phrasc(N1,NP). 
determiner(Num,det(W)) --~ \[W\], is_determiner(W,Num). 
noun(Num,n(l~oot)) --+\[W\], is_noun(W,Num,Root). 
name(name(W)) --* \[W\], is_name(W). 
trans_verb(Num,tv(Root)) ~ \[W\], is_trans(W,Num,Root). 
(Figure 2) 
is_determiner(all, p ural). 
is_noun(man, singular, ma 
isJ~oun(men, phlral, man). 
is_name(mary). 
is_trans(likes,singular, ike) 
is_trans(like, plural, like). 
In a specification like this, there are three different 
kinds of information mixed together. Take for exam- 
ple the rule "noun_phrase(Num, np(Det, Noun)) 
determiner(Num, Det), noun(Num, Noun)". In "this 
rule we find: 
1. a specification of a set of wcll-formed sub- 
strings using the CF skeleton: noun_phrase --~ 
determiner, noun; 
2. a specification of well-formed (partial) syntac- 
tic structures: the structure np(Det, Noun) is 
well-formed if Det and Noun are a well-formed 
structure and if its agreement value (variable 
Num) is the same for the Det, the Noun, and 
the noun_phrase; 
3. a specification of a relation between well- 
formed (partial) syntactic structures and well- 
formed substrings by augmenting the CF skele- 
ton with annotations representing those struc- 
tures. 
4.2 A TFS  spec i f i ca t ion  
All this information mixed together can be separated 
out and specified in a more modular way. 
1. The set of well-formed strings of words is de- 
fined as in Sect.2.1, where WORD = allVmen... 
2. The set of well-formed partial syntactic struc- 
tures, i.e. every syntactic onstraint like agree- 
\[Rent or subcategorisation, should be expressed 
in this part of the specification. 
PI4RASAL_CATEGORY = S V NP V VP. 
S = \[NP: NP\[AaR: \[~NUM\], vP: VP\[*oR: \[g3\]\]. 
\[DET: DET\[AGR: Z\]NUM\]I \[NAME: PN\] 
NP = Vo, . :  N\[,oR: ~\ ]  / V L *aR: SG ' 
LAOR: \[\]~\] J 
VP = \[V: TV\[AoR: \[~INUM\], NP: NP: AGR: \[1~\]\]. 
. 
LEXICAL_CATEGORY : DET V N V PN V V. 
DET : ALL V EVERY V A V THE. 
ALL : \[WORD: all, AOR: PL\]. 
N : MAN V WOMAN. 
MAN = \[WORD:man, AGR:SG\] V \[WORD:men, *GR:Pq. 
PN = JOHN V MARY. 
MARY = \[WORD: Mary\]. 
V :: IV V TV. 
TV ---- LIKE V LOVE. 
L,KE = \[WORD:,ike,, ,on:SG\] V \[WORD:,ike, hoR:PL\]. 
The relation between strings and struc- 
tures should be stated independently of well- 
formedness conditions on syntactic structures. 
It is expressed here in CF manner by using 
the APPEND relation on strings. (However, we 
do not advocate the exclusive use of CF-like 
relations; more complex ones can be specified 
to gain expressive power, e.g. by incorporating 
linear precedence rules). 
SENTENCE == 
NOUN_PHRASE\[sTm\[Na: ~ LIST, C-STR:\[n~\] 
VERB_PHRASE\[sTRINO: ~ ,  C-STm \ [~J  
APPEND\ [P :~,  . : ~ ,  w:~\ ]  
NOUN_PHRASE =- 
\[STmNO: ~.___~,  C-STR: NP\[DI~T: \[~\], ,OUN: IK\]\] : -- 
DETERMINER\[STmNQ: ~ ,  C-STm ~\]\] 
NOUN\[sTmNO: ~ ,  c-s~.rm \[Ell 
APPEND\ [F :~B:~,  w : ~ \ ]  
V 
\ [STR INO:~,  C-STR: PN\[NAME: ~\] \ ]  : -- 
NAMErs'rmNG: ~ ,  C-STm ~\ ]  
VERB-PHRASE : 
\[STRING: ~ ,  C-STR: VP\[v: \[~TV, NP: ~\]  : -- 
TRANS_VERB\[sTRINO: ~ ,  C-ST,R: \[~1\] 
NOUN_PHRASE\[sTmNO: ~ ,  C-STR: ~\ ] ,  
APPEND\ [~:~ B: ~ ,  w : ~ \ ]  
LEXICAL-RULE = \[STRING: (\[~\]), C-STR: \[WORD: \[~\]\]. 
DETERMINER --= LEXICAL_RULE\[c-sTm DET\]. 
NOUN = LEXlCAL.RULE\[c-STR: N\]. 
NAME = LEXICAL-RULE\[C-STm PN\]. 
TRANS_VERB = LEXICAL-RULE\[C-STR: TV\]. 
4.3 Parsing and generation 
Both parsing and generation i  the system amount 
to type inference. Either (1) for parsing or (2) gen- 
eration yield the same result (3). 
(1 )  SENTENCE\[sTRING: (Mary likes all men)\] 
(2) SENTENCE 
NP: NP\[NAME: MARY\] 
Iv: LIKE 
ALL, o  : MANIJ 
(3) SENTENCE 
"sTR,NG: (lEMony \[\]like, \[\]a, Rime.) 
C-STR: S 
~:  NP\[...E: MARY\[woRD: I~\], .oR: aSG\] 
vP: VP 
I 
v: UKE\[woRD: \[\], AGR: ~\] 
NP: NP 
\[DET: ALL\[WORD: \[~, AOR: \[~PL\]\] 
|NOUN: MAN\[woRD: \[~, AGR: \[b\]\] / 
L^o~: \[\] J 
AGR: \[\] 
This shows that the formalism has the same power as 
PI~OLOG to synthesize unspecified arguments, and 
the same evaluation mechanism can be used for both 
generation and parsing, depending on the input. 
295 
4.4 From DCG to HPSG 
In the following, we explain how one can general- 
ize the principles used for describing a DCG gram- 
mar in TFS to write an HPSG-like grammar. HPSG 
linguistic objects of all kinds, be they syntactic, 
phrase-structural, or semantic, are modeled by fea- 
ture structures \[14\]. In addition, HPSG relies heavily 
on the notion of type. Hence, TFS is perfectly suited 
for an implementation f HPSG. The grammar itself 
is purely declarative in the sense that it characterizes 
what constraints should hold on linguistic objects in- 
dependently of the order in which these constraints 
are actually applied. 
We first generalize the description of linguistic struc- 
tures: instead of defining explicit types for sen- 
tences, noun phrases, etc., we define a generic con- 
stituent structure for any kind of phrase. According 
to the specification of ItPSG linguistic objects, we 
define SIGNs as being either of type PHRASAL_SIGN 
or of type LEXICAL-SIGN \[15\]. A SIGN has a phono- 
logical value, represented as a list of words, and 
syntactic and semantic information (omitted for 
this comparison). The subtypes PHttASAL.SIGN and 
LEXICAL-SIGN inherit all the attributes and type re- 
strictions of SIGN. 
(4) SIGN -= (PHRASAL_SIGN V LEXICAL-SIGN) A 
|PHON: LIST_OF-STRINGS 
ISYN: CATEGORY 
LSEM: SEMANTIC_OBJECT 
PHRASAL_SIGNs (5) differ from LEXICAL_SIGNs (6) 
by having an additional dtrs ('daughters") attribute 
that gives information about the (lexical or phrasal) 
signs which are their immediate constituents. This 
attribute encodes the kind of information about 
constituency conventionally described as constituent 
structures. In addition, the various daughters are 
distinguished according to what kinds of informa- 
tion they contribute to the sign as a whole. Thus, 
daughters are classified as heads and complements 
as in the standard X-bar theory. In order to be a 
well formed object of type PHRASAL-SIGN, a lin- 
guistic object has to obey some general principles 
such as the "Head Feature Principle" and the "Sub- 
categorization Feature Principle". 
(5) phrasal-sign----(HEAD_FP A SUBCAT-FP A ... A 
(CH_CO_FP V HC*.CO-FP ...)) A 
LCOMP- DTI~S: LIST_OF_SIGNSJ 
(6) lexical_sign ----VERB V PNOUN V NOUN V DET V . 
Genera l  pr inciples The "Head Feature Princi- 
ple" ensures that the head features of the head- 
daughter always be shared with their phrasal pro- 
jections. It generalizes the passing of agreement in- 
formation from e.g. a verb to the VP for all kind of 
constituent and for all information related to agree- 
ment and subcatcgorisation. 
\[s.,.N: \[..E.,,o: I-~-al\] \] 
(7') HEAD_FP .--- L D~rp's: \[.EA~o"rp.: \[sv.: \[HEAD: I i i~\] \ ] \ ]  
In the DCG example, subcategorization was ex- 
pressed by introducing different kinds of lexical cat- 
egories like transitive verb (TV) vs. intransitive verbs 
IV). In HPSG, subcategorization is expressed by us- 
ng a list of signs. Thisl ist specifies the number and 
kind of signs that the head subcategorizes for the 
formation of a complete sign. Subcategorization in- 
formation is described in lexical entries. The "Subcat 
Feature Principle" ensures that in any phrasal sign, 
the subcat list of the head-daughter is the concate- 
nation of the list of complement daughters and the 
subcat list of the mother. (The order of the elements 
in the complements li t does not reflect the surface 
order but rather the more abstract "obliqueness hi- 
erarchy" (\[14\] Chap.7)). 
(8) SUBCAT-FP .---- 
? 
Grammar  rules Just as we have generalized the 
notion of constituency, we are also able to generalize 
the relations between phonological representations 
and their desired constituent structure representa- 
tions. The specialized CF-like relations for a sen- 
tence, a noun phrase, and so on in the DCG exam- 
ple can be replaced by two more general rules which 
specify constituent structure configurations accord- 
ing to the X-bar theory. 
The "Complement Head Constituent Order Fea- 
ture Principle" (9) simply states that a "saturated 
phrasal sign" (i.e. with \[syn: \ [ subcat :0 \ ] \ ]  ) is the 
combination of an unsaturated phrasal head with 
one phrasal complement (e.g. S --+ NP VP). 
(9) CH-CO-FP ---- 
SYI',I: SU~BCAT: 01PH RASAL.SIGN \[PHON" hJh_~.a~ 
DTRS: \[HEAD-DTR: : . \] 
\[ooMP-DT.~: (S,GN \[P.oN: l?omp-pho-\]\]) 
: -  APPEND 
The "Head Complements Constituent Order Feature 
Principle" (13) states that an "unsaturated phrasal 
sign" is the combination of a lexical head and any 
number of complements (e.g. VP --* V XP*). The 
relation ORDER_COMPL is used for specifyinl,,?-the or- 
dering of the phonological values of all complements. 
The phonological value of the whole phrase can then 
be specified as the concatenation f the head phonol- 
ogy value with the complement phonology value. 
(13) HC*-CO-FP -~ 
\[F: igeaa-pnonl " 
APPEND |B: Icomp-phonl 
\[w. ~ 
ORDER-COMPL I ~:oM:S: ~ hon,\] Lw : \[comp-pnonl j
296  
(10) S'GNI}',,o,:{"M,~F "likes" "all" "men")\]. 
(11) SIGN L,K   DrRS MAN,I   \]\]J 1COMP-DTF~S: { \[COMP-D'r.s: (ALL) ) ' 
(12) 
PItRASAL-SI(;N 
"PHON: (l-~"Mary" . \[2\]( .\[~\]"likes". \[~\]("all" "men"))) 
IIEAD-DTn: PHRASAL-SIGN |ItNAD-DTR: LEXlCAL-SIGN / /ftNAD: 
\[DTRS: SYN: SUBCAT: ! L \[ 
:)TRS: LCOMP-D'!"itS: {\[~ PHRASAL.:SIGN\[PltON:\[~\] ...\]) 
COMP-O'rRs: ( I~PHRASAL_SIGN ton 1\] y.: F  .Ex: 
Lsu cA : UM  \] 
Lexical entr ies  
AI,L = DET\[sYN IIINAD: \[LEX:"aII", NUM:pl\]\]\] , 
MAN =: NOUN\[~YN:IIn~A ): \[bEX:"man", NUM:sg\]V \]\]. 
\[Lr~X:" me.", NUM:pl\] 
MARY = PNOUN \[SYN:I),BAD: (Lt~X:" mary", NUM:sg\]\]\]. 
LIKE = TRANS A (3RD-S GISYN: ha,~Ao:\[u~x:" likes"\]\]\] V).
3RD-SG :~ \[sYN:rHI';AD:\[PFmSON:3, ug:sg\]\]\]. 
~RANS = \[SYN: SVr~CAT:(ISYN:\[m':AD:ICASI'-':acc\]\]\] )\] . 
,5 Pars ing  and generat ion  
Either (10) for parsing or (11) generation, tile eval- 
uation yields I, he same fully specified sign (12). 
6 Conelus ion 
'I'he main characteristics of the formalism we pre-- 
s(.nted are (1, type inheritance which provides a 
clean way of itetining classes and subclasses of ob-. 
jects, and (2) an evaluation mechanism based on 
typed unitication which provides avery powerful and 
semantically (:lear means of specifying and cornput- 
irlg relations between classes of objects. 
'lThe possibility of defining types as (conditional) ex= 
pressions of typed FSs encourages a very different 
approach to grammar specification than integrated 
CF based approaches like DCG or LFG: the gram- 
mar writer has to deline the set of linguistic objects 
relevant for the problem, define the possible rela- 
tions between these objects, and specify explicitly 
the constraints between objects and relations. 
The TFS system has been implemented in Common- 
Lisp and has been tested on Symbolics, TI Explorer, 
VAX and Allegro Common-Lisp. Sample grammars 
have been developed(\[6\], \[18\]) in order to demon- 
strate the feasibility of the approach. 
Acknowledgments  The current system is based 
ol~ a previous implementation carried out by the au- 
thors at ATR, Kyoto, as a part of' a visiting research 
program. We would like to thank Dr. Akira Kure- 
matsu, president of ATIL Interpreting Telephony 
Research Laboratories for making our stay possi- 
ble, and Mr. Teruaki Aizawa, head of the Natural 
Language Understanding Department for his con- 
stant support. We owe many clarifications to Son- 
dra Ahlen with whom we had many lively discus- 
sions. This paper has benefited from rnany comments 
fi'om our collegues at the IMS of the University of 
Stuttgart. 
Y{eferences 
\[1\] Ilassan Ait-Kaci: A Lattice Theoretic Approach 
1o Computation Based on a Calculus of Par- 
tially Ordered Type Structures, Ph.D. Thesis, 
University of Pennsylvania. 1983 
\[2\] Itassan Ai't-Kaci: "An Algebraic Semantics Ap- 
proach to the effective I{esolution of Type Equa- 
tions." in: Theoretical Compuler Science, Vol. 
45, p. 293-351. 1986 
\[3\] tlassan Ai't-Kaci, Patrick Lincoln: LIFE: a nat- 
ural language for natural anguage, MCC Tech- 
nical Report ACA-ST-074~88. 
\[d\] D.J. Arnold, S. Krauwer, M. Rosner, L. des 
Tornbes, G.B. Varile: "The <C,A>,T frame- 
work in Eurotra: a theoretically committed no- 
tation for MT", llth International Conference 
on Computational Linguistics (COLING-86), 
Bonn. 1986. 
\[5\] lfdl~ne Bestougeff, G~rard Ligozat: "Parame- 
terized abstract objects for linguistic informa- 
tion processing", 2nd European A CL Confer- 
ence, Geneva. 1985. 
\[6\] Martin C. Emele: "A Typed Feature Structure 
Unification-based Approach to Generation" in: 
Proceedings of the WGNLC of the \[ECE 1988, 
(Japan: Oiso University) 1989. 
\[7\] Martin Emele, R~mi Zajae: "RETIF: A Rewrit- 
ing System for Typed Feature Structures", (Ky- 
oto) 1989, \[ATR Technical Report TR-I-0071\] 
\[8\] Martin Emele, ~mi  Zajac: "Multiple Inheri- 
tance in RETIF", (Kyoto) 1989, \[ATR Techni- 
cal Report TR-I-0114\] 
297 
\[9\] 
\[lO\] 
\[11\] 
\[12\] 
\[13\] 
Roger Evans , Gerald Gazdar: "Inference in 
DATR", in: 4th European ACL Conference, 
Manchester. 1989. 
Jens E. Fenstad, Per-Kristian Halvorsen, Tore 
Langholm, Johan van Benthem: Situation, lan- 
guage, and logic, 1987,(Dordrecht: Reidel) 
Marc Moens, Jo Calder, Ewan Klein, Mike 
Reape, ttenk Zeevat: "Expressing generaliza- 
tions in unification-based formalisms", in: 4th 
European A CL Conference, 1989, (Manchester) 
Fernando C.N. Pereira, David H.D. War- 
ren: "Definite Clause Grammars for Language 
Analysis-A Survey of the Formalism and a Com- 
parison with Augmented Transition Networks", 
in: Artificial Intelligence 13: 231-278. 1988. 
Harry H. Porter: "Incorporating Inheritance 
and Feature Structures into a Logic Grammar 
Formalism", in: 25th Annual Meeting of the 
ACL, 1987, (Stanford) 
\[14\] Carl PolLard, Ivan A. Sag: Information-based 
Syntax and Semantics. CSLI, Lectures Notes 
Number 13, Chicago University Press, 1987 
\[15\] Carl Pollard: "Sorts in unification-based gram- 
mar and what they mean", To appear in M. 
Pinkal and B. Gregor (eds.), Unification in nat- 
ural language analysis, 1988. 
\[16\] Stuart M. Shieber: An Introduction 
to Unification-based Approaches to Grammar, 
CSLI, Lecture Notes Number 4, Chicago Uni- 
versity Press, 1986. 
\[17\] Gert Smolka: A feature logic with subsorts, 
LILOG report 33, IBM Deutschland, Stuttgart, 
1987. 
\[18\] R4mi Zajac: "A Transfer Model Using a Typed 
Feature Structure Rewriting System with In- 
heritance.", in: Proceedings of the 27th Annual 
Meeting of the A CL-89 (Vancouver, Canada) 
1989. 
298 
