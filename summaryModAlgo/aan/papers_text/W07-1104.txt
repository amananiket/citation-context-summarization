Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 25?32,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semantics-based Multiword Expression Extraction
Tim Van de Cruys and Begon?a Villada Moiro?n
Alfa Informatica, University of Groningen
Oude Kijk in ?t Jatstraat 26
9712 EK Groningen, The Netherlands
{T.Van.de.Cruys|M.B.Villada.Moiron}@rug.nl
Abstract
This paper describes a fully unsupervised
and automated method for large-scale ex-
traction of multiword expressions (MWEs)
from large corpora. The method aims at cap-
turing the non-compositionality of MWEs;
the intuition is that a noun within a MWE
cannot easily be replaced by a semanti-
cally similar noun. To implement this intu-
ition, a noun clustering is automatically ex-
tracted (using distributional similarity mea-
sures), which gives us clusters of semanti-
cally related nouns. Next, a number of statis-
tical measures ? based on selectional prefer-
ences ? is developed that formalize the intu-
ition of non-compositionality. Our approach
has been tested on Dutch, and automatically
evaluated using Dutch lexical resources.
1 Introduction
MWEs are expressions whose linguistic behaviour is
not predictable from the linguistic behaviour of their
component words. Baldwin (2006) characterizes the
idiosyncratic behavior of MWEs as ?a lack of com-
positionality manifest at different levels of analysis,
namely, lexical, morphological, syntactic, seman-
tic, pragmatic and statistical?. Some MWEs show
productive morphology and/or syntactic flexibility.
Therefore, these two aspects are not sufficient con-
ditions to discriminate actual MWEs from productive
expressions. Nonetheless, the mentioned character-
istics are useful indicators to distinguish literal and
idiomatic expressions (Fazly and Stevenson, 2006).
One property that seems to affect MWEs the most
is semantic non-compositionality. MWEs are typi-
cally non-compositional. As a consequence, it is not
possible to replace the noun of a MWE by semanti-
cally related nouns. Take for example the expres-
sions in (1) and (2):
(1) a. break the vase
b. break the cup
c. break the dish
(2) a. break the ice
b. *break the snow
c. *break the hail
Expression (1-a) is fully compositional. Therefore,
vase can easily be replaced with semantically re-
lated nouns such as cup and dish. Expression (2-a),
on the contrary, is non-compositional; ice cannot be
replaced with semantically related words, such as
snow and hail without loss of the original meaning.
Due to the idiosyncratic behavior, current propos-
als argue that MWEs need to be described in the lexi-
con (Sag et al, 2002). In most languages, electronic
lexical resources (such as dictionaries, thesauri, on-
tologies) suffer from a limited coverage of MWEs.
To facilitate the update and expansion of language
resources, the NLP community would clearly bene-
fit from automated methods that extract MWEs from
large text collections. This is the main motivation to
pursue an automated and fully unsupervised MWE
extraction method.
25
2 Previous Work
Recent proposals that attempt to capture seman-
tic compositionality (or lack thereof) employ vari-
ous strategies. Approaches evaluated so far make
use of dictionaries with semantic annotation (Piao
et al, 2006), WordNet (Pearce, 2001), automati-
cally generated thesauri (Lin, 1999; McCarthy et
al., 2003; Fazly and Stevenson, 2006), vector-based
methods that measure semantic distance (Baldwin et
al., 2003; Katz and Giesbrecht, 2006), translations
extracted from parallel corpora (Villada Moiro?n
and Tiedemann, 2006) or hybrid methods that use
machine learning techniques informed by features
coded using some of the above methods (Venkata-
pathy and Joshi, 2005).
Pearce (2001) describes a method to extract collo-
cations from corpora by measuring semantic compo-
sitionality. The underlying assumption is that a fully
compositional expression allows synonym replace-
ment of its component words, whereas a collocation
does not. Pearce measures to what degree a collo-
cation candidate allows synonym replacement. The
measurement is used to rank candidates relative to
their compositionality.
Building on Lin (1998), McCarthy et al (2003)
measure the semantic similarity between expres-
sions (verb particles) as a whole and their compo-
nent words (verb). They exploit contextual features
and frequency information in order to assess mean-
ing overlap. They established that human composi-
tionality judgements correlate well with those mea-
sures that take into account the semantics of the par-
ticle. Contrary to these measures, standard associ-
ation measures poorly correlate with human judge-
ments.
A different approach proposed by Villada Moiro?n
and Tiedemann (2006) measures translational en-
tropy as a sign of meaning predictability, and there-
fore non-compositionality. The entropy observed
among word alignments of a potential MWE varies:
highly predictable alignments show less entropy and
probably correspond to compositional expressions.
Data sparseness and polysemy pose problems be-
cause the entropy cannot be accurately calculated.
Fazly and Stevenson (2006) use lexical and
syntactic fixedness as partial indicators of non-
compositionality. Their method uses Lin?s (1998)
automatically generated thesaurus to compute a met-
ric of lexical fixedness. Lexical fixedness mea-
sures the deviation between the pointwise mutual
information of a verb-object phrase and the aver-
age pointwise mutual information of the expres-
sions resulting from substituting the noun by its
synonyms in the original phrase. This measure is
similar to Lin?s (1999) proposal for finding non-
compositional phrases. Separately, a syntactic flexi-
bility score measures the probability of seeing a can-
didate in a set of pre-selected syntactic patterns. The
assumption is that non-compositional expressions
score high in idiomaticity, that is, a score resulting
from the combination of lexical fixedness and syn-
tactic flexibility. The authors report an 80% accu-
racy in distinguishing literal from idiomatic expres-
sions in a test set of 200 expressions. The perfor-
mance of both metrics is stable across all frequency
ranges.
In this study, we are interested in establishing
whether a fully unsupervised method can capture
the (partial or) non-compositionality of MWEs. The
method should not depend on the existence of large
(open domain) parallel corpora or sense tagged cor-
pora. Also, the method should not require numer-
ous adjustments when applied to new subclasses
of MWEs, for instance, when coding empirical at-
tributes of the candidates. Similar to Lin (1999),
McCarthy et al (2003) and Fazly and Stevenson
(2006), our method makes use of automatically gen-
erated thesauri; the technique used to compile the
thesauri differs from previous work. Aiming at find-
ing a method of general applicability, the measures
to capture non-compositionality differ from those
employed in earlier work.
3 Methodology
In the description and evaluation of our algorithm,
we focus on the extraction of verbal MWEs that con-
tain prepositional complements, although we believe
the method can be easily generalized to other kinds
of MWEs.
In our semantics-based approach, we want to for-
malize the intuition of non-compositionality, so that
MWE extraction can be done in a fully automated
way. A number of statistical measures are developed
that try to capture the MWE?s non-compositional
26
bond between a verb-preposition combination and
its noun by comparing the particular noun of a MWE
candidate to other semantically related nouns.
3.1 Data extraction
The MWE candidates (verb + prepositional phrase)
are automatically extracted from the Twente Nieuws
Corpus (Ordelman, 2002), a large corpus of Dutch
newspaper texts (500 million words), which has
been automatically parsed by the Dutch dependency
parser Alpino (van Noord, 2006). Next, a matrix is
created of the 5,000 most frequent verb-preposition
combinations by the 10,000 most frequent nouns,
containing the frequency of each MWE candidate.1
To this matrix, a number of statistical measures are
applied to determine the non-compositionality of the
candidate MWEs. These statistical measures are ex-
plained in 3.3.
3.2 Clustering
In order to compare a noun to its semantically re-
lated nouns, a noun clustering is created. These
clusters are automatically extracted using standard
distributional similarity techniques (Weeds, 2003;
van der Plas and Bouma, 2005). First, depen-
dency triples are extracted from the Twente Nieuws
Corpus. Next, feature vectors are created for each
noun, containing the frequency of the dependency
relations in which the noun occurs.2 This way, a
frequency matrix of 10K nouns by 100K depen-
dency relations is constructed. The cell frequencies
are replaced by pointwise mutual information scores
(Church et al, 1991), so that more informative fea-
tures get a higher weight. The noun vectors are then
clustered into 1,000 clusters using a simple K-means
clustering algorithm (MacQueen, 1967) with cosine
similarity. During development, several other clus-
tering algorithms and parameters have been tested,
but the settings described above gave us the best
EuroWordNet similarity score (using Wu and Palmer
(1994)).
Note that our clustering algorithm is a hard clus-
tering algorithm, which means that a certain noun
1The lowest frequency verb-preposition combination (with
regard to the 10,000 nouns) appears 3 times.
2e.g. dependency relations that qualify apple might be ?ob-
ject of eat? and ?adjective red?. This gives us dependency triples
like < apple, obj, eat >.
can only be assigned to one cluster. This may pose a
problem for polysemous nouns. On the other hand,
this makes the computation of our metrics straight-
forward, since we do not have to decide among var-
ious senses of a word.
3.3 Measures
The measures used to find MWEs are inspired by
Resnik?s method to find selectional preferences
(Resnik, 1993; Resnik, 1996). Resnik uses a number
of measures based on the Kullback-Leibler diver-
gence, to measure the difference between the prior
probability of a noun class p(c) and the probabil-
ity of the class given a verb p(c|v). We adopt the
method for particular nouns, and add a measure for
determining the ?unique preference? of a noun given
other nouns in the cluster, which, we claim, yields
a measure of non-compositionality. In total, 4 mea-
sures are used, the latter two being the symmetric
counterpart of the former two.
The first two measures, Av?n (equation 2) and
Rv?n (equation 3), formalize the unique prefer-
ence of the verb3 for the noun. Equation 1 gives
the Kullback-Leibler divergence between the overall
probability distribution of the nouns and the proba-
bility distribution of the nouns given a verb; it is used
as a normalization constant in equation 2. Equa-
tion 2 models the actual preference of the verb for
the noun.
Sv =
?
n
p(n | v) logp(n | v)p(n) (1)
Av?n =
p(n | v) log p(n|v)p(n)
Sv
(2)
When p(n|v) is 0, Av?n is undefined. In this
case, we assign a score of 0.
Equation 3 gives the ratio of the verb preference
for a particular noun, compared to the other nouns
that are present in the cluster.
Rv?n =
Av?n
?
n??C Av?n?
(3)
When Rv?n is more or less equally divided
among the different nouns in the cluster, there is no
3We will use ?verb? to designate a prepositional verb, i.e. a
combination of a verb and a preposition.
27
preference of the verb for a particular noun in the
cluster, whereas scores close to 1 indicate a ?unique?
preference of the verb for a particular noun in the
cluster. Candidates whose Rv?n value approaches
1 are likely to be non-compositional expressions.
In the latter two measures, An?v and Rn?v, the
direction of preference is changed: equations 4 and 5
are the symmetric counterparts of equations 2 and 3.
Instead of the preference of the verb for the noun,
the preference of the noun for the verb is modelled.
Except for the change of preference direction, the
characteristics of the former and the latter two mea-
sures are the same.
An?v =
p(v | n) log p(v|n)p(v)
Sn
(4)
Rn?v =
An?v
?
n??C An??v
(5)
Note that, despite their symmetry, the measures
for verb preference and the measures for noun pref-
erence are different in nature. It is possible that
a certain verb only selects a restricted number of
nouns, while the nouns themselves can co-occur
with many different verbs. This brings about differ-
ent probability distributions. In our evaluation, we
want to investigate the impact of both preferences.
3.4 Example
In this section, an elaborated example is presented,
to show how our method works. Take for example
the two MWE candidates in (3):
(3) a. in
in
de
the
smaak
taste
vallen
fall
to be appreciated
b. in
in
de
the
put
well
vallen
fall
to fall down the well
In the first expression, smaak cannot be replaced
with other semantically similar nouns, such as geur
?smell? and zicht ?sight?, whereas in the second ex-
pression, put can easily be replaced with other se-
mantically similar words, such as kuil ?hole? and
krater ?crater?.
The first step in the formalization of this intuition,
is the extraction of the clusters in which the words
smaak and put appear from our clustering database.
This gives us the clusters in (4).
(4) a. smaak: aroma ?aroma?, gehoor ?hear-
ing?, geur ?smell?, gezichtsvermogen
?sight?, reuk ?smell?, spraak ?speech?,
zicht ?sight?
b. put: afgrond ?abyss?, bouwput ?build-
ing excavation?, gaatje ?hole?, gat
?hole?, hiaat ?gap?, hol ?cave?, kloof
?gap?, krater ?crater?, kuil ?hole?, lacune
?lacuna?, leemte ?gap?, valkuil ?pitfall?
Next, the various measures described in section 3.3
are applied. Resulting scores are given in tables 1
and 2.
MWE candidate Av?n Rv?n An?v Rn?v
val#in smaak .12 1.00 .04 1.00
val#in geur .00 .00 .00 .00
val#in zicht .00 .00 .00 .00
Table 1: Scores for MWE candidate in de smaak
vallen and other nouns in the same cluster.
Table 1 gives the scores for the MWE in de smaak
vallen, together with some other nouns that are
present in the same cluster. Av?n shows that there
is a clear preference (.12) of the verb val in for the
noun smaak. Rv?n shows that there is a unique
preference of the verb for the particular noun smaak.
For the other nouns (geur, zicht, . . . ), the verb has no
preference whatsoever. Therefore, the ratio of verb
preference for smaak compared to the other nouns
in the cluster is 1.00.
An?v and Rn?v show similar behaviour. There
is a preference (.04) of the noun smaak for the verb
val in, and this preference is unique (1.00).
MWE candidate Av?n Rv?n An?v Rn?v
val#in put .00 .05 .00 .05
val#in kuil .01 .11 .02 .37
val#in kloof .00 .02 .00 .03
val#in gat .04 .71 .01 .24
Table 2: Scores for MWE candidate in de put vallen
and other nouns in same cluster.
28
Table 2 gives the scores for the instance in de put
vallen ? which is not a MWE ? together with other
nouns from the same cluster. The results are quite
different from the ones in table 1. Av?n ? the pref-
erence of the verb for the noun ? is quite low in most
cases, the highest score being a score of .04 for gat.
Furthermore, Rv?n does not show a unique pref-
erence of val in for put (a low ratio score of .05).
Instead, the preference mass is divided among the
various nouns in the cluster, the highest preference
of val in being assigned to the noun gat (.71).4
The other two scores show again a similar ten-
dency; An?v ? the preference of the noun for the
verb ? is low in all cases, and when all nouns in the
cluster are considered (Rn?v), there is no ?unique?
preference of one noun for the verb val in. Instead,
the preference mass is divided among all nouns in
the cluster.
4 Results & Evaluation
4.1 Quantitative evaluation
In this section, we quantitatively evaluate our
method, and compare it to the lexical and syntactic
fixedness measures proposed by Fazly and Steven-
son (2006). More information about Fazly and
Stevenson?s measures can be found in their paper.
The potential MWEs that are extracted with the
fully unsupervised method described above and with
Fazly and Stevenson?s (2006) method (FS from here
onwards) are automatically evaluated by compar-
ing the extracted list to handcrafted MWE databases.
Since we have extracted Dutch MWEs, we are us-
ing the two Dutch resources available: the Refer-
entie Bestand Nederlands (RBN, Martin and Maks
(2005)) and the Van Dale Lexicographical Informa-
tion System (VLIS) database. Evaluation scores are
calculated with regard to the MWEs that are present
in our evaluation resources. Among the MWEs in our
reference data, we consider only those expressions
that are present in our frequency matrix: if the verb
is not among the 5,000 most frequent verbs, or the
noun is not among the 10,000 most frequent nouns,
the frequency information is not present in our input
4The expression is ambiguous: it can be used in a lit-
eral sense (in een gat vallen, ?to fall down a hole?) and in a
metaphorical sense (in een zwart gat vallen, ?to get depressed
after a joyful or busy period?).
data. Consequently, our algorithm would never be
able to find those MWEs.
The first six rows of table 3 show precision, re-
call and f-measure for various parameter thresholds
with regard to the measures Av?n, Rv?n, An?v
and Rn?v, together with the number of candidates
found (n). The last 3 rows show the highest val-
ues we were able to reach by using FS?s fixedness
scores.
Using only two parameters ? Av?n and Rv?n ?
gives the highest f-measure (? 14%), with a pre-
cision and recall of about 17% and about 12% re-
spectively. Adding parameter Rn?v increases preci-
sion but degrades recall, and this tendency continues
when adding both parameters An?v and Rn?v. In
all cases, a higher threshold increases precision but
degrades recall. When using a high threshold for all
parameters, the algorithm is able to reach a precision
of ? 38%, but recall is low (? 4%).
Lexical fixedness reaches an f-measure of ? 12%
(threshold of 3.00). These scores show the best per-
formance that we reached using lexical fixedness.
Following FS, we evaluated the syntactic fixedness
scores of expressions falling above a frequency cut-
off. Since our corpus is much larger than that used
by FS, a frequency cutoff of 50 was chosen. The pre-
cision, recall and f-measure of the syntactic fixed-
ness measure (shown on table 3) are ? 10%, 41%
and 16% respectively, showing worse precision than
our method but much better recall and f-measure.
As shown by FS, syntactic fixedness performs better
than lexical fixedness; Fixednessoverall improves
on the syntactic fixedness results and also reaches
better overall performance than our method.
The compared methods show a different behav-
ior. FS?s method favours high recall whereas our
method prefers the best trade-off between precision
and recall. We wish to highlight that our method
reaches better precision than FS?s method while han-
dling many low frequency candidates (minimum fre-
quency is 3); this makes our method preferable in
some NLP tasks. It is possible that the two methods
are capturing different properties of MWEs; in future
work, we want to analyse whether the expressions
extracted by the two methods differ.
29
parameters precision recall f-measure
Av?n Rv?n An?v Rn?v n (%) (%) (%)
.10 .80 ? ? 3175 16.09 13.11 14.45
.10 .90 ? ? 2655 17.59 11.98 14.25
.10 .80 ? .80 2225 19.19 10.95 13.95
.10 .90 ? .90 1870 20.70 9.93 13.42
.10 .80 .01 .80 1859 20.33 9.69 13.13
.20 .99 .05 .99 404 38.12 3.95 7.16
Fixednesslex(v, n) 3.00 3899 15.14 9.92 11.99
Fixednesssyn(v, n) 50 15,630 10.20 40.90 16.33
Fixednessoverall(v, n) 50 7819 13.73 27.54 18.33
Table 3: Evaluation results compared to RBN & VLIS
4.2 Qualitative evaluation
Next, we elaborate upon advantages and disadvan-
tages of our semantics-based MWE extraction algo-
rithm by examining the output of the procedure, and
looking at the characteristics of the MWEs found and
the errors made by the algorithm.
First of all, our algorithm is able to filter out gram-
matical collocations that cause problems in tradi-
tional MWE extraction paradigms. An example is
given in (5).
(5) voldoen
meet
aan
to
eisen,
demands,
voorwaarden
conditions
meet the {demands, conditions}
In traditional MWE extraction algorithms, based on
collocations, highly frequent expressions like the
ones in (5) often get classified as a MWE, even
though they are fully compositional. Such algo-
rithms correctly identify a strong lexical affinity be-
tween two component words (voldoen, aan), which
make up a grammatical collocation; however, they
fail to capture the fact that the noun may be filled in
by a semantic class of nouns. Our algorithm filters
out those expressions, because semantic similarity
between nouns that fill in the object slot is taken into
account.
Our quantitative evaluation shows that the algo-
rithm reaches the best results (i.e. the highest f-
measures) when using only two parameters (Av?n
and Rv?n). Upon closer inspection of the output,
we noticed that An?v and Rn?v are often able to
filter out non-MWEs like the expressions b in (6)
and (7).
(6) a. verschijnen
appear
op
on
toneel
stage
to appear
b. zingen
sing
op
on
toneel
stage
to sing on the stage
(7) a. lig
lie
in
in
geheugen
memory
be in memory
b. lig
lie
in
in
ziekenhuis
hospital
lie in the hospital
It is only when the two other measures (a unique
preference of the noun for the verb) are taken into
account that the b expressions are filtered out ? ei-
ther because the noun preference for the verb is very
low, or because it is more evenly distributed among
the cluster. The b expressions, which are non-MWEs,
result from the combination of a verb with a highly
frequent PP. These PPs are typically locative, direc-
tional or predicative PPs, that may combine with nu-
merous verbs.
Also, expressions like the ones in (8), where the
fixedness of the expression lies not so much in the
verb-noun combination, but more in the noun part
(naar school, naar huis) are filtered out by the lat-
ter two measures. These preposition-noun combina-
tions seem to be institutionalized PPs, so-called de-
terminerless PPs.
30
(8) a. naar
to
school
school
willen
want
want to go to school
b. naar
to
huis
home
willen
want
want to go home
We will now look at some errors made by our algo-
rithm. First of all, our algorithm highly depends on
the quality of the noun clustering. If a noun appears
in a cluster with unrelated words, the measures will
overrate the semantic uniqueness of the expressions
in which the noun appears.
Secondly, syntax might play an important role.
Sometimes, there are syntactic restrictions between
the preposition and the noun. A noun like pagina
?page? can only appear with the preposition op ?on?,
as in lees op pagina ?read on page?. Other, semanti-
cally related nouns, such as hoofdstuk ?chapter?, pre-
fer in ?in?. Due to these restrictions, the measures
will again overrate the semantic uniqueness of the
noun (pagina in the example).
Finally, our hard clustering method does not take
polysemous nouns into account. A noun may only
occur in one cluster, ignoring other possible mean-
ings. Schaal, for example, means ?dish? as well as
?scale?. In our clustering, it only appears in a cluster
of dish-related nouns. Therefore, expressions like
maak gebruik op [grote] schaal ?make use of [sth.]
on a [large] scale?, receive again overrated measures
of semantic uniqueness, because the ?scale? sense of
the noun is compared to nouns related to the ?dish?
sense.
5 Conclusions and further work
Our algorithm based on non-compositionality ex-
plores a new approach aimed at large-scale MWE
extraction. Using only two parameters, Av?n and
Rv?n, yields the highest f-measure. Using the two
other parameters, An?v and Rn?v, increases preci-
sion but degrades recall. Due to the formalization of
the intuition of non-compositionality (using an auto-
matic noun clustering), our algorithm is able to rule
out various expressions that are coined MWEs by tra-
ditional algorithms.
Note that our algorithm has taken on a purely
semantics-based approach. ?Syntactic fixedness? of
the expressions is not taken into account. Combin-
ing our semantics-based approach with other extrac-
tion techniques such as the syntactic fixedness mea-
sure proposed by Fazly and Stevenson (2006) might
improve the results significantly.
We conclude with some issues saved for future
work. First of all, we would like to combine our
semantics-based method with other methods that are
used to find MWEs (especially syntax-based meth-
ods), and implement the method in general classifi-
cation models (decision tree classifier and maximum
entropy model). This includes the use of a more
principled (machine learning) framework in order to
establish the optimal threshold values.
Next, we would like to investigate a number of
topics to improve on our semantics-based method.
First of all, using the top k similar nouns for a certain
noun ? instead of the cluster in which a noun appears
? might be more beneficial to get a grasp of the com-
positionality of MWE candidates. Also, making use
of a verb clustering in addition to the noun clustering
might help in determining the non-compositionality
of expressions. Disambiguating among the various
senses of nouns should also be a useful improve-
ment. Furthermore, we would like to generalize our
method to other syntactic patterns (e.g. verb object
combinations), and test the approach for English.
One final issue is the realization of a manual eval-
uation of our semantics-based algorithm, by hav-
ing human judges decide whether a MWE candidate
found by our algorithm is an actual MWE. Our au-
tomated evaluation framework is error-prone due to
mistakes and incompleteness of our resources. Dur-
ing qualitative evaluation, we found many actual
MWEs found by our algorithm, that were not con-
sidered correct by our resources (e.g. [iemand] in
de gordijnen jagen ?to drive s.o. mad?, op het [ver-
keerde] paard gokken ?back the wrong horse?, [de
kat] uit de boom kijken ?wait to see which way the
wind blows?, uit het [goede] hout gesneden ?be a
trustworthy person?). Conversely, there were also
questionable MWE candidates that were described
as actual MWEs in our evaluation resources (val op
woensdag ?fall on a wednesday?, neem als voorzitter
?take as chairperson?, ruik naar haring ?smell like
herring?, ben voor [. . . ] procent ?to be . . . percent?).
A manual evaluation could overcome these difficul-
ties.
We believe that our method provides a genuine
31
and successful approach to get a grasp of the non-
compositionality of MWEs in a fully automated way.
We also believe that it is one of the first methods
able to extract MWEs based on non-compositionality
on a large scale, and that traditional MWE extrac-
tion algorithms will benefit from taking this non-
compositionality into account.
Acknowledgements
This research was carried out as part of the research
program IRME STEVIN project. We would also like
to thank Gertjan van Noord and the two anonymous
reviewers for their helpful comments on an earlier
version of this paper.
References
T. Baldwin, C. Bannard, T. Tanaka, and D. Widdows. 2003. An
Empirical Model of Multiword Expressions Decomposabil-
ity. In Proc. of the ACL-2003 Workshop on Multiword Ex-
pressions: Analysis, Acquisition and Treatment, pages 89?
96, Sapporo, Japan.
T. Baldwin. 2006. Compositionality and Multiword Expres-
sions: Six of One, Half a Dozen of the Other? Invited talk
given at the COLING/ACL?06 Workshop on Multiword Ex-
pressions: Identifying and Exploiting Underlying Properties,
July.
K. Church, W. Gale, P. Hanks, and D. Hindle. 1991. Using
statistics in lexical analysis. In Uri Zernik, editor, Lexical
Acquisition: Exploiting On-line resources to build a lexicon,
pages 115?164. Lawrence Erlbaum Associates, New Jersey.
A. Fazly and S. Stevenson. 2006. Automatically constructing
a lexicon of verb phrase idiomatic combinations. In Pro-
ceedings of the 11th Conference of the European Chapter of
the Association for Computational Linguistics (EACL-2006),
Trento, Italy.
G. Katz and E. Giesbrecht. 2006. Automatic identification of
non-compositional multi-word expressions using Latent Se-
mantic Analysis. In Proc. of the COLING/ACL?06 Work-
shop on Multiword Expressions: Identifying and Exploiting
Underlying Properties, pages 12?19, Sydney, Australia.
D. Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING/ACL 98, Montreal,
Canada.
D. Lin. 1999. Automatic identification of non-compositional
phrases. In Proceedings of ACL-99, pages 317?324. Univer-
sity of Maryland.
J. B. MacQueen. 1967. Some methods for classification and
analysis of multivariate observations. In Proceedings of 5-th
Berkeley Symposium on Mathematical Statistics and Prob-
ability, volume 1, pages 281?297, Berkeley. University of
California Press.
W. Martin and I. Maks, 2005. Referentie Bestand Nederlands.
Documentatie, April.
D. McCarthy, B. Keller, and J. Carroll. 2003. Detecting a Con-
tinuum of Compositionality in Phrasal Verbs. In Proc. of
the ACL-2003 Workshop on Multiword Expressions: Analy-
sis, Acquisition and Treatment, Sapporo, Japan.
R.J.F. Ordelman. 2002. Twente Nieuws Corpus (TwNC), Au-
gust. Parlevink Language Techonology Group. University of
Twente.
D. Pearce. 2001. Synonymy in collocation extraction. In Word-
Net and Other lexical resources: applications, extensions
& customizations (NAACL 2001), pages 41?46, Pittsburgh.
Carnegie Mellon University.
S. Piao, P. Rayson, O. Mudraya, A. Wilson, and R. Garside.
2006. Measuring mwe compositionality using semantic an-
notation. In Proceedings of the Workshop on Multiword
Expressions: Identifying and Exploiting Underlying Prop-
erties, pages 2?11, Sydney, Australia. Association for Com-
putational Linguistics.
P. Resnik. 1993. Selection and Information: A Class-Based
Approach to Lexical Relationships. PhD Thesis, University
of Pennsylvania.
P. Resnik. 1996. Selectional constraints: An information-
theoretic model and its computational realization. Cogni-
tion, 61:127?159.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword Expressions: a pain in the neck for NLP.
In Proceedings of the Third International Conference on
Intelligent Text Processing and Computational Linguistics,
pages 1?15, Mexico City, Mexico.
L. van der Plas and G. Bouma. 2005. Syntactic contexts for
finding semantically similar words. Computational Linguis-
tics in the Netherlands 2004. Selected Papers from the Fif-
teenth CLIN Meeting, pages 173?184.
G. van Noord. 2006. At Last Parsing Is Now Operational.
In P. Mertens, C. Fairon, A. Dister, and P. Watrin, editors,
TALN06. Verbum Ex Machina. Actes de la 13e conference
sur le traitement automatique des langues naturelles, pages
20?42, Leuven.
S. Venkatapathy and A. Joshi. 2005. Measuring the relative
compositionality of verb-noun collocations by integrating
features. In Proceedings of the Human Language Technol-
ogy Conference and Conference on Empirical Methods in
Natural Language Processing, pages 899?906, Vancouver.
B. Villada Moiro?n and J. Tiedemann. 2006. Identifying id-
iomatic expressions using automatic word-alignment. In
Proceedings of the EACL 2006 Workshop on Multi-word-
expressions in a multilingual context?, pages 33?40, Trento,
Italy.
J. Weeds. 2003. Measures and Applications of Lexical Distri-
butional Similarity. PhD Thesis, University of Sussex.
Z. Wu and M. Palmer. 1994. Verb semantics and lexical selec-
tion. In 32nd. Annual Meeting of the Association for Com-
putational Linguistics, pages 133?138, New Mexico State
University, Las Cruces, New Mexico.
32
