Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 17?20,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Extractive Summaries for Educational Science Content 
Sebastian de la Chica, Faisal Ahmad, James H. Martin, Tamara Sumner 
Institute of Cognitive Science 
Department of Computer Science 
University of Colorado at Boulder 
sebastian.delachica, faisal.ahmad, james.martin, 
tamara.sumner@colorado.edu 
 
 
Abstract 
This paper describes an extractive summarizer 
for educational science content called 
COGENT. COGENT extends MEAD based 
on strategies elicited from an empirical study 
with domain and instructional experts. 
COGENT implements a hybrid approach inte-
grating both domain independent sentence 
scoring features and domain-aware features. 
Initial evaluation results indicate that 
COGENT outperforms existing summarizers 
and generates summaries that closely resem-
ble those generated by human experts. 
1 Introduction 
Knowledge maps consist of nodes containing rich 
concept descriptions interconnected using a limited 
set of relationship types (Holley and Dansereau, 
1984). Learning research indicates that knowledge 
maps may be useful for learners to understand the 
macro-level structure of an information space 
(O'Donnell et al, 2002). Knowledge maps have 
also emerged as an effective computational infra-
structure to support the automated generation of 
conceptual browsers. Such conceptual browsers 
appear to allow students to focus on the science 
content of large educational digital libraries (Sum-
ner et al, 2003), such as the Digital Library for 
Earth System Education (DLESE.org). Knowledge 
maps have also shown promise as domain and stu-
dent knowledge representations to support person-
alized learning interactions (de la Chica et al, 
2008). 
In this paper we describe our progress towards 
the generation of science concept inventories as 
summaries of digital library collections. Such in-
ventories provide the basis for the construction of 
knowledge maps useful both as computational 
knowledge representations and as learning re-
sources for presentation to the student. 
2 Related Work 
Our work is informed by efforts to automate the 
acquisition of ontology concepts from text. On-
toLearn extracts candidate domain terms from texts 
using a syntactic parse and updates an existing on-
tology with the identified concepts and relation-
ships (Navigli and Velardi, 2004). Knowledge 
Puzzle focuses on n-gram identification to produce 
a list of candidate terms pruned using information 
extraction techniques to derive the ontology 
(Zouaq et al, 2007). Lin and Pantel (2002) dis-
cover concepts using clustering by committee to 
group terms into conceptually related clusters. 
These approaches produce ontologies of very fine 
granularity and therefore graphs that may not be 
suitable for presentation to a student. 
Multi-document summarization (MDS) re-
search also informs our work. XDoX analyzes 
large document sets to extract important themes 
using n-gram scoring and clustering (Hardy et al, 
2002). Topic representation and topic themes have 
also served as the basis for the exploration of 
promising MDS techniques (Harabagiu and Laca-
tusu, 2005). Finally, MEAD is a widely used MDS 
and evaluation platform (Radev et al, 2000). 
While all these systems have produced promising 
results in automated evaluations, none have di-
rectly targeted educational content collections. 
17
3 Empirical Study 
We have conducted a study to capture how human 
experts processed digital library resources to create 
a domain knowledge map. Four geology and in-
structional design experts selected 20 resources 
from DLESE to construct a knowledge map on 
earthquakes and plates tectonics for high school 
age learners. The resulting knowledge map con-
sists of 564 concepts and 578 relationships. 
 
Figure 1. Expert knowledge map excerpt 
The concepts include 7,846  words, or 5% of 
the resources. Our experts relied on copying-and-
pasting (58%) and paraphrasing (37%) to create 
most concepts. Only 5% of the concepts could not 
be traced directly to the original resources. Rela-
tionship types were used in a Zipf-like distribution 
with the top 2 relationship types each accounting 
for more than 10% of all relationships: elabora-
tions (19%) and examples (14%). 
Analysis by an independent instructional expert 
indicates that this knowledge map provides ade-
quate coverage of nationally-recognized educa-
tional goals on earthquakes and plate tectonics for 
high school learners using the American Associa-
tion for the Advancement of Science (AAAS) 
Benchmarks (Project 2061, 1993). 
Verbal protocol analysis shows that all experts 
used external sources to create the knowledge map, 
including their own expertise, other digital library 
resources, and the National Science Education 
Standards (NSES), a comprehensive collection of 
nationally-recognized science learning goals for K-
12 students (National Research Council, 1996). 
We have examined sentence extraction agree-
ment between experts using the prevalence-
adjusted bias-adjusted (PABA) kappa to account 
for prevalence of judgments and conflicting biases 
amongst experts (Byrt et al, 1993). The average 
PABA-kappa value of 0.62 indicates that experts 
substantially agree on sentence extraction from 
digital library resources. This level of agreement 
suggests that these concepts may serve as the ref-
erence summary to evaluate our system. 
4 Summarizer for Science Education 
We have implemented an extractive summarizer 
for educational science content, COGENT, based 
on MEAD version 3.11 (Radev et al, 2000). 
COGENT complements the default MEAD sen-
tence scoring features with features based on find-
ings from the empirical study. COGENT 
represents a hybrid approach integrating bottom-up 
(hypertext and content word density) and top-down 
(educational standards and gazetteer) features.  
We model how human experts used external in-
formation sources with the educational standards 
feature. This feature leverages the text of the rele-
vant AAAS Benchmarks and associated NSES. 
Each sentence receives a score based on its TFIDF 
similarity to the textual contents of these learning 
goals and educational standards. 
We have developed a feature that reflects the 
large number of examples extracted by the experts. 
Earth science examples often refer to geographical 
locations and geological formations. The gazetteer 
feature checks named entities from each sentence 
against the Alexandria Digital Library (ADL) Gaz-
etteer (Hill, 2000). A gazetteer is a geo-referencing 
resource containing location and type information 
about place-names. Each sentence receives a 
TFIDF score based on place-name term frequency 
and overall uniqueness in the gazetteer. Our as-
sumption is that geographical locations with more 
unique names may be more pedagogically relevant. 
Based on the intuition that the HTML structure 
of a resource reflects relevancy, we have devel-
oped the hypertext feature. This feature computes a 
sentence score directly proportional to the HTML 
heading level and inversely proportional to the 
relative paragraph number within a heading and to 
the relative sentence position within a paragraph. 
18
To promote the extraction of sentences contain-
ing science concepts, we have developed the con-
tent word density feature. This feature computes 
the ratio of content to function words in a sentence. 
Function words are identified using a stopword list, 
and the feature only keeps sentences featuring 
more content words than function words. 
We compute the final sentence score by adding 
the MEAD default feature scores (centroid and 
position) to the COGENT feature scores (educa-
tional standards, gazetteer, and hypertext). 
COGENT keeps sentences that pass the cut-off 
constraints, including the MEAD sentence length 
of 9 and COGENT content word density of 50%. 
The default MEAD cosine re-ranker eliminates 
redundant sentences. Since the experts used 5% of 
the total word count in the resources, we produce 
summaries of that same length. 
5 Evaluation 
We have evaluated COGENT by processing the 20 
digital library resources used in the empirical study 
and comparing the output against the concepts 
identified by the experts. Three configurations are 
considered: Random, Default, and COGENT. The 
Random summary uses MEAD to extract random 
sentences. The Default summary uses the MEAD 
centroid, position and length default features. Fi-
nally, the COGENT summary extends MEAD with 
the COGENT features. 
We use ROUGE (Lin, 2004) to assess summary 
quality using common n-gram counts and longest 
common subsequence (LCS) measures. We report 
on ROUGE-1 (unigrams), ROUGE-2 (bigrams), 
ROUGE W-1.2 (weighted LCS), and ROUGE-S* 
(skip bigrams) as they have been shown to corre-
late well with human judgments for longer multi-
document summaries (Lin, 2004). Table 1 shows 
the results for recall (R), precision (P), and bal-
anced f-measure (F). 
  Random Default COGENT 
R 0.4855 0.4976 0.6073 
P 0.5026 0.5688 0.6034 R-1 
F 0.4939 0.5308 0.6054 
R 0.0972 0.1321 0.1907 
P 0.1006 0.1510 0.1895 R-2 
F 0.0989 0.1409 0.1901 
R 0.0929 0.0951 0.1185 
P 0.1533 0.1733 0.1877 R-W-1.2 
F 0.1157 0.1228 0.1453 
  Random Default COGENT 
R 0.2481 0.2620 0.3820 
P 0.2657 0.3424 0.3772 R-S* 
F 0.2566 0.2969 0.3796 
Table 1. Quality evaluation results 
Table 1 indicates that COGENT consistently 
outperforms the Random and Default summaries. 
These results indicate the promise of our approach 
to generate extractive summaries of educational 
science content. Given our interest in generating a 
pedagogically effective domain knowledge map, 
we have also conducted a content-centric evalua-
tion. 
To characterize the COGENT summary con-
tents, one of the authors manually constructed a 
summary corresponding to the best case output for 
an extractive summarizer. This Best Case summary 
comprises all the sentences from the resources that 
align to all the concepts selected by the experts. 
This summary comprises 621 sentences consisting 
of 13,116 words, or about a 9% word compression.  
We use ROUGE-L to examine the union LCS 
between the reference and candidate summaries, 
thus capturing their linguistic surface structure 
similarity. We also use MEAD to report on cosine 
similarity. Table 2 shows the results for recall (R), 
precision (P), and balanced f-measure (F). 
  Random 
(5%) 
Default 
(5%) 
COGENT 
(5%) 
Best Case  
(9%) 
R 0.4814 0.4919 0.6021 0.9669 
P 0.4982 0.5623 0.5982 0.6256 R-L 
F 0.4897 0.5248 0.6001 0.7597 
Cosine 0.5382 0.6748 0.8325 0.9323 
Table 2. Content evaluation results (word compression) 
The ROUGE-L scores consistently indicate that 
the COGENT summary may be closer to the refer-
ence in linguistic surface structure than either the 
Random or Default summaries. Since the 
COGENT ROUGE-L recall score (R=0. 6021) is 
lower than the Best Case (R=0.9669), it is likely 
that COGENT may be extracting different sen-
tences than those selected by the experts. Based on 
the high cosine similarity with the reference 
(0.8325), we hypothesize that COGENT may be 
selecting sentences that cover very similar con-
cepts to those selected by the experts, but ex-
pressed differently. 
Given the difference in word compression for 
the Best Case summary, we have performed an 
19
incremental analysis using the ROUGE-L measure 
shown in Figure 2. 
ROUGE-L COGENT Evaluation
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0 5 10 15 20 25 30
MEAD Word Percent Compression
Recall Precision F-Measure
 
Figure 2. Incremental COGENT ROUGE-L analysis 
Figure 2 indicates that COGENT can match the 
Best Case recall (R=0.9669) by generating a longer 
summary. For educational applications, lengthier 
summaries may be better suited for computational 
purposes, such as diagnosing student understand-
ing, while shorter summaries may be more appro-
priate for display to the student. 
6 Conclusions 
COGENT extends MEAD based on strategies elic-
ited from an empirical study with domain and in-
structional experts. Initial evaluation results 
indicate that COGENT holds promise for identify-
ing important domain pedagogical concepts. We 
are exploring portability to other science education 
domains and machine learning techniques to con-
nect concepts into a knowledge map. Automating 
the creation of inventories of pedagogically impor-
tant concepts may represent an important step to-
wards scalable intelligent tutoring systems. 
Acknowledgements 
This research is funded in part by the National Sci-
ence Foundation under NSF IIS/ALT Award 
0537194. Any opinions, findings, and conclusions 
or recommendations expressed in this material are 
those of the author(s) and do not necessarily reflect 
the views of the NSF. 
References 
T. Byrt, J. Bishop and J. B. Carlin. Bias, prevalence, and 
kappa. Journal of Clinical Epidemiology, 46, 5 
(1993), 423-429. 
S. de la Chica, F. Ahmad, T. Sumner, J. H. Martin and 
K. Butcher. Computational foundations for personal-
izing instruction with digital libraries. International 
Journal of Digital Libraries, to appear in the Special 
Issue on Digital Libraries and Education. 
S. Harabagiu and F. Lacatusu. Topic themes for multi-
document summarization. In Proc. of the 28th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval, 
(Salvador, Brazil, 2005), 202-209. 
H. Hardy, N. Shimizu, T. Strzalkowski, L. Ting, G. B. 
Wise and X. Zhang. Summarizing large document 
sets using concept-based clustering. In Proc. of the 
Human Language Technology Conference 2002, 
(San Diego, California, United States, 2002), 222-
227. 
L. L. Hill. Core elements of digital gazetteers: place-
names, categories, and footprints. In Proc. of the 4th 
European Conference on Digital Libraries, (Lisbon, 
Portugal, 2000), 280-290. 
C. D. Holley and D. F. Dansereau. Spatial learning 
strategies: Techniques, applications, and related is-
sues. Academic Press, Orlando, Florida, 1984. 
C. Y. Lin. ROUGE: A package for automatic evaluation 
of summaries. In Proc. of the Workshop on Text 
Summarization Branches Out, (Barcelona, Spain, 
2004). 
D. Lin and P. Pantel. Concept discovery from text. In 
Proc. of the 19th International Conference on Com-
putational Linguistics, (Taipei, Taiwan, 2002), 1-7. 
National Research Council. National Science Education 
Standards. National Academy Press, Washington, 
DC, 1996. 
R. Navigli and P. Velardi. Learning domain ontologies 
from document warehouses and dedicated websites. 
Computational Linguistics, 30, 2 (2004), 151-179. 
A. M. O'Donnell, D. F. Dansereau and R. H. Hall. 
Knowledge maps as scaffolds for cognitive process-
ing. Educational Psychology Review, 14, 1 (2002), 
71-86. 
Project 2061. Benchmarks for science literacy. Oxford 
University Press, New York, New York, United 
States, 1993. 
D. R. Radev, H. Jing and M. Budzikowska. Centroid-
based summarization of multiple documents: sen-
tence extraction, utility-based evaluation, and user 
studies. In Proc. of the ANLP/NAACL 2000 Work-
shop on Summarization, (2000), 21-30. 
T. Sumner, S. Bhushan, F. Ahmad and Q. Gu. Design-
ing a language for creating conceptual browsing in-
terfaces for digital libraries. In Proc. of the 3rd 
ACM/IEEE-CS Joint Conference on Digital Librar-
ies, (Houston, Texas, 2003), 258-260. 
A. Zouaq, R. Nkambou and C. Frasson. Learning a do-
main ontology in the Knowledge Puzzle project. In 
Proc. of the Fifth International Workshop on Ontolo-
gies and Semantic Web for E-Learning, (Marina del 
Rey, California, 2007). 
20
