Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1090?1099,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Soft-Supervised Learning for Text Classification
Amarnag Subramanya & Jeff Bilmes
Dept. of Electrical Engineering,
University of Washington, Seattle, WA 98195, USA.
{asubram,bilmes}@ee.washington.edu
Abstract
We propose a new graph-based semi-
supervised learning (SSL) algorithm and
demonstrate its application to document
categorization. Each document is represented
by a vertex within a weighted undirected
graph and our proposed framework minimizes
the weighted Kullback-Leibler divergence
between distributions that encode the class
membership probabilities of each vertex. The
proposed objective is convex with guaranteed
convergence using an alternating minimiza-
tion procedure. Further, it generalizes in
a straightforward manner to multi-class
problems. We present results on two stan-
dard tasks, namely Reuters-21578 and
WebKB, showing that the proposed algorithm
significantly outperforms the state-of-the-art.
1 Introduction
Semi-supervised learning (SSL) employs small
amounts of labeled data with relatively large
amounts of unlabeled data to train classifiers. In
many problems, such as speech recognition, doc-
ument classification, and sentiment recognition,
annotating training data is both time-consuming
and tedious, while unlabeled data are easily ob-
tained thus making these problems useful appli-
cations of SSL. Classic examples of SSL algo-
rithms include self-training (Yarowsky, 1995) and
co-training (Blum and Mitchell, 1998). Graph-
based SSL algorithms are an important class of SSL
techniques that have attracted much of attention of
late (Blum and Chawla, 2001; Zhu et al, 2003).
Here one assumes that the data (both labeled and
unlabeled) is embedded within a low-dimensional
manifold expressed by a graph. In other words,
each data sample is represented by a vertex within
a weighted graph with the weights providing a mea-
sure of similarity between vertices.
Most graph-based SSL algorithms fall under one
of two categories ? those that use the graph structure
to spread labels from labeled to unlabeled samples
(Szummer and Jaakkola, 2001; Zhu and Ghahra-
mani, 2002) and those that optimize a loss function
based on smoothness constraints derived from the
graph (Blum and Chawla, 2001; Zhu et al, 2003;
Joachims, 2003; Belkin et al, 2005). Sometimes the
two categories are similar in that they can be shown
to optimize the same underlying objective (Zhu and
Ghahramani, 2002; Zhu et al, 2003). In general
graph-based SSL algorithms are non-parametric and
transductive.1 A learning algorithm is said to be
transductive if it is expected to work only on a closed
data set, where a test set is revealed at the time of
training. In practice, however, transductive learners
can be modified to handle unseen data (Zhu, 2005a;
Sindhwani et al, 2005). A common drawback of
many graph-based SSL algorithms (e.g. (Blum and
Chawla, 2001; Joachims, 2003; Belkin et al, 2005))
is that they assume binary classification tasks and
thus require the use of sub-optimal (and often com-
putationally expensive) approaches such as one vs.
rest to solve multi-class problems, let alne struc-
tured domains such as strings and trees. There are
also issues related to degenerate solutions (all un-
labeled samples classified as belonging to a single
1Excluding Manifold Regularization (Belkin et al, 2005).
1090
class) (Blum and Chawla, 2001; Joachims, 2003;
Zhu and Ghahramani, 2002). For more background
on graph-based and general SSL and their applica-
tions, see (Zhu, 2005a; Chapelle et al, 2007; Blitzer
and Zhu, 2008).
In this paper we propose a new algorithm for
graph-based SSL and use the task of text classifica-
tion to demonstrate its benefits over the current state-
of-the-art. Text classification involves automatically
assigning a given document to a fixed number of se-
mantic categories. Each document may belong to
one, many, or none of the categories. In general,
text classification is a multi-class problem (more
than 2 categories). Training fully-supervised text
classifiers requires large amounts of labeled data
whose annotation can be expensive (Dumais et al,
1998). As a result there has been interest is us-
ing SSL techniques for text classification (Joachims,
1999; Joachims, 2003). However past work in semi-
supervised text classification has relied primarily on
one vs. rest approaches to overcome the inherent
multi-class nature of this problem. We believe such
an approach may be sub-optimal because, disregard-
ing data overlap, the different classifiers have train-
ing procedures that are independent of one other.
In order to address the above drawback we pro-
pose a new framework based on optimizing a loss
function composed of Kullback-Leibler divergence
(KL-divergence) (Cover and Thomas, 1991) terms
between probability distributions defined for each
graph vertex. The use of probability distributions,
rather than fixed integer labels, not only leads to a
straightforward multi-class generalization, but also
allows us to exploit other well-defined functions of
distributions, such as entropy, to improve system
performance and to allow for the measure of uncer-
tainty. For example, with a single integer, at most all
we know is its assignment. With a distribution, we
can continuously move from knowing an assignment
with certainty (i.e., an entropy of zero) to expres-
sions of doubt or multiple valid possibilities (i.e., an
entropy greater than zero). This is particularly use-
ful for document classification as we will see. We
also show how one can use the alternating minimiza-
tion (Csiszar and Tusnady, 1984) algorithm to op-
timize our objective leading to a relatively simple,
fast, easy-to-implement, guaranteed to converge, it-
erative, and closed form update for each iteration.
2 Proposed Graph-Based Learning
Framework
We consider the transductive learning problem, i.e.,
given a training setD = {Dl,Du}, whereDl andDu
are the sets of labeled and unlabeled samples respec-
tively, the task is to infer the labels for the samples
in Du. In other words, Du is the ?test-set.? Here
Dl = {(xi, yi)}li=1, Du = {xi}
l+u
i=l+1, xi ? X (the
input space of the classifier, and corresponds to vec-
tors of features) and yi ? Y (the space of classifier
outputs, and for our case is the space of non-negative
integers). Thus |Y| = 2 yields binary classifica-
tion while |Y| > 2 yields multi-class. We define
n = l + u, the total number of samples in the train-
ing set. Given D, most graph-based SSL algorithms
utilize an undirected weighted graph G = (V,E)
where V = {1, . . . , n} are the data points in D
and E = V ? V are the set of undirected edges
between vertices. We use wij ? W to denote the
weight of the edge between vertices i and j. W is
referred to as the weight (or affinity) matrix of G.
As will be seen shortly, the input features xi effect
the final classification results via W, i.e., the graph.
Thus graph construction is crucial to the success of
any graph-based SSL algorithm. Graph construction
?is more of an art, than science? (Zhu, 2005b) and
is an active research area (Alexandrescu and Kirch-
hoff, 2007). In general the weights are formed as
wij = sim(xi,xj)?(j ? K(i)). Here K(i) is the set
of i?s k-nearest-neighbors (KNN), sim(xi,xj) is a
given measure of similarity between xi and xj , and
?(c) returns a 1 if c is true and 0 otherwise. Getting
the similarity measure right is crucial for the success
of any SSL algorithm as that is what determines the
graph. Note that setting K(i) = |V | = n results
in a fully-connected graph. Some popular similarity
measures include
sim(xi,xj) = e
?
?xi?xj?
2
2
?2 or
sim(xi,xj) = cos(xi,xj) =
?xi,xj?
? xi ?2
2
? xj ?2
2
where ? xi ?2 is the L2 norm, and ?xi,xj? is the
inner product of xi and xj . The first similarity mea-
sure is an RBF kernel applied on the squared Eu-
clidean distance while the second is cosine similar-
ity. In this paper all graphs are constructed using
cosine similarity.
1091
We next introduce our proposed approach. For
every i ? V , we define a probability distribution pi
over the elements of Y. In addition let rj , j = 1 . . . l
be another set of probability distributions again over
the elements of Y (recall, Y is the space of classi-
fier outputs). Here {rj}j represents the labels of the
supervised portion of the training data. If the label
for a given labeled data point consists only of a sin-
gle integer, then the entropy of the corresponding rj
is zero (the probability of that integer will be unity,
with the remaining probabilities being zero). If, on
the other hand, the ?label? for a given labeled data
point consists of a set of integers (e.g., if the object
is a member of multiple classes), then rj is able to
represent this property accordingly (see below). We
emphasize again that both pi and rj are probability
distributions, with rj fixed throughout training. The
goal of learning in this paper is to find the best set
of distributions pi, ?i that attempt to: 1) agree with
the labeled data rj wherever it is available; 2) agree
with each other (when they are close according to a
graph); and 3) be smooth in some way. These cri-
teria are captured in the following new multi-class
SSL optimization procedure:
min
p
C
1
(p), where C
1
(p) =
[
l?
i=1
DKL
(
ri||pi
)
+?
n?
i
?
j
wijDKL
(
pi||pj
)
? ?
n?
i=1
H(pi)
?
?
,
(1)
and where p , (p
1
, . . . , pn) denotes the en-
tire set of distributions to be learned, H(pi) =
?
?
y
pi(y) log pi(y) is the standard Shannon en-
tropy function of pi, DKL(pi||qj) is the KL-
divergence between pi and qj , and ? and ? are hy-
perparameters whose selection we discuss in section
5. The distributions ri are derived from Dl (as men-
tioned above) and this can be done in one of the fol-
lowing ways: (a) if y?i is the single supervised label
for input xi then ri(y) = ?(y = y?i), which means
that ri gives unity probability for y equaling the la-
bel y?i; (b) if y?i = {y?
(1)
i , . . . , y?
(k)
i }, k ? |Y| is a set
of possible outputs for input xi, meaning an object
validly falls into all of the corresponding categories,
we set ri(y) = (1/k)?(y ? y?i) meaning that ri is
uniform over only the possible categories and zero
otherwise; (c) if the labels are somehow provided
in the form of a set of non-negative scores, or even
a probability distribution itself, we just set ri to be
equal to those scores (possibly) normalized to be-
come a valid probability distribution. Among these
three cases, case (b) is particularly relevant to text
classification as a given document many belong to
(and in practice may be labeled as) many classes.
The final classification results, i.e., the final labels
for Du, are then given by y? = argmax
y?Y
pi(y).
We next provide further intuition on our objective
function. SSL on a graph consists of finding a la-
belingDu that is consistent with both the labels pro-
vided in Dl and the geometry of the data induced
by the graph. The first term of C
1
will penalize
the solution pi i ? {1, . . . , l}, when it is far away
from the labeled training data Dl, but it does not in-
sist that pi = ri, as allowing for deviations from ri
can help especially with noisy labels (Bengio et al,
2007) or when the graph is extremely dense in cer-
tain regions. As explained above, our framework al-
lows for the case where supervised training is uncer-
tain or ambiguous. We consider it reasonable to call
our approach soft-supervised learning, generalizing
the notion of semi-supervised learning, since there
is even more of a continuum here between fully su-
pervised and fully unsupervised learning than what
typically exists with SSL. Soft-supervised learning
allows uncertainty to be expressed (via a probability
distribution) about any of the labels individually.
The second term of C
1
penalizes a lack of con-
sistency with the geometry of the data and can be
seen as a graph regularizer. If wij is large, we prefer
a solution in which pi and pj are close in the KL-
divergence sense. While KL-divergence is asym-
metric, given that G is undirected implies W is sym-
metric (wij = wji) and as a result the second term
is inherently symmetric.
The last term encourages each pi to be close to
the uniform distribution if not preferred to the con-
trary by the first two terms. This acts as a guard
against degenerate solutions commonly encountered
in SSL (Blum and Chawla, 2001; Joachims, 2003).
For example, consider the case where part of the
graph is almost completely disconnected from any
labeled vertex (which is possible in the k-nearest
neighbor case). In such situations the third term en-
1092
sures that the nodes in this disconnected region are
encouraged to yield a uniform distribution, validly
expressing the fact that we do not know the labels of
these nodes based on the nature of the graph. More
generally, we conjecture that by maximizing the en-
tropy of each pi, the classifier has a better chance of
producing high entropy results in graph regions of
low confidence (e.g. close to the decision boundary
and/or low density regions). This overcomes a com-
mon drawback of a large number of state-of-the-art
classifiers that tend to be confident even in regions
close to the decision boundary.
We conclude this section by summarizing some of
the features of our proposed framework. It should
be clear that C
1
uses the ?manifold assumption?
for SSL (see chapter 2 in (Chapelle et al, 2007))
? it assumes that the input data can be embed-
ded within a low-dimensional manifold (the graph).
As the objective is defined in terms of probability
distributions over integers rather than just integers
(or to real-valued relaxations of integers (Joachims,
2003; Zhu et al, 2003)), the framework general-
izes in a straightforward manner to multi-class prob-
lems. Further, all the parameters are estimated
jointly (compare to one vs. rest approaches which
involve solving |Y| independent problems). Fur-
thermore, the objective is capable of handling label
training data uncertainty (Pearl, 1990). Of course,
this objective would be useless if it wasn?t possible
to efficiently and easily optimize it on large data sets.
We next describe a method that can do this.
3 Learning with Alternating Minimization
As long as ?, ? ? 0, the objective C
1
(p) is con-
vex. This follows since DKL(pi||pj) is convex in
the pair (pi, pj) (Cover and Thomas, 1991), nega-
tive entropy is convex, and a positive-weighted lin-
ear combination of a set of convex functions is con-
vex. Thus, the problem of minimizing C
1
over the
space of collections of probability distributions (a
convex set) constitutes a convex programming prob-
lem (Bertsekas, 2004). This property is extremely
beneficial since there is a unique global optimum
and there are a variety of methods that can be used
to yield that global optimum. One possible method
might take the derivative of the objective along with
Lagrange multipliers to ensure that we stay within
the space of probability distributions. This method
can sometimes yield a closed form single-step an-
alytical expression for the globally optimum solu-
tion. Unfortunately, however, our problem does not
admit such a closed form solution because the gra-
dient of C
1
(p) with respect to pi(y) is of the form,
k
1
pi(y) log pi(y) + k2pi(y) + k3 (where k1, k2, k3
are fixed constants). Sometimes, optimizing the dual
of the objective can also produce a solution, but un-
fortunately again the dual of our objective also does
not yield a closed form solution. The typical next
step, then, is to resort to iterative techniques such
as gradient descent along with modifications to en-
sure that the solution stays within the set of proba-
bility distributions (the gradient of C
1
alone will not
necessarily point in the direction where p is still a
valid distribution) - one such modification is called
the method of multipliers (MOM). Another solu-
tion would be to use computationally complex (and
complicated) algorithms like interior point methods
(IPM). While all of the above methods (described
in detail in (Bertsekas, 2004)) are feasible ways to
solve our problem, they each have their own draw-
backs. Using MOM, for example, requires the care-
ful tuning of a number of additional parameters such
as learning rates, growth factors, and so on. IPM in-
volves inverting a matrix of the order of the number
of variables and constraints during each iteration.
We instead adopt a different strategy based on al-
ternating minimization (Csiszar and Tusnady, 1984).
This approach has a single additional optimization
parameter (contrasted with MOM), admits a closed
form solution for each iteration not involving any
matrix inversion (contrasted with IPM), and yields
guaranteed convergence to the global optimum. In
order to render our approach amenable to AM, how-
ever, we relax our objective C
1
by defining a new
(third) set of distributions for all training samples qi,
i = 1, . . . , n denoted collectively like the above us-
ing the notation q , (q
1
, . . . , qn). We define a new
objective to be optimized as follows:
min
p,q
C
2
(p, q), where C
2
(p, q) =
[
l?
i=1
DKL
(
ri||qi
)
+?
n?
i=1
?
j?N (i)
w
?
ijDKL
(
pi||qj
)
? ?
n?
i=1
H(pi)
?
?
.
1093
Before going further, the reader may be wondering
at this juncture how might it be desirable for us to
have apparently complicated the objective function
in an attempt to yield a more computationally and
methodologically superior machine learning proce-
dure. This is indeed the case as will be spelled out
below. First, in C
2
we have defined a new weight
matrix [W ?]ij = w?ij of the same size as the original
where W ? = W + ?In, where In is the n? n iden-
tity matrix, and where ? ? 0 is a non-negative con-
stant (this is the optimization related parameter men-
tioned above). This has the effect that w?ii ? wii.
In the original objective C
1
, wii is irrelevant since
DKL(p||p) = 0 for all p, but since there are now two
distributions for each training point, there should be
encouragement for the two to approach each other.
Like C
1
, the first term of C
2
ensures that the la-
beled training data is respected and the last term is
a smoothness regularizer, but these are done via dif-
ferent sets of distributions, q and p respectively ?
this choice is what makes possible the relatively sim-
ple analytical update equations given below. Next,
we see that the two objective functions in fact have
identical solutions when the optimization enforces
the constraint that p and q are equal:
min
(p,q):p=q
C
2
(p, q) = min
p
C
1
(p).
Indeed, as ? gets large, the solutions considered vi-
able are those only where p = q. We thus have that:
lim
???
min
p,q
C
2
(p, q) = min
p
C
1
(p).
Therefore, the two objectives should yield the same
solution as long as ? ? wij for all i, j. A key advan-
tage of this relaxed objective is that it is amenable to
alternating minimization, a method to produce a se-
quence of sets of distributions (pn, qn) as follows:
pn = argmin
p
C
2
(p, qn?1), qn = argmin
q
C
2
(pn, q).
It can be shown (we omit the rather lengthy proof
due to space constraints) that the sequence gener-
ated using the above minimizations converges to the
minimum of C
2
(p, q), i.e.,
lim
n??
C
2
(p(n), q(n)) = inf
p,q
C
2
(p, q),
provided we start with a distribution that is initial-
ized properly q(0)(y) > 0 ? y ? Y. The update
equations for p(n) and q(n) are given by
p
(n)
i (y) =
1
Zi
exp
?
(n?1)
i (y)
?i
,
q
(n)
i (y) =
ri(y)?(i ? l) + ?
?
j w
?
jip
(n)
j (y)
?(i ? l) + ?
?
j w
?
ji
,
where
?i = ? + ?
?
j
w
?
ij ,
?
(n?1)
i (y) = ?? + ?
?
j
w
?
ij(log q
(n?1)
j (y)? 1)
and where Zi is a normalizing constant to ensure pi
is a valid probability distribution. Note that each it-
eration of the proposed framework has a closed form
solution and is relatively simple to implement, even
for very large graphs. Henceforth we refer to the
proposed objective optimized using alternating min-
imization as AM.
4 Connections to Other Approaches
Label propagation (LP) (Zhu and Ghahramani,
2002) is a graph-based SSL algorithms that per-
forms Markov random walks on the graph and has
a straightforward extension to multi-class problems.
The update equations for LP (which also we use for
our LP implementations) may be written as
p
(n)
i (y) =
ri(y)?(i ? l) + ?(i > l)
?
j wijp
(n?1)
j (y)
?(i ? l) + ?(i > l)
?
j wij
Note the similarity to the update equation for q(n)i in
our AM case. It has been shown that the squared-
loss based SSL algorithm (Zhu et al, 2003) and LP
have similar updates (Bengio et al, 2007).
The proposed objective C
1
is similar in spirit to
the squared-loss based objective in (Zhu et al, 2003;
Bengio et al, 2007). Our method, however, differs
in that we are optimizing the KL-divergence over
probability distributions. We show in section 5 that
KL-divergence based loss significantly outperforms
the squared-loss. We believe that this could be due
1094
to the following: 1) squared loss is appropriate un-
der a Gaussian loss model which may not be opti-
mal under many circumstances (e.g. classification);
2) KL-divergence DKL(p||q) is based on a relative
(relative to p) rather than an absolute error; and 3)
under certain natural assumptions, KL-divergence is
asymptotically consistent with respect to the under-
lying probability distributions.
AM is also similar to the spectral graph trans-
ducer (Joachims, 2003) in that they both attempt
to find labellings over the unlabeled data that re-
spect the smoothness constraints of the graph. While
spectral graph transduction is an approximate solu-
tion to a discrete optimization problem (which is NP
hard), AM is an exact solution obtained by optimiz-
ing a convex function over a continuous space. Fur-
ther, while spectral graph transduction assumes bi-
nary classification problems, AM naturally extends
to multi-class situations without loss of convexity.
Entropy Minimization (EnM) (Grandvalet and
Bengio, 2004) uses the entropy of the unlabeled data
as a regularizer while optimizing a parametric loss
function defined over the labeled data. While the
objectives in the case of both AM and EnM make
use of the entropy of the unlabeled data, there are
several important differences: (a) EnM is not graph-
based, (b) EnM is parametric whereas our proposed
approach is non-parametric, and most importantly,
(c) EnM attempts to minimize entropy while the pro-
posed approach aims to maximize entropy. While
this may seem a triviality, it has catastrophic conse-
quences in terms of both the mathematics and mean-
ing. The objective in case of EnM is not convex,
whereas in our case we have a convex formulation
with simple update equations and convergence guar-
antees.
(Wang et al, 2008) is a graph-based SSL al-
gorithm that also employs alternating minimiza-
tion style optimization. However, it is inherently
squared-loss based which our proposed approach
out-performs (see section 5). Further, they do not
provide or state convergence guarantees and one
side of their update approximates an NP-complete
optimization procedure.
The information regularization (IR) (Corduneanu
and Jaakkola, 2003) algorithm also makes use of
a KL-divergence based loss for SSL. Here the in-
put space is divided into regions {Ri} which might
or might not overlap. For a given point xi ? Ri,
IR attempts to minimize the KL-divergence between
pi(yi|xi) and p?Ri(y), the agglomerative distribution
for region Ri. Given a graph, one can define a re-
gion to be a vertex and its neighbor thus making IR
amenable to graph-based SSL. In (Corduneanu and
Jaakkola, 2003), the agglomeration is performed by
a simple averaging (arithmetic mean). While IR sug-
gests (without proof of convergence) the use of al-
ternating minimization for optimization, one of the
steps of the optimization does not admit a closed-
form solution. This is a serious practical drawback
especially in the case of large data sets. (Tsuda,
2005) (hereafter referred to as PD) is an extension of
the IR algorithm to hypergraphs where the agglom-
eration is performed using the geometric mean. This
leads to closed form solutions in both steps of the al-
ternating minimization. There are several important
differences between IR and PD on one side and our
proposed approach: (a) neither IR nor PD use an
entropy regularizer, and (b) the update equation for
one of the steps of the optimization in the case of
PD (equation 13 in (Tsuda, 2005)) is actually a spe-
cial case of our update equation for pi(y) and may
be obtained by setting wij = 1/2. Further, our work
here may be easily extended to hypergraphs.
5 Results
We compare our algorithm (AM) with other
state-of-the-art SSL-based text categorization al-
gorithms, namely, (a) SVM (Joachims, 1999),
(b) Transductive-SVM (TSVM) (Joachims, 1999),
(c) Spectral Graph Transduction (SGT) (Joachims,
2003), and (d) Label Propagation (LP) (Zhu and
Ghahramani, 2002). Note that only SGT and LP
are graph-based algorithms, while SVM is fully-
supervised (i.e., it does not make use of any of the
unlabeled data). We implemented SVM and TSVM
using SVM Light (Joachims, b) and SGT using SGT
Light (Joachims, a). In the case of SVM, TSVM and
SGT we trained |Y| classifiers (one for each class) in
a one vs. rest manner precisely following (Joachims,
2003).
5.1 Reuters-21578
We used the ?ModApte? split of the Reuters-21578
dataset collected from the Reuters newswire in
1095
1987 (Lewis et al, 1987). The corpus has 9,603
training (not to be confused with D) and 3,299 test
documents (which representsDu). Of the 135 poten-
tial topic categories only the 10 most frequent cate-
gories are used (Joachims, 1999). Categories outside
the 10 most frequent were collapsed into one class
and assigned a label ?other?. For each document i
in the training and test sets, we extract features xi in
the following manner: stop-words are removed fol-
lowed by the removal of case and information about
inflection (i.e., stemming) (Porter, 1980). We then
compute TFIDF features for each document (Salton
and Buckley, 1987). All graphs were constructed us-
ing cosine similarity with TFIDF features.
For this task Y = { earn, acq, money, grain,
crude, trade, interest, ship, wheat, corn, average}.
For LP and AM, we use the output space Y? = Y?{
other }. For documents in Dl that are labeled with
multiple categories, we initialize ri to have equal
non-zero probability for each such category. For
example, if document i is annotated as belonging
to classes { acq, grain, wheat}, then ri(acq) =
ri(grain) = ri(wheat) = 1/3.
We created 21 transduction sets by randomly sam-
pling l documents from the training set with the con-
straint that each of 11 categories (top 10 categories
and the class other) are represented at least once in
each set. These samples constitute Dl. All algo-
rithms used the same transduction sets. In the case
of SGT, LP and AM, the first transduction set was
used to tune the hyperparameters which we then held
fixed for all the remaining 20 transduction sets. For
all the graph-based approaches, we ran a search over
K ? {2, 10, 50, 100, 250, 500, 1000, 2000, n} (note
K = n represents a fully connected graph). In addi-
tion, in the case of AM, we set ? = 2 for all exper-
iments, and we ran a search over ? ? {1e?8, 1e?4,
0.01, 0.1, 1, 10, 100} and ? ? {1e?8, 1e?6, 1e?4,
0.01, 0.1}, for SGT the search was over c ? {3000,
3200, 3400, 3800, 5000, 100000} (see (Joachims,
2003)).
We report precision-recall break even point
(PRBEP) results on the 3,299 test documents in Ta-
ble 1. PRBEP has been a popular measure in infor-
mation retrieval (see e.g. (Raghavan et al, 1989)).
It is defined as that value for which precision and
recall are equal. Results for each category in Ta-
ble 1 were obtained by averaging the PRBEP over
Category SVM TSVM SGT LP AM
earn 91.3 95.4 90.4 96.3 97.9
acq 67.8 76.6 91.9 90.8 97.2
money 41.3 60.0 65.6 57.1 73.9
grain 56.2 68.5 43.1 33.6 41.3
crude 40.9 83.6 65.9 74.8 55.5
trade 29.5 34.0 36.0 56.0 47.0
interest 35.6 50.8 50.7 47.9 78.0
ship 32.5 46.3 49.0 26.4 39.6
wheat 47.9 44.4 59.1 58.2 64.3
corn 41.3 33.7 51.2 55.9 68.3
average 48.9 59.3 60.3 59.7 66.3
Table 1: P/R Break Even Points (PRBEP) for the top
10 categories in the Reuters data set with l = 20 and
u = 3299. All results are averages over 20 randomly
generated transduction sets. The last row is the macro-
average over all the categories. Note AM is the proposed
approach.
the 20 transduction sets. The final row ?average?
was obtained by macro-averaging (average of av-
erages). The optimal value of the hyperparame-
ters in case of LP was K = 100; in case of AM,
K = 2000, ? = 1e?4, ? = 1e?2; and in the case
of SGT, K = 100, c = 3400. The results show
that AM outperforms the state-of-the-art on 6 out of
10 categories and is competitive in 3 of the remain-
ing 4 categories. Further it significantly outperforms
all other approaches in case of the macro-averages.
AM is significant over its best competitor SGT at
the 0.0001 level according to the difference of pro-
portions significance test.
Figure 1 shows the variation of ?average? PRBEP
against the number of labeled documents (l). For
each value of l, we tuned the hyperparameters over
the first transduction set and used these values for
all the other 20 sets. Figure 1 also shows error-
bars (? standard deviation) all the experiments. As
expected, the performance of all the approaches
improves with increasing number of labeled docu-
ments. Once again in this case, AM, outperforms
the other approaches for all values of l.
5.2 WebKB Collection
World Wide Knowledge Base (WebKB) is a collec-
tion of 8282 web pages obtained from four academic
1096
0 50 100 150 200 250 300 350 400 450 50045
50
55
60
65
70
75
80
85
Number of Labeled Documents
Ave
rage
 PR
BEP
 
 
AMSGTLPTSVMSVM
Figure 1: Average PRBEP over all classes vs.
number of labeled documents (l) for Reuters data
set
0 100 200 300 400 500 600
20
30
40
50
60
70
80
Number of Labeled Documents
Ave
rage
 PR
BEP
 
 
AMSGTLPTSVMSVM
Figure 2: Average PRBEP over all classes vs.
number of labeled documents (l) for WebKB col-
lection.
domains. The web pages in the WebKB set are la-
beled using two different polychotomies. The first
is according to topic and the second is according to
web domain. In our experiments we only consid-
ered the first polychotomy, which consists of 7 cat-
egories: course, department, faculty, project, staff,
student, and other. Following (Nigam et al, 1998)
we only use documents from categories course, de-
partment, faculty, project which gives 4199 docu-
ments for the four categories. Each of the documents
is in HTML format containing text as well as other
information such as HTML tags, links, etc. We used
both textual and non-textual information to construct
the feature vectors. In this case we did not use ei-
ther stop-word removal or stemming as this has been
found to hurt performance on this task (Nigam et al,
1998). As in the the case of the Reuters data set
we extracted TFIDF features for each document and
constructed the graph using cosine similarity.
As in (Bekkerman et al, 2003), we created four
roughly-equal random partitions of the data set. In
order to obtain Dl, we first randomly choose a split
and then sample l documents from that split. The
other three splits constitute Du. We believe this is
more realistic than sampling the labeled web-pages
from a single university and testing web-pages from
the other universities (Joachims, 1999). This method
of creating transduction sets allows us to better eval-
uate the generalization performance of the various
algorithms. Once again we create 21 transduction
sets and the first set was used to tune the hyperpa-
rameters. Further, we ran a search over the same grid
as used in the case of Reuters. We report precision-
Class SVM TSVM SGT LP AM
course 46.5 43.9 29.9 45.0 67.6
faculty 14.5 31.2 42.9 40.3 42.5
project 15.8 17.2 17.5 27.8 42.3
student 15.0 24.5 56.6 51.8 55.0
average 23.0 29.2 36.8 41.2 51.9
Table 2: P/R Break Even Points (PRBEP) for the WebKB
data set with l = 48 and u = 3148. All results are aver-
ages over 20 randomly generated transduction sets. The
last row is the macro-average over all the classes. AM is
the proposed approach.
recall break even point (PRBEP) results on the 3,148
test documents in Table 2. For this task, we found
that the optimal value of the hyperparameter were:
in the case of LP, K = 1000; in case of AM,
K = 1000, ? = 1e?2, ? = 1e?4; and in case of
SGT, K = 100, c = 3200. Once again, AM is sig-
nificant at the 0.0001 level over its closest competi-
tor LP. Figure 2 shows the variation of PRBEP with
number of labeled documents (l) and was generated
in a similar fashion as in the case of the Reuters data
set.
6 Discussion
We note that LP may be cast into an AM-like frame-
work by using the following sequence of updates,
p
(n)
i (y) = ?(i ? l)ri(y) + ?(i > l)q
(n?1)
i ,
q
(n)
i (y) =
?
j wijp
(n)
i (y)
?
j wij
1097
To compare the behavior of AM and LP, we ap-
plied this form of LP along with AM on a simple
5-node binary-classification SSL graph where two
nodes are labeled (node 1 and 2) and the remaining
nodes are unlabeled (see Figure 3, top). Since this is
binary classification (|Y | = 2), each distribution pi
or qi can be depicted using only a single real num-
ber between 0 and 1 corresponding to the probability
that each vertex is class 2 (yes two). We show how
both LP and AM evolve starting from exactly the
same random starting point q0 (Figure 3, bottom).
For each algorithm, the figure shows that both algo-
rithms clearly converge. Each alternate iteration of
LP is such that the labeled vertices oscillate due to
its clamping back to the labeled distribution, but that
is not the case for AM. We see, moreover, qualitative
differences in the solutions as well ? e.g., AM?s so-
lution for the pendant node 5 is less confident than is
LP?s solution. More empirical comparative analysis
between the two algorithms of this sort will appear
in future work.
We have proposed a new algorithm for semi-
supervised text categorization. Empirical results
show that the proposed approach significantly out-
performs the state-of-the-art. In addition the pro-
posed approach is relatively simple to implement
and has guaranteed convergence properties. While
in this work, we use relatively simple features to
construct the graph, use of more sophisticated fea-
tures and/or similarity measures could lead to further
improved results.
Acknowledgments
This work was supported by ONR MURI grant
N000140510388, by NSF grant IIS-0093430, by
the Companions project (IST programme under EC
grant IST-FP6-034434), and by a Microsoft Re-
search Fellowship.
References
Alexandrescu, A. and Kirchhoff, K. (2007). Data-driven
graph construction for semi-supervised graph-based
learnning in nlp. In Proc. of the Human Language
Technologies Conference (HLT-NAACL).
Bekkerman, R., El-Yaniv, R., Tishby, N., and Winter, Y.
(2003). Distributional word clusters vs. words for text
categorization. J. Mach. Learn. Res., 3:1183?1208.
0.8
0.6 0.2
0.8
0.8
Node 1
Label 1
Node 2
Label 2
Node 3
Unlabeled
Node 4
Unlabeled
Node 5
Unlabeled
1
2
3
4
5 0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
AM iteration (and distribution pair) number
ve
rte
x  (d
ata 
pion
t) nu
mbe
r
 
q
(0)
p
(1)
q
(1)
p
(2)
q
(2)
p
(3)
q
(3)
p
(4)
q
(4)
p
(5)
q
(5)
p
(6)
q
(6)
p
(7)
q
(7)
p
(8)
q
(8)
p
(9)
q
(9)
p
(15)
q
(15)
p
(14)
q
(14)
p
(13)
q
(13)
p
(12)
q
(12)
p
(11)
q
(11)
p
(10)
q
(10)
 
LP iteration (and distribution pair) number
ve
rte
x  (d
ata 
pion
t) nu
mbe
r
 
 
1
2
3
4
5 0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
q
(0)
p
(1)
q
(1)
p
(2)
q
(2)
p
(3)
q
(3)
p
(4)
q
(4)
p
(5)
q
(5)
p
(6)
q
(6)
p
(7)
q
(7)
p
(8)
q
(8)
p
(9)
q
(9)
p
(15)
q
(15)
p
(14)
q
(14)
p
(13)
q
(13)
p
(12)
q
(12)
p
(11)
q
(11)
p
(10)
q
(10)
 
Figure 3: Graph (top), and alternating values of pn, qn
for increasing n for AM and LP.
1098
Belkin, M., Niyogi, P., and Sindhwani, V. (2005). On
manifold regularization. In Proc. of the Conference on
Artificial Intelligence and Statistics (AISTATS).
Bengio, Y., Delalleau, O., and Roux, N. L. (2007). Semi-
Supervised Learning, chapter Label Propogation and
Quadratic Criterion. MIT Press.
Bertsekas, D. (2004). Nonlinear Programming. Athena
Scientific Publishing.
Blitzer, J. and Zhu, J. (2008). ACL 2008 tutorial on
Semi-Supervised learning. http://ssl-acl08.
wikidot.com/.
Blum, A. and Chawla, S. (2001). Learning from labeled
and unlabeled data using graph mincuts. In Proc. 18th
International Conf. on Machine Learning, pages 19?
26. Morgan Kaufmann, San Francisco, CA.
Blum, A. and Mitchell, T. (1998). Combining labeled
and unlabeled data with co-training. In COLT: Pro-
ceedings of the Workshop on Computational Learning
Theory.
Chapelle, O., Scholkopf, B., and Zien, A. (2007). Semi-
Supervised Learning. MIT Press.
Corduneanu, A. and Jaakkola, T. (2003). On informa-
tion regularization. In Uncertainty in Artificial Intelli-
gence.
Cover, T. M. and Thomas, J. A. (1991). Elements of In-
formation Theory. Wiley Series in Telecommunica-
tions. Wiley, New York.
Csiszar, I. and Tusnady, G. (1984). Information Geome-
try and Alternating Minimization Procedures. Statis-
tics and Decisions.
Dumais, S., Platt, J., Heckerman, D., and Sahami, M.
(1998). Inductive learning algorithms and represen-
tations for text categorization. In CIKM ?98: Proceed-
ings of the seventh international conference on Infor-
mation and knowledge management, New York, NY,
USA.
Grandvalet, Y. and Bengio, Y. (2004). Semi-supervised
learning by entropy minimization. In Advances in
Neural Information Processing Systems (NIPS).
Joachims, T. SGT Light. http://sgt.joachims.
org.
Joachims, T. SVM Light. http://svmlight.
joachims.org.
Joachims, T. (1999). Transductive inference for text clas-
sification using support vector machines. In Proc. of
the International Conference on Machine Learning
(ICML).
Joachims, T. (2003). Transductive learning via spectral
graph partitioning. In Proc. of the International Con-
ference on Machine Learning (ICML).
Lewis, D. et al (1987). Reuters-21578. http:
//www.daviddlewis.com/resources/
testcollections/reuters21578.
Nigam, K., McCallum, A., Thrun, S., and Mitchell, T.
(1998). Learning to classify text from labeled and un-
labeled documents. In AAAI ?98/IAAI ?98: Proceed-
ings of the fifteenth national/tenth conference on Arti-
ficial intelligence/Innovative applications of artificial
intelligence, pages 792?799.
Pearl, J. (1990). Jeffrey?s Rule, Passage of Experience
and Neo-Bayesianism in Knowledge Representation
and Defeasible Reasoning. Kluwer Academic Pub-
lishers.
Porter, M. (1980). An algorithm for suffix stripping. Pro-
gram, 14(3):130?137.
Raghavan, V., Bollmann, P., and Jung, G. S. (1989). A
critical investigation of recall and precision as mea-
sures of retrieval system performance. ACM Trans.
Inf. Syst., 7(3):205?229.
Salton, G. and Buckley, C. (1987). Term weighting ap-
proaches in automatic text retrieval. Technical report,
Ithaca, NY, USA.
Sindhwani, V., Niyogi, P., and Belkin, M. (2005). Be-
yond the point cloud: from transductive to semi-
supervised learning. In Proc. of the International Con-
ference on Machine Learning (ICML).
Szummer, M. and Jaakkola, T. (2001). Partially la-
beled classification with Markov random walks. In
Advances in Neural Information Processing Systems,
volume 14.
Tsuda, K. (2005). Propagating distributions on a hyper-
graph by dual information regularization. In Proceed-
ings of the 22nd International Conference on Machine
Learning.
Wang, J., Jebara, T., and Chang, S.-F. (2008). Graph
transduction via alternating minimization. In Proc. of
the International Conference on Machine Learning
(ICML).
Yarowsky, D. (1995). Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association for
Computational Linguistics.
Zhu, X. (2005a). Semi-supervised learning literature sur-
vey. Technical Report 1530, Computer Sciences, Uni-
versity of Wisconsin-Madison.
Zhu, X. (2005b). Semi-Supervised Learning with
Graphs. PhD thesis, Carnegie Mellon University.
Zhu, X. and Ghahramani, Z. (2002). Learning from
labeled and unlabeled data with label propagation.
Technical report, Carnegie Mellon University.
Zhu, X., Ghahramani, Z., and Lafferty, J. (2003). Semi-
supervised learning using gaussian fields and har-
monic functions. In Proc. of the International Con-
ference on Machine Learning (ICML).
1099
