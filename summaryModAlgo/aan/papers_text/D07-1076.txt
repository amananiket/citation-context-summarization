Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 728?736, Prague, June 2007. c?2007 Association for Computational Linguistics
Tree Kernel-based Relation Extraction  
with Context-Sensitive Structured Parse Tree Information 
  
    GuoDong ZHOU12     Min ZHANG 2     Dong Hong JI 2     QiaoMing ZHU 1 
         1School of Computer Science & Technology         2  Institute for Infocomm Research 
                      Soochow Univ.                                              Heng Mui Keng Terrace 
         Suzhou, China 215006                                           Singapore 119613 
                 Email: {gdzhou,qmzhu}@suda.edu.cn      Email: {zhougd, mzhang, dhji}@i2r.a-star.edu.sg 
  
Abstract 
This paper proposes a tree kernel with context-
sensitive structured parse tree information for re-
lation extraction. It resolves two critical problems 
in previous tree kernels for relation extraction in 
two ways. First, it automatically determines a dy-
namic context-sensitive tree span for relation ex-
traction by extending the widely-used Shortest 
Path-enclosed Tree (SPT) to include necessary 
context information outside SPT. Second, it pro-
poses a context-sensitive convolution tree kernel, 
which enumerates both context-free and context-
sensitive sub-trees by considering their  ancestor 
node paths as their contexts. Moreover, this paper 
evaluates the complementary nature between our 
tree kernel and a state-of-the-art linear kernel. 
Evaluation on the ACE RDC corpora shows that 
our dynamic context-sensitive tree span is much 
more suitable for relation extraction than SPT and 
our tree kernel outperforms the state-of-the-art 
Collins and Duffy?s convolution tree kernel. It 
also shows that our tree kernel achieves much bet-
ter performance than the state-of-the-art linear 
kernels . Finally, it shows that feature-based and 
tree kernel-based methods much complement each 
other and the composite kernel can well integrate 
both flat and structured features.  
1 Introduction 
Relation extraction is to find various predefined se-
mantic relations between pairs of entities in text. The 
research in relation extraction has been promoted by 
the Message Understanding Conferences (MUCs) 
(MUC, 1987-1998) and the NIST Automatic Content 
Extraction (ACE) program (ACE, 2002-2005). Ac-
cording to the ACE Program, an entity is an object or 
a set of objects in the world and a relation is an ex-
plicitly or implicitly stated relationship among enti-
ties. For example, the sentence ?Bill Gates is the 
chairman and chief software architect of Microsoft 
Corporation.? conveys the ACE-style relation 
?EMPLOYMENT.exec? between the entities ?Bill 
Gates? (person name) and ?Microsoft Corporation? 
(organization name). Extraction of semantic relations 
between entities can be very useful in many applica-
tions such as question answering, e.g. to answer the 
query ?Who is the president of the United States??, 
and information  retrieval, e.g. to expand the query 
?George W. Bush? with ?the president of the United 
States? via his relationship with ?the United States?. 
Many researches have been done in relation extrac-
tion. Among them, feature-based methods (Kamb-
hatla 2004; Zhou et al, 2005) achieve certain success 
by employing a large amount of diverse linguistic 
features, varying from lexical knowledge, entity-
related information to syntactic parse trees, depend-
ency trees and semantic information. However, it is 
difficult for them to effectively capture structured 
parse tree information (Zhou et al2005), which is 
critical for further performance improvement in rela-
tion extraction.  
As an alternative to feature-based methods, tree 
kernel-based methods provide an elegant solution to 
explore implicitly structured features by directly 
computing the similarity between two trees. Although 
earlier researches (Zelenko et al2003; Culotta and 
Sorensen 2004; Bunescu and Mooney 2005a) only 
achieve success on simple tasks and fail on complex 
tasks, such as the ACE RDC task, tree kernel-based 
methods achieve much progress recently. As the 
state-of-the-art, Zhang et al(2006) applied the convo-
lution tree kernel (Collins and Duffy 2001) and 
achieved comparable performance with a state-of-the-
art linear kernel (Zhou et al2005) on the 5 relation  
types in the ACE RDC 2003 corpus.  
However, there are two problems in Collins and 
Duffy?s convolution tree kernel for relation extraction.  
The first is that the sub-trees enumerated in the tree 
kernel computation are context-free. That is, each 
sub-tree enumerated in the tree kernel computation 
728
does not consider the context information outside the 
sub-tree. The second is to decide a proper tree span in 
relation extraction. Zhang et al(2006) explored five 
tree spans in relation extraction and it was  a bit sur-
prising to find that the Shortest Path-enclosed Tree 
(SPT, i.e. the sub-tree enclosed by the shortest path 
linking two involved entities in the parse tree) per-
formed best. This is contrast to our intuition. For ex-
ample, ?got married? is critical to determine the 
relationship between ?John? and ?Mary? in the sen-
tence ?John and Mary got married? ? as shown in 
Figure 1(e). It is obvious that the information con-
tained in SPT (?John and Marry?) is not enough to 
determine their relationship. 
This paper proposes a context-sensitive convolu-
tion tree kernel for relation extraction to resolve the 
above two problems. It first automatically determines 
a dynamic context-sensitive tree span for relation ex-
traction by extending the Shortest Path-enclosed Tree 
(SPT) to include necessary context information out-
side SPT. Then it proposes a context-sensitive convo-
lution tree kernel, whic h not only enumerates context-
free sub-trees but also context-sensitive sub-trees by 
considering their ancestor node paths as their contexts. 
Moreover, this paper evaluates the complementary 
nature of different linear kernels and tree kernels via a 
composite kernel.  
The layout of this paper is as follows. In Section 2, 
we review related work in more details. Then, the 
dynamic context-sensitive tree span and the context-
sensitive convolution tree kernel are proposed in Sec-
tion 3 while Section 4 shows the experimental results. 
Finally, we conclude our work in Sec tion 5.  
2 Related Work 
The relation extraction task was first introduced as 
part of the Template Element task in MUC6 and then 
formulated as the Template Relation task in MUC7. 
Since then, many methods, such as feature-based 
(Kambhatla 2004; Zhou et al2005, 2006), tree ker-
nel-based (Zelenko et al2003; Culotta and Sorensen 
2004; Bunescu and Mooney 2005a; Zhang et al2006) 
and composite kernel-based (Zhao and Gris hman 
2005; Zhang et al2006), have been proposed in lit-
erature. 
For the feature-based methods, Kambhatla (2004) 
employed Maximum Entropy models to combine di-
verse lexical, syntactic and semantic features in rela-
tion extraction, and achieved the F-measure of 52.8 
on the 24 relation subtypes in the ACE RDC 2003 
corpus. Zhou et al(2005) further systematically ex-
plored diverse features through a linear kernel and 
Support Vector Machines, and achieved the F-
measures of 68.0 and 55.5 on the 5 relation types and 
the 24 relation subtypes in the ACE RDC 2003 cor-
pus respectively. One problem with the feature-based 
methods is that they need extensive feature engineer-
ing. Another problem is that, although they can ex-
plore some structured information in the parse tree 
(e.g. Kambhatla (2004) used the non-terminal path 
connecting the given two entities in a parse tree while 
Zhou et al (2005) introduced additional chunking 
features to enhance the performance), it is found dif-
ficult to well preserve structured information in the 
parse trees using the feature-based methods. Zhou et 
al (2006) further improved the performance by ex-
ploring the commonality among related classes in a 
class hierarchy using hierarchical learning strategy. 
As an alternative to the feature-based methods, the 
kernel-based methods (Haussler, 1999) have been 
proposed to implicitly explore various features in a 
high dimensional space by employing a kernel to cal-
culate the similarity between two objects directly. In 
particular, the kernel-based methods could be very 
effective at reducing the burden of feature engineer-
ing for structured objects in NLP researches, e.g. the 
tree structure in relation extraction.   
Zelenko et al (2003) proposed a kernel between 
two parse trees, which recursively matches nodes 
from roots to leaves in a top-down manner. For each 
pair of matched nodes, a subsequence kernel on their 
child nodes is invoked. They achieved quite success 
on two simple relation extraction tasks. Culotta and 
Sorensen (2004) extended this work to estimate simi-
larity between augmented dependency trees and 
achieved the F-measure of 45.8 on the 5 relation 
types in the ACE RDC 2003 corpus. One problem 
with the above two tree kernels is that matched nodes 
must be at the same height and have the same path to 
the root node. Bunescu and Mooney (2005a) pro-
posed a shortest path dependency tree kernel, which 
just sums up the number of common word classes 
at each position in the two paths, and achieved the 
F-measure of 52.5 on the 5 relation types in the ACE 
RDC 2003 corpus. They argued that the information 
to model a relationship between two entities can be 
typically captured by the shortest path between them 
in the dependency graph. While the shortest path 
may not be able to well preserve structured de-
pendency tree information, another problem with 
their kernel is that the two paths should have same 
length. This makes it suffer from the similar behavior 
with that of Culotta and Sorensen (2004): high preci-
sion but very low recall.  
As the state-of-the-art tree kernel-based method, 
Zhang et al(2006) explored various structured feature 
729
spaces and used the convolution tree kernel over 
parse trees (Collins and Duffy 2001) to model syntac-
tic structured information for relation extraction. 
They achieved the F-measures of 61.9 and 63.6 on the 
5 relation types of the ACE RDC 2003 corpus and the 
7 relation types of the ACE RDC 2004 corpus respec-
tively without entity-related information while the F-
measure on the 5 relation types in the ACE RDC 
2003 corpus reached 68.7 when entity-related infor-
mation was included in the parse tree. One problem 
with Collins and Duffy?s convolution tree kernel is 
that the sub-trees involved in the tree kernel computa-
tion are context-free, that is, they do not consider the 
information outside the sub-trees. This is different 
from the tree kernel in Culota and Sorensen (2004), 
where the sub-trees involved in the tree kernel com-
putation are context-sensitive (that is, with the path 
from the tree root node to the sub-tree root node in 
consideration). Zhang et al(2006) also showed that 
the widely-used Shortest Path-enclosed Tree (SPT) 
performed best. One problem with SPT is that it fails 
to capture the contextual information outside the 
shortest path, which is important for relation extrac-
tion in many cases. Our random selection of 100 pos i-
tive training instances from the ACE RDC 2003 
training corpus shows that ~25% of the cases need 
contextual information outside the shortest path. 
Among other kernels, Bunescu and Mooney (2005b) 
proposed a subsequence kernel and applied it in pro-
tein interaction and ACE relation extraction tasks. 
In order to integrate the advantages of feature-
based and tree kernel-based methods, some research-
ers have turned to composite kernel-based methods. 
Zhao and Grishman (2005) defined several feature-
based composite kernels to integrate diverse features 
for relation extraction and achieved the F-measure of 
70.4 on the 7 relation types of the ACE RDC 2004 
corpus. Zhang et al(2006) proposed two composite 
kernels to integrate a linear kernel and Collins and 
Duffy?s convolution tree kernel. It achieved the F-
measure of 70.9/57.2 on the 5 relation types/24 rela-
tion subtypes in the ACE RDC 2003 corpus and the 
F-measure of 72.1/63.6 on the 7 relation types/23 
relation subtypes in the ACE RDC 2004 corpus. 
The above discussion suggests that structured in-
formation in the parse tree may not be fully utilized in 
the previous works, regardless of feature-based, tree 
kernel-based or composite kernel-based methods. 
Compared with the previous works, this paper pro-
poses a dynamic context-sensitive tree span trying to 
cover necessary structured information and a context-
sensitive convolution tree kernel considering both 
context-free and context-sensitive sub-trees. Further-
more, a composite kernel is applied to combine our 
tree kernel and a state-of-the-art linear kernel for in-
tegrating both flat and structured features in relation 
extraction as well as validating their complementary 
nature. 
3 Context Sensitive Convolution Tree 
Kernel for Relation Extraction 
In this section, we first propose an algorithm to dy-
namically determine a proper context-sensitive tree 
span and then a context-sensitive convolution tree 
kernel for relation extraction.  
3.1 Dynamic Context-Sensitive Tree Span in 
Relation Extraction 
A relation instance between two entities is encaps u-
lated by a parse tree. Thus, it is critical to understand 
which portion of a parse tree is important in the tree 
kernel calculation. Zhang et al(2006) systematically 
explored seven different tree spans, including the 
Shortest Path-enclosed Tree (SPT) and a Context-
Sensitive Path-enclosed Tree1 (CSPT), and found that 
SPT per formed best. That is, SPT even outperforms 
CSPT. This is contrary to our intuition. For example, 
?got married? is critical to determine the relationship 
between ?John? and ?Mary? in the sentence ?John 
and Mary got married? ? as shown in Figure 1(e), 
and the information contained in SPT (?John and 
Mary?) is not enough to determine their relationship. 
Obviously, context-sensitive tree spans should have 
the potential for better performance. One problem 
with the context-sensitive tree span explored in Zhang 
et al(2006) is that it only considers the availability of 
entities? siblings and fails to consider following two 
factors: 
1) Whether is the information contained in SPT 
enough to determine the relationship between 
two entities? It depends. In the embedded cases, 
SPT is enough. For example, ?John?s wife? is 
enough to determine the relationship between 
?John? and ?John?s wife? in the sentence ?John?s 
wife got a good job? ? as shown in Figure 1(a) . 
However, SPT is not enough in the coordinated 
cases, e.g. to determine the relationship between 
?John? and ?Mary? in the sentence ?John and 
Mary got married? ? as shown in Figure 1(e). 
                                                               
1 CSPT means SPT extending with the 1st left sibling of 
the node of entity 1 and the 1st right sibling of the node 
of entity 2.  In the case of no available  sibling, it moves 
to the parent of current node and repeat the same proc-
ess until a sibling is available or the root is reached. 
730
2) How can we extend SPT to include necessary 
context information if there is no enough infor-
mation in SPT for relation extraction?  
To answer the above two questions, we randomly 
chose 100 positive instances from the ACE RDC 
2003 training data and studied their necessary tree 
spans. It was observed that we can classify them into 
5 categories: 1) embedded (37 instances), where one 
entity is embedded in another entity, e.g. ?John? and 
?John?s wife? as shown in Figure 1(a); 2) PP-linked 
(21 instances), where one entity is linked to another 
entity via PP attachment, e.g. ?CEO? and ?Microsoft? 
in the sentence ?CEO of Microsoft announced ? ? as 
shown in Figure 1(b); 3) semi-structured (15 in-
stances), where the sentence consists of a sequence of 
noun phrases (including the two given entities), e.g. 
?Jane? and ?ABC news? in the sentence ?Jane, ABC 
news, California.? as shown in Figure 1(c); 4) de-
scriptive (7 instances), e.g. the citizenship between 
?his mother? and ?Lebanese? in the sentence ?his 
mother Lebanese landed at ?? as shown in Figure 
1(d); 5) predicate-linked and others (19 instances, 
including coordinated cases), where the predicate 
information is necessary to determine the relationship 
between two entities, e.g.  ?John? and ?Mary? in the 
sentence ?John and Mary got married?? as shown in 
Figure 1(e); 
Based on the above observations, we implement an 
algorithm to determine the necessary tree span for the 
relation extract task. The idea behind the algorithm is 
that the necessary tree span for a relation should be 
determined dynamically according to its tree span 
category and context. Given a parsed tree and two 
entities in consideration, it first determin es the tree 
span category and then extends the tree span accord-
ingly. By default, we adopt the Shortest Path-
enclosed Tree (SPT) as our tree span. We only ex-
pand the tree span when the tree span belongs to the 
?predicate-linked? category. This is based on our ob-
servation that the tree spans belonging to the ?predi-
cate-linked? category vary much syntactically and 
majority (~70%) of them need information outside 
SPT while it is quite safe (>90%) to use SPT as the 
tree span for the remaining categories. In our algo-
rithm, the expansion is done by first moving up until 
a predicate-headed phrase is found and then moving 
down along the predicated-headed path to the predi-
cate terminal node. Figure 1(e) shows an example for 
the ?predicate-linked? category where the lines with 
arrows indicate the expansion path.  
 
   
 
e) predicate-linked: SPT and the dynamic context-sensitive tree span  
Figure 1: Different tree span categories with SPT (dotted circle) and an ex-
ample of the dynamic context-sensitive tree span (solid circle) 
  
 
Figure 2: Examples of context-
free and context-sensitive sub-
trees related with Figure 1(b). 
Note: the bold node is the root 
for a sub-tree. 
A problem with our algorithm is how to deter-
mine whether an entity pair belongs to the ?predi-
cate-linked? category. In this paper, a simple 
method is applied by regarding the ?predicate-
linked? category as the default category. That is, 
those entity pairs, which do not belong to the four 
well defined and easily detected categories (i.e. 
embedded, PP-liked, semi-structured and descrip-
tive), are classified into the ?predicate-linked? cate-
gory. 
His mother Lebanese  landed 
PRP$ NNP VBD IN 
NP-E1-PER NP-E2-GPE PP 
S 
d)  descriptive 
NP 
NN 
at 
?  
VP 
Jane ABC news ,  
NNP , NNP NNS , NNP . 
NP NP-E1-PER NP-E2-ORG
NP 
c) semi-structured  
California . . 
, 
, 
, 
NP(NN) 
of Microsoft 
IN NNP 
NP-E2-ORG 
PP(IN)-subroot 
b) context -sensitive 
NP(NN) 
of Microsoft 
IN NNP 
NP-E2-ORG 
S(VBD) 
PP(IN)-subroot 
c) context -sensitive 
PP(IN)-subtoot 
NP-E2-ORG 
of Microsoft 
IN NNP 
a) context -free 
?  
NP 
John and Mary  got 
NNP CC NNP VBD 
married  
NP-E1-PER NP-E2-PER VP 
S 
VP 
VBN ?  
John and Mary  got 
NNP CC NNP VBD  
married 
NP-E1-PER NP-E2-PER VP 
 
NP VP 
 
?  
NP 
CEO of Microsoft announced 
NN IN NNP VBD ?  
NP-E1-PER NP-E2-ORG 
VP 
S 
b)  PP -linked  
PP 
?  
John ?s wife found a  job 
NNP POS NN VBD DT JJ NN 
NP NP-E1-PER 
NP-E2-PER VP 
S 
a) embedded  
good 
731
Since ?predicate -linked? instances only occupy 
~20% of cases, this explains why SPT performs 
better than the Context-Sensitive Path-enclosed 
Tree (CSPT) as described in Zhang et al(2006): 
consistently adopting CSPT may introduce too 
much noise/unnecessary information in the tree 
kernel. 
3.2 Context-Sensitive Convolution Tree Kernel 
Given any tree span, e.g. the dynamic context-
sensitive tree span in the last subsection, we now 
study how to measure the similarity between two 
trees, using a convolution tree kernel.A convolution 
kernel (Haussler D., 1999) aims to capture structured 
information in terms of substructures . As a special-
ized convolution kernel, Collins and Duffy?s convolu-
tion tree kernel ),( 21 TTKC  (?C? for convolution) 
counts the number of common sub-trees (sub-
structures) as the syntactic structure similarity be-
tween two parse trees T1 and T2 (Collins and Duffy 
2001): 
?
??
D=
2211 ,
2121 ),(),(
NnNn
C nnTTK    (1) 
where Nj is the set of nodes in tree Tj , and 1 2( , )n nD  
evaluates the common sub-trees rooted at n1 and n2 2 
and is computed recursively as follows:  
1) If the context-free productions (Context-Free 
Grammar(CFG) rules) at 1n  and 2n  are different, 
1 2( , ) 0n nD = ; Otherwise go to 2. 
2) If both 1n  and 2n  are POS tags, 1 2( , ) 1n n lD = ? ; 
Otherwise go to 3. 
3)  Calculate 1 2( , )n nD recursively as: 
?
=
D+=D
)(#
1
2121
1
)),(),,((1(),(
nch
k
knchknchnn l  (2) 
where )(# nch is the number of children of node n , 
),( knch  is the k th child of node n  andl (0< l <1) is 
the decay factor in order to make the kernel value less 
variable with respect to different sub-tree sizes.  
This convolution tree kernel has been successfully 
applied by Zhang et al(2006) in relation extraction. 
However, there is one problem with this tree kernel: 
the sub-trees involved in the tree kernel computation 
are context-free (That is, they do not consider the 
information outside the sub-trees). This is contrast to 
                                                               
2 That is, each node n encodes the identity of a sub-
tree rooted at n and, if there are two nodes in the 
tree with the same label, the summation will go over 
both of them. 
the tree kernel proposed in Culota and Sorensen 
(2004) which is context-sensitive, that is, it considers 
the path from the tree root node to the sub-tree root 
node. In order to integrate the advantages of both tree 
kernels and resolve the problem in Collins and 
Duffy?s convolution tree kernel, this paper proposes a 
context-sensitive convolution tree kernel. It works by 
taking ancestral information (i.e. the root node path) 
of sub-trees into consideration: 
? ?
= ??
D=
m
i NnNn
ii
C
iiii
nnTTK
1 ]2[]2[],1[]1[
11
1111
])2[],1[(])2[],1[(  (3) 
Where 
? ][1 jN i is the set of root node paths with length i 
in tree T[j] while the maximal length of a root 
node path is defined by m.  
? ])[...(][ 211 jnnnjn ii = is a root node path with 
length i in tree T[j] , which takes into account the 
i-1 ancestral nodes in2 [j] of 1n [j] in T[j]. Here, 
][1 jn k+  is the parent of ][ jn k and ][1 jn  is the 
root node of a context-free sub-tree in T[j]. For 
better differentiation, the label of each ancestral 
node in in1 [j] is augmented with the POS tag of 
its head word.  
? ])2[],1[( 11 ii nnD  measures the common context-
sensitive sub-trees rooted at root node paths 
]1[1in  and ]2[1in
3. In our tree kernel, a sub-tree 
becomes context-sensitive with its dependence on 
the root node path instead of the root node itself. 
Figure 2 shows a few examples of context-
sensitive sub-trees with comparison to context-
free sub-trees. 
Similar to Collins and Duffy (2001),   our tree ker-
nel computes ])2[],1[( 11 ii nnD recursively as follows:  
1) If the context-sensitive productions (Context-
Sensitive Grammar (CSG) rules with root node 
paths as their left hand sides) rooted at ]1[1in  and 
]2[1
in  are different, return ])2[],1[( 11
ii nnD =0; 
Otherwise go to Step 2. 
2) If both ]1[1n  and ]2[1n  are POS tags, 
l=D ])2[],1[( 11 ii nn ; Otherwise go to Step 3. 
                                                               
3 That is, each root node path in1  encodes the identity 
of a context-sensitive sub-tree rooted at in1  and, if 
there are two root node paths in the tree with the 
same label sequence, the summation will go over 
both of them.  
732
3) Calculate ])2[],1[( 11 ii nnD  recursively as: 
?
=
D+=
D
])1[(#
1
11
11
1
))],2[(),],1[((1(
])2[],1[(
inch
k
ii
ii
knchknch
nn
l
 (4) 
where ])],[( 1 kjnch i  is the k
th context-sensitive 
child of the context-sensitive sub-tree rooted at 
][1 jn i  with ])[(# 1 jnch i the number of the con-
text-sensitive children. Here, l (0< l <1) is the 
decay factor in order to make the kernel value 
less variable with respect to different sizes of the 
context-sensitive sub-trees. 
It is worth comparing our tree kernel with previous 
tree kernels. Obviously, our tree kernel is an exten-
sion of Collins and Duffy?s convolution tree kernel, 
which is a special case of our tree kernel (if m=1 in 
Equation (3)). Our tree kernel not only counts the 
occurrence of each context-free sub-tree, which does 
not consider its ancestors, but also counts the occur-
rence of each context-sensitive sub-tree, which con-
siders its ancestors. As a result, our tree kernel is not 
limited by the constraints in previous tree kernels (as 
discussed in Section 2), such as Collins and Duffy 
(2001), Zhang et al(2006), Culotta and Sorensen 
(2004) and Bunescu and Mooney (2005a). Finally, 
let?s study the computational issue with our tree ker-
nel. Although our tree kernel takes the context-
sensitive sub-trees into consideration, it only slightly 
increases the computational burden, compared with 
Collins and Duffy?s convolution tree kernel. This is 
due to that 0])2[],1[( 11 =D nn  holds for the major-
ity of context-free sub-tree pairs (Collins and Duffy 
2001) and that computation for context-sensitive sub-
tree pairs is necessary only when 
0])2[],1[( 11 ?D nn  and the context-sensitive sub-
tree pairs have the same root node path(i.e. 
]2[]1[ 11 ii nn =  in Equation (3)). 
4 Experimentation 
This paper uses the ACE RDC 2003 and 2004 cor-
pora provided by LDC in all our experiments. 
4.1 Experimental Setting  
The ACE RDC corpora are gathered from various 
newspapers, newswire and broadcasts. In the 2003 
corpus , the training set consists of 674 documents and 
9683 positive relation instances w hile the test set con-
sists of 97 documents and 1386 positive relation in-
stances. The 2003 corpus defines 5 entity types, 5 
major relation types and 24 relation subtypes. All the 
reported performances in this paper on the ACE RDC 
2003 corpus are evaluated on the test data. The 2004 
corpus  contains 451 documents and 5702 positive 
relation instances. It redefines 7 entity types, 7 major 
relation types and 23 relation subtypes. For compari-
son, we use the same setting as Zhang et al(2006) by 
applying a 5-fold cross-validation on a subset of the 
2004 data, containing 348 documents and 4400 rela-
tion instances. That is, all the reported performances 
in this paper on the ACE RDC 2004 corpus are evalu-
ated using 5-fold cross validation on the entire corpus . 
Both corpora are parsed using Charniak?s parser 
(Charniak, 2001) with the boundaries of all the entity 
mentions kept 4 . We iterate over all pairs of entity 
mentions occurring in the same sentence to generate 
potential relation instances5. In our experimentation, 
SVM (SVMLight, Joachims(1998)) is selected as our 
classifier. For efficiency, we apply the one vs. others 
strategy, which builds K classifiers so as to separate 
one class from all others. The training parameters are 
chosen using cross-validation on the ACE RDC 2003 
training data.  In particular, l  in our tree kernel is 
fine-tuned to 0.5. This suggests that about 50% dis-
count is done as our tree kernel moves down one 
level in computing ])2[],1[( 11 ii nnD .  
4.2 Experimental Results  
First, we systematically evaluate the context-sensitive 
convolution tree kernel and the dynamic context-
sensitive tree span proposed in this paper. 
Then, we evaluate the complementary nature be-
tween our tree kernel and a state-of-the-art linear ker-
nel via a composite kernel. Generally different 
feature-based methods and tree kernel-based methods 
have their own merits. It is usually easy to build a 
system using a feature-based method and achieve the 
state-of-the-art performance, while tree kernel-based 
methods  hold the potential for further performance 
improvement. Therefore, it is always a good idea to 
integrate them via a composite kernel.  
                                                               
4 This can be done by first representing all entity men-
tions with their head words and then restoring all the 
entity mentions after parsing. Moreover, please note 
that the final performance of relation extraction may 
change much with different range of parsing errors. 
We will study this issue in the near future. 
5 In this paper, we only measure the performance of rela-
tion extraction on ?true? mentions with ?true? chain-
ing of co-reference (i.e. as annotated by LDC 
annotators ). Moreover, we only model explicit relations and 
explicitly model the argument order of the two mentions in-
volved. 
733
Finally, we compare our system with the state-of-
the-art systems in the literature.  
Context-Sensitive Convolution Tree Kernel 
In this paper, the m parameter of our context-sensitive 
convolution tree kernel as shown in Equation (3) 
indicates the maximal length of root node paths and is 
optimized to 3 using 5-fold cross validation on the 
ACE RDC 2003 training data. Table 1 compares the 
impact of different m in context-sensitive convolution 
tree kernels using the Shortest Path-enclosed Tree 
(SPT) (as described in Zhang et al(2006)) on the 
major relation types of the ACE RDC 2003 and 2004 
corpora, in details. It also shows that our tree kernel 
achieves best performance on the test data using SPT 
with m = 3, which outperforms the one with m = 1 by 
~2.3 in F-measure. This suggests the parent and 
grandparent nodes of a sub-tree  contains much 
information for relation extraction while considering 
more ancestral nodes may not help. This may be due 
to that, although our experimentation on the 
training data indicates that  more than 80% (on 
average) of subtrees has a root node path longer 
than 3 (since most of the subtrees are deep from the 
root node and more than 90% of the parsed trees in 
the training data are deeper than 6 levels), 
including a root node path longer than 3 may be 
vulnerable to the full parsing errors and have 
negative impact. Table 1 also evaluates the impact of 
entity-related information in our tree kernel by 
attaching entity type information (e.g. ?PER? in the 
entity node 1 of Figure 1(b)) into both entity nodes. 
It shows that such information can significantly 
improve the performance by ~6.0 in F-measure. In all 
the following experiments, we will apply our tree 
kernel with m=3 and entity-related information by 
default. 
Table 2 compares the dynamic context-sensitive 
tree span with SPT using our tree kernel. It shows that 
the dynamic tree span can futher improve the 
performance by ~1.2 in F-measure6. This suggests the 
usefulness of extending the tree span beyond SPT for 
the ?predicate-linked? tree span category. In the 
future work, we will further explore expanding the 
dynamic tree span beyond SPT for the remaining tree 
span categories. 
  
  
  
                                                               
6 Significance test shows that the dynamic tree span per-
forms s tatistically significantly better than SPT with p-
values smaller than 0.05. 
m P(%) R(%) F 
1 72.3(72.7)  56.6(53.8) 63.5(61.8)  
2 74.9(75.2)  57.9(54.7) 65.3(63.5)  
3 75.7(76.1)  58.3(55.1) 65.9(64.0)  
4 76.0(75.9)  58.3(55.3) 66.0(63.9)  
a) without entity-related information 
m P(%) R(%) F 
1 77.2(76.9)  63.5(60.8) 69.7(67.9)  
2 79.1(78.6)  65.0(62.2) 71.3(69.4)  
3 79.6(79.4)  65.6(62.5) 71.9(69.9)  
4 79.4(79.1)  65.6(62.3) 71.8(69.7)  
b) with entity-related information 
Table 1: Evaluation of context-sensitive convolution 
tree kernels using SPT on the major relation types of 
the ACE RDC 2003 (inside the parentheses) and 2004 
(outside the parentheses) corpora. 
Tree Span P(%) R(%) F 
Shortest Path-  
enclosed Tree 
79.6 
(79.4) 
65.6 
(62.5) 
71.9 
(69.9) 
Dynamic Context- 
Sensitive Tee 
81.1 
(80.1) 
66.7 
(63.8) 
73.2 
(71.0) 
Table 2: Comparison of dynamic context-sensitive 
tree span with SPT using our context-sensitive 
convolution tree kernel on the major relation types of 
the ACE RDC 2003 (inside the parentheses) and 2004 
(outside the parentheses) corpora. 18% of positive 
instances in the ACE RDC 2003 test data belong to 
the predicate-linked category. 
  
Composite Kernel 
In this paper, a composite kernel via polynomial in-
terpolation, as described Zhang et al(2006), is ap-
plied to integrate the proposed context-sensitive 
convolution tree kernel with a state-of-the-art linear 
kernel (Zhou et al2005) 7: 
),()1(),(),(1 ???-+???=?? CPL KKK aa  (5) 
Here, ),( ??LK  and ),( ??CK  indicates the normal-
ized linear kernel and context-sensitive convolution 
tree kernel respectively while  ( , )pK ? ?  is the poly-
nomial expansion of ( , )K ? ?  with degree d=2, i.e. 
2( , ) ( ( , ) 1)pK K? ? ? ?= +  and a  is the coefficient (a  is 
set to 0.3 using cross-validation). 
                                                               
7 Here, we use the same set of flat features (i.e. word, 
entity type, mention level, overlap, base phrase chunk-
ing, dependency tree, parse tree and semantic informa-
tion) as Zhou et al(2005). 
734
Table 3 evaluates the performance of the 
composite kernel. It shows that the composite kernel 
much further improves the performance beyond that 
of either the state-of-the-art linear kernel or our tree 
kernel and achieves the F-measures of 74.1 and 75.8 
on the major relation types of the ACE RDC 2003 
and 2004 corpora respectively. This suggests that our 
tree kernel and the state-of-the-art linear kernel are 
quite complementary, and that our composite kernel 
can effectively integrate both flat and structured 
features. 
System P(%) R(%) F 
Linear Kernel 78.2 (77.2) 
63.4 
(60.7) 
70.1 
(68.0) 
Context-Sensitive Con-
volution Tree Kernel 
81.1 
(80.1) 
66.7 
(63.8) 
73.2 
(71.0) 
Composite Kernel 82.2 (80.8) 
70.2 
(68.4) 
75.8 
(74.1) 
Table 3: Performance of the compos ite kernel via 
polynomial interpolation on the major relation types 
of the ACE RDC 2003 (inside the parentheses) and 
2004 (outside the parentheses) corpora 
  
Comparison with Other Systems  
ACE RDC 2003 P(%) R(%) F 
Ours:  
composite kernel 
80.8 
(65.2) 
68.4 
(54.9) 
74.1 
(59.6) 
Zhang et al(2006):  
composite kernel 
77.3 
(64.9) 
65.6 
(51.2) 
70.9 
(57.2) 
Ours: context-sensitive  
convolution tree kernel 
80.1 
(63.4) 
63.8 
(51.9) 
71.0 
(57.1) 
Zhang et al(2006):  
convolution tree kernel 
76.1 
(62.4) 
62.6 
(48.5) 
68.7 
(54.6) 
Bunescu et al(2005):  
shortest path  
dependency kernel 
65.5 
(-) 
43.8 
(-) 
52.5 
(-) 
Culotta et al(2004):  
dependency kernel 
67.1 
(-) 
35.0 
(-) 
45.8 
(-) 
Zhou et al (2005):  
feature-based 
77.2 
(63.1) 
60.7 
(49.5) 
68.0 
(55.5) 
Kambhatla (2004):  
feature-based 
- 
(63.5) 
- 
(45.2) 
- 
(52.8) 
Table 4: Comparison of difference systems on the 
ACE RDC 2003 corpus over both 5 types (outside the 
parentheses) and 24 subtypes (inside the parentheses) 
  
  
  
ACE RDC 2004 P(%) R(%) F 
Ours:  
composite kernel 
82.2 
(70.3) 
70.2 
(62.2) 
75.8 
(66.0) 
Zhang et al(2006):  
composite kernel 
76.1 
(68.6) 
68.4 
(59.3) 
72.1 
(63.6) 
Zhao et al(2005):8  
composite kernel 
69.2 
(-) 
70.5 
(-) 
70.4 
(-) 
Ours: context-sensitive  
convolution tree kernel 
81.1 
(68.8) 
66.7 
(60.3) 
73.2 
(64.3) 
Zhang et al(2006):  
convolution tree kernel 
72.5 
(-) 
56.7 
(-) 
63.6 
(-) 
Table 5: Comparison of difference systems on the 
ACE RDC 2004 corpus over both 7 types (outside the 
parentheses) and 23 subtypes (inside the parentheses) 
  
Finally, Tables 4 and 5 compare our system with 
other state-of-the-art systems9 on the ACE RDC 2003 
and 2004 corpora, respectively. They show that our 
tree kernel-based system outperforms previous tree 
kernel-based systems. This is largely due to the con-
text-sensitive nature of our tree kernel which resolves 
the limitations of the previous tree kernels. They also 
show that our tree kernel-based system outperforms 
the state-of-the-art feature-based system. This proves 
the great potential inherent in the parse tree structure 
for relation extraction and our tree kernel takes a big 
stride towards the right direction. Finally, they also 
show that our composite kernel-based system outper-
forms other composite kernel-based systems. 
5 Conclusion 
Structured parse tree information holds great potential 
for relation extraction. This paper proposes a context-
sensitive convolution tree kernel to resolve two criti-
cal problems in previous tree kernels for relation ex-
traction by first automatically determining a dynamic 
context-sensitive tree span and then applying a con-
text-sensitive convolution tree kernel. Moreover, this 
paper evaluates the complementary nature between 
our tree kernel and a state-of-the-art linear kernel. 
Evaluation on the ACE RDC corpora shows that our 
dynamic context-sensitive tree span is much more 
suitable for relation extraction than the widely -used 
Shortest Path-enclosed Tree and our tree kernel out-
performs the state-of-the-art Collins and Duffy?s con-
volution tree kernel. It also shows that feature-based 
                                                               
8 There might be some typing errors for the performance 
reported in Zhao and Grishman(2005) since P, R and F 
do not match. 
9 All the state-of-the-art systems apply the entity-related 
information. It is not supervising: our experiments 
show that using the entity-related information gives a 
large performance improvement.  
735
and tree kernel-based methods well complement each 
other and the composite kernel can effectively inte-
grate both flat and structured features.  
To our knowledge, this is the first research to dem-
onstrate that, without extensive feature engineer ing, 
an individual tree kernel can achieve much better per-
formance than the state-of-the-art linear kernel in re-
lation extraction. This shows the great potential of 
structured parse tree information for relation extrac-
tion and our tree kernel takes a big stride towards the 
right direction.  
For the future work, we will focus on improving 
the context-sensitive convolution tree kernel by ex-
ploring more useful context information. Moreover, 
we will explore more entity-related information in the 
parse tree. Our preliminary work of including the en-
tity type information significantly improves the per-
formance. Finally, we will study how to resolve the 
data imbalance and sparseness issues from the learn-
ing algorithm viewpoint.  
Acknowledgement 
This research is supported by Project 60673041 under 
the National Natural Science Foundation of China 
and Project 2006AA01Z147 under the ?863? National 
High-Tech Research and Development of China. We 
would also like to thank the critical and insightful 
comments from the four anonymous reviewers. 
References  
ACE. (2000-2005). Automatic Content Extraction. 
http://www.ldc.upenn.edu/Projects/ACE/  
Bunescu R. & Mooney R.J. (2005a). A shortest path 
dependency kernel for relation extraction. 
HLT/EMNLP?2005 : 724-731. 6-8 Oct 2005. Van-
cover, B.C. 
Bunescu R. & Mooney R.J. (2005b). Subsequence Ker-
nels for Relation Extraction  NIPS?2005. Vancouver, 
BC, December 2005  
Charniak E. (2001). Immediate-head Parsing for Lan-
guage Models. ACL?2001: 129-137. Toulouse, France 
Collins M. and Duffy N. (2001). Convolution Ke rnels 
for Natural Language. NIPS?2001: 625-632. Ca m-
bridge, MA 
Culotta A. and Sorensen J. (2004). Dependency tree 
kernels for relation extraction. ACL?2004 . 423-429. 
21-26 July 2004. Ba rcelona, Spain. 
Haussler D. (1999). Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, Uni-
versity of California, Santa Cruz 
Joachims T. (1998). Text Categorization with Su pport 
Vector Machine: learning with many relevant fea-
tures. ECML-1998 : 137-142.  Chemnitz, Germany 
Kambhatla N. (2004). Combining lexical, syntactic and 
semantic features with Maximum Entropy models for 
extracting relations. ACL?2004(Poster). 178-181. 21-
26 July 2004. Barcelona, Spain. 
MUC. (1987-1998). The NIST MUC website: http: 
//www.itl.nist.gov/iaui/894.02/related_projects/muc/ 
Zelenko D., Aone C. and Richardella. (2003). Kernel 
methods for relation extraction. Journal of Machine 
Learning Research. 3(Feb):1083-1106. 
Zhang M., Zhang J., Su J. and Zhou G.D. (2006). A 
Composite Kernel to Extract Relations between Enti-
ties with both Flat and Structured Features . COLING-
ACL-2006: 825-832. Sydney, Australia 
Zhao S.B. and Grishman R. (2005). Extracting relations 
with integrated information using kernel methods. 
ACL?2005: 419-426. Univ of Michigan-Ann Arbor,  
USA,  25-30 June 2005. 
Zhou G.D., Su J. Zhang J. and Zhang M. (2005). Ex-
ploring various knowledge in relation extraction. 
ACL?2005. 427-434. 25-30 June, Ann Arbor, Mich-
gan, USA.  
Zhou G.D., Su J. and Zhang M. (2006). Modeling com-
monality among related classes in relation extraction, 
COLING-ACL?2006: 121-128. Sydney, Australia. 
736
