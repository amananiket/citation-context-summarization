Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 150?153,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Annotating Semantic Relations Combining Facts and Opinions
Koji Murakami? Shouko Masuda?? Suguru Matsuyoshi?
Eric Nichols? Kentaro Inui? Yuji Matsumoto?
?Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara 630-0192 JAPAN
?Osaka Prefecture University
1-1, Gakuen, Naka-ku, Sakai, Osaka 599-8531 JAPAN
{kmurakami,shouko,matuyosi,eric-n,inui,matsu}@is.naist.jp
Abstract
As part of the STATEMENT MAP project,
we are constructing a Japanese corpus an-
notated with the semantic relations bridg-
ing facts and opinions that are necessary
for online information credibility evalua-
tion. In this paper, we identify the se-
mantic relations essential to this task and
discuss how to efficiently collect valid ex-
amples from Web documents by splitting
complex sentences into fundamental units
of meaning called ?statements? and an-
notating relations at the statement level.
We present a statement annotation scheme
and examine its reliability by annotating
around 1,500 pairs of statements. We are
preparing the corpus for release this win-
ter.
1 Introduction
The goal of the STATEMENT MAP project (Mu-
rakami et al, 2009) is to assist internet users with
evaluating the credibility of online information by
presenting them with a comprehensive survey of
opinions on a topic and showing how they relate
to each other. However, because real text on the
Web is often complex in nature, we target a sim-
pler and more fundamental unit of meaning which
we call the ?statement.? To summarize opinions
for the statement map users, we first convert all
sentences into statements and then, organize them
into groups of agreeing and conflicting opinions
that show the logical support for each group.
For example, a user who is concerned about po-
tential connections between vaccines and autism
would be presented with a visualization of the
opinions for and against such a connection to-
gether with the evidence supporting each view as
shown in Figure 1.
When the concerned user in our example looks
at this STATEMENT MAP, he or she will see that
some opinions support the query ?Do vaccines
cause autism?? while other opinions do not, but
it will also show what support there is for each of
these viewpoints. So, STATEMENT MAP can help
user come to an informed conclusion.
2 Semantic Relations between
Statements
2.1 Recognizing Semantic Relations
To generate STATEMENT MAPs, we need to an-
alyze a lot of online information retrieved on a
given topic, and STATEMENT MAP shows users
a summary with three major semantic relations.
AGREEMENT to group similar opinions
CONFLICT to capture differences of opinions
EVIDENCE to show support for opinions
Identifying logical relations between texts is the
focus of Recognizing Textual Entailment (RTE).
A major task of the RTE Challenge (Dagan et al,
2005) is the identification of [ENTAILMENT] or
[CONTRADICTION] between Text (T) and Hy-
pothesis (H). For this task, several corpora have
been constructed over the past few years, and an-
notated with thousands of (T,H) pairs.
While our research objective is to recognize se-
mantic relations as well, our target domain is text
from Web documents. The definition of contradic-
tion in RTE is that T contradicts H if it is very un-
likely that both T and H can be true at the same
time. However, in real documents on the Web,
there are many examples which are partially con-
tradictory, or where one statement restricts the ap-
plicability of another like in the example below.
(1) a. Mercury-based vaccines actually cause autism in
children.
150
!Mercury-based vaccine preservatives actually have caused autism in 
children. 
!It?s biologically plausible that the MMR vaccine causes autism. 
VACCINES CAUSE AUTISM 
!There is no valid scientific evidence that vaccines  
cause autism. 
!The weight of the evidence indicates that vaccines  
are not associated with autism. 
VACCINES DON?T CAUSE AUTISM 
!My son then had the MMR, and then when he was three he was 
diagnosed with autism. 
!He then had the MMR, and then when he was three he was  
diagnosed with autism. 
MY CHILD WAS DIAGNOSED WITH AUTISM 
RIGHT AFTER THE VACCINE 
!Vaccinations are given around the same time  
children can be first diagnosed. 
!The plural of anecdote is not data.  
ANECDOTES ARE NOT EVIDENCE 
[CONFLICT]!
[FOCUS]!
[EVIDENCE]!
[EVIDENCE]!
Query : Do vaccines cause autism?!
[CONFLICT]!
Figure 1: An example STATEMENT MAP for the query ?Do vaccines cause autism??
b. Vaccines can trigger autism in a vulnerable subset of
children.
While it is difficult to assign any relation to this
pair in an RTE framework, in order to construct
statement maps we need to recognize a contradic-
tion between (1a) and (1b).
There is another task of recognizing relations
between sentences, CST (Cross-Document Struc-
ture Theory) which was developed by Radev
(2000). CST is an expanded rhetorical structure
analysis based on RST (Mann and Thompson,
1988), and attempts to describe relations between
two or more sentences from both single and mul-
tiple document sets. The CSTBank corpus (Radev
et al, 2003) was constructed to annotate cross-
document relations. CSTBank is divided into clus-
ters in which topically-related articles are gath-
ered. There are 18 kinds of relations in this corpus,
including [EQUIVALENCE], [ELABORATION],
and [REFINEMENT].
2.2 Facts and Opinions
RTE is used to recognize logical and factual re-
lations between sentences in a pair, and CST is
used for objective expressions because newspa-
per articles related to the same topic are used as
data. However, the task specifications of both RTE
and CST do not cover semantic relations between
opinions and facts as illustrated in the following
example.
(2) a. There must not be a connection between vaccines
and autism.
b. I do believe that there is a link between vaccinations
and autism.
Subjective statements, such as opinions, are re-
cently the focus of many NLP research topics,
such as review analysis, opinion extraction, opin-
ion QA, or sentiment analysis. In the corpus con-
structed by the MPQA Project (Multi-Perspective
Question Answering) (Wiebe et al, 2005), indi-
vidual expressions are marked that correspond to
explicit mentions of private states, speech events,
and expressive subjective elements.
Our goal is to annotate instances of the three
major relation classes: [AGREEMENT], [CON-
FLICT] and [EVIDENCE], between pairs of state-
ments in example texts. However, each relation
has a wide range, and it is very difficult to define
a comprehensive annotation scheme. For exam-
ple, different kinds of information can act as clues
to recognize the [AGREEMENT] relations. So,
we have prepared a wide spectrum of semantic re-
lations depending on different types of informa-
tion regarded as clues to identify a relation class,
such as [AGREEMENT] or [CONFLICT]. Table 1
shows the semantic relations needed for carry-
ing out the anotation. Although detecting [EVI-
DENCE] relations is also essential to the STATE-
MENT MAP project, we do not include them in our
current corpus construction.
3 Constructing a Japanese Corpus
3.1 Targeting Semantic Relations Between
Statements
Real data on the Web generally has complex sen-
tence structures. That makes it difficult to rec-
ognize semantic relations between full sentences.
but it is possible to annotate semantic relation be-
tween parts extracted from each sentence in many
cases. For example, the two sentences A and B
in Figure 2 cannot be annotated with any of the
semantic relations in Table 1, because each sen-
tence include different types of information. How-
ever, if two parts extracted from these sentences C
and D are compared, the parts can be identified as
[EQUIVALENCE] because they are semantically
close and each extracted part does not contain a
different type of information. So, we attempt to
break sentences from the Web down into reason-
able text segments, which we call ?statements.?
When a real sentence includes several pieces of se-
151
Table 1: Definition of semantic relations and example in the corpus
Relation Class Relation Label Example
AGREEMENT
Equivalence A: The overwhelming evidence is that vaccines are unrelated to autism.B: There is no link between the MMR vaccine and autism.
Equivalent Opinion
A: We think vaccines cause autism.
B: I am the mother of a 6 year old that regressed into autism because of his 18
month vaccinations.
Specific A: Mercury-based vaccine preservatives actually have caused autism in children.B: Vaccines cause autism.
CONFLICT
Contradiction A: Mercury-based vaccine preservatives actually have caused autism in children.B: Vaccines don?t cause autism.
Confinement A: Vaccines can trigger autism in a vulnerable subset of children.B: Mercury-based vaccine actually have caused autism in children.
Conflicting Opinion A: I don?t think vaccines cause autism.B: I believe vaccines are the cause of my son?s autism.
According to Department 
of Medicine, there is no 
link between the MMR 
vaccine and autism.!
There is no link between the 
MMR vaccine and autism.!
The weight of the 
evidence indicates that 
vaccines are not 
associated with autism.!
Vaccines are not 
associated with autism.!
(A) Real sentence (1) (B) Real sentence (2)!
(C) Statement (1)! (D) Statement (2)!(E) [EQUIVALENCE]!
Figure 2: Extracting statements from sentences
and annotating a semantic relation between them
mantic segments, more than one statement can be
extracted. So, a statement can reflect the writer?s
affirmation in the original sentence. If the ex-
tracted statements lack semantic information, such
as pronouns or other arguments, human annota-
tors manually add the missing information. Fi-
nally we label pairs of statements with either one
of the semantic relations from Table 1 or with ?NO
RELATION,? which means that two sentences (1)
are not semantically related, or (2) have a relation
other than relations defined in Table 1.
3.2 Corpus Construction Procedure
We automatically gather sentences on related top-
ics by following the procedure below:
1. Retrieve documents related to a set number of
topics using a search engine
2. Extract real sentences that include major sub-
topic words which are detected based on TF or
DF in the document set
3. Reduce noise in data by using heuristics to
eliminate advertisements and comment spam
4. Reduce the search space for identifying sen-
tence pairs and prepare pairs, which look fea-
sible to annotate.
Dolan and Brockett (2005) proposed a method
to narrow the range of sentence pair candidates
and collect candidates of sentence-level para-
phrases which correspond [EQUIVALENCE] in
[AGREEMENT] class in our task. It worked well
for collecting valid sentence pairs from a large
cluster which was constituted by topic-related sen-
tences. The method also seem to work well for
[CONFLICT] relations, because lexical similar-
ity based on bag-of-words (BOW) can narrow the
range of candidates with this relation as well.
We calculate the lexical similarity between the
two sentences based on BOW. We also used hy-
ponym and synonym dictionaries (Sumida et al,
2008) and a database of relations between predi-
cate argument structures (Matsuyoshi et al, 2008)
as resources. According to our preliminary exper-
iments, unigrams of KANJI and KATAKANA ex-
pressions, single and compound nouns, verbs and
adjectives worked well as features, and we calcu-
late the similarity using cosine distance. We did
not use HIRAGANA expressions because they are
also used in function words.
4 Analyzing the Corpus
Five annotators annotated semantic relations ac-
cording to our specifications in 22 document sets
as targets. We have annotated target statement
pairs with either [AGREEMENT], [CONFLICT]
or [NO RELATION]. We provided 2,303 real
sentence pairs to human annotators, and they
identified 1,375 pairs as being invalid and 928
pairs as being valid. The number of annotated
statement pairs are 1,505 ([AGREEMENT]:862,
[CONFLICT]:126, [NO RELATION]:517).
Next, to evaluate inter annotator agreement, 207
randomly selected statement pairs were annotated
by two human annotators. The annotators agreed
in their judgment for 81.6% of the examples,
which corresponds to a kappa level of 0.49. The
annotation results are evaluated by calculating re-
call and precision in which one annotation result
is treated as a gold standard and the other?s as the
output of the system, as shown in Talbe 2.
152
Table 2: Inter-annotator agreement for 2 annota-
tors
Annotator A
AGR. CON. NONE TOTAL
AGR. 146 7 9 162
Anno- CON. 0 13 1 14
tator B NONE 17 4 10 31
TOTAL 163 24 20 207
5 Discussion
The number of sentence pairs that annotators iden-
tified as invalid examples shows that around 60%
of all pairs were invalid, showing that there is still
room to improve our method of collecting sen-
tence pairs for the annotators. Developing more
effective methods of eliminating sentences pairs
that are unlikely to contain statements with plau-
sible relations is important to improve annotator
efficiency. We reviewed 50 such invalid sentence
pairs, and the results indicate two major consider-
ations: (1) negation, or antonyms have not been re-
garded as key information, and (2) verbs in KANJI
have to be handled more carefully. The polarities
of sentences in all pairs were the same although
there are sentences which can be paired up with
opposite polarities. So, we will consider the po-
larity of words and sentences as well as similarity
when considering candidate sentence pairs.
In Japanese, the words which consist of
KATAKANA expressions are generally nouns, but
those which contain KANJI can be nouns, verbs,
or adjectives. Sharing KATAKANA words was
the most common way of increasing the simi-
larity between sentences. We need to assign a
higher weight to verbs and adjectives that contain
KANJI, to more accurately calculate the similarity
between sentences.
Another approach to reducing the search space
for statement pairs is taken by Nichols et al
(2009), who use category tags and in-article hyper-
links to organize scientific blog posts into discus-
sions on the same topic, making it easier to iden-
tify relevant statements. We are investigating the
applicability of these methods to the construction
of our Japanese corpus but suffer from the lack of
a richly-interlinked data source comparable to En-
glish scientific blogs.
6 Conclusion
In this paper, we described the ongoing construc-
tion of a Japanese corpus consisting of statement
pairs annotated with semantic relations for han-
dling web arguments. We designed an annotation
scheme complete with the necessary semantic re-
lations to support the development of statement
maps that show [AGREEMENT], [CONFLICT],
and [EVIDENCE] between statements for assist-
ing users in analyzing credibility of information
in Web. We discussed the revelations made from
annotating our corpus, and discussed future direc-
tions for refining our specifications of the corpus.
We are planning to annotate relations for more
than 6,000 sentence pairs in this summer, and the
finished corpus will consist of around 10,000 sen-
tence pairs. The first release of our annotation
specifications and the corpus will be made avail-
able on the Web1 this winter.
Acknowledgments
This work is supported by the National Institute
of Information and Communications Technology
Japan.
References
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The pascal recognising textual entailment challenge. In
Proc. of the PASCAL Challenges Workshop on RTE.
Bill Dolan and Chris Brockett. 2005. Automatical ly con-
structing a corpus of sentential paraphrases. In Proc. of
the IWP 2005, pages 9?16.
William Mann and Sandra Thompson. 1988. Rhetorical
structure theory: towards a functional theory of text or-
ganization. Text, 8(3):243?281.
Suguru Matsuyoshi, Koji Murakami, Yuji Matsumoto, , and
Kentaro Inui. 2008. A database of relations between
predicate argument structures for recognizing textual en-
tailment and contradiction. In Proc. of the ISUC 2008.
Koji Murakami, Eric Nichols, Suguru Matsuyoshi, Asuka
Sumida, Shouko Masuda, Kentaro Inui, and Yuji Mat-
sumoto. 2009. Statement map: Assisting information
credibility analysis by visualizing arguments. In Proc. of
the WICOW 2009, pages 43?50.
Eric Nichols, Koji Murakami, Kentaro Inui, and Yuji Mat-
sumoto. 2009. Constructing a scientific blog corpus for
information credibility analysis. In Proc. of the Annual
Meeting of ANLP.
Dragomir Radev, Jahna Otterbacher, and Zhu Zhang.
2003. CSTBank: Cross-document Structure Theory Bank.
http://tangra.si.umich.edu/clair/CSTBank.
Dragomir R. Radev. 2000. Common theory of informa-
tion fusion from multiple text sources step one: Cross-
document structure. In Proc. of the 1st SIGdial workshop
on Discourse and dialogue, pages 74?83.
Asuka Sumida, Naoki Yoshinaga, and Kentaro Torisawa.
2008. Boosting precision and recall of hyponymy rela-
tion acquisition from hierarchical layouts in wikipedia. In
Proc. of the LREC 2008.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in lan-
guage. Language Resources and Evaluation, 39(2-
3):165?210.
1http://cl.naist.jp/stmap/corpus/ja
153
