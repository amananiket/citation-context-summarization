Decoding Algorithm in Statistical Machine Translation 
Ye-Y i  Wang and  A lex  Waibe l  
Language Techno logy  Ins t i tu te  
School  of Computer  Science 
Carnegie  Mel lon Un ivers i ty  
5000 Forbes Avenue 
P i t t sburgh ,  PA 15213, USA 
{yyw, waibel}@cs, cmu. edu 
Abst ract  
Decoding algorithm is a crucial part in sta- 
tistical machine translation. We describe 
a stack decoding algorithm in this paper. 
We present he hypothesis scoring method 
and the heuristics used in our algorithm. 
We report several techniques deployed to 
improve the performance of the decoder. 
We also introduce a simplified model to 
moderate the sparse data problem and to 
speed up the decoding process. We evalu- 
ate and compare these techniques/models 
in our statistical machine translation sys- 
tem. 
1 In t roduct ion  
1.1 Stat is t ica l  Machine Trans lat ion 
Statistical machine translation is based on a channel 
model. Given a sentence T in one language (Ger- 
man) to be translated into another language (En- 
glish), it considers T as the target of a communi- 
cation channel, and its translation S as the source 
of the channel. Hence the machine translation task 
becomes to recover the source from the target. Ba- 
sically every English sentence is a possible source for 
a German target sentence. If we assign a probability 
P(S I T) to each pair of sentences (S, T), then the 
problem of translation is to find the source S for a 
given target T, such that P(S \[ T) is the maximum. 
According to Bayes rule, 
P(S IT)  = P (S)P (T  I S) 
P(T)  (1) 
Since the denominator is independent of S, we have 
-- arg maxP(S)P(T  I S) (2) 
S 
Therefore a statistical machine translation system 
must deal with the following three problems: 
? Modeling Problem: How to depict the process 
of generating a sentence in a source language, 
and the process used by a channel to generate 
a target sentence upon receiving a source sen- 
tence? The former is the problem of language 
modeling, and the later is the problem of trans- 
lation modeling. They provide a framework for 
calculating P(S) and P(W I S) in (2). 
? Learning Problem: Given a statistical language 
model P(S) and a statistical translation model 
P (T  I S), how to estimate the parameters in 
these models from a bilingual corpus of sen- 
tences? 
? Decoding Problem: With a fully specified 
(framework and parameters) language and 
translation model, given a target sentence T, 
how to efficiently search for the source sentence 
that satisfies (2). 
The modeling and learning issues have been dis- 
cussed in (Brown et ah, 1993), where ngram model 
was used for language modeling, and five different 
translation models were introduced for the transla- 
tion process. We briefly introduce the model 2 here, 
for which we built our decoder. 
In model 2, upon receiving a source English sen- 
tence e = el,. ? -, el, the channel generates a German 
sentence g = gl, ? ? ", g,n at the target end in the fol- 
lowing way: 
1. With a distribution P(m I e), randomly choose 
the length m of the German translation g. In 
model 2, the distribution is independent of m 
and e: 
P(m \[ e) = e 
where e is a small, fixed number. 
2. For each position i (0 < i < m) in g, find the 
corresponding position ai in e according to an 
al ignment distribution P(a i  I i, a~ -1, m, e). In 
model 2, the distribution only depends on i, ai 
and the length of the English and German sen- 
tences: 
P(ai  l i, a~- l ,m,e)  = a(ai l i, m, l )  
3. Generate the word gl at the position i of the 
German sentence from the English word ea~ at 
366 
the aligned position ai of gi, according to a 
translation distribution P(gi t ~t~'~, st~i-t, e) = 
t(gl I ea~). The distribution here only depends 
on gi and eai. 
Therefore, P(g l e) is the sum of the probabilities 
of generating  from e over all possible alignments 
A, in which the position i in the target sentence g is 
aligned to the position ai in the source sentence : 
P(gle) = 
I l m 
e ~,  ... ~" IT t(g# le=jla(a~ Ij, l ,m)= 
a l=0 amm0j=l  
m ! 
e 1"I ~ t(g# le,)a(ilj, t, m) (3) 
j= l  i=0  
(Brown et al, 1993) also described how to use 
the EM algorithm to estimate the parameters a(i I 
j, l, m) and $(g I e) in the aforementioned model. 
1.2 Decod ing  in Statistical Machine 
Trans lat ion  
(Brown et al, 1993) and (Vogel, Ney, and Tillman, 
1996) have discussed the first two of the three prob- 
lems in statistical machine translation. Although 
the authors of (Brown et al, 1993) stated that they 
would discuss the search problem in a follow-up arti- 
? cle, so far there have no publications devoted to the 
decoding issue for statistical machine translation. 
On the other side, decoding algorithm is a crucial 
part in statistical machine translation. Its perfor- 
mance directly affects the quality and efficiency of 
translation. Without a good and efficient decoding 
algorithm, a statistical machine translation system 
may miss the best translation of an input sentence 
even if it is perfectly predicted by the model. 
2 S tack  Decod ing  A lgor i thm 
Stack decoders are widely used in speech recognition 
systems. The basic algorithm can be described as 
following: 
1. Initialize the stack with a null hypothesis. 
2. Pop the hypothesis with the highest score off 
the stack, name it as current -hypothes is .  
3. if cur rent -hypothes is  is a complete sentence, 
output it and terminate. 
4. extend cur rent -hypothes is  by appending a 
word in the lexicon to its end. Compute the 
score of the new hypothesis and insert it into 
the stack. Do this for all the words in the lexi- 
con. 
5. Go  to 2. 
2.1 Scoring the hypotheses 
In stack search for statistical machine translation, 
a hypothesis H includes (a) the length l of the 
source sentence, and (b) the prefix words in the 
sentence. Thus a hypothesis can be written as 
H = l : ere2.. "ek, which postulates a source sen- 
tence of length l and its first k words. The score 
of H, fit, consists of two parts: the prefix score gH 
for ele2"" ek and the heuristic score hH for the part 
ek+lek+2"-et that is yet to be appended to H to 
complete the sentence. 
2.1.1 Prefix score gH 
(3) can be used to assess a hypothesis. Although 
it was obtained from the alignment model, it would 
be easier for us to describe the scoring method if 
we interpret the last expression in the equation in 
the following way: each word el in the hypothesis 
contributes the amount e t(gj \[ ei)a(i l J, l, m) to the 
probability of the target sentence word gj. For each 
hypothesis H = l : el,e2,-",ek, we use SH(j) to 
denote the probability mass for the target word gl 
contributed by the words in the hypothesis: 
k 
SH(j) = e~'~t(g~ lei)a(ilj, t,m) (4) 
i=0  
Extending H with a new word will increase 
Sn( j ) , l  < j < m. 
To make the score additive, the logarithm of the 
probability in (3) was used. So the prefix score con- 
tributed by the translation model is :~'\]~=0 log St/(j). 
Because our objective is to maximize P(e, g), we 
have to include as well the logarithm of the language 
model probability of the hypothesis in the score, 
therefore we have 
m 
g. = ~IogS.( j)  + 
j=0  
k 
E log P(el l ei-N+t'" el-l). 
i=0 
here N is the order of the ngram language model. 
The above g-score gH of a hypothesis H = l : 
ele?...ek can be calculated from the g-score of its 
parent hypothesis P = l : ele2.. "ek-t: 
gH = gp+logP(ek lek -N+t ' ' ' ek - t )  
m 
+ ~-'~ log\[1 + et(gj l ek)a(k Ij, l, m) 
~=0 se( j )  \] 
SH(j) = Sp( j )+et(g j lek)a(k l j ,  l,m) (5) 
A practical problem arises here. For a many early 
stage hypothesis P, Sp(j) is close to 0. This causes 
problems because it appears as a denominator in (5) 
and the argument of the log function when calculat- 
ing gp. We dealt with this by either limiting the 
translation probability from the null word (Brown 
367 
et al, 1993) at the hypothetical 0-position(Brown et
al., 1993) over a threshold during the EM training, 
or setting SHo (j) to a small probability 7r instead of 
0 for the initial null hypothesis H0. Our experiments 
show that lr = 10 -4 gives the best result. 
2.1.2 Heur i s t i cs  
To guarantee an optimal search result, the heuris- 
tic function must be an upper-bound of the score 
for all possible extensions ek+le/c+2...et(Nilsson, 
1971) of a hypothesis. In other words, the benefit 
of extending a hypothesis hould never be under- 
estimated. Otherwise the search algorithm will con- 
clude prematurely with a non-optimal hypothesis. 
On the other hand, if the heuristic function over- 
estimates the merit of extending a hypothesis too 
much, the search algorithm will waste a huge amount 
of time after it hits a correct result to safeguard the 
optimality. 
To estimate the language model score h LM of the 
unrealized part of a hypothesis, we used the nega- 
tive of the language model perplexity PPtrain on the 
training data as the logarithm of the average proba- 
bility of predicting a new word in the extension from 
a history. So we have 
h LM = -(1 - k)PPtrai, + C. (6) 
Here is the motivation behind this. We assume that 
the perplexity on training data overestimates the 
likelihood of the forthcoming word string on av- 
erage.  However, when there are only a few words 
to be extended (k is close to 1), the language model 
probability for the words to be extended may be 
much higher than the average. This is why the con- 
stant term C was introduced in (6). When k << l, 
- ( l -k )PPtra in  is the dominating term in (6), so the 
heuristic language model score is close to the aver- 
age. This can avoid overestimating the score too 
much. As k is getting closer to l, the constant erm 
C plays a more important role in (6) to avoid un- 
derestimating the language model score. In our ex- 
periments, we used C = PPtrain +log(Pmax), where 
Pm== is the maximum ngram probability in the lan- 
guage model. 
To estimate the translation model score, we intro- 
duce a variable va(j), the maximum contribution to 
the probability of the target sentence word gj from 
any possible source language words at any position 
between i and l: 
vit(j) = max t(g~ \[e)a(klj, l ,m ). (7) 
i<_/c<_l ,eEL~ " " 
here LE is the English lexicon. 
Since vit (j) is independent of hypotheses, it only 
needs to be calculated once for a given target sen- 
tence. 
When k < 1, the heuristic function for the hypoth- 
esis H = 1 : ele2 -..e/c, is 
171 
hH = ~max{0,1og(v(/c+Dl(j)) - -  logSH(j)} 
j= l  
- ( t  - k)PP,~=., + c (8) 
where log(v(k+l)t( j ))-  logSg(j))  is the maximum 
increasement that a new word can bring to the like- 
lihood of the j - th target word. 
When k = l, since no words can be appended to 
the hypothesis, it is obvious that h~ = O. 
This heuristic function over-estimates the score 
of the upcoming words. Because of the constraints 
from language model and from the fact that a posi- 
tion in a source sentence cannot be occupied by two 
different words, normally the placement of words in 
those unfilled positions cannot maximize the likeli- 
hood of all the target words simultaneously. 
2.2 P run ing  and  abor t ing  search 
Due to physical space limitation, we cannot keep all 
hypotheses alive. We set a constant M, and when- 
ever the number of hypotheses exceeds M, the al- 
gorithm will prune the hypotheses with the lowest 
scores. In our experiments, M was set to 20,000. 
There is time limitation too. It is of little practical 
interest o keep a seemingly endless earch alive too 
long. So we set a constant T, whenever the decoder 
extends more than T hypotheses, it will abort the 
search and register a failure. In our experiments, T 
was set to 6000, which roughly corresponded to 2 
and half hours of search effort. 
2.3 Mu l t i -S tack  Search 
The above decoder has one problem: since the 
heuristic function overestimates the merit of ex- 
tending a hypothesis, the decoder always prefers 
hypotheses of a long sentence, which have a bet- 
ter chance to maximize the likelihood of the target 
words. The decoder will extend the hypothesis with 
large I first, and their children will soon occupy the 
stack and push the hypotheses of a shorter source 
sentence out of the stack. If the source sentence is 
a short one, the decoder will never be able to find 
it, for the hypotheses leading to it have been pruned 
permanently. 
This "incomparable" problem was solved with 
multi-stack search(Magerman, 1994). A separate 
stack was used for each hypothesized source sentence 
length 1. We do compare hypotheses in different 
stacks in the following cases. First, we compare a 
complete sentence in a stack with the hypotheses in 
other stacks to safeguard the optimality of search 
result; Second, the top hypothesis in a stack is com- 
pared with that of another stack. If the difference 
is greater than a constant ~, then the less probable 
one will not be extended. This is called soft-pruning, 
since whenever the scores of the hypotheses in other 
stacks go down, this hypothesis may revive. 
368 
Z 
2 
5000 
400O 
3000 
2000 
1000 
0 
0 5 l0 15 20 25 
Sentence Length 
Engfish - -  
30 35 40 
5OOO 
4OOO 
3OOO 
111011 
0 5 I0 15 20 25 
Sentence Length 
German - -  
30 35 40 
Figure 1: Sentence Length Distribution 
3 S tack  Search  w i th  a S impl i f ied  
Mode l  
In the IBM translation model 2, the alignment pa- 
rameters depend on the source and target sentence 
length I and m. While this is an accurate model, it 
causes the following difficulties: 
1. there are too many parameters and therefore 
too few trainingdata per parameter. This may 
not be a problem when massive training data 
are available. However, in our application, this 
is a severe problem. Figure 1 plots the length 
distribution for the English and German sen- 
tences. When sentences get longer, there are 
fewer training data available. 
2. the search algorithm has to make multiple hy- 
potheses of different source sentence l ngth. For 
each source sentence l ngth, it searches through 
almost the same prefix words and finally set- 
tles on a sentence length. This is a very time 
consuming process and makes the decoder very 
inefficient. 
To solve the first problem, we adjusted the count 
for the parameter a(i \[ j, l, m) in the EM parameter 
estimation by adding to it the counts for the pa- 
rameters a(i l j, l', m'), assuming (l, m) and (1', m') 
are close enough. The closeness were measured in 
m 
m'  
. . . .  , . . . . . . .  . . . . . . .  
. . . .  . . . . . . . .  . . . . . . .  
- : , "  . . . . . . .  . . . . . . .  : , ' '  
. . . . . .  . . . . . . .  . . . . . . .  . . .  
...# ....... ~ . . .~ . .#  ....... #..~ 
. : . .  . . . . . . .  ? . .~ . . .~ . .~ . . .~  . . . . . . .  . . .~  
1' 1 
Figure 2: Each x/y position represents a different 
source/target sentence length. The dark dot at the 
intersection (l, m) corresponds to the set of counts 
for the alignment parameters a(.  \[ o,l, m) in the 
EM estimation. The adjusted counts are the sum 
of the counts in the neighboring sets residing inside 
the circle centered at (1, m) with radius r. We took 
r = 3 in our experiment. 
Euclidean distance (Figure 2). So we have 
e(i I J, t, m) = 
e(i l j ,  l ' ,m';e,g ) (9) 
(I-l')~+(m-m')~<r~;e,g 
where ~(i I J, l, m) is the adjusted count for the pa- 
rameter a(i I J, 1, m), c(i I J, l, m; e, g) is the expected 
count for a(i I J, l, m) from a paired sentence (e g), 
and c( i l j ,  l ,m;e,g) = 0 when lel  l, or Igl ? m, 
or i > l, or j > m. 
Although (9) can moderate the severity of the first 
data sparse problem, it does not ease the second 
inefficiency problem at all. We thus made a radi- 
cal change to (9) by removing the precondition that 
(l, m) and (l', m') must be close enough. This re- 
sults in a simplified translation model, in which the 
alignment parameters are independent of the sen- 
tence length 1 and m: 
P( i l j ,  m,e) = P( i l j ,  l,m) 
-- a(i l J) 
here i , j  < Lm, and L,n is the maximum sentence 
length allowed in the translation system. A slight 
change to the EM algorithm was made to estimate 
the parameters. 
There is a problem with this model: given a sen- 
tence pair g and e, when the length of e is smaller 
than  Lm, then the alignment parameters do not sum 
to 1: 
lel 
a(ilj) < 1. (10) 
i--0 
We deal with this problem by padding e to length 
Lm with dummy words that never gives rise to any 
word in the target of the channel. 
Since the parameters are independent of the 
source sentence length, we do not have to make an 
369 
assumption about the length in a hypothesis. When- 
ever a hypothesis ends with the sentence nd sym- 
bol </s> and its score is the highest, the decoder 
reports it as the search result. In this case, a hypoth- 
esis can be expressed as H = e l ,e2, . . . ,ek ,  and IHI 
is used to denote the length of the sentence prefix of 
the hypothesis H, in this case, k. 
3.1 Heur i s t i cs  
Since we do not make assumption of the source sen- 
tence length, the heuristics described above can no 
longer be applied. Instead, we used the following 
heuristic function: 
h~./ = ~ max{0,1og( v IHI+I)(IHI+n)(j))} S.(j) 
-n  * PPt~ain + C (11) 
L. - IHI  
h.  = Pp(IHl+nlm)*h  (12) 
n----I 
here h~ is the heuristics for the hypothesis that ex- 
tend H with n more words to complete the source 
sentence (thus the final source sentence length is 
\[H\[ + n.) Pp(x \[ y) is the eoisson distribution of the 
source sentence length conditioned on the target sen- 
tence length. It is used to calculate the mean of the 
heuristics over all possible source sentence length, m 
is the target sentence length. The parameters of the 
Poisson distributions can be estimated from training 
data. 
4 Imp lementat ion  
Due to historical reasons, stack search got its current 
name. Unfortunately, the requirement for search 
states organization is far beyond what a stack and 
its push pop operations can handle. What we really 
need is a dynamic set which supports the following 
operations: 
1. INSERT: to insert a new hypothesis into the 
set. 
2. DELETE: to delete a state in hard pruning. 
3. MAXIMUM: to find the state with the best 
score to extend. 
4. MINIMUM: to find the state to be pruned. 
We used the Red-Black tree data structure (Cor- 
men, Leiserson, and Rivest, 1990) to implement the 
dynamic set, which guarantees that the above oper- 
ations take O(log n) time in the worst case, where n 
is the number of search states in the set. 
5 Per fo rmance  
We tested the performance of the decoders with 
the scheduling corpus(Suhm et al, 1995). Around 
30,000 parallel sentences (400,000 words altogether 
for both languages) were used to train the IBM 
model 2 and the simplified model with the EM algo- 
rithm. A larger English monolingual corpus with 
around 0.5 million words was used to train a bi- 
gram for language modelling. The lexicon contains 
2,800 English and 4,800 German words in morpho- 
logically inflected form. We did not do any prepro- 
cessing/analysis of the data as reported in (Brown 
et al, 1992). 
5.1 Decoder  Success Rate  
Table 1 shows the success rate of three mod- 
els/decoders. As we mentioned before, the compari- 
son between hypotheses of different sentence length 
made the single stack search for the IBM model 2 
fail (return without a result) on a majority of the 
test sentences. While the multi-stack decoder im- 
proved this, the simplified model/decoder p oduced 
an output for all the 120 test sentences. 
5.2 T rans la t ion  Accuracy  
Unlike the case in speech recognition, it is quite 
arguable what "accurate translations" means. In 
speech recognition an output can be compared with 
the sample transcript of the test data. In machine 
translation, a sentence may have several egitimate 
translations. It is difficult to compare an output 
from a decoder with a designated translation. In- 
stead, we used human subjects to judge the machine- 
made translations. The translations are classified 
into three categories 1. 
1.  Correct translations: translations that are 
grammatical nd convey the same meaning as 
the inputs. 
2. Okay translations: translations that convey the 
same meaning but with small grammatical mis- 
takes or translations that convey most but not 
the entire meaning of the input. 
3.  Incorrect translations: Translations that are 
ungrammatical or convey little meaningful in- 
formation or the information is different from 
the input. 
Examples of correct, okay, and incorrect ransla- 
tions are shown in Table 2. 
Table 3 shows the statistics of the translation re- 
sults. The accuracy was calculate by crediting a cor- 
rect translation 1 point and an okay translation 1/2 
point. 
There are two different kinds of errors in statis- 
tical machine translation. A modeling erivr occurs 
when the model assigns a higher score to an incor- 
rect translation than a correct one. We cannot do 
anything about this with the decoder. A decoding 
1 This is roughly the same as the classification i IBM 
statistical translation, except we do not have "legitimate 
translation that conveys different meaning from the in- 
put" - -  we did not observed this case in our outputs. 
370 
Model 2, Single Stack 
Model 2, Multi-Stack 
Simplified Model 
Total Test Sentences Decoded Sentenced Failed sentences 
120 32 88 
120 83 37 
120 120 0 
Table 1: Decoder Success Rate 
Correct 
Okay 
Incorrect 
German 
English (target) 
English (output) 
German 
English/target) 
English (output) 
German 
English (target) 
English/output/ 
German 
English/target) 
English (output) 
German 
English (target) 
English (output) 
German 
English (target) 
English (output) 
ich habe ein Meeting yon halb zehn bis um zwSlf 
I have a meeting from nine thirty to twelve 
I have a meeting from nine thirty to twelve 
versuchen wir sollten es vielleicht mit einem anderen Termin 
we might want to try for some other time 
we should try another time 
ich glaube nicht diis ich noch irgend etwas im Januar frei habe 
I do not think I have got anything open m January 
I think I will not free in January 
ich glaube wit sollten em weiteres Meeting vereinbaren 
I think we have to have another meeting 
I think we should fix a meeting 
schlagen Sie doch einen Termin vor 
why don't you suggest a time 
why you an appointment 
ich habe Zeit fiir den Rest des Tages 
I am free the rest of it 
I have time for the rest of July 
Table 2: Examples of Correct, Okay, and Incorrect Translations: for each translation, the first line is an 
input German sentence, the second line is the human made (target) translation for that input sentence, and 
the third line is the output from the decoder. 
error or search error happens when the search al- 
gorithm misses a correct translation with a higher 
score. 
When evaluating a decoding algorithm, it would 
be attractive if we can tell how many errors are 
caused by the decoder. Unfortunately, this is not 
attainable. Suppose that we are going to translate a
German sentence g, and we know from the sample 
that e is one of its possible English translations. The 
decoder outputs an incorrect e~ as the translation of 
g. If the score of e' is lower than that of e, we know 
that a search error has occurred. On the other hand, 
if the score of e' is higher, we cannot decide if it is a 
modeling error or not, since there may still be other 
legitimate translations with a score higher than e ~ 
- -  we just do not know what they are. 
Although we cannot distinguish a modeling error 
from a search error, the comparison between the de- 
coder output's core and that of a sample transla- 
tion can still reveal some information about the per- 
formance of the decoder. If we know that the de- 
coder can find a sentence with a better score than 
a "correct" translation, we will be more confident 
that the decoder is less prone to cause errors. Ta- 
ble 4 shows the comparison between the score of the 
outputs from the decoder and the score of the sam- 
ple translations when the outputs are incorrect. In 
most cases, the incorrect outputs have a higher score 
than the sample translations. Again, we consider a 
"okay" translation a half error here. 
This result hints that model deficiencies may be a 
major source of errors. The models we used here are 
very simple. With a more sophisticated model, more 
training data, and possibly some preprocessing, the 
total error rate is expected to decrease. 
5.3 Decod ing  Speed 
Another important issue is the efficiency of the de- 
coder. Figure 3 plots the average number of states 
being extended by the decoders. It is grouped ac- 
cording to the input sentence length, and evaluated 
on those sentences on which the decoder succeeded. 
The average number of states being extended in 
the model 2 single stack search is not available for 
long sentences, ince the decoder failed on most of 
the long sentences. 
The figure shows that the simplified model/decoder 
works much more efficiently than the other mod- 
371 
Total 
Model 2, Multi-Stack 83 
Simplified Model 120 
Correct Okay Incorrect Accuracy 
39 12 32 54.2% 
64 15 41 59.6% 
Table 3: Translation Accuracy 
Model 2, Multi-Stack 
Simplified Model 
Total Errors Scoree > Scoree, Scoree < Seoree, 
38 3.5 (7.9%) 34.5 (92.1%) 
48.5 4.5 (9.3%) 44 (90.7%) 
Table 4: Sample Translations versus Machine-Made Translations 
6000 
5000 
~d 
4000 
3000 
=~ 2ooo Z 
10oo 
< 
0 j..Zh 
1-4 
"Model2-Single-S tack" , , 
"Model2-Mult i-Stack" - -~  
"Simpli f ied-Moder'  , ......... 
i 
i 
5-8 9-12 13-16 17-20 
Target Sentence Length 
Figure 3: Extended States versus Target Sentence 
Length 
els/decoders. 
6 Conc lus ions  
We have reported astack decoding algorithm for the 
IBM statistical translation model 2 and a simpli- 
fied model. Because the simplified model has fewer 
uarameters and does not have to posit hypotheses 
with the same prefixes but different length, it out- 
performed the IBM model 2 with regard to both 
accuracy and efficiency, especially in our application 
that lacks a massive amount of training data. In 
most cases, the erroneous outputs from the decoder 
have a higher score than the human made transla- 
tions. Therefore it is less likely that the decoder is 
a major contributor of translation errors. 
7 Acknowledgements  
We would like to thank John Lafferty for enlight- 
ening discussions on this work. We would also like 
to thank the anonymous ACL reviewers for valuable 
comments. This research was partly supported by 
ATR and the Verbmobil Project. The vmws and 
conclusions in this document are those of the au- 
thors. 
Re ferences  
Brown, P. F., S. A. Dellaopietra, V. J Della-Pietra, 
and R. L. Mercer. 1993. The Mathematics ofSta- 
tistical Machine Translation: Parameter Estima- 
tion. Computational Linguistics, 19(2):263-311. 
Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, 
J. D. Lafferty, and R. L. Mercer. 1992. Analy- 
sis, Statistical Transfer, and Synthesis in Machine 
Translation. In Proceedings of the fourth Interna- 
tional Conference on Theoretical and Methodolog- 
ical Issues in Machine Translation, pages 83-100. 
Cormen, Thomas H., Charles E. Leiserson, and 
Ronald L. Rivest. 1990. Introduction to Al- 
gorithms. The MIT Press, Cambridge, Mas- 
sachusetts. 
Magerman, D. 1994. Natural Language Parsing 
as Statistical Pattern Recognition. Ph.D. thesis, 
Stanford University. 
Nilsson, N. 1971. Problem-Solving Methods in Arti- 
ficial Intelligence. McGraw Hill, New York, New 
York. 
Suhm, B., P.Geutner, T. Kemp, A. Lavie, L. May- 
field, A. McNair, I. Rogina, T. Schultz, T. Slo- 
boda, W. Ward, M. Woszczyna, and A. Waibel. 
1995. JANUS: Towards multilingual spoken lan- 
guage translation. In Proceedings of the ARPA 
Speech Spoken Language Technology Workshop, 
Austin, TX, 1995. 
Vogel, S., H. Ney, and C. Tillman. 1996. HMM- 
Based Word Alignment in Statistical Transla- 
tion. In Proceedings of the Seventeenth Interna- 
tional Conference on Computational Linguistics: 
COLING-96, pages 836-841, Copenhagen, Den- 
mark. 
372 
