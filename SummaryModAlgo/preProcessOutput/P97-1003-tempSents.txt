24	head-lexicalized stochastic grammars have recently become increasingly popular . 
17	another consequence of not generating posthead conjunctions and punctuation as first-class words is that they 19 in fact, if punctuation occurs before the head, it is not generated at allâ€”a deficiency in the parsing model that appears to be a holdover from the deficient punctuation handling in the model of collins . 
31	in parsing, the most relevant previous work is due to collins , who considered three binary features of the intervening material: did it contain any word tokens at all, any verbs, any commas or colons? 
21	as in most other statistical parsing systems we therefore use the pruning technique described in goodman and collins which assigns a score to each item in the chart equal to the product of the inside probability of the item and its prior probability. 
39	previous parsing models maximize the joint probability p of a sentence s and its parse tree t. we maximize the conditional probability p . 
32	as discussed in footnote 3, collins and mcdonald et al considered the pos tags intervening between a head and child. 
44	on the other hand, the best available parsers trained on the penn treebank, those of collins and charniak , use statistical models for disambiguation that make crucial use of dependency relations. 
38	here, we extract part-of-speech tags from the collins parserâ€™s output for section 23 instead of reinventing a tagger. 
34	nasrÂ©lia, univ-avignon, fr) :~cogentex, inc. 1 introduction dependency grammar has a long tradition in syntactic theory, dating back to at least tesni~re's work from the thirties3 recently, it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words ), which is what dependency grammars model explicitly do, but context-free phrasestructure grammars do not. 
1	in the probabilistic lr model, probabilities are assigned to tree 696 precision recall f-score time best-first classifier-based 88.1 87.8 87.9 17 deterministic 85.4 84.8 85.1 < 1 charniak & johnson 91.3 90.6 91.0 unk bod 90.8 90.7 90.7 145* charniak 89.5 89.6 89.5 23 collins 88.3 88.1 88.2 39 ratnaparkhi 87.5 86.3 86.9 unk tsuruoka & tsujii : deterministic 86.5 81.2 83.8 < 1* tsuruoka & tsujii : search 86.8 85.0 85.9 2* sagae & lavie 86.0 86.1 86.0 11* table 1: summary of results on labeled precision and recall of constituents, and time required to parse the test set. 
12	eisner , charniak , collins , and many subsequent researchers1 annotated every node with lexical features passed up from its head-child in order to more precisely reflect the node's inside contents. 
42	moreover, the deterministic dependency parser of yamada and matsumoto , when trained on the penn treebank, gives a dependency accuracy that is almost as good as that of collins and charniak . 
18	503 bikel intricacies of collinsâ€™ parsing model table 4 overall parsing results using only details found in collins . 
37	parsers that attempt to disambiguate the input completely â€” full parsing â€” typically first employ some kind of dynamic programming algorithm to derive a packed parse forest and then applies a probabilistic top-down model in order to select the most probable analysis . 
25	context-free rules charniak collins , eisner context-free rules, headwords charniak context-free rules, headwords, grandparent nodes collins context-free rules, headwords, grandparent nodes/rules, bigrams, two-level rules, two-level bigrams, nonheadwords bod all fragments within parse trees scope of statistical dependencies model figure 4. schematic overview of the increase of statistical dependencies by stochastic parsers thus there seems to be a convergence towards a maximalist model which "takes all fragments and lets the statistics decide" . 
8	of particular relevance is other work on parsing the penn wsj treebank . 
40	there has been a great deal of progress in statistical parsing in the past decade . 
13	317 citation observed data hidden data collins treebank tree with head child annotated on each nonterminal no hidden data. 
3	we used the collins parser to generate the constituency parse and a dependency converter to obtain the dependency parse of english sentences. 
50	collins et al applied the parser of collins developed for english, to czech, and found thatthe performance was substantially lower when compared to the results for english. 
28	many stochastic parsing models use linguistic intuitions to find this minimal set, for example by restricting the statistical dependencies to the locality of headwords of constituents , leaving it as an open question whether there exist important statistical dependencies that go beyond linguistically motivated dependencies. 
45	many probabilistic evaluation models have been published inspired by one or more of these feature types , but discrepancies between training sets, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively. 
11	 
5	training on about 40,000 sentences achieves a crossing brackets rate of 1.07, a better value than our 1.63 value for regular parsing or the 1.13 value assuming perfect segmentation/tagging, but even for similar text types, comparisons across languages are of course problematic. 
46	in the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types . 
2	some of the more popular and more accurate of these approaches to data-driven parsing have been based on generative models that are closely related to probabilistic context-free grammars. 
9	the models were originally introduced in collins ; the current article 1 gives considerably more detail about the models and discusses them in greater depth. 
19	michael collins parsing models have been quite influential in the field of natural language processing. 
47	mainstream approaches in statistical parsing are based on nondeterministic parsing techniques, usually employing some kind of dynamic programming, in combination with generative probabilistic models that provide an n-best ranking of the set of candidate analyses derived by the parser . 
43	table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser in comparison with collins , charniak , and yamada and matsumoto . 
20	this was done for supervised parsing in different ways by collins , klein and manning , and mcdonald et al , all of whom considered intervening material or coarse distance classes when predicting children in a tree. 
6	previous work the generative models described in collins and the earlier version of these models described in collins conditioned on punctuation as surface features of the string, treating it quite differently from lexical items. 
26	a major difference between our approach and most other models tested on the wsj is that the dop model uses frontier lexicalization while most other models use constituent lexicalization . 
33	statistical disambiguation such as for pp-attachment or for generative parsing greatly improve disambiguation, but as they model by imitation instead of by understanding, complete soundness has to remain elusive. 
15	evaluation 8.1 effects of unpublished details in this section we present the results of effectively doing a â€œclean-roomâ€ implementation of collinsâ€™ parsing model, that is, using only information available in , as shown in table 4. the clean-room model has a 10.6% increase in f-measure error compared to collinsâ€™ parser and an 11.0% increase in f-measure error compared to our engine in its complete emulation of collinsâ€™ model 2. this is comparable to the increase in 32 although we have implemented a version of this type of pruning that limits the number of items that can be collected in any one cell, that is, the maximum number of items that cover a particular span. 
23	 , and the current paper has shown the importance of including two and more nonhead words. 
22	the base line for our base line parse accuracy, we used the now standard division of the wsj with sections 2 through 21 for training (approx. 
30	we should note, however, that most other stochastic parsers do include counts of single nonheadwords: they appear in the backed-off statistics of these parsers . 
7	in particular, the model in collins failed to generate punctuation, a deficiency of the model. 
35	most recently, mcdonald et al have implemented a dependency parser with good accuracy ) and very impressive speed and four times faster than charniak ). 
14	collins 's parser and its reimplementation and extension by bikel have by now been applied to a variety of languages: english , czech , german , spanish , french , chinese and, according to dan bikel's web page, arabic. 
27	collins 1996; eisner 1996), later models showed the importance of including context from higher nodes in the tree . 
49	thus, model c uses a reduced version of equation and an extended version of 2other models, as for pp-attachment resolution, or for probabilistic parsing, are somewhat related, but their supervised nature makes any direct comparison impossible. 
41	this permits us to make exact comparisons with the parser of yamada and matsumoto , but also the parsers of collins and charniak , which are evaluated on the same data set in yamada and matsumoto . 
16	these confidence values can be derived in a number of sensible ways; the technique used by collins was adapted from that used in bikel et al , which makes use of a quantity called the diversity of the history context , which is equal to the number of unique futures observed in training for that history context. 
29	table 1 shows the lp and lr scores obtained with our base line subtree set, and compares these scores with those of previous stochastic parsers tested on the wsj . 
10	in the models described in collins , there was a third question concerning punctuation: does the string contain 0, 1, 2 or more than 2 commas? 
48	deterministic parsing algorithms for building dependency graphs 2. history-based models for predicting the next parser action 3. discriminative learning to map histories to parser actions in this section we will define dependency graphs, describe the parsing algorithm used in the experiments and finally explain the extraction of features for the history-based models. 
36	 only the words that occur more than d times in training data. 
4	this is well illustrated by the collins parser , scrutinized by bikel , where several transformations are applied in order to improve the analysis of noun phrases, coordination and punctuation. 
